2022-11-02 22:49:26,989:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-02 22:49:26,989:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-02 22:49:26,989:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-02 22:49:26,989:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-02 22:49:28,915:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2022-11-03 10:31:55,687:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:31:55,687:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:31:55,687:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:31:55,687:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:31:56,430:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2022-11-03 10:38:55,942:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:38:55,942:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:38:55,942:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:38:55,942:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:38:56,435:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2022-11-03 10:46:41,703:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:46:41,703:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:46:41,703:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:46:41,703:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:46:42,184:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2022-11-03 10:48:05,317:INFO:PyCaret RegressionExperiment
2022-11-03 10:48:05,317:INFO:Logging name: reg-default-name
2022-11-03 10:48:05,317:INFO:ML Usecase: MLUsecase.REGRESSION
2022-11-03 10:48:05,317:INFO:version 3.0.0.rc4
2022-11-03 10:48:05,317:INFO:Initializing setup()
2022-11-03 10:48:05,317:INFO:self.USI: b4dc
2022-11-03 10:48:05,317:INFO:self.variable_keys: {'html_param', 'memory', 'X_test', '_all_models', 'variable_keys', '_available_plots', '_all_models_internal', 'seed', '_all_metrics', 'logging_param', 'idx', 'y_train', 'display_container', 'gpu_param', 'USI', 'exp_name_log', '_gpu_n_jobs_param', 'X_train', 'y_test', 'X', 'n_jobs_param', 'fold_groups_param', 'master_model_container', 'y', 'transform_target_param', '_ml_usecase', 'transform_target_method_param', 'log_plots_param', 'pipeline', 'fold_generator', 'fold_shuffle_param', 'exp_id', 'data', 'target_param'}
2022-11-03 10:48:05,317:INFO:Checking environment
2022-11-03 10:48:05,317:INFO:python_version: 3.10.7
2022-11-03 10:48:05,317:INFO:python_build: ('tags/v3.10.7:6cc6b13', 'Sep  5 2022 14:08:36')
2022-11-03 10:48:05,317:INFO:machine: AMD64
2022-11-03 10:48:05,355:INFO:platform: Windows-10-10.0.22000-SP0
2022-11-03 10:48:05,355:INFO:Memory: svmem(total=16901767168, available=4720353280, percent=72.1, used=12181413888, free=4720353280)
2022-11-03 10:48:05,355:INFO:Physical Core: 4
2022-11-03 10:48:05,355:INFO:Logical Core: 8
2022-11-03 10:48:05,355:INFO:Checking libraries
2022-11-03 10:48:05,355:INFO:System:
2022-11-03 10:48:05,355:INFO:    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
2022-11-03 10:48:05,355:INFO:executable: C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\python.exe
2022-11-03 10:48:05,355:INFO:   machine: Windows-10-10.0.22000-SP0
2022-11-03 10:48:05,355:INFO:PyCaret required dependencies:
2022-11-03 10:48:05,355:INFO:                 pip: 22.3
2022-11-03 10:48:05,355:INFO:          setuptools: 63.2.0
2022-11-03 10:48:05,363:INFO:             pycaret: 3.0.0rc4
2022-11-03 10:48:05,363:INFO:             IPython: 8.5.0
2022-11-03 10:48:05,363:INFO:          ipywidgets: 8.0.2
2022-11-03 10:48:05,363:INFO:                tqdm: 4.64.1
2022-11-03 10:48:05,363:INFO:               numpy: 1.22.4
2022-11-03 10:48:05,363:INFO:              pandas: 1.4.4
2022-11-03 10:48:05,363:INFO:              jinja2: 3.1.2
2022-11-03 10:48:05,363:INFO:               scipy: 1.8.1
2022-11-03 10:48:05,363:INFO:              joblib: 1.2.0
2022-11-03 10:48:05,363:INFO:             sklearn: 1.1.2
2022-11-03 10:48:05,363:INFO:                pyod: 1.0.6
2022-11-03 10:48:05,363:INFO:            imblearn: 0.9.1
2022-11-03 10:48:05,363:INFO:   category_encoders: 2.5.1.post0
2022-11-03 10:48:05,363:INFO:            lightgbm: 3.3.3
2022-11-03 10:48:05,363:INFO:               numba: 0.55.2
2022-11-03 10:48:05,363:INFO:            requests: 2.28.1
2022-11-03 10:48:05,363:INFO:          matplotlib: 3.5.3
2022-11-03 10:48:05,363:INFO:          scikitplot: 0.3.7
2022-11-03 10:48:05,363:INFO:         yellowbrick: 1.5
2022-11-03 10:48:05,363:INFO:              plotly: 5.10.0
2022-11-03 10:48:05,363:INFO:             kaleido: 0.2.1
2022-11-03 10:48:05,363:INFO:         statsmodels: 0.13.4
2022-11-03 10:48:05,363:INFO:              sktime: 0.13.4
2022-11-03 10:48:05,363:INFO:               tbats: 1.1.1
2022-11-03 10:48:05,363:INFO:            pmdarima: 1.8.5
2022-11-03 10:48:05,363:INFO:              psutil: 5.9.2
2022-11-03 10:48:05,363:INFO:PyCaret optional dependencies:
2022-11-03 10:48:05,403:INFO:                shap: Not installed
2022-11-03 10:48:05,403:INFO:           interpret: Not installed
2022-11-03 10:48:05,403:INFO:                umap: Not installed
2022-11-03 10:48:05,403:INFO:    pandas_profiling: 3.4.0
2022-11-03 10:48:05,403:INFO:  explainerdashboard: Not installed
2022-11-03 10:48:05,403:INFO:             autoviz: Not installed
2022-11-03 10:48:05,403:INFO:           fairlearn: Not installed
2022-11-03 10:48:05,403:INFO:             xgboost: 1.6.2
2022-11-03 10:48:05,403:INFO:            catboost: Not installed
2022-11-03 10:48:05,403:INFO:              kmodes: Not installed
2022-11-03 10:48:05,403:INFO:             mlxtend: Not installed
2022-11-03 10:48:05,403:INFO:       statsforecast: Not installed
2022-11-03 10:48:05,403:INFO:        tune_sklearn: Not installed
2022-11-03 10:48:05,403:INFO:                 ray: Not installed
2022-11-03 10:48:05,403:INFO:            hyperopt: Not installed
2022-11-03 10:48:05,403:INFO:              optuna: Not installed
2022-11-03 10:48:05,403:INFO:               skopt: Not installed
2022-11-03 10:48:05,411:INFO:              mlflow: Not installed
2022-11-03 10:48:05,411:INFO:              gradio: Not installed
2022-11-03 10:48:05,411:INFO:             fastapi: Not installed
2022-11-03 10:48:05,411:INFO:             uvicorn: Not installed
2022-11-03 10:48:05,411:INFO:              m2cgen: Not installed
2022-11-03 10:48:05,411:INFO:           evidently: Not installed
2022-11-03 10:48:05,411:INFO:                nltk: Not installed
2022-11-03 10:48:05,411:INFO:            pyLDAvis: Not installed
2022-11-03 10:48:05,411:INFO:              gensim: Not installed
2022-11-03 10:48:05,411:INFO:               spacy: Not installed
2022-11-03 10:48:05,411:INFO:           wordcloud: Not installed
2022-11-03 10:48:05,411:INFO:            textblob: Not installed
2022-11-03 10:48:05,411:INFO:               fugue: Not installed
2022-11-03 10:48:05,411:INFO:           streamlit: 1.14.0
2022-11-03 10:48:05,411:INFO:             prophet: Not installed
2022-11-03 10:48:05,411:INFO:None
2022-11-03 10:48:05,411:INFO:Set up data.
2022-11-03 10:48:05,493:INFO:Set up train/test split.
2022-11-03 10:48:05,533:INFO:Set up index.
2022-11-03 10:48:05,533:INFO:Set up folding strategy.
2022-11-03 10:48:05,533:INFO:Assigning column types.
2022-11-03 10:48:05,549:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-11-03 10:48:05,549:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-11-03 10:48:05,566:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-03 10:48:05,581:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-03 10:48:05,755:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-03 10:48:05,907:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-03 10:48:05,907:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:06,349:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:06,349:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-11-03 10:48:06,364:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-03 10:48:06,380:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-03 10:48:06,571:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-03 10:48:06,715:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-03 10:48:06,715:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:06,723:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:06,723:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-11-03 10:48:06,739:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-03 10:48:06,755:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-03 10:48:06,932:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-03 10:48:07,074:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-03 10:48:07,074:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:07,082:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:07,099:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-03 10:48:07,116:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-03 10:48:07,298:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-03 10:48:07,434:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-03 10:48:07,434:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:07,451:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:07,451:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-11-03 10:48:07,483:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-03 10:48:07,666:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-03 10:48:07,804:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-03 10:48:07,804:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:07,822:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:07,837:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-03 10:48:08,031:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-03 10:48:08,161:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-03 10:48:08,161:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:08,177:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:08,177:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-11-03 10:48:08,399:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-03 10:48:08,546:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-03 10:48:08,546:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:08,546:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:08,771:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-03 10:48:08,899:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-03 10:48:08,915:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:08,916:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:08,916:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-11-03 10:48:09,137:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-03 10:48:09,275:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:09,283:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:09,500:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-03 10:48:09,650:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:09,658:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:09,658:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-11-03 10:48:10,029:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:10,046:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:10,403:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:10,403:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:10,419:INFO:Preparing preprocessing pipeline...
2022-11-03 10:48:10,419:INFO:Set up label encoding.
2022-11-03 10:48:10,419:INFO:Set up simple imputation.
2022-11-03 10:48:10,435:INFO:Set up encoding of ordinal features.
2022-11-03 10:48:10,452:INFO:Set up encoding of categorical features.
2022-11-03 10:48:10,452:INFO:Set up variance threshold.
2022-11-03 10:48:13,961:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:16,468:INFO:Finished creating preprocessing pipeline.
2022-11-03 10:48:16,531:INFO:Pipeline: Pipeline(memory=Memory(location=C:\Users\HARISH~1\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(include=['Senior Citizen', 'tenure',
                                             'Monthly Charges'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Customer ID', 'Ge...
                                                                    'Payment '
                                                                    'Method'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['Customer ID', 'Total Charges'],
                                    transformer=LeaveOneOutEncoder(cols=['Customer '
                                                                         'ID',
                                                                         'Total '
                                                                         'Charges'],
                                                                   handle_missing='return_nan',
                                                                   random_state=4411))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-11-03 10:48:16,531:INFO:Creating final display dataframe.
2022-11-03 10:48:19,974:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:225: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(transformer, X, y)

2022-11-03 10:48:21,669:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:225: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(transformer, X, y)

2022-11-03 10:48:35,039:INFO:Setup display_container:                  Description             Value
0                 Session id              4411
1                     Target             Churn
2                Target type        Regression
3                 Data shape        (7044, 41)
4           Train data shape        (4930, 41)
5            Test data shape        (2114, 41)
6           Ordinal features                 5
7           Numeric features                 3
8       Categorical features                17
9                 Preprocess              True
10           Imputation type            simple
11        Numeric imputation              mean
12    Categorical imputation          constant
13  Maximum one-hot encoding                 5
14           Encoding method              None
15    Low variance threshold                 0
16            Fold Generator             KFold
17               Fold Number                10
18                  CPU Jobs                -1
19                   Use GPU             False
20            Log Experiment             False
21           Experiment Name  reg-default-name
22                       USI              b4dc
2022-11-03 10:48:35,453:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:35,453:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:35,828:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:48:35,845:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:48:35,869:INFO:setup() successfully completed in 30.57s...............
2022-11-03 10:48:35,893:INFO:Initializing compare_models()
2022-11-03 10:48:35,893:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-11-03 10:48:35,893:INFO:Checking exceptions
2022-11-03 10:48:35,901:INFO:Preparing display monitor
2022-11-03 10:48:35,909:INFO:Initializing Linear Regression
2022-11-03 10:48:35,917:INFO:Total runtime is 0.00013345877329508463 minutes
2022-11-03 10:48:35,917:INFO:SubProcess create_model() called ==================================
2022-11-03 10:48:35,917:INFO:Initializing create_model()
2022-11-03 10:48:35,917:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:48:35,917:INFO:Checking exceptions
2022-11-03 10:48:35,924:INFO:Importing libraries
2022-11-03 10:48:35,924:INFO:Copying training dataset
2022-11-03 10:48:35,940:INFO:Defining folds
2022-11-03 10:48:35,940:INFO:Declaring metric variables
2022-11-03 10:48:35,940:INFO:Importing untrained model
2022-11-03 10:48:35,940:INFO:Linear Regression Imported successfully
2022-11-03 10:48:35,940:INFO:Starting cross validation
2022-11-03 10:48:35,969:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:48:47,216:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:47,375:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:47,489:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:47,523:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:47,675:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:47,729:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:47,782:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.28s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:47,866:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:50,203:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:50,437:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:50,437:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:50,628:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:50,707:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:50,797:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:50,814:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:50,888:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:51,508:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:48:56,708:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.22759221 -0.18281955 -0.16490057 -0.15916288 -0.15481519 -0.15037475
 -0.13391122 -0.13320558 -0.1327074  -0.13256409 -0.13095249 -0.1273081
 -0.12681571 -0.11797125 -0.11663401 -0.11609342 -0.11249005 -0.11221214
 -0.11213424 -0.10815903 -0.10812711 -0.10807845 -0.10758513 -0.10469526
 -0.0964037  -0.0916023  -0.08400326 -0.08322505 -0.08234236 -0.08132376
 -0.0720365  -0.06928709 -0.06874596 -0.06701821 -0.06437475 -0.06393701
 -0.06095805 -0.06010752 -0.05762167 -0.05535918 -0.05518988 -0.05499943
 -0.05433063 -0.05096152 -0.04819062 -0.04507993 -0.04477371 -0.04432384
 -0.04179743 -0.0401627  -0.03346355 -0.03186098 -0.03166437 -0.03161845
 -0.03058267 -0.02874367 -0.0254831  -0.02315566 -0.0230948  -0.02199682
 -0.02141183 -0.02011469 -0.02000063 -0.01965465 -0.01419046 -0.00459904
 -0.00370943  0.00344583  0.00743552  0.00922258  0.01040018  0.0104983
  0.01109955  0.01173035  0.01186176  0.01589742  0.01788842  0.01920366
  0.0209784   0.0224974   0.02332723  0.024382    0.02537359  0.02565929
  0.02701675  0.02720935  0.03319138  0.03319866  0.03585579  0.03632771
  0.0379261   0.03922978  0.04357103  0.04404904  0.04595345  0.04675752
  0.04693946  0.04733717  0.04830229  0.04927382  0.05036855  0.05162026
  0.05168833  0.0517132   0.05349545  0.05500137  0.05801273  0.059001
  0.06008231  0.06542563  0.0662237   0.06654805  0.06689119  0.06689152
  0.06715118  0.07511242  0.07638561  0.07672291  0.07921667  0.08006163
  0.0804636   0.08244954  0.08311129  0.0844589   0.08631289  0.0864061
  0.0864236   0.08902445  0.09070019  0.09162993  0.09411949  0.09867068
  0.10100462  0.10298721  0.10305137  0.10331588  0.1035254   0.10391193
  0.10476436  0.10572539  0.10633997  0.10669534  0.10720733  0.10776001
  0.11027698  0.11124508  0.11234039  0.11433397  0.11704384  0.11960591
  0.12041164  0.120981    0.12404506  0.12456287  0.12544055  0.12613948
  0.12630957  0.12776268  0.12807984  0.12896698  0.12943014  0.12993136
  0.13685346  0.13805388  0.13995182  0.14010507  0.14119334  0.14462429
  0.14526182  0.14573409  0.14937214  0.14980869  0.15156332  0.15356078
  0.15398515  0.15449947  0.15494677  0.1580319   0.16153808  0.16399483
  0.16748496  0.16863745  0.17026985  0.17176277  0.17322491  0.17497194
  0.17587985  0.17809644  0.17903139  0.17982566  0.18071381  0.18201188
  0.18243059  0.18262188  0.18650394  0.18659539  0.18701319  0.18843688
  0.18873573  0.18893998  0.18909032  0.19056766  0.19591832  0.19707374
  0.1999712   0.20048607  0.20240882  0.20259527  0.20309472  0.20359552
  0.20568186  0.20571297  0.2057855   0.20628684  0.20747989  0.20809745
  0.20832389  0.20833996  0.20874108  0.20937992  0.20957316  0.20975356
  0.21412598  0.21517826  0.21625468  0.21640577  0.21790559  0.22074496
  0.22157666  0.22342703  0.22368264  0.2251784   0.2277293   0.22968501
  0.23295245  0.23347432  0.23499852  0.23538645  0.23539654  0.23582126
  0.23603424  0.23642982  0.23659726  0.23868069  0.23978823  0.24301063
  0.24355835  0.24448087  0.24456466  0.24678786  0.24839935  0.24983298
  0.25031738  0.25078023  0.25172539  0.2555624   0.25571729  0.25607812
  0.25821318  0.26038021  0.26167455  0.26273589  0.2639788   0.26477114
  0.26688956  0.26711272  0.26794053  0.27137969  0.27519268  0.28404647
  0.28613917  0.28725794  0.28843805  0.28940204  0.28990584  0.29073417
  0.29091754  0.2914618   0.29211427  0.29349374  0.29395974  0.29503095
  0.29539312  0.29616104  0.29861094  0.30004213  0.30040813  0.30077668
  0.30216647  0.30245036  0.30297473  0.30372261  0.30444101  0.30465111
  0.30495027  0.30612323  0.30745607  0.30750639  0.30890722  0.30983346
  0.31109311  0.31270066  0.31300694  0.31419388  0.31439174  0.31497002
  0.31939761  0.31993411  0.32317877  0.32413821  0.32471538  0.32759252
  0.32829213  0.33022821  0.33043179  0.33057407  0.33143016  0.33179264
  0.33257427  0.33307936  0.33441494  0.33585127  0.33759265  0.34426155
  0.34484972  0.34530559  0.34995965  0.35152731  0.35165376  0.35177651
  0.35224241  0.35560022  0.35636658  0.35756157  0.36069095  0.36074197
  0.36146014  0.36219325  0.36570567  0.3665477   0.36753312  0.36872521
  0.36970901  0.37142197  0.37416747  0.37427992  0.37692361  0.37709295
  0.37785455  0.38199079  0.38312878  0.38400479  0.3847381   0.38541466
  0.38662125  0.39010744  0.40350929  0.40379373  0.40463114  0.40647878
  0.40855606  0.41048669  0.41094187  0.41545822  0.41792328  0.41871622
  0.42240963  0.42330054  0.42399341  0.42484933  0.43007681  0.43067912
  0.43184134  0.43184597  0.43320167  0.43446911  0.4357637   0.43740654
  0.43767834  0.43769191  0.44033269  0.44194887  0.45035741  0.45098822
  0.45137831  0.45191732  0.45274253  0.45410296  0.46303508  0.46442587
  0.46516541  0.46795817  0.46945319  0.4736979   0.47629589  0.47694102
  0.48043288  0.48055402  0.48098343  0.48103068  0.48169626  0.48461644
  0.48699976  0.48811948  0.49014283  0.49596642  0.50214313  0.50358034
  0.50415208  0.5059043   0.5108498   0.51290831  0.51356367  0.51523019
  0.51537383  0.51634971  0.51821451  0.52017281  0.526025    0.52602647
  0.52897194  0.53028964  0.530424    0.53124961  0.53548438  0.53642586
  0.53971278  0.54121636  0.54300089  0.54558479  0.54776939  0.54787775
  0.54957297  0.55174319  0.55176146  0.55359277  0.55538556  0.56015626
  0.56598114  0.56725437  0.57363187  0.57373614  0.57565645  0.57967081
  0.58276696  0.58669196  0.58790188  0.59037985  0.59214094  0.59299113
  0.59657468  0.60081719  0.60090122  0.60145224  0.60398068  0.60590994
  0.60767836  0.60788558  0.60935213  0.61303197  0.61460339  0.61995438
  0.62228099  0.62256566  0.62413935  0.62679056  0.63050591  0.63286426
  0.64401532  0.64468527  0.64572663  0.64745305  0.65767456  0.66336743
  0.66341367  0.66477482  0.66571625  0.6687756   0.67402886  0.67773434
  0.68360734  0.68472522  0.68601512  0.68642374  0.69594331  0.6964306
  0.70121238  0.71192317  0.71772971  0.71823367  0.72196     0.7238786
  0.73099813]

  warnings.warn(

2022-11-03 10:48:56,796:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.87642799e-01 -1.62929345e-01 -1.60162427e-01 -1.58967038e-01
 -1.49996106e-01 -1.48520718e-01 -1.45839637e-01 -1.44535895e-01
 -1.36062380e-01 -1.35368127e-01 -1.31472286e-01 -1.29789551e-01
 -1.26857581e-01 -1.26821190e-01 -1.25075438e-01 -1.23499152e-01
 -1.22744739e-01 -1.20360530e-01 -1.18281998e-01 -1.17041329e-01
 -1.16388591e-01 -1.05400292e-01 -1.05175092e-01 -1.04566606e-01
 -1.02554677e-01 -1.01982367e-01 -9.95851288e-02 -9.94075324e-02
 -9.39880865e-02 -9.38074635e-02 -8.87095177e-02 -8.74564878e-02
 -8.26639326e-02 -8.24956336e-02 -8.12749238e-02 -7.85131408e-02
 -7.29077810e-02 -7.28610671e-02 -7.07008035e-02 -6.50091643e-02
 -6.25510481e-02 -6.17937901e-02 -5.97256383e-02 -5.90432096e-02
 -5.66585347e-02 -5.44045935e-02 -5.00019052e-02 -4.79105706e-02
 -4.68107225e-02 -4.45706298e-02 -4.30460656e-02 -4.22403918e-02
 -4.14605625e-02 -4.12665629e-02 -3.40479921e-02 -3.21608961e-02
 -2.93881989e-02 -2.49552349e-02 -2.36141441e-02 -2.30354425e-02
 -2.25116879e-02 -2.16528656e-02 -2.04574368e-02 -1.89296222e-02
 -1.72249146e-02 -1.50459201e-02 -1.48453113e-02 -1.39537737e-02
 -1.39176199e-02 -1.30167713e-02 -1.00186534e-02 -9.91533735e-03
 -6.50311709e-03 -6.18656152e-03 -6.04569513e-03 -5.59021719e-03
 -3.34309443e-03 -1.93572124e-04  1.92909981e-03  2.00665242e-03
  4.39905735e-03  6.29112528e-03  7.87904395e-03  8.11402176e-03
  9.22605776e-03  1.19062152e-02  1.24869587e-02  1.42590691e-02
  1.46193136e-02  1.55391651e-02  1.72981722e-02  1.93099944e-02
  2.14215947e-02  2.14729578e-02  2.24912537e-02  2.25961562e-02
  2.37122757e-02  2.44030916e-02  2.73589284e-02  2.87383692e-02
  3.22678447e-02  3.41588734e-02  3.47856752e-02  3.63089886e-02
  3.69345538e-02  3.80267976e-02  3.89539777e-02  4.02885408e-02
  4.26622098e-02  4.27317133e-02  4.42641729e-02  4.69037099e-02
  4.72883379e-02  4.74978040e-02  5.12560360e-02  5.19134922e-02
  5.27478539e-02  5.62950270e-02  5.64154038e-02  5.66689762e-02
  6.18457680e-02  6.22441594e-02  6.35577000e-02  6.73780637e-02
  7.08251965e-02  7.34343383e-02  7.35994039e-02  7.44864632e-02
  7.53012800e-02  7.88370595e-02  7.97405312e-02  8.25498576e-02
  8.27308971e-02  8.34556842e-02  8.34683224e-02  8.37209191e-02
  8.39040067e-02  8.44168425e-02  8.54589310e-02  8.62568184e-02
  9.21006012e-02  9.26420178e-02  9.80412298e-02  1.00049970e-01
  1.02768318e-01  1.04128562e-01  1.07319535e-01  1.09636651e-01
  1.09791272e-01  1.10610426e-01  1.10710670e-01  1.11126803e-01
  1.12161062e-01  1.15188573e-01  1.15492382e-01  1.16160219e-01
  1.18068923e-01  1.18096463e-01  1.18269632e-01  1.25523558e-01
  1.26145253e-01  1.29659764e-01  1.29785154e-01  1.31800847e-01
  1.33283851e-01  1.33783746e-01  1.41728349e-01  1.41858556e-01
  1.41987128e-01  1.42223266e-01  1.42877329e-01  1.44068371e-01
  1.45688391e-01  1.46052902e-01  1.46912359e-01  1.47082540e-01
  1.47170099e-01  1.50259122e-01  1.50956896e-01  1.51515407e-01
  1.52306230e-01  1.60070503e-01  1.60454134e-01  1.61536229e-01
  1.61950558e-01  1.64058083e-01  1.71369373e-01  1.72137094e-01
  1.72616002e-01  1.73361381e-01  1.73511826e-01  1.77369323e-01
  1.79201884e-01  1.82698729e-01  1.84041196e-01  1.88501020e-01
  1.89683639e-01  1.90465342e-01  1.91924466e-01  1.93211061e-01
  2.00568520e-01  2.03230297e-01  2.07605805e-01  2.08744784e-01
  2.09935556e-01  2.10091291e-01  2.11171732e-01  2.11870033e-01
  2.12353085e-01  2.14096703e-01  2.15464351e-01  2.16932966e-01
  2.18397816e-01  2.19580589e-01  2.19643224e-01  2.22603013e-01
  2.23262883e-01  2.26310869e-01  2.26601516e-01  2.26925592e-01
  2.27342885e-01  2.27554011e-01  2.29362886e-01  2.32087027e-01
  2.36546587e-01  2.36719738e-01  2.37183399e-01  2.37829328e-01
  2.42092710e-01  2.42283697e-01  2.44767261e-01  2.45238759e-01
  2.45890698e-01  2.46526682e-01  2.47457106e-01  2.54214899e-01
  2.57533783e-01  2.59262868e-01  2.59607796e-01  2.62988227e-01
  2.64221802e-01  2.64624672e-01  2.65488076e-01  2.65546625e-01
  2.67565007e-01  2.69560333e-01  2.73780238e-01  2.74356860e-01
  2.75554279e-01  2.76456389e-01  2.78960145e-01  2.79630862e-01
  2.80520280e-01  2.81188552e-01  2.81545550e-01  2.82200503e-01
  2.83567035e-01  2.84156059e-01  2.85689221e-01  2.86515096e-01
  2.91560497e-01  2.93971208e-01  2.96658162e-01  2.97045073e-01
  2.97415723e-01  2.97490849e-01  2.99586637e-01  3.02330095e-01
  3.03442333e-01  3.03667284e-01  3.06440238e-01  3.07228292e-01
  3.07415878e-01  3.09502868e-01  3.10450018e-01  3.10735826e-01
  3.14441565e-01  3.14904611e-01  3.15264740e-01  3.15378786e-01
  3.17639072e-01  3.21915960e-01  3.27243627e-01  3.27494099e-01
  3.27540728e-01  3.28617786e-01  3.29039621e-01  3.35243623e-01
  3.35611300e-01  3.35895248e-01  3.36970781e-01  3.38370223e-01
  3.41095153e-01  3.44838909e-01  3.46597488e-01  3.46845148e-01
  3.48949972e-01  3.49897400e-01  3.53388354e-01  3.53441426e-01
  3.59849772e-01  3.61070057e-01  3.62685074e-01  3.63329742e-01
  3.64969418e-01  3.66501593e-01  3.67735752e-01  3.68671071e-01
  3.68966040e-01  3.69970343e-01  3.71643888e-01  3.72578511e-01
  3.73822829e-01  3.75307688e-01  3.75681096e-01  3.76174231e-01
  3.76960013e-01  3.78681559e-01  3.79257046e-01  3.79891856e-01
  3.80488855e-01  3.80703880e-01  3.81479574e-01  3.87507114e-01
  3.87799133e-01  3.93145166e-01  3.93427179e-01  3.97857742e-01
  3.97952038e-01  3.98301441e-01  3.99426993e-01  3.99927728e-01
  4.00684371e-01  4.02450653e-01  4.02507173e-01  4.03223961e-01
  4.03567165e-01  4.05650452e-01  4.05750299e-01  4.05799645e-01
  4.06219971e-01  4.09421126e-01  4.09826486e-01  4.15508987e-01
  4.16892513e-01  4.16935117e-01  4.17495288e-01  4.18113241e-01
  4.18124656e-01  4.21524030e-01  4.28639113e-01  4.28863134e-01
  4.29919530e-01  4.33173972e-01  4.34287829e-01  4.36173773e-01
  4.38228384e-01  4.38263755e-01  4.38896454e-01  4.39478441e-01
  4.42510976e-01  4.46305032e-01  4.48621055e-01  4.51334448e-01
  4.51905507e-01  4.52684946e-01  4.54244173e-01  4.54372180e-01
  4.54625192e-01  4.55817252e-01  4.60141339e-01  4.65424431e-01
  4.66070109e-01  4.66312578e-01  4.67019058e-01  4.69648288e-01
  4.70509857e-01  4.72298648e-01  4.72394268e-01  4.73093670e-01
  4.76376316e-01  4.77929320e-01  4.78325143e-01  4.79357807e-01
  4.80371132e-01  4.80398676e-01  4.84585457e-01  4.89363787e-01
  4.90287667e-01  4.90853054e-01  4.91887013e-01  4.95000094e-01
  4.98401697e-01  4.98803715e-01  4.99975751e-01  5.01644459e-01
  5.02348304e-01  5.04147716e-01  5.07815126e-01  5.13104971e-01
  5.14794884e-01  5.15144914e-01  5.15726194e-01  5.16404778e-01
  5.17747453e-01  5.18565883e-01  5.18846024e-01  5.24585273e-01
  5.26414857e-01  5.26903758e-01  5.27293594e-01  5.30208672e-01
  5.32892440e-01  5.33415600e-01  5.33512187e-01  5.36191860e-01
  5.36280787e-01  5.36298006e-01  5.36532269e-01  5.38799937e-01
  5.41979651e-01  5.42336203e-01  5.42980468e-01  5.44971325e-01
  5.45575933e-01  5.50558429e-01  5.53972263e-01  5.56369955e-01
  5.57908059e-01  5.60959009e-01  5.61101686e-01  5.61506294e-01
  5.67428974e-01  5.70362038e-01  5.73129921e-01  5.77001002e-01
  5.78013754e-01  5.78711657e-01  5.78880363e-01  5.90001433e-01
  5.91699194e-01  5.96771265e-01  5.96801756e-01  5.99740817e-01
  6.00948590e-01  6.01157320e-01  6.01719458e-01  6.03685143e-01
  6.07256454e-01  6.07289830e-01  6.09859882e-01  6.10533165e-01
  6.12626069e-01  6.13095228e-01  6.13191443e-01  6.14471269e-01
  6.15279296e-01  6.16920322e-01  6.17933046e-01  6.20416982e-01
  6.20516285e-01  6.21530324e-01  6.22272388e-01  6.25102231e-01
  6.25284184e-01  6.26863613e-01  6.27005415e-01  6.27899985e-01
  6.28172117e-01  6.29494033e-01  6.32837213e-01  6.36623702e-01
  6.39374061e-01  6.42411313e-01  6.52281056e-01  6.60158862e-01
  6.61339925e-01  6.62504958e-01  6.63079132e-01  6.65121129e-01
  6.67425003e-01  6.68150175e-01  6.73829832e-01  6.76979878e-01
  6.84394826e-01  6.86163744e-01  6.88033416e-01  7.07335490e-01
  7.08941587e-01  7.18138843e-01  7.42895870e-01  7.43626792e-01
  7.58767626e-01]

  warnings.warn(

2022-11-03 10:48:56,846:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.83048829e-01 -1.81381742e-01 -1.58083276e-01 -1.50423314e-01
 -1.47290788e-01 -1.46354752e-01 -1.44953723e-01 -1.43663561e-01
 -1.39135457e-01 -1.39121192e-01 -1.38846414e-01 -1.36962409e-01
 -1.36295000e-01 -1.35574397e-01 -1.34490460e-01 -1.32417308e-01
 -1.32181235e-01 -1.31680899e-01 -1.30806367e-01 -1.23025449e-01
 -1.22979860e-01 -1.19803774e-01 -1.16641620e-01 -1.15912482e-01
 -1.12983592e-01 -1.09661321e-01 -1.04346591e-01 -1.02850955e-01
 -1.01394430e-01 -1.01322001e-01 -1.01019470e-01 -9.90403359e-02
 -9.86937054e-02 -9.76875809e-02 -9.35593976e-02 -9.27037889e-02
 -8.97175890e-02 -8.65876122e-02 -8.57566814e-02 -8.21570689e-02
 -8.06957057e-02 -7.59068905e-02 -7.53589118e-02 -7.45425115e-02
 -7.35776245e-02 -7.30700111e-02 -6.74251300e-02 -6.64703563e-02
 -6.55812342e-02 -6.50859608e-02 -6.38233592e-02 -6.32419485e-02
 -6.13002663e-02 -5.55202542e-02 -5.42327303e-02 -5.28575714e-02
 -5.08925434e-02 -4.97711477e-02 -4.84626563e-02 -4.82071409e-02
 -4.80388558e-02 -4.60122762e-02 -4.58537614e-02 -4.55407771e-02
 -4.44548253e-02 -4.38736815e-02 -4.29009634e-02 -4.27723018e-02
 -3.77706389e-02 -3.71000656e-02 -3.68305264e-02 -3.59785006e-02
 -3.59693976e-02 -3.44127290e-02 -3.42522932e-02 -3.18979640e-02
 -3.09344657e-02 -3.03713326e-02 -2.89210928e-02 -2.81037383e-02
 -2.65394298e-02 -2.64132184e-02 -2.31327830e-02 -2.28205878e-02
 -2.23960791e-02 -1.91505964e-02 -1.69828336e-02 -1.49059804e-02
 -5.02951294e-03 -4.53640452e-03 -3.37920074e-03 -3.17077229e-03
 -2.00576923e-03 -4.12962613e-04  3.84644342e-05  6.47029689e-04
  2.02591225e-03  3.55584178e-03  1.04510286e-02  2.00994147e-02
  2.08936594e-02  2.10963536e-02  2.11091828e-02  2.21361531e-02
  2.70294292e-02  2.76347989e-02  2.94859472e-02  3.02914946e-02
  3.12935476e-02  3.13581519e-02  3.24732444e-02  3.43543862e-02
  3.48155540e-02  3.64898033e-02  3.79243168e-02  3.91456549e-02
  4.09129045e-02  4.09775934e-02  4.20045487e-02  4.26542435e-02
  4.73489886e-02  4.84663442e-02  4.91197403e-02  5.24057005e-02
  5.47165033e-02  5.54034856e-02  5.71060289e-02  5.77064599e-02
  5.94476548e-02  5.97530493e-02  6.05337892e-02  6.33792394e-02
  6.61212095e-02  6.65641182e-02  6.69028977e-02  6.83130576e-02
  6.84685586e-02  6.85714852e-02  7.09178601e-02  7.17167882e-02
  7.63956746e-02  7.70585196e-02  7.93785708e-02  8.18951791e-02
  8.27921469e-02  8.40313354e-02  9.14757524e-02  9.17496285e-02
  9.19602769e-02  9.62260920e-02  9.91101952e-02  9.93495933e-02
  1.00828545e-01  1.01775900e-01  1.03608069e-01  1.03651973e-01
  1.05147773e-01  1.05894018e-01  1.08886375e-01  1.12213278e-01
  1.12719848e-01  1.16141000e-01  1.17206507e-01  1.18349666e-01
  1.19100314e-01  1.20058879e-01  1.20614912e-01  1.22719138e-01
  1.25039035e-01  1.26383969e-01  1.26948408e-01  1.27025164e-01
  1.28000158e-01  1.30415989e-01  1.31137466e-01  1.33192228e-01
  1.33408312e-01  1.33507087e-01  1.37176670e-01  1.38294168e-01
  1.40818776e-01  1.41242629e-01  1.42158674e-01  1.43161697e-01
  1.43245271e-01  1.43958963e-01  1.45138818e-01  1.48467920e-01
  1.51345103e-01  1.51512602e-01  1.54130883e-01  1.56234720e-01
  1.58744424e-01  1.58846398e-01  1.60969175e-01  1.61567933e-01
  1.61661569e-01  1.61747228e-01  1.62586310e-01  1.63108814e-01
  1.63538063e-01  1.63552633e-01  1.64043132e-01  1.64446052e-01
  1.65101453e-01  1.66775580e-01  1.66972373e-01  1.70403636e-01
  1.70435043e-01  1.75787594e-01  1.76835768e-01  1.78574042e-01
  1.79221174e-01  1.79703223e-01  1.79922373e-01  1.83370532e-01
  1.84991518e-01  1.89180863e-01  1.89982551e-01  1.90301442e-01
  1.91461686e-01  1.91527375e-01  1.93648357e-01  1.93836648e-01
  1.95871901e-01  1.96793778e-01  1.98576664e-01  1.98660504e-01
  2.00533306e-01  2.02476918e-01  2.04709370e-01  2.07143055e-01
  2.11647216e-01  2.11692583e-01  2.12079570e-01  2.12322905e-01
  2.12856854e-01  2.16898405e-01  2.17165559e-01  2.17697251e-01
  2.18692699e-01  2.18787521e-01  2.20028052e-01  2.22393065e-01
  2.23582531e-01  2.25880592e-01  2.27012645e-01  2.27277640e-01
  2.31205819e-01  2.31622764e-01  2.32726465e-01  2.37413119e-01
  2.41274633e-01  2.41877594e-01  2.41945433e-01  2.44910773e-01
  2.45512359e-01  2.47503643e-01  2.49174932e-01  2.51050436e-01
  2.51709938e-01  2.54775059e-01  2.54969560e-01  2.56615302e-01
  2.58952564e-01  2.59990898e-01  2.60952556e-01  2.61514434e-01
  2.61900818e-01  2.64349536e-01  2.65821207e-01  2.68014074e-01
  2.69325018e-01  2.77179933e-01  2.78069440e-01  2.78763932e-01
  2.79571529e-01  2.80084492e-01  2.82652454e-01  2.83236529e-01
  2.86614532e-01  2.90282417e-01  2.98424887e-01  2.98731174e-01
  2.99723056e-01  3.02705473e-01  3.03957206e-01  3.11260165e-01
  3.11599002e-01  3.13584598e-01  3.14167658e-01  3.15599045e-01
  3.16862258e-01  3.22630922e-01  3.24489622e-01  3.24850151e-01
  3.26619350e-01  3.26946666e-01  3.30851700e-01  3.30876534e-01
  3.32676011e-01  3.33583875e-01  3.37158134e-01  3.41818029e-01
  3.41924677e-01  3.42201580e-01  3.42227665e-01  3.44168420e-01
  3.48985329e-01  3.50466549e-01  3.50741864e-01  3.52372643e-01
  3.52742256e-01  3.55380364e-01  3.56474234e-01  3.59607566e-01
  3.62514562e-01  3.64347463e-01  3.65329354e-01  3.67540601e-01
  3.70273049e-01  3.71416331e-01  3.71434427e-01  3.76381467e-01
  3.79176821e-01  3.80211902e-01  3.81874197e-01  3.82178535e-01
  3.85433937e-01  3.85870952e-01  3.93308997e-01  3.93532140e-01
  3.93736966e-01  3.94538431e-01  3.95973191e-01  3.98033899e-01
  3.98094357e-01  3.98108755e-01  4.01248287e-01  4.03920691e-01
  4.06356776e-01  4.06858637e-01  4.07590118e-01  4.09669724e-01
  4.15119757e-01  4.19512558e-01  4.19618430e-01  4.20966547e-01
  4.26407912e-01  4.28269102e-01  4.28957450e-01  4.30061115e-01
  4.30831156e-01  4.31781771e-01  4.32584197e-01  4.33523502e-01
  4.35232843e-01  4.36262235e-01  4.37711000e-01  4.38328832e-01
  4.39340866e-01  4.40357516e-01  4.40841310e-01  4.42374534e-01
  4.42917506e-01  4.44129054e-01  4.45163818e-01  4.47874273e-01
  4.48026298e-01  4.50864512e-01  4.54480902e-01  4.55526093e-01
  4.56120028e-01  4.56874392e-01  4.57798794e-01  4.62059886e-01
  4.63103449e-01  4.64818529e-01  4.66637968e-01  4.68587963e-01
  4.69344322e-01  4.72129364e-01  4.72696067e-01  4.73231436e-01
  4.74200958e-01  4.74243080e-01  4.74599944e-01  4.75572735e-01
  4.76763100e-01  4.76951190e-01  4.80489528e-01  4.81878219e-01
  4.86524971e-01  4.88722192e-01  4.89144035e-01  4.90136945e-01
  4.90923619e-01  4.96369069e-01  4.98094177e-01  4.99480826e-01
  5.00651560e-01  5.05540388e-01  5.05639573e-01  5.05685233e-01
  5.06098778e-01  5.06239437e-01  5.11154296e-01  5.17055936e-01
  5.21085298e-01  5.23569596e-01  5.24363026e-01  5.25583556e-01
  5.27619435e-01  5.31362577e-01  5.31426996e-01  5.33732739e-01
  5.34136792e-01  5.35656287e-01  5.40202116e-01  5.40753646e-01
  5.43228897e-01  5.51621930e-01  5.53227630e-01  5.53585598e-01
  5.54284892e-01  5.54429767e-01  5.55678074e-01  5.56108209e-01
  5.56838660e-01  5.57015731e-01  5.59792622e-01  5.60404115e-01
  5.60421495e-01  5.60871878e-01  5.64127456e-01  5.66071219e-01
  5.68547211e-01  5.69393878e-01  5.69956533e-01  5.70334329e-01
  5.74668826e-01  5.75559078e-01  5.75630174e-01  5.75714462e-01
  5.76130333e-01  5.76206475e-01  5.77583699e-01  5.79698807e-01
  5.79811865e-01  5.85880168e-01  5.89544830e-01  5.91914875e-01
  5.92437699e-01  6.07017452e-01  6.07918891e-01  6.09256391e-01
  6.10704552e-01  6.12244325e-01  6.15720098e-01  6.16198873e-01
  6.16612515e-01  6.22852063e-01  6.27184999e-01  6.27389391e-01
  6.31008719e-01  6.33559077e-01  6.34504263e-01  6.36979582e-01
  6.38641839e-01  6.40299705e-01  6.47600730e-01  6.48037639e-01
  6.50575548e-01  6.50668424e-01  6.55159508e-01  6.56796814e-01
  6.59114071e-01  6.63057602e-01  6.69459020e-01  6.71326068e-01
  6.78791750e-01  6.83756634e-01  6.87845241e-01  6.92638561e-01
  6.94242623e-01  6.94265206e-01  6.97263983e-01  6.97767292e-01
  7.09144843e-01  7.16116857e-01  7.19600921e-01  7.25472248e-01
  7.34536799e-01]

  warnings.warn(

2022-11-03 10:48:56,847:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.21995375 -0.20156224 -0.19517494 -0.17532196 -0.17526044 -0.17190587
 -0.16542606 -0.16485416 -0.16473621 -0.16382731 -0.15718968 -0.1567626
 -0.15370434 -0.15249719 -0.14336911 -0.13846612 -0.13716038 -0.13617887
 -0.13612657 -0.12946446 -0.12688151 -0.11988778 -0.11883769 -0.11591722
 -0.11344735 -0.11168056 -0.11134392 -0.11102781 -0.10922339 -0.10921945
 -0.10552838 -0.09637341 -0.09617047 -0.09268386 -0.09246421 -0.09106299
 -0.09049436 -0.08483331 -0.08423254 -0.08395837 -0.08332911 -0.08278316
 -0.08254977 -0.08042081 -0.07882946 -0.07486019 -0.06887859 -0.06816356
 -0.06648795 -0.06282146 -0.06253692 -0.06240397 -0.06055665 -0.0561319
 -0.05561258 -0.05423256 -0.05390765 -0.0529403  -0.05242863 -0.04733807
 -0.04646914 -0.04635257 -0.04583403 -0.04499208 -0.04491378 -0.04151733
 -0.03846217 -0.03759608 -0.03595636 -0.03356973 -0.03300069 -0.03244074
 -0.02533164 -0.01986503 -0.01693559 -0.01556187 -0.01546881 -0.01449584
 -0.01336515 -0.01099972 -0.01066485 -0.00811392 -0.00693457 -0.00489688
 -0.00175242  0.00122484  0.00182201  0.00604517  0.0096298   0.01149551
  0.01206894  0.01391786  0.01686972  0.01817437  0.01833533  0.02119341
  0.02133919  0.02310854  0.02466553  0.02953944  0.03039273  0.03070166
  0.03143429  0.03695     0.03903229  0.03980566  0.04090561  0.04462778
  0.05041741  0.050648    0.05099537  0.05499355  0.05512309  0.05532132
  0.05673146  0.05906322  0.05919481  0.06075396  0.06105576  0.06563767
  0.06850039  0.06963128  0.07166113  0.07637359  0.07687042  0.07828668
  0.07857365  0.0807738   0.08452663  0.08629059  0.08664293  0.08746303
  0.09033173  0.09243359  0.09263014  0.09350479  0.09474971  0.0978683
  0.09975243  0.10090523  0.1027711   0.10537573  0.10595945  0.10804259
  0.10932383  0.11028343  0.11029149  0.11100749  0.11376965  0.1172224
  0.12107135  0.12406677  0.12412644  0.1243029   0.12777708  0.12947169
  0.13038808  0.13263488  0.13294959  0.13309386  0.13636565  0.1377309
  0.13850561  0.13903155  0.14279615  0.14347773  0.14434866  0.1462418
  0.14681006  0.14896265  0.1550299   0.15634256  0.15789115  0.15820876
  0.15889219  0.15946524  0.16078833  0.16362414  0.16471883  0.16584496
  0.16971421  0.1720216   0.17376434  0.17603084  0.18207719  0.18220787
  0.18256065  0.18358076  0.18529167  0.18637677  0.19150296  0.192734
  0.19293644  0.19298236  0.19393749  0.19637504  0.1967431   0.19709383
  0.19759569  0.19800886  0.19844034  0.19845084  0.20106684  0.20250968
  0.20418742  0.2055052   0.20589124  0.20734537  0.20737285  0.21048647
  0.21133808  0.21171044  0.21825507  0.21903192  0.21981598  0.22053948
  0.22075728  0.22134801  0.22257426  0.22345965  0.22424499  0.22493684
  0.2253667   0.22542378  0.22548906  0.23096236  0.23248797  0.23663138
  0.24005468  0.24022641  0.24166679  0.24249524  0.24364751  0.24532052
  0.24730132  0.2476877   0.24929943  0.25248203  0.25406676  0.25471994
  0.25474042  0.25522692  0.2554401   0.25686827  0.25690413  0.25717003
  0.26223076  0.26396139  0.26407468  0.26540716  0.26647102  0.26658703
  0.27034574  0.2706378   0.27387613  0.27402494  0.27479193  0.27557641
  0.27621449  0.27737494  0.2777618   0.27838998  0.28099577  0.28187683
  0.28401214  0.2844458   0.28529793  0.28580957  0.28663927  0.28682262
  0.28850114  0.28851251  0.28989243  0.28997739  0.29233565  0.29494311
  0.29846965  0.29876389  0.30701713  0.31155038  0.31236627  0.31319372
  0.31334198  0.31425054  0.31583074  0.31657161  0.31722067  0.31729027
  0.3173681   0.31820598  0.32144875  0.32197988  0.32299161  0.32400343
  0.32463548  0.32531887  0.32605303  0.32634561  0.32803443  0.32852128
  0.32980238  0.33104302  0.33282144  0.33466423  0.34188786  0.34218012
  0.34652675  0.34665565  0.35022167  0.35136931  0.35162105  0.3524659
  0.35280341  0.35389056  0.3555991   0.35628712  0.35736158  0.35760006
  0.35801337  0.36007729  0.3623172   0.36423307  0.3665527   0.36909796
  0.37117461  0.37315326  0.37334409  0.37745377  0.37860499  0.38003817
  0.38471284  0.38692133  0.38791566  0.38908373  0.38993828  0.39022611
  0.39029691  0.39032836  0.39141929  0.3915439   0.39312261  0.39569472
  0.39693782  0.39920439  0.40293068  0.40632054  0.40723917  0.4127757
  0.41464425  0.41635175  0.41636185  0.41972     0.42007464  0.42231677
  0.42249514  0.4239188   0.42465395  0.42979767  0.43118197  0.43225241
  0.43263666  0.4328125   0.43445567  0.44102219  0.44105451  0.44132504
  0.44151715  0.44247983  0.44289565  0.44375305  0.44558489  0.44572274
  0.44706832  0.44790358  0.44878909  0.44988428  0.45407443  0.45600448
  0.4630452   0.46444079  0.4660532   0.46811018  0.46973204  0.47142993
  0.4729564   0.47480479  0.47871436  0.4807098   0.48099816  0.48452403
  0.48531511  0.48765297  0.48803672  0.48902529  0.48996073  0.49022132
  0.49195976  0.49254325  0.494444    0.49510512  0.49814578  0.49938263
  0.50053628  0.50286795  0.50304959  0.50337797  0.50373531  0.50579938
  0.50614557  0.50715503  0.50971351  0.51410136  0.51556894  0.51611288
  0.51937683  0.52121574  0.52411329  0.53216051  0.53281377  0.53415622
  0.53609685  0.53611704  0.53653496  0.53725898  0.53925269  0.54315348
  0.54456024  0.54486036  0.549304    0.55417157  0.55764519  0.56153977
  0.56160231  0.56248144  0.5641628   0.56581901  0.56804971  0.57039778
  0.5735457   0.57504472  0.5758139   0.57661747  0.57937371  0.57949028
  0.58038332  0.58094584  0.58804753  0.59242457  0.59480299  0.59621177
  0.59741719  0.59807833  0.60348283  0.60895934  0.60994314  0.61016638
  0.61197567  0.61222734  0.61263175  0.61504288  0.61590109  0.62467272
  0.62727327  0.62780469  0.63030568  0.63690553  0.642505    0.64462021
  0.64474076  0.64492361  0.64642337  0.64751352  0.64930491  0.65005229
  0.65053256  0.65497884  0.6617082   0.66282176  0.66337642  0.66470706
  0.66494939  0.66512453  0.66537536  0.67329833  0.67536626  0.69493366
  0.7048751   0.71299944  0.73657167  0.74688423  0.75921636  0.76776268
  0.76948426]

  warnings.warn(

2022-11-03 10:48:56,922:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.17859909 -0.17137604 -0.16883698 -0.16142804 -0.15823029 -0.15433136
 -0.14520169 -0.14441111 -0.14382384 -0.14100634 -0.14087518 -0.13970847
 -0.13939833 -0.13249522 -0.13246282 -0.13130347 -0.12968202 -0.12649065
 -0.12527451 -0.12306027 -0.11891008 -0.10731138 -0.09906699 -0.09631797
 -0.09589221 -0.09027215 -0.08319542 -0.08295828 -0.08277739 -0.08250432
 -0.08228986 -0.07808097 -0.07650528 -0.07380014 -0.07271407 -0.07262291
 -0.07233932 -0.07211808 -0.06746815 -0.06702841 -0.06594035 -0.06342977
 -0.06150551 -0.05364698 -0.05151769 -0.0501369  -0.04992765 -0.04987424
 -0.04494082 -0.04027569 -0.03818179 -0.03732028 -0.03453102 -0.03381415
 -0.02955203 -0.02864871 -0.02553229 -0.02487619 -0.02268468 -0.02252069
 -0.02037185 -0.01877173 -0.01822087 -0.0182084  -0.01770274 -0.01543315
 -0.01412414 -0.0114035  -0.00778528 -0.00710542 -0.00449067 -0.00204671
  0.00162219  0.00443329  0.00625776  0.00653145  0.00771633  0.00801741
  0.01023331  0.01061632  0.0147856   0.01687977  0.01835336  0.01943637
  0.02416442  0.02781641  0.02928354  0.02986251  0.03016783  0.03094979
  0.03379028  0.0356441   0.03925998  0.03962373  0.03985412  0.0399344
  0.04082751  0.05174638  0.05261206  0.05682378  0.06012914  0.06078503
  0.06159185  0.06564717  0.06577092  0.06666856  0.07102967  0.07256755
  0.07474553  0.07520501  0.07544624  0.0761721   0.07779038  0.08238989
  0.08386085  0.0849662   0.08584834  0.08643958  0.08680819  0.08706165
  0.09031636  0.09085284  0.09183286  0.09228593  0.09255045  0.0950769
  0.09526852  0.09544835  0.09714589  0.09834657  0.10211893  0.10374185
  0.10443462  0.10474312  0.10564904  0.10602678  0.10996826  0.11186988
  0.11197397  0.11202978  0.11359033  0.11478966  0.11549043  0.11689716
  0.11987762  0.12011251  0.120418    0.12100201  0.12263108  0.12330828
  0.12556199  0.12646656  0.12895193  0.13195149  0.13256507  0.13548904
  0.13652015  0.13769934  0.13859216  0.13906358  0.14001971  0.14274009
  0.14311831  0.14466241  0.14576557  0.14609819  0.14691511  0.14735177
  0.15345267  0.15387854  0.15460962  0.15594909  0.15669579  0.15758231
  0.1581733   0.15844873  0.16242785  0.16278127  0.16348791  0.16398071
  0.16564139  0.16606266  0.169004    0.17093509  0.17124833  0.17288807
  0.17335896  0.17355232  0.17437053  0.17473283  0.17526983  0.17527054
  0.17556138  0.17615514  0.17648087  0.17742806  0.17818027  0.17841982
  0.18097947  0.18112012  0.18208847  0.18282333  0.18437182  0.18544378
  0.18639886  0.19001275  0.1968446   0.19817807  0.19827033  0.19854711
  0.1987812   0.20017674  0.20150764  0.20176155  0.2030076   0.20403427
  0.20443419  0.20550196  0.20724564  0.20821553  0.20852778  0.20862954
  0.20890153  0.2092309   0.21137079  0.21314535  0.21396375  0.21419139
  0.2151643   0.21852511  0.21897764  0.22006489  0.22109162  0.22195463
  0.22231064  0.224986    0.22541104  0.22575761  0.2287525   0.23075427
  0.23160618  0.23546496  0.23548466  0.23573909  0.23750414  0.23869479
  0.24245129  0.24427656  0.24890434  0.25108094  0.25133329  0.25228159
  0.25435941  0.25461977  0.25813919  0.25989824  0.26381099  0.2644135
  0.2654833   0.26568439  0.26632215  0.26758752  0.27389795  0.27584366
  0.27982775  0.28013009  0.2802278   0.28081642  0.28565539  0.28809932
  0.28872465  0.28976764  0.29438848  0.29439729  0.2983794   0.298952
  0.30056516  0.30396865  0.30469516  0.30497693  0.30510832  0.3081813
  0.3087887   0.31129456  0.31191352  0.31202232  0.31205255  0.31214926
  0.313166    0.3141328   0.31855322  0.31918478  0.31972045  0.31984586
  0.32020819  0.32424274  0.3243946   0.32924798  0.33138723  0.33226646
  0.33506402  0.34231866  0.34290593  0.344175    0.34521683  0.34669929
  0.34687919  0.34791121  0.34799461  0.35228108  0.35248076  0.35273546
  0.35314949  0.35368953  0.35470757  0.35701237  0.35812474  0.3583853
  0.3596053   0.36101767  0.36270634  0.36839226  0.37608674  0.37659653
  0.37719176  0.37730064  0.37743132  0.38113479  0.38170251  0.38194438
  0.38312869  0.38468645  0.38689591  0.3903517   0.39379965  0.39383144
  0.39476262  0.39510278  0.39548872  0.39557743  0.39954061  0.40436677
  0.40909104  0.40975384  0.41720849  0.41763515  0.41920264  0.41936779
  0.42128908  0.42320586  0.42370236  0.4245139   0.43250952  0.43255183
  0.43431112  0.43611767  0.43903664  0.44193085  0.44444533  0.44481374
  0.44646204  0.44807096  0.44877735  0.45180219  0.45188271  0.45317501
  0.45476071  0.45576707  0.455824    0.45782896  0.45955087  0.45955986
  0.45970005  0.46424165  0.46591635  0.46655379  0.47076474  0.47498066
  0.47525658  0.47712874  0.47722008  0.48031336  0.48348754  0.48348842
  0.48663683  0.48929393  0.49136601  0.4921309   0.49815352  0.49820391
  0.49837914  0.50112939  0.50143085  0.50150219  0.50165706  0.50192396
  0.50549932  0.5062357   0.50665115  0.51135318  0.51287801  0.51404285
  0.51632054  0.51890148  0.51918744  0.52057425  0.52158057  0.52888547
  0.53218272  0.53260741  0.532886    0.5341401   0.53905577  0.5398203
  0.54003297  0.54101899  0.54191458  0.54329125  0.54891503  0.55005171
  0.55057397  0.55115858  0.55142052  0.55145255  0.55158167  0.55173629
  0.55192204  0.55280502  0.55319444  0.5541294   0.55763616  0.55763957
  0.56216761  0.5628237   0.56291041  0.56939642  0.56980925  0.57118016
  0.5862847   0.5877238   0.588692    0.59615516  0.59779372  0.6039677
  0.60609727  0.60803595  0.60876772  0.60899448  0.60904007  0.61031502
  0.61039874  0.61094132  0.61565771  0.61801732  0.62576125  0.62897278
  0.62922915  0.63256296  0.63393807  0.63661445  0.63812846  0.64145848
  0.6450304   0.64578623  0.64631825  0.64980956  0.65322722  0.6575099
  0.65754569  0.6580613   0.66211529  0.66728319  0.66754401  0.66778824
  0.67265571  0.67359233  0.67695794  0.68081029  0.68150292  0.68302453
  0.69239014  0.69457139  0.69503269  0.70163067  0.71779953  0.71909945
  0.72530793  0.7340377   0.75742178  0.76269903  0.76518511  0.76941151
  0.77693273]

  warnings.warn(

2022-11-03 10:48:57,022:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.76828970e-01 -1.71590919e-01 -1.60074520e-01 -1.54233416e-01
 -1.53754253e-01 -1.35760584e-01 -1.33551417e-01 -1.32461881e-01
 -1.25838367e-01 -1.17038122e-01 -1.16500262e-01 -1.15088990e-01
 -1.13429493e-01 -1.11939679e-01 -1.11383469e-01 -1.09295667e-01
 -1.08794800e-01 -1.05104520e-01 -1.02609051e-01 -1.01706842e-01
 -1.00838178e-01 -1.00060102e-01 -9.92363730e-02 -9.65399041e-02
 -9.46506116e-02 -9.17803428e-02 -9.06399124e-02 -8.03530817e-02
 -7.89968938e-02 -7.75164309e-02 -7.71067875e-02 -7.69268583e-02
 -7.57457605e-02 -6.90883016e-02 -6.83036690e-02 -6.17971968e-02
 -6.04433692e-02 -6.01744042e-02 -5.86243117e-02 -5.81042684e-02
 -5.63905720e-02 -5.55752302e-02 -5.30256916e-02 -5.25311828e-02
 -5.18352273e-02 -4.95095232e-02 -4.75784811e-02 -4.73616742e-02
 -4.57105651e-02 -4.46881460e-02 -4.37709708e-02 -4.34436666e-02
 -4.05282443e-02 -3.80974203e-02 -3.69345616e-02 -3.62506776e-02
 -3.53203570e-02 -3.46977167e-02 -3.38618021e-02 -3.31728175e-02
 -2.97655185e-02 -2.77641212e-02 -2.58381100e-02 -2.56689700e-02
 -2.51336614e-02 -2.50571707e-02 -2.48193959e-02 -2.05577396e-02
 -1.99662980e-02 -1.69089378e-02 -1.59841051e-02 -1.48097797e-02
 -1.46612823e-02 -9.85119430e-03 -8.24179322e-03 -3.09162107e-03
 -2.16165421e-03 -1.65463332e-03 -2.03126882e-04  8.15561744e-06
  4.98402147e-03  7.44600851e-03  1.02434234e-02  1.05811733e-02
  1.52127800e-02  1.58290733e-02  1.86435511e-02  1.86641337e-02
  2.14386405e-02  2.21812666e-02  2.32399765e-02  2.45357686e-02
  2.52718538e-02  2.70195193e-02  2.76674864e-02  2.77928925e-02
  3.04522416e-02  3.08062873e-02  3.09463859e-02  3.46253724e-02
  3.53403464e-02  3.58346150e-02  3.62356384e-02  3.74417980e-02
  3.82654510e-02  3.92358557e-02  4.14142745e-02  4.21686074e-02
  4.21755080e-02  4.22210887e-02  4.37041475e-02  4.45280409e-02
  4.52454903e-02  5.01641867e-02  5.14701976e-02  5.31305104e-02
  5.49214926e-02  5.56424330e-02  5.59853385e-02  5.60559178e-02
  5.71679815e-02  5.84685098e-02  5.98297980e-02  6.24660123e-02
  6.46230377e-02  6.59403227e-02  6.73136198e-02  6.79505493e-02
  6.87671957e-02  6.91527335e-02  6.93247992e-02  6.99943193e-02
  7.06540423e-02  7.40828575e-02  7.69243052e-02  7.74713345e-02
  7.75698252e-02  7.84758769e-02  8.07963095e-02  8.14315408e-02
  8.27171309e-02  8.30835894e-02  8.56577914e-02  8.61202990e-02
  8.74514238e-02  8.92051393e-02  9.11343239e-02  9.13090491e-02
  9.18111157e-02  9.21068953e-02  9.51817366e-02  9.57133179e-02
  9.71698666e-02  9.95515120e-02  1.00874940e-01  1.02105049e-01
  1.04753719e-01  1.05660935e-01  1.06040994e-01  1.06668271e-01
  1.06862947e-01  1.08689657e-01  1.10108758e-01  1.13412228e-01
  1.14168775e-01  1.14886257e-01  1.16700158e-01  1.17372249e-01
  1.21480659e-01  1.24704294e-01  1.27825327e-01  1.28326517e-01
  1.29905645e-01  1.31903976e-01  1.33956037e-01  1.39452550e-01
  1.40565854e-01  1.49006685e-01  1.52111030e-01  1.53052374e-01
  1.55638544e-01  1.55852230e-01  1.56153701e-01  1.58908051e-01
  1.58916757e-01  1.59320338e-01  1.59632253e-01  1.61553977e-01
  1.62308063e-01  1.62631600e-01  1.63239273e-01  1.63970966e-01
  1.65400834e-01  1.67744732e-01  1.69409182e-01  1.70200607e-01
  1.71577613e-01  1.72650654e-01  1.74332081e-01  1.76668793e-01
  1.78249035e-01  1.81248698e-01  1.81459875e-01  1.82512820e-01
  1.83982105e-01  1.88441880e-01  1.88655686e-01  1.89848852e-01
  1.90736604e-01  1.92298419e-01  1.94421113e-01  1.98386300e-01
  1.99683331e-01  2.00619640e-01  2.00989311e-01  2.02818361e-01
  2.04494302e-01  2.05432037e-01  2.06751453e-01  2.06929719e-01
  2.07346486e-01  2.07809537e-01  2.07992895e-01  2.10484206e-01
  2.11319619e-01  2.11459960e-01  2.15608003e-01  2.19177550e-01
  2.20660602e-01  2.23231563e-01  2.23328837e-01  2.27690324e-01
  2.28084914e-01  2.28258642e-01  2.28906586e-01  2.29709847e-01
  2.29876168e-01  2.34117328e-01  2.36648358e-01  2.37783622e-01
  2.39450639e-01  2.40454788e-01  2.41435331e-01  2.43926043e-01
  2.44454053e-01  2.47078210e-01  2.47522202e-01  2.49226023e-01
  2.49489784e-01  2.53536602e-01  2.56838799e-01  2.57268689e-01
  2.59456426e-01  2.60866806e-01  2.61045491e-01  2.63134590e-01
  2.64950540e-01  2.66969778e-01  2.67653795e-01  2.68719169e-01
  2.69415381e-01  2.69599170e-01  2.70587445e-01  2.72027198e-01
  2.75375763e-01  2.77384029e-01  2.78114262e-01  2.79386590e-01
  2.80013012e-01  2.80238803e-01  2.84934635e-01  2.86328533e-01
  2.87644820e-01  2.88524457e-01  2.92390551e-01  2.92742897e-01
  2.93208408e-01  2.93312143e-01  2.96445530e-01  2.98256748e-01
  2.98365610e-01  2.98767273e-01  2.99033117e-01  2.99419398e-01
  2.99778438e-01  3.03531510e-01  3.04870281e-01  3.06160536e-01
  3.06819081e-01  3.07362572e-01  3.07481063e-01  3.07593984e-01
  3.07769271e-01  3.10477076e-01  3.13067822e-01  3.14376786e-01
  3.14624999e-01  3.15849516e-01  3.18379503e-01  3.18554257e-01
  3.19027010e-01  3.20464702e-01  3.22056721e-01  3.22808893e-01
  3.22809392e-01  3.28589192e-01  3.29107710e-01  3.30089319e-01
  3.31001306e-01  3.31920809e-01  3.33944077e-01  3.36306619e-01
  3.39043529e-01  3.41649017e-01  3.45053127e-01  3.47811905e-01
  3.49203531e-01  3.50015594e-01  3.51526776e-01  3.53123103e-01
  3.55200558e-01  3.55388683e-01  3.57168020e-01  3.58612946e-01
  3.58989094e-01  3.60087481e-01  3.60592265e-01  3.63474439e-01
  3.63763798e-01  3.66045287e-01  3.66621314e-01  3.67057851e-01
  3.68296817e-01  3.69975278e-01  3.70732555e-01  3.72438024e-01
  3.73466860e-01  3.75312022e-01  3.75977138e-01  3.81285658e-01
  3.83788074e-01  3.84157003e-01  3.84493599e-01  3.88150338e-01
  3.90226797e-01  3.93561955e-01  3.96175629e-01  4.01164172e-01
  4.04266362e-01  4.05352072e-01  4.12868229e-01  4.12932899e-01
  4.13247697e-01  4.13609466e-01  4.14089768e-01  4.14667631e-01
  4.15573631e-01  4.19887153e-01  4.21804021e-01  4.23553221e-01
  4.24276146e-01  4.26017007e-01  4.26203539e-01  4.26262530e-01
  4.34462828e-01  4.36573217e-01  4.37353302e-01  4.38545434e-01
  4.41785737e-01  4.44099947e-01  4.46395743e-01  4.49317434e-01
  4.49329431e-01  4.49447897e-01  4.50491964e-01  4.51289664e-01
  4.55347155e-01  4.56609081e-01  4.58432099e-01  4.59064721e-01
  4.59356465e-01  4.60865732e-01  4.60949787e-01  4.63882765e-01
  4.68299103e-01  4.68738903e-01  4.69909047e-01  4.73102315e-01
  4.76480006e-01  4.76760631e-01  4.77177494e-01  4.78797523e-01
  4.81139207e-01  4.85327175e-01  4.89884066e-01  4.91808739e-01
  4.92500030e-01  4.93823502e-01  4.96253001e-01  4.97267539e-01
  4.99699543e-01  5.01555007e-01  5.03398937e-01  5.06210835e-01
  5.09613397e-01  5.13528798e-01  5.13587699e-01  5.13869720e-01
  5.19318546e-01  5.22035845e-01  5.22507502e-01  5.25244696e-01
  5.28032313e-01  5.28759864e-01  5.30013458e-01  5.30127594e-01
  5.30508558e-01  5.35903881e-01  5.39752239e-01  5.45661764e-01
  5.45890323e-01  5.47712762e-01  5.47891902e-01  5.50044906e-01
  5.52096380e-01  5.53671158e-01  5.60032958e-01  5.61498711e-01
  5.68278403e-01  5.74696319e-01  5.78761926e-01  5.83139858e-01
  5.85206103e-01  5.85641166e-01  5.86640733e-01  5.93164100e-01
  5.94548245e-01  5.95417966e-01  5.96593657e-01  5.97208020e-01
  5.98201852e-01  5.98325363e-01  6.00104142e-01  6.02111061e-01
  6.03521359e-01  6.03553934e-01  6.05935122e-01  6.06261823e-01
  6.06507172e-01  6.06953774e-01  6.12001248e-01  6.12560296e-01
  6.12849245e-01  6.13269654e-01  6.15091948e-01  6.17064177e-01
  6.18273199e-01  6.19377878e-01  6.24037071e-01  6.27114018e-01
  6.27962170e-01  6.29657402e-01  6.32783546e-01  6.35194715e-01
  6.38056403e-01  6.41347368e-01  6.42440633e-01  6.46976870e-01
  6.49446479e-01  6.53948993e-01  6.54646347e-01  6.57946539e-01
  6.58463372e-01  6.58914496e-01  6.61215993e-01  6.61245463e-01
  6.65931179e-01  6.66746575e-01  6.66840200e-01  6.73361811e-01
  6.84028019e-01  6.84917464e-01  7.02234432e-01  7.08033739e-01
  7.09314070e-01  7.12785068e-01  7.21896567e-01  7.22050583e-01
  7.22933656e-01  7.22964407e-01  7.26397217e-01  7.64551607e-01]

  warnings.warn(

2022-11-03 10:48:57,038:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.57098522e-01 -1.90276567e-01 -1.84791237e-01 -1.81700163e-01
 -1.73283491e-01 -1.72072705e-01 -1.69474249e-01 -1.67413124e-01
 -1.62080317e-01 -1.60307348e-01 -1.58089089e-01 -1.54173531e-01
 -1.48806832e-01 -1.47473492e-01 -1.47383739e-01 -1.42274144e-01
 -1.34965359e-01 -1.34467337e-01 -1.29753666e-01 -1.29130748e-01
 -1.23284294e-01 -1.19709248e-01 -1.19666193e-01 -1.17022843e-01
 -1.14800802e-01 -1.08273609e-01 -1.06839435e-01 -1.02226160e-01
 -1.02076222e-01 -1.01560206e-01 -9.76948841e-02 -9.27560785e-02
 -9.16132010e-02 -9.04240472e-02 -8.60033288e-02 -8.21041070e-02
 -8.04125499e-02 -7.97534025e-02 -7.91426305e-02 -7.81334926e-02
 -6.97768516e-02 -6.65307697e-02 -6.47949780e-02 -6.20977157e-02
 -6.16713787e-02 -6.12899027e-02 -6.10213848e-02 -6.07583885e-02
 -5.84702709e-02 -5.80947032e-02 -5.76241856e-02 -5.73024145e-02
 -5.45768569e-02 -5.42866421e-02 -5.41226286e-02 -5.27850171e-02
 -5.13149061e-02 -4.96293095e-02 -4.95120057e-02 -4.90559553e-02
 -4.80499291e-02 -4.38216421e-02 -4.19838701e-02 -4.18050138e-02
 -3.68897975e-02 -3.20842058e-02 -3.03875759e-02 -2.94943417e-02
 -2.77258915e-02 -2.68047659e-02 -2.57874167e-02 -2.57256088e-02
 -2.19911237e-02 -2.03458876e-02 -1.73413192e-02 -1.63704047e-02
 -1.48965538e-02 -1.08185202e-02 -1.02286995e-02 -7.66147551e-03
 -7.46692561e-03 -3.19626726e-03 -1.21900889e-03 -3.84875387e-04
  4.68971821e-03  9.76086495e-03  1.23753543e-02  1.47307486e-02
  1.56033762e-02  1.68913557e-02  2.13327076e-02  2.48371499e-02
  2.57660925e-02  2.63663929e-02  2.68190936e-02  2.83016678e-02
  2.88055717e-02  3.22450710e-02  3.40416737e-02  3.43037652e-02
  3.50289859e-02  3.56156049e-02  3.93706224e-02  3.99754504e-02
  4.13935993e-02  4.27806414e-02  4.42238874e-02  4.57581928e-02
  4.65822941e-02  4.72979311e-02  4.73772284e-02  4.81873452e-02
  4.84494751e-02  4.88100143e-02  5.10536402e-02  5.11815542e-02
  5.37074770e-02  5.62526344e-02  5.76972412e-02  5.92101373e-02
  6.13238744e-02  6.14703161e-02  6.21760107e-02  6.24169709e-02
  6.26666406e-02  6.40789061e-02  6.41046598e-02  6.45024969e-02
  6.51649713e-02  6.53828001e-02  6.66338949e-02  6.70466791e-02
  6.95711930e-02  6.97856180e-02  7.01258338e-02  7.15713384e-02
  7.32799637e-02  7.35766151e-02  7.40297267e-02  7.74471223e-02
  7.79642726e-02  7.96424591e-02  8.10578585e-02  8.16240640e-02
  8.26467671e-02  8.39592988e-02  8.53595815e-02  8.60345643e-02
  8.64545644e-02  8.81574691e-02  8.81801691e-02  9.81310549e-02
  9.96679323e-02  9.99084597e-02  1.00662646e-01  1.05057615e-01
  1.10076251e-01  1.12864223e-01  1.19473486e-01  1.20301917e-01
  1.23037878e-01  1.23039701e-01  1.24250932e-01  1.25387531e-01
  1.25885490e-01  1.29722055e-01  1.30121237e-01  1.31071163e-01
  1.33570512e-01  1.35615323e-01  1.37216605e-01  1.37888972e-01
  1.38484390e-01  1.39316522e-01  1.40041507e-01  1.42655312e-01
  1.42995788e-01  1.43308429e-01  1.44416990e-01  1.45095771e-01
  1.45424960e-01  1.48050954e-01  1.48352059e-01  1.51404631e-01
  1.51522322e-01  1.52101878e-01  1.52687564e-01  1.55827518e-01
  1.56182960e-01  1.56735904e-01  1.57088157e-01  1.60111464e-01
  1.61213708e-01  1.62834379e-01  1.66448057e-01  1.68935842e-01
  1.71792748e-01  1.72282885e-01  1.75008190e-01  1.76794053e-01
  1.81990880e-01  1.82403503e-01  1.82609918e-01  1.84079355e-01
  1.85835347e-01  1.85863123e-01  1.86105934e-01  1.86988167e-01
  1.87518151e-01  1.87913576e-01  1.90122297e-01  1.97606573e-01
  1.97843148e-01  1.98087975e-01  1.98719827e-01  1.99397006e-01
  2.00102074e-01  2.00278967e-01  2.00529567e-01  2.01338425e-01
  2.03654485e-01  2.03959195e-01  2.04448377e-01  2.07881738e-01
  2.08123842e-01  2.08884957e-01  2.10569119e-01  2.10729070e-01
  2.11191703e-01  2.12245414e-01  2.15064662e-01  2.15276514e-01
  2.15506044e-01  2.20994510e-01  2.21380421e-01  2.22071023e-01
  2.23185654e-01  2.23300984e-01  2.24104397e-01  2.24408829e-01
  2.28060359e-01  2.28608373e-01  2.28819466e-01  2.29272015e-01
  2.35092842e-01  2.35404808e-01  2.37261991e-01  2.38232077e-01
  2.38805031e-01  2.39514229e-01  2.40225753e-01  2.40250601e-01
  2.40519188e-01  2.41198395e-01  2.43578065e-01  2.44495448e-01
  2.46360237e-01  2.49850073e-01  2.54581252e-01  2.56783587e-01
  2.56880774e-01  2.57685838e-01  2.57798347e-01  2.58084999e-01
  2.59902860e-01  2.65422292e-01  2.65908196e-01  2.66141727e-01
  2.67147273e-01  2.68838346e-01  2.70171687e-01  2.70966436e-01
  2.71216888e-01  2.72575763e-01  2.72948585e-01  2.73727274e-01
  2.74417932e-01  2.76304513e-01  2.76851082e-01  2.76861395e-01
  2.78282540e-01  2.78415648e-01  2.80407268e-01  2.83297386e-01
  2.83967197e-01  2.84303237e-01  2.86569143e-01  2.86891152e-01
  2.87396618e-01  2.89266839e-01  2.89863121e-01  2.91364711e-01
  2.92778576e-01  2.92928281e-01  2.95448076e-01  3.00106680e-01
  3.03546737e-01  3.05005683e-01  3.06532283e-01  3.06848656e-01
  3.07424500e-01  3.08567999e-01  3.08758509e-01  3.09061246e-01
  3.10145707e-01  3.11767568e-01  3.13826246e-01  3.15206504e-01
  3.15369299e-01  3.16405922e-01  3.21666672e-01  3.22156380e-01
  3.23736491e-01  3.24132312e-01  3.28752243e-01  3.34501219e-01
  3.34933960e-01  3.36218132e-01  3.40218632e-01  3.44284448e-01
  3.44705246e-01  3.45013767e-01  3.47943442e-01  3.48070998e-01
  3.49056462e-01  3.49090713e-01  3.52360436e-01  3.53447891e-01
  3.54410544e-01  3.56803465e-01  3.57436721e-01  3.59592085e-01
  3.59611388e-01  3.60329064e-01  3.60461129e-01  3.63828879e-01
  3.64202664e-01  3.64218832e-01  3.65580519e-01  3.67897916e-01
  3.71890817e-01  3.73360774e-01  3.75739501e-01  3.76677423e-01
  3.79713903e-01  3.80022315e-01  3.85546767e-01  3.86550593e-01
  3.90132587e-01  3.95275045e-01  3.97964106e-01  3.99488962e-01
  4.02592886e-01  4.04817904e-01  4.10327053e-01  4.11367859e-01
  4.15692022e-01  4.16830051e-01  4.16996118e-01  4.22214807e-01
  4.22616523e-01  4.26578408e-01  4.26707495e-01  4.27364719e-01
  4.29620381e-01  4.30578659e-01  4.32426289e-01  4.33509605e-01
  4.40014116e-01  4.42692498e-01  4.43378795e-01  4.46447549e-01
  4.48058060e-01  4.48516579e-01  4.49393394e-01  4.50803095e-01
  4.51007014e-01  4.59366911e-01  4.63792250e-01  4.67091210e-01
  4.71738216e-01  4.74585755e-01  4.79907825e-01  4.80327475e-01
  4.80635817e-01  4.81796272e-01  4.83789050e-01  4.83952202e-01
  4.84612210e-01  4.85867536e-01  4.87226434e-01  4.90308244e-01
  4.92958699e-01  4.93387339e-01  4.94394091e-01  4.95000518e-01
  4.98113036e-01  5.00029639e-01  5.02796802e-01  5.04754184e-01
  5.05678441e-01  5.09721432e-01  5.10572943e-01  5.11544412e-01
  5.17048611e-01  5.21589759e-01  5.22161410e-01  5.22306736e-01
  5.25792913e-01  5.26132583e-01  5.29098277e-01  5.30462875e-01
  5.30845847e-01  5.31785229e-01  5.32123073e-01  5.33833669e-01
  5.34596809e-01  5.35405135e-01  5.38975976e-01  5.40632938e-01
  5.41891364e-01  5.42015580e-01  5.42154713e-01  5.43465704e-01
  5.46994535e-01  5.47237655e-01  5.49341498e-01  5.51269743e-01
  5.51342977e-01  5.51606260e-01  5.53353063e-01  5.54878563e-01
  5.55530814e-01  5.55673018e-01  5.56703222e-01  5.59422040e-01
  5.60162298e-01  5.62152003e-01  5.71379589e-01  5.76848920e-01
  5.76950764e-01  5.79114725e-01  5.80415434e-01  5.80845394e-01
  5.82737276e-01  5.87672924e-01  5.87736020e-01  5.89152561e-01
  5.89462371e-01  5.91455857e-01  5.92079908e-01  5.97686862e-01
  6.01238704e-01  6.01475124e-01  6.01888424e-01  6.05526487e-01
  6.12175766e-01  6.14797291e-01  6.16048052e-01  6.24031462e-01
  6.25022928e-01  6.25705113e-01  6.26370827e-01  6.28140389e-01
  6.28286658e-01  6.29349863e-01  6.30434256e-01  6.34693271e-01
  6.39487003e-01  6.40769688e-01  6.47050756e-01  6.50850149e-01
  6.54544507e-01  6.56453451e-01  6.56798868e-01  6.68292444e-01
  6.70476313e-01  6.73863258e-01  6.75845863e-01  6.75860894e-01
  6.77110841e-01  6.78615531e-01  6.79057772e-01  6.79689543e-01
  6.79723671e-01  6.83777457e-01  6.88320732e-01  6.89230348e-01
  6.90767690e-01  7.13812092e-01  7.17168723e-01  7.23735694e-01
  7.29849648e-01]

  warnings.warn(

2022-11-03 10:48:57,080:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.91536643e-01 -1.67359420e-01 -1.50870734e-01 -1.44159664e-01
 -1.38960061e-01 -1.32436532e-01 -1.30889801e-01 -1.28575354e-01
 -1.25375597e-01 -1.19656661e-01 -1.18150440e-01 -1.16905858e-01
 -1.15855640e-01 -1.14869827e-01 -1.10000053e-01 -1.08742246e-01
 -1.02658434e-01 -1.00956932e-01 -9.73111792e-02 -9.45469244e-02
 -9.38658895e-02 -8.94110251e-02 -8.83384577e-02 -8.72819415e-02
 -8.61035589e-02 -8.50650186e-02 -8.43474356e-02 -7.10468026e-02
 -7.04816209e-02 -6.87108069e-02 -6.37387019e-02 -6.28943016e-02
 -5.85875125e-02 -5.80858280e-02 -5.60564666e-02 -4.94512165e-02
 -4.90296134e-02 -4.88748001e-02 -4.45132564e-02 -4.43538630e-02
 -4.40967398e-02 -4.26252005e-02 -4.22766377e-02 -4.20604365e-02
 -3.92637758e-02 -3.83042172e-02 -3.62594597e-02 -3.54494103e-02
 -3.39927225e-02 -3.22665592e-02 -2.88161229e-02 -2.84128299e-02
 -2.83729897e-02 -2.60828247e-02 -2.39400514e-02 -2.32741770e-02
 -2.30260760e-02 -2.20656206e-02 -2.12055933e-02 -2.07070292e-02
 -1.83971892e-02 -1.60854585e-02 -1.11865461e-02 -9.60151851e-03
 -8.56415259e-03 -7.73981651e-03 -5.00728721e-03 -4.55203496e-03
 -3.64030117e-03 -2.47441635e-03 -1.84942937e-03 -5.46357679e-04
  3.97419267e-04  1.80096009e-03  1.82125363e-03  3.06468205e-03
  4.00706690e-03  5.84919655e-03  7.08961715e-03  9.09800058e-03
  1.03393827e-02  1.05297693e-02  1.27093672e-02  1.39506779e-02
  1.81484029e-02  1.85039729e-02  2.05039796e-02  2.23952074e-02
  2.42888276e-02  2.52986602e-02  2.64413677e-02  2.92927366e-02
  3.07691202e-02  3.15242904e-02  3.34991029e-02  3.46064463e-02
  3.55142691e-02  3.75459441e-02  3.92452318e-02  4.03918660e-02
  4.09253172e-02  4.11507475e-02  4.31378714e-02  4.50620786e-02
  4.55038215e-02  5.03371588e-02  5.08658530e-02  5.19539264e-02
  5.29843303e-02  5.41493830e-02  5.70246293e-02  5.93864979e-02
  6.10636622e-02  6.13139407e-02  6.48214776e-02  6.66912225e-02
  6.80486759e-02  8.36861943e-02  8.36933788e-02  8.43881223e-02
  8.57264399e-02  8.61214207e-02  8.69781282e-02  8.76503042e-02
  9.02553172e-02  9.20395944e-02  9.21943250e-02  9.21977971e-02
  9.22061069e-02  9.22336508e-02  9.27073663e-02  9.31882115e-02
  9.60494080e-02  9.63813149e-02  9.71826886e-02  9.75017600e-02
  9.87076957e-02  1.01190009e-01  1.01393881e-01  1.01450635e-01
  1.01495022e-01  1.02324181e-01  1.05597463e-01  1.06451733e-01
  1.07266935e-01  1.07517864e-01  1.16184338e-01  1.16716781e-01
  1.16826367e-01  1.20516159e-01  1.20635318e-01  1.21128695e-01
  1.21694195e-01  1.21795334e-01  1.21834318e-01  1.24970041e-01
  1.25118558e-01  1.25189682e-01  1.27316463e-01  1.28369007e-01
  1.28692825e-01  1.29577824e-01  1.29755724e-01  1.30321635e-01
  1.31631805e-01  1.34477280e-01  1.34986612e-01  1.36280298e-01
  1.37193143e-01  1.37699771e-01  1.37755461e-01  1.37959923e-01
  1.38161964e-01  1.44475733e-01  1.46114891e-01  1.47220300e-01
  1.47758796e-01  1.48081184e-01  1.51093358e-01  1.54781716e-01
  1.55712602e-01  1.55997642e-01  1.58113010e-01  1.59763918e-01
  1.60571794e-01  1.60797117e-01  1.63693205e-01  1.66537112e-01
  1.68539896e-01  1.71726594e-01  1.73918319e-01  1.73964749e-01
  1.74545681e-01  1.77316540e-01  1.78326132e-01  1.83367154e-01
  1.85435065e-01  1.86867145e-01  1.87019805e-01  1.89839919e-01
  1.94422132e-01  1.99536814e-01  2.00489722e-01  2.03530992e-01
  2.04817151e-01  2.05491322e-01  2.05966375e-01  2.06335933e-01
  2.07720454e-01  2.08740852e-01  2.08886326e-01  2.11783441e-01
  2.12010797e-01  2.13235793e-01  2.13572110e-01  2.13855980e-01
  2.14921852e-01  2.17055062e-01  2.18418133e-01  2.20202868e-01
  2.22421645e-01  2.23203353e-01  2.23533429e-01  2.27028625e-01
  2.28473450e-01  2.29210831e-01  2.32129963e-01  2.32626870e-01
  2.34638819e-01  2.35020610e-01  2.35106903e-01  2.35314619e-01
  2.37640732e-01  2.38041173e-01  2.39601716e-01  2.39677194e-01
  2.41551143e-01  2.41884232e-01  2.42746266e-01  2.43491566e-01
  2.45788293e-01  2.46428161e-01  2.46551139e-01  2.46721802e-01
  2.47415341e-01  2.48045752e-01  2.48844830e-01  2.48992325e-01
  2.49067212e-01  2.51036084e-01  2.51925249e-01  2.52376578e-01
  2.54552439e-01  2.56807930e-01  2.58692842e-01  2.58757014e-01
  2.59946085e-01  2.61976636e-01  2.62379740e-01  2.62706582e-01
  2.62861226e-01  2.63108193e-01  2.63804758e-01  2.63823336e-01
  2.64367743e-01  2.66615005e-01  2.66951226e-01  2.70205988e-01
  2.71447682e-01  2.71783207e-01  2.72010905e-01  2.72827817e-01
  2.72907220e-01  2.74952794e-01  2.75491700e-01  2.79469991e-01
  2.81996056e-01  2.82294479e-01  2.83853631e-01  2.86007186e-01
  2.86289887e-01  2.89077274e-01  2.95115315e-01  2.99569027e-01
  3.01769414e-01  3.09999139e-01  3.12326758e-01  3.12736518e-01
  3.13670556e-01  3.14759692e-01  3.15406198e-01  3.15956362e-01
  3.17854405e-01  3.18409436e-01  3.19196082e-01  3.19772226e-01
  3.23459591e-01  3.26347833e-01  3.26486051e-01  3.29380107e-01
  3.30493962e-01  3.30914543e-01  3.31556717e-01  3.32286323e-01
  3.32563149e-01  3.33160609e-01  3.33593845e-01  3.34459624e-01
  3.37699055e-01  3.37843768e-01  3.39145169e-01  3.40315803e-01
  3.42680218e-01  3.46397073e-01  3.51344889e-01  3.56294430e-01
  3.58882193e-01  3.61634306e-01  3.63340967e-01  3.63956521e-01
  3.64008378e-01  3.65432756e-01  3.65883448e-01  3.67102814e-01
  3.69351246e-01  3.71370904e-01  3.73254827e-01  3.74506060e-01
  3.74676228e-01  3.75517968e-01  3.76805650e-01  3.78060316e-01
  3.81806214e-01  3.84192011e-01  3.86478058e-01  3.86995876e-01
  3.89424358e-01  3.92010317e-01  3.93962002e-01  3.95289046e-01
  3.96091013e-01  3.96895918e-01  3.97283908e-01  4.02794716e-01
  4.05305972e-01  4.10243379e-01  4.10622452e-01  4.12596019e-01
  4.20545426e-01  4.20863573e-01  4.22774009e-01  4.23131319e-01
  4.24839658e-01  4.26670200e-01  4.32282090e-01  4.32714529e-01
  4.32829690e-01  4.33279559e-01  4.34204057e-01  4.36694750e-01
  4.39460093e-01  4.39680338e-01  4.48311560e-01  4.49166113e-01
  4.49722148e-01  4.50418509e-01  4.51015568e-01  4.51565791e-01
  4.53510080e-01  4.57773749e-01  4.58220552e-01  4.59966294e-01
  4.60802966e-01  4.61139132e-01  4.61291091e-01  4.61805462e-01
  4.62594966e-01  4.65824252e-01  4.67167090e-01  4.67296781e-01
  4.67518541e-01  4.68380396e-01  4.69058356e-01  4.69131342e-01
  4.69619104e-01  4.69663813e-01  4.70717059e-01  4.73423934e-01
  4.74338783e-01  4.76293653e-01  4.77634827e-01  4.78615950e-01
  4.78647934e-01  4.78856741e-01  4.82089552e-01  4.82495921e-01
  4.86557177e-01  4.86757866e-01  4.88552614e-01  4.88752072e-01
  4.91410939e-01  4.91918905e-01  4.92848246e-01  4.96948599e-01
  4.97674764e-01  4.98651134e-01  5.01183617e-01  5.01354555e-01
  5.02641144e-01  5.03203411e-01  5.07567064e-01  5.08724173e-01
  5.09267201e-01  5.10605132e-01  5.11336496e-01  5.11890025e-01
  5.18909118e-01  5.20727852e-01  5.21601739e-01  5.23036185e-01
  5.28846937e-01  5.31834730e-01  5.33438523e-01  5.34981273e-01
  5.36705237e-01  5.38826779e-01  5.39623551e-01  5.39820240e-01
  5.46043864e-01  5.47859601e-01  5.53252308e-01  5.54452252e-01
  5.64841343e-01  5.65029514e-01  5.67129649e-01  5.68497847e-01
  5.69127893e-01  5.69420600e-01  5.70882558e-01  5.76068122e-01
  5.76512105e-01  5.77408426e-01  5.79633590e-01  5.83147773e-01
  5.85003364e-01  5.88695094e-01  5.88879332e-01  5.90208191e-01
  5.92126964e-01  5.92946725e-01  5.94383573e-01  6.05977211e-01
  6.07875684e-01  6.10498520e-01  6.11088281e-01  6.12979036e-01
  6.14828938e-01  6.15578813e-01  6.15786892e-01  6.16536838e-01
  6.25331307e-01  6.26666691e-01  6.30485604e-01  6.30858969e-01
  6.31475245e-01  6.32517642e-01  6.36848552e-01  6.37108377e-01
  6.41339326e-01  6.42807978e-01  6.45803629e-01  6.47796298e-01
  6.49530005e-01  6.51708312e-01  6.56994861e-01  6.57718430e-01
  6.58356301e-01  6.59537916e-01  6.63803527e-01  6.65764377e-01
  6.76686596e-01  6.82522271e-01  6.86354446e-01  6.89426665e-01
  6.89473322e-01  6.90933830e-01  6.97097964e-01  7.00935149e-01
  7.11969663e-01  7.15067857e-01  7.19851108e-01  7.20450391e-01
  7.35815757e-01]

  warnings.warn(

2022-11-03 10:49:00,624:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:49:00,704:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:49:02,787:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:49:02,788:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:49:07,021:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.13808844e-01 -2.09173753e-01 -2.07443349e-01 -1.90821735e-01
 -1.86030049e-01 -1.71647435e-01 -1.68266034e-01 -1.51747971e-01
 -1.51216949e-01 -1.51137418e-01 -1.49946208e-01 -1.49577891e-01
 -1.39052724e-01 -1.32655682e-01 -1.32163734e-01 -1.31465927e-01
 -1.28257956e-01 -1.27042841e-01 -1.26413752e-01 -1.25099816e-01
 -1.24888447e-01 -1.24662051e-01 -1.23411706e-01 -1.21309139e-01
 -1.20499269e-01 -1.18221138e-01 -1.06674896e-01 -1.05730834e-01
 -1.01341223e-01 -9.91349154e-02 -9.71267462e-02 -9.55788703e-02
 -9.35183332e-02 -9.26751554e-02 -9.25446485e-02 -9.21121974e-02
 -9.10499746e-02 -8.91215239e-02 -8.37524877e-02 -7.67716699e-02
 -7.62168318e-02 -7.38828992e-02 -7.28941467e-02 -7.08876713e-02
 -7.07886706e-02 -6.48864923e-02 -6.00769827e-02 -5.93176616e-02
 -5.84790659e-02 -5.73694146e-02 -5.41268509e-02 -5.28297489e-02
 -4.98184211e-02 -4.88872160e-02 -4.70962935e-02 -4.70477157e-02
 -4.42225112e-02 -4.30382081e-02 -4.27737501e-02 -4.06245406e-02
 -3.98367651e-02 -3.90618496e-02 -3.76747594e-02 -3.53186775e-02
 -3.40138254e-02 -3.37692003e-02 -2.97115328e-02 -2.76807581e-02
 -2.71841268e-02 -2.26024178e-02 -2.14616189e-02 -2.03326902e-02
 -1.89302134e-02 -1.82186680e-02 -1.58550346e-02 -1.51932204e-02
 -1.42069042e-02 -1.24354957e-02 -1.04429323e-02 -9.84225601e-03
 -9.47102271e-03 -5.57915398e-03 -4.06564158e-03 -4.06219675e-03
 -1.71271546e-03 -1.35901474e-03 -1.64804897e-04  1.13630779e-03
  2.81053167e-03  2.98279770e-03  5.34608793e-03  6.23609957e-03
  6.99276014e-03  8.66732583e-03  9.12070687e-03  1.55780243e-02
  2.04126261e-02  2.20247119e-02  2.26399563e-02  2.38025114e-02
  2.42864367e-02  2.98602848e-02  2.99523028e-02  3.21878503e-02
  3.23974064e-02  3.34262185e-02  3.52450004e-02  3.64047101e-02
  4.08066266e-02  4.34728324e-02  4.76591241e-02  4.85309113e-02
  4.91113118e-02  4.95568937e-02  5.32121522e-02  5.60030061e-02
  5.86330113e-02  5.95480323e-02  6.01138896e-02  6.11387418e-02
  6.28092944e-02  6.44947334e-02  6.49225071e-02  6.54623712e-02
  6.62700548e-02  6.84301360e-02  6.85575174e-02  7.02576278e-02
  7.14701277e-02  7.20266155e-02  7.37930795e-02  7.47211379e-02
  7.49301379e-02  7.54888294e-02  8.03495252e-02  8.41906933e-02
  8.52403699e-02  8.54353497e-02  8.71562391e-02  8.72230915e-02
  8.72522669e-02  8.77712202e-02  8.79049848e-02  9.19845458e-02
  9.26359475e-02  9.35275423e-02  9.55800594e-02  9.61981687e-02
  9.80993610e-02  1.01326888e-01  1.01379761e-01  1.02150401e-01
  1.03862949e-01  1.05245777e-01  1.09135275e-01  1.11602020e-01
  1.12327002e-01  1.12375769e-01  1.12690676e-01  1.13478792e-01
  1.13647890e-01  1.16199187e-01  1.16646952e-01  1.21482893e-01
  1.21566378e-01  1.22385368e-01  1.22586609e-01  1.22908380e-01
  1.23641196e-01  1.25467003e-01  1.25939344e-01  1.26463328e-01
  1.28662109e-01  1.31436952e-01  1.31971491e-01  1.33557354e-01
  1.35755289e-01  1.39277032e-01  1.39380372e-01  1.40015679e-01
  1.40442969e-01  1.41247015e-01  1.41321471e-01  1.42406469e-01
  1.42553363e-01  1.45331022e-01  1.48157813e-01  1.53653170e-01
  1.53914943e-01  1.54648976e-01  1.55411547e-01  1.55493257e-01
  1.56873935e-01  1.58485337e-01  1.59468259e-01  1.61962540e-01
  1.63196848e-01  1.64283153e-01  1.64836651e-01  1.65822618e-01
  1.65828972e-01  1.67998539e-01  1.70842505e-01  1.73517748e-01
  1.75996485e-01  1.76553807e-01  1.78440602e-01  1.78522057e-01
  1.81072047e-01  1.81362642e-01  1.83209427e-01  1.84015134e-01
  1.84492719e-01  1.89159068e-01  1.89449419e-01  1.91881875e-01
  1.93597891e-01  1.94607487e-01  1.95508371e-01  1.96911187e-01
  2.01110297e-01  2.02157095e-01  2.02283618e-01  2.02754127e-01
  2.02967545e-01  2.03276532e-01  2.03431101e-01  2.04808617e-01
  2.04944351e-01  2.10152747e-01  2.13086500e-01  2.14789513e-01
  2.20916272e-01  2.24292164e-01  2.25903939e-01  2.26346976e-01
  2.26590945e-01  2.30847592e-01  2.31407937e-01  2.32745969e-01
  2.32872781e-01  2.33180475e-01  2.34352402e-01  2.34510388e-01
  2.36650533e-01  2.38437111e-01  2.38814057e-01  2.40450276e-01
  2.43376916e-01  2.44331706e-01  2.46421728e-01  2.48293216e-01
  2.49406172e-01  2.50916534e-01  2.54196815e-01  2.59047286e-01
  2.59170262e-01  2.63747523e-01  2.64566535e-01  2.65438429e-01
  2.65456883e-01  2.67219232e-01  2.71140403e-01  2.73352796e-01
  2.80369657e-01  2.81218396e-01  2.81890963e-01  2.82532060e-01
  2.85913129e-01  2.87387377e-01  2.89071082e-01  2.92742157e-01
  2.93385913e-01  2.94401844e-01  2.95786774e-01  2.96022781e-01
  2.96373416e-01  2.96708133e-01  2.97156914e-01  2.97603129e-01
  2.98665260e-01  2.98782636e-01  2.99539381e-01  3.01233927e-01
  3.01786419e-01  3.02490253e-01  3.02665091e-01  3.06900083e-01
  3.07321522e-01  3.09579416e-01  3.12008884e-01  3.14856474e-01
  3.20370569e-01  3.23071474e-01  3.26139729e-01  3.26681600e-01
  3.27542361e-01  3.27734089e-01  3.28185879e-01  3.28211300e-01
  3.28496129e-01  3.29135310e-01  3.29726159e-01  3.30192559e-01
  3.33955802e-01  3.39569255e-01  3.40340182e-01  3.41220302e-01
  3.42363645e-01  3.45405152e-01  3.47015863e-01  3.52102259e-01
  3.53883476e-01  3.54112185e-01  3.55651029e-01  3.56076815e-01
  3.59260748e-01  3.60720008e-01  3.60878626e-01  3.62679372e-01
  3.62865799e-01  3.64790794e-01  3.65054422e-01  3.67225841e-01
  3.67709328e-01  3.68547060e-01  3.70889601e-01  3.71235160e-01
  3.72822835e-01  3.72948267e-01  3.78279150e-01  3.81116836e-01
  3.83182599e-01  3.86533218e-01  3.92319014e-01  3.93500952e-01
  3.95810935e-01  3.96484314e-01  3.97800334e-01  3.99712586e-01
  4.00830665e-01  4.01555282e-01  4.02176136e-01  4.03412045e-01
  4.03560211e-01  4.05581825e-01  4.05645234e-01  4.05886903e-01
  4.08174661e-01  4.09063228e-01  4.11614884e-01  4.15675585e-01
  4.16746175e-01  4.16877740e-01  4.18935483e-01  4.19832094e-01
  4.21344981e-01  4.22765367e-01  4.28365472e-01  4.28468383e-01
  4.29070004e-01  4.30957046e-01  4.34426134e-01  4.34801624e-01
  4.36953329e-01  4.37502610e-01  4.38630548e-01  4.38886638e-01
  4.40077151e-01  4.40494529e-01  4.47110453e-01  4.47782274e-01
  4.49100743e-01  4.52154928e-01  4.52976624e-01  4.53226038e-01
  4.55844351e-01  4.55900398e-01  4.58553743e-01  4.59575991e-01
  4.60726428e-01  4.63824396e-01  4.64081750e-01  4.67481095e-01
  4.74954071e-01  4.79180975e-01  4.81097062e-01  4.82182753e-01
  4.82436848e-01  4.83125116e-01  4.84771358e-01  4.87865412e-01
  4.88593489e-01  4.88945050e-01  4.90300419e-01  4.92203269e-01
  4.96293917e-01  4.96711286e-01  4.98616414e-01  4.99202896e-01
  5.00534115e-01  5.01877557e-01  5.03415304e-01  5.07635951e-01
  5.08117992e-01  5.09927175e-01  5.10277890e-01  5.14009217e-01
  5.14856356e-01  5.15529418e-01  5.17128279e-01  5.18926774e-01
  5.21676836e-01  5.23349949e-01  5.24210273e-01  5.24719021e-01
  5.27072811e-01  5.29270402e-01  5.29454778e-01  5.29555710e-01
  5.29986320e-01  5.30952820e-01  5.31748171e-01  5.32716472e-01
  5.33411574e-01  5.33418829e-01  5.35428162e-01  5.41209878e-01
  5.41835609e-01  5.47198486e-01  5.48030342e-01  5.51476415e-01
  5.52107598e-01  5.54393060e-01  5.54561878e-01  5.59259888e-01
  5.64725473e-01  5.64895775e-01  5.74208127e-01  5.74941720e-01
  5.76497442e-01  5.77721314e-01  5.86299935e-01  5.87372260e-01
  5.88078149e-01  5.89483849e-01  5.92350769e-01  5.92693255e-01
  5.93175017e-01  5.94906585e-01  5.97409262e-01  6.02261132e-01
  6.02651612e-01  6.02988262e-01  6.03190021e-01  6.14590807e-01
  6.18052881e-01  6.19551354e-01  6.19935930e-01  6.20634446e-01
  6.22975487e-01  6.25623671e-01  6.26173328e-01  6.26444585e-01
  6.27638656e-01  6.28177573e-01  6.28229711e-01  6.30938548e-01
  6.32527958e-01  6.37082199e-01  6.41447573e-01  6.42862080e-01
  6.50009861e-01  6.51269545e-01  6.51723063e-01  6.58075050e-01
  6.60717942e-01  6.63822374e-01  6.67209940e-01  6.80513386e-01
  6.87751210e-01  6.88374859e-01  6.94661092e-01  7.02351090e-01
  7.06838203e-01  7.11749803e-01  7.13185848e-01  7.13612622e-01
  7.16448531e-01  7.20580983e-01  7.21338357e-01  7.27856171e-01
  7.60407830e-01]

  warnings.warn(

2022-11-03 10:49:07,021:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16455598 -0.15778297 -0.15584259 -0.15453334 -0.14849395 -0.14315033
 -0.13549726 -0.13391493 -0.13092513 -0.12850628 -0.1265135  -0.12535711
 -0.12224866 -0.1193439  -0.11881186 -0.11873383 -0.11732448 -0.11569529
 -0.11314379 -0.10904988 -0.10743556 -0.1025979  -0.10251461 -0.10217005
 -0.10074024 -0.09667925 -0.09355669 -0.09277183 -0.09155345 -0.08912912
 -0.08654163 -0.08557394 -0.08167376 -0.07992157 -0.07899935 -0.0783477
 -0.0777423  -0.07165881 -0.07141679 -0.06979461 -0.0695772  -0.0670305
 -0.06532054 -0.06284141 -0.0558495  -0.05387225 -0.05320725 -0.05211233
 -0.05154854 -0.05042014 -0.05007438 -0.04802195 -0.042985   -0.03848683
 -0.0358297  -0.03378194 -0.03255749 -0.03067341 -0.02655675 -0.02367425
 -0.02304337 -0.0213852  -0.01833675 -0.01681467 -0.01595889 -0.0131883
 -0.01084359 -0.0107814  -0.00998209 -0.00943123 -0.00814051 -0.00666967
 -0.00637135 -0.00531709 -0.0050711  -0.00162262  0.004776    0.0064463
  0.00650589  0.00685774  0.00777641  0.00866011  0.01048657  0.01187997
  0.01264618  0.01423359  0.0153113   0.01689186  0.0182379   0.02046429
  0.0235247   0.03014092  0.03029067  0.03151567  0.03313989  0.03670066
  0.0375063   0.03884322  0.0397251   0.04036364  0.04624663  0.04719934
  0.05010679  0.05212244  0.05318683  0.0566189   0.05671034  0.05717231
  0.06067153  0.06337224  0.06412842  0.06708053  0.06746839  0.06899463
  0.07071534  0.07401867  0.07404729  0.07431167  0.07577905  0.07618448
  0.07784181  0.08010258  0.08220794  0.08267931  0.0829428   0.08414403
  0.08452838  0.08579855  0.0861617   0.08628654  0.08669903  0.08700396
  0.08898464  0.09011807  0.09093103  0.09296087  0.09354578  0.09473868
  0.09610742  0.09821659  0.10454076  0.10487273  0.10515482  0.1080247
  0.10946941  0.10977978  0.11027147  0.11407098  0.11588676  0.12051609
  0.12083601  0.12265716  0.12311345  0.12354525  0.12659852  0.12762
  0.13059643  0.13081086  0.13606004  0.13613556  0.13801754  0.13906093
  0.14126262  0.14356596  0.1444361   0.14672869  0.14720568  0.14817836
  0.14892235  0.15017333  0.15639726  0.1565849   0.15680308  0.15692252
  0.1570772   0.15899642  0.16359925  0.16369203  0.16557286  0.16652421
  0.16735154  0.17359178  0.17737772  0.17785204  0.17962142  0.18054704
  0.18138746  0.18235075  0.18256157  0.18339173  0.18526015  0.18661709
  0.18867369  0.18888297  0.18933407  0.19027121  0.19096162  0.19117123
  0.19145938  0.19324049  0.19490223  0.19502955  0.19883532  0.202083
  0.2030645   0.20404851  0.20568153  0.20629976  0.20686797  0.20692162
  0.2084808   0.20917771  0.20947736  0.20976179  0.20988336  0.21113262
  0.21179101  0.21248435  0.21323655  0.21371093  0.21531245  0.21599969
  0.21614883  0.21654662  0.21854467  0.21930623  0.21993208  0.22073415
  0.22156349  0.22274246  0.22278041  0.22720704  0.22830442  0.2332163
  0.23489719  0.23793795  0.23798222  0.23810334  0.23813025  0.23919514
  0.24121704  0.24121897  0.24382049  0.24426308  0.24438422  0.24579294
  0.24950079  0.25058249  0.25127984  0.25391784  0.25689539  0.25752722
  0.25919776  0.25975673  0.26054113  0.26199368  0.2627892   0.266113
  0.267596    0.2677555   0.26855543  0.26994063  0.27149926  0.2749072
  0.27572721  0.27668083  0.27723246  0.27891908  0.2795171   0.2852167
  0.28718837  0.28816986  0.288233    0.28957725  0.2926715   0.29343022
  0.29858306  0.29933116  0.30096634  0.30673595  0.30748607  0.30839589
  0.30881805  0.31003286  0.31204793  0.31354719  0.31468601  0.31478193
  0.31534462  0.31630826  0.31669164  0.31741234  0.32035988  0.32275089
  0.32295846  0.32296774  0.32610306  0.32690321  0.33016479  0.33063453
  0.3365534   0.33711034  0.33758261  0.3398276   0.3404029   0.34175345
  0.34317305  0.34431955  0.34441871  0.34906417  0.34943426  0.35069074
  0.35178318  0.35386561  0.35507485  0.35520169  0.36000921  0.36127103
  0.3634267   0.36498604  0.36632394  0.36904074  0.37077548  0.37127057
  0.37234057  0.37614403  0.37671164  0.37869298  0.38032491  0.38262657
  0.3832484   0.38357193  0.38882382  0.39017232  0.39029141  0.39819147
  0.40211042  0.40405046  0.40446996  0.40537587  0.40637906  0.40858866
  0.40871353  0.41061641  0.41317601  0.41370397  0.41378037  0.414047
  0.41418434  0.41476985  0.41503963  0.41568815  0.41599707  0.417041
  0.4223422   0.42328665  0.42580445  0.42595621  0.42666557  0.43313382
  0.43768636  0.43909392  0.44396685  0.44651809  0.44972773  0.45021357
  0.4506329   0.45230057  0.4542367   0.45534688  0.45553455  0.45820875
  0.45889525  0.46279305  0.47159525  0.47180697  0.47200251  0.4739427
  0.47546551  0.47595654  0.47685123  0.4769996   0.47979505  0.48010894
  0.48079795  0.48266795  0.48307097  0.4893361   0.4914372   0.49191037
  0.49506786  0.49519648  0.49839749  0.4984186   0.49932992  0.49953322
  0.49953608  0.50192336  0.50234478  0.5031838   0.50570424  0.50571781
  0.50651754  0.51438328  0.51494117  0.51691389  0.52289214  0.52718604
  0.52880228  0.52916953  0.52934278  0.53108128  0.53381469  0.53774012
  0.538041    0.53940862  0.54091367  0.5427898   0.54846781  0.5507628
  0.5509917   0.55229711  0.55420798  0.55969835  0.56250878  0.56257692
  0.56338437  0.56527307  0.5654313   0.56582798  0.5673238   0.56846705
  0.57371191  0.57424081  0.57468846  0.57498406  0.57529896  0.5771116
  0.58227578  0.58331974  0.58434484  0.5845451   0.5862193   0.58790336
  0.58892423  0.59106725  0.59125056  0.59128576  0.59148063  0.59328897
  0.59769325  0.59789754  0.60243083  0.60455956  0.60617333  0.60658932
  0.60880277  0.6094485   0.60958097  0.6109957   0.61345492  0.6214292
  0.62197663  0.62342066  0.6243093   0.62498979  0.62772984  0.62935509
  0.6302732   0.64229785  0.64618179  0.64751414  0.64967769  0.65241776
  0.6552164   0.65620149  0.65638687  0.66185991  0.66244199  0.66724164
  0.66881012  0.67133324  0.67631782  0.67723411  0.68396019  0.6849062
  0.69769604  0.69861703  0.69937474  0.70666365  0.72730881  0.73101792
  0.73268752]

  warnings.warn(

2022-11-03 10:49:07,021:INFO:Calculating mean and std
2022-11-03 10:49:07,037:INFO:Creating metrics dataframe
2022-11-03 10:49:07,037:INFO:Uploading results into container
2022-11-03 10:49:07,037:INFO:Uploading model into container now
2022-11-03 10:49:07,037:INFO:master_model_container: 1
2022-11-03 10:49:07,037:INFO:display_container: 2
2022-11-03 10:49:07,037:INFO:LinearRegression(n_jobs=-1)
2022-11-03 10:49:07,037:INFO:create_model() successfully completed......................................
2022-11-03 10:49:07,329:WARNING:create_model() for LinearRegression(n_jobs=-1) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:49:07,345:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:49:07,345:INFO:Initializing create_model()
2022-11-03 10:49:07,345:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:49:07,345:INFO:Checking exceptions
2022-11-03 10:49:07,345:INFO:Importing libraries
2022-11-03 10:49:07,345:INFO:Copying training dataset
2022-11-03 10:49:07,361:INFO:Defining folds
2022-11-03 10:49:07,361:INFO:Declaring metric variables
2022-11-03 10:49:07,361:INFO:Importing untrained model
2022-11-03 10:49:07,361:INFO:Linear Regression Imported successfully
2022-11-03 10:49:07,361:INFO:Starting cross validation
2022-11-03 10:49:07,361:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:49:13,188:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.87642799e-01 -1.62929345e-01 -1.60162427e-01 -1.58967038e-01
 -1.49996106e-01 -1.48520718e-01 -1.45839637e-01 -1.44535895e-01
 -1.36062380e-01 -1.35368127e-01 -1.31472286e-01 -1.29789551e-01
 -1.26857581e-01 -1.26821190e-01 -1.25075438e-01 -1.23499152e-01
 -1.22744739e-01 -1.20360530e-01 -1.18281998e-01 -1.17041329e-01
 -1.16388591e-01 -1.05400292e-01 -1.05175092e-01 -1.04566606e-01
 -1.02554677e-01 -1.01982367e-01 -9.95851288e-02 -9.94075324e-02
 -9.39880865e-02 -9.38074635e-02 -8.87095177e-02 -8.74564878e-02
 -8.26639326e-02 -8.24956336e-02 -8.12749238e-02 -7.85131408e-02
 -7.29077810e-02 -7.28610671e-02 -7.07008035e-02 -6.50091643e-02
 -6.25510481e-02 -6.17937901e-02 -5.97256383e-02 -5.90432096e-02
 -5.66585347e-02 -5.44045935e-02 -5.00019052e-02 -4.79105706e-02
 -4.68107225e-02 -4.45706298e-02 -4.30460656e-02 -4.22403918e-02
 -4.14605625e-02 -4.12665629e-02 -3.40479921e-02 -3.21608961e-02
 -2.93881989e-02 -2.49552349e-02 -2.36141441e-02 -2.30354425e-02
 -2.25116879e-02 -2.16528656e-02 -2.04574368e-02 -1.89296222e-02
 -1.72249146e-02 -1.50459201e-02 -1.48453113e-02 -1.39537737e-02
 -1.39176199e-02 -1.30167713e-02 -1.00186534e-02 -9.91533735e-03
 -6.50311709e-03 -6.18656152e-03 -6.04569513e-03 -5.59021719e-03
 -3.34309443e-03 -1.93572124e-04  1.92909981e-03  2.00665242e-03
  4.39905735e-03  6.29112528e-03  7.87904395e-03  8.11402176e-03
  9.22605776e-03  1.19062152e-02  1.24869587e-02  1.42590691e-02
  1.46193136e-02  1.55391651e-02  1.72981722e-02  1.93099944e-02
  2.14215947e-02  2.14729578e-02  2.24912537e-02  2.25961562e-02
  2.37122757e-02  2.44030916e-02  2.73589284e-02  2.87383692e-02
  3.22678447e-02  3.41588734e-02  3.47856752e-02  3.63089886e-02
  3.69345538e-02  3.80267976e-02  3.89539777e-02  4.02885408e-02
  4.26622098e-02  4.27317133e-02  4.42641729e-02  4.69037099e-02
  4.72883379e-02  4.74978040e-02  5.12560360e-02  5.19134922e-02
  5.27478539e-02  5.62950270e-02  5.64154038e-02  5.66689762e-02
  6.18457680e-02  6.22441594e-02  6.35577000e-02  6.73780637e-02
  7.08251965e-02  7.34343383e-02  7.35994039e-02  7.44864632e-02
  7.53012800e-02  7.88370595e-02  7.97405312e-02  8.25498576e-02
  8.27308971e-02  8.34556842e-02  8.34683224e-02  8.37209191e-02
  8.39040067e-02  8.44168425e-02  8.54589310e-02  8.62568184e-02
  9.21006012e-02  9.26420178e-02  9.80412298e-02  1.00049970e-01
  1.02768318e-01  1.04128562e-01  1.07319535e-01  1.09636651e-01
  1.09791272e-01  1.10610426e-01  1.10710670e-01  1.11126803e-01
  1.12161062e-01  1.15188573e-01  1.15492382e-01  1.16160219e-01
  1.18068923e-01  1.18096463e-01  1.18269632e-01  1.25523558e-01
  1.26145253e-01  1.29659764e-01  1.29785154e-01  1.31800847e-01
  1.33283851e-01  1.33783746e-01  1.41728349e-01  1.41858556e-01
  1.41987128e-01  1.42223266e-01  1.42877329e-01  1.44068371e-01
  1.45688391e-01  1.46052902e-01  1.46912359e-01  1.47082540e-01
  1.47170099e-01  1.50259122e-01  1.50956896e-01  1.51515407e-01
  1.52306230e-01  1.60070503e-01  1.60454134e-01  1.61536229e-01
  1.61950558e-01  1.64058083e-01  1.71369373e-01  1.72137094e-01
  1.72616002e-01  1.73361381e-01  1.73511826e-01  1.77369323e-01
  1.79201884e-01  1.82698729e-01  1.84041196e-01  1.88501020e-01
  1.89683639e-01  1.90465342e-01  1.91924466e-01  1.93211061e-01
  2.00568520e-01  2.03230297e-01  2.07605805e-01  2.08744784e-01
  2.09935556e-01  2.10091291e-01  2.11171732e-01  2.11870033e-01
  2.12353085e-01  2.14096703e-01  2.15464351e-01  2.16932966e-01
  2.18397816e-01  2.19580589e-01  2.19643224e-01  2.22603013e-01
  2.23262883e-01  2.26310869e-01  2.26601516e-01  2.26925592e-01
  2.27342885e-01  2.27554011e-01  2.29362886e-01  2.32087027e-01
  2.36546587e-01  2.36719738e-01  2.37183399e-01  2.37829328e-01
  2.42092710e-01  2.42283697e-01  2.44767261e-01  2.45238759e-01
  2.45890698e-01  2.46526682e-01  2.47457106e-01  2.54214899e-01
  2.57533783e-01  2.59262868e-01  2.59607796e-01  2.62988227e-01
  2.64221802e-01  2.64624672e-01  2.65488076e-01  2.65546625e-01
  2.67565007e-01  2.69560333e-01  2.73780238e-01  2.74356860e-01
  2.75554279e-01  2.76456389e-01  2.78960145e-01  2.79630862e-01
  2.80520280e-01  2.81188552e-01  2.81545550e-01  2.82200503e-01
  2.83567035e-01  2.84156059e-01  2.85689221e-01  2.86515096e-01
  2.91560497e-01  2.93971208e-01  2.96658162e-01  2.97045073e-01
  2.97415723e-01  2.97490849e-01  2.99586637e-01  3.02330095e-01
  3.03442333e-01  3.03667284e-01  3.06440238e-01  3.07228292e-01
  3.07415878e-01  3.09502868e-01  3.10450018e-01  3.10735826e-01
  3.14441565e-01  3.14904611e-01  3.15264740e-01  3.15378786e-01
  3.17639072e-01  3.21915960e-01  3.27243627e-01  3.27494099e-01
  3.27540728e-01  3.28617786e-01  3.29039621e-01  3.35243623e-01
  3.35611300e-01  3.35895248e-01  3.36970781e-01  3.38370223e-01
  3.41095153e-01  3.44838909e-01  3.46597488e-01  3.46845148e-01
  3.48949972e-01  3.49897400e-01  3.53388354e-01  3.53441426e-01
  3.59849772e-01  3.61070057e-01  3.62685074e-01  3.63329742e-01
  3.64969418e-01  3.66501593e-01  3.67735752e-01  3.68671071e-01
  3.68966040e-01  3.69970343e-01  3.71643888e-01  3.72578511e-01
  3.73822829e-01  3.75307688e-01  3.75681096e-01  3.76174231e-01
  3.76960013e-01  3.78681559e-01  3.79257046e-01  3.79891856e-01
  3.80488855e-01  3.80703880e-01  3.81479574e-01  3.87507114e-01
  3.87799133e-01  3.93145166e-01  3.93427179e-01  3.97857742e-01
  3.97952038e-01  3.98301441e-01  3.99426993e-01  3.99927728e-01
  4.00684371e-01  4.02450653e-01  4.02507173e-01  4.03223961e-01
  4.03567165e-01  4.05650452e-01  4.05750299e-01  4.05799645e-01
  4.06219971e-01  4.09421126e-01  4.09826486e-01  4.15508987e-01
  4.16892513e-01  4.16935117e-01  4.17495288e-01  4.18113241e-01
  4.18124656e-01  4.21524030e-01  4.28639113e-01  4.28863134e-01
  4.29919530e-01  4.33173972e-01  4.34287829e-01  4.36173773e-01
  4.38228384e-01  4.38263755e-01  4.38896454e-01  4.39478441e-01
  4.42510976e-01  4.46305032e-01  4.48621055e-01  4.51334448e-01
  4.51905507e-01  4.52684946e-01  4.54244173e-01  4.54372180e-01
  4.54625192e-01  4.55817252e-01  4.60141339e-01  4.65424431e-01
  4.66070109e-01  4.66312578e-01  4.67019058e-01  4.69648288e-01
  4.70509857e-01  4.72298648e-01  4.72394268e-01  4.73093670e-01
  4.76376316e-01  4.77929320e-01  4.78325143e-01  4.79357807e-01
  4.80371132e-01  4.80398676e-01  4.84585457e-01  4.89363787e-01
  4.90287667e-01  4.90853054e-01  4.91887013e-01  4.95000094e-01
  4.98401697e-01  4.98803715e-01  4.99975751e-01  5.01644459e-01
  5.02348304e-01  5.04147716e-01  5.07815126e-01  5.13104971e-01
  5.14794884e-01  5.15144914e-01  5.15726194e-01  5.16404778e-01
  5.17747453e-01  5.18565883e-01  5.18846024e-01  5.24585273e-01
  5.26414857e-01  5.26903758e-01  5.27293594e-01  5.30208672e-01
  5.32892440e-01  5.33415600e-01  5.33512187e-01  5.36191860e-01
  5.36280787e-01  5.36298006e-01  5.36532269e-01  5.38799937e-01
  5.41979651e-01  5.42336203e-01  5.42980468e-01  5.44971325e-01
  5.45575933e-01  5.50558429e-01  5.53972263e-01  5.56369955e-01
  5.57908059e-01  5.60959009e-01  5.61101686e-01  5.61506294e-01
  5.67428974e-01  5.70362038e-01  5.73129921e-01  5.77001002e-01
  5.78013754e-01  5.78711657e-01  5.78880363e-01  5.90001433e-01
  5.91699194e-01  5.96771265e-01  5.96801756e-01  5.99740817e-01
  6.00948590e-01  6.01157320e-01  6.01719458e-01  6.03685143e-01
  6.07256454e-01  6.07289830e-01  6.09859882e-01  6.10533165e-01
  6.12626069e-01  6.13095228e-01  6.13191443e-01  6.14471269e-01
  6.15279296e-01  6.16920322e-01  6.17933046e-01  6.20416982e-01
  6.20516285e-01  6.21530324e-01  6.22272388e-01  6.25102231e-01
  6.25284184e-01  6.26863613e-01  6.27005415e-01  6.27899985e-01
  6.28172117e-01  6.29494033e-01  6.32837213e-01  6.36623702e-01
  6.39374061e-01  6.42411313e-01  6.52281056e-01  6.60158862e-01
  6.61339925e-01  6.62504958e-01  6.63079132e-01  6.65121129e-01
  6.67425003e-01  6.68150175e-01  6.73829832e-01  6.76979878e-01
  6.84394826e-01  6.86163744e-01  6.88033416e-01  7.07335490e-01
  7.08941587e-01  7.18138843e-01  7.42895870e-01  7.43626792e-01
  7.58767626e-01]

  warnings.warn(

2022-11-03 10:49:13,221:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.83048829e-01 -1.81381742e-01 -1.58083276e-01 -1.50423314e-01
 -1.47290788e-01 -1.46354752e-01 -1.44953723e-01 -1.43663561e-01
 -1.39135457e-01 -1.39121192e-01 -1.38846414e-01 -1.36962409e-01
 -1.36295000e-01 -1.35574397e-01 -1.34490460e-01 -1.32417308e-01
 -1.32181235e-01 -1.31680899e-01 -1.30806367e-01 -1.23025449e-01
 -1.22979860e-01 -1.19803774e-01 -1.16641620e-01 -1.15912482e-01
 -1.12983592e-01 -1.09661321e-01 -1.04346591e-01 -1.02850955e-01
 -1.01394430e-01 -1.01322001e-01 -1.01019470e-01 -9.90403359e-02
 -9.86937054e-02 -9.76875809e-02 -9.35593976e-02 -9.27037889e-02
 -8.97175890e-02 -8.65876122e-02 -8.57566814e-02 -8.21570689e-02
 -8.06957057e-02 -7.59068905e-02 -7.53589118e-02 -7.45425115e-02
 -7.35776245e-02 -7.30700111e-02 -6.74251300e-02 -6.64703563e-02
 -6.55812342e-02 -6.50859608e-02 -6.38233592e-02 -6.32419485e-02
 -6.13002663e-02 -5.55202542e-02 -5.42327303e-02 -5.28575714e-02
 -5.08925434e-02 -4.97711477e-02 -4.84626563e-02 -4.82071409e-02
 -4.80388558e-02 -4.60122762e-02 -4.58537614e-02 -4.55407771e-02
 -4.44548253e-02 -4.38736815e-02 -4.29009634e-02 -4.27723018e-02
 -3.77706389e-02 -3.71000656e-02 -3.68305264e-02 -3.59785006e-02
 -3.59693976e-02 -3.44127290e-02 -3.42522932e-02 -3.18979640e-02
 -3.09344657e-02 -3.03713326e-02 -2.89210928e-02 -2.81037383e-02
 -2.65394298e-02 -2.64132184e-02 -2.31327830e-02 -2.28205878e-02
 -2.23960791e-02 -1.91505964e-02 -1.69828336e-02 -1.49059804e-02
 -5.02951294e-03 -4.53640452e-03 -3.37920074e-03 -3.17077229e-03
 -2.00576923e-03 -4.12962613e-04  3.84644342e-05  6.47029689e-04
  2.02591225e-03  3.55584178e-03  1.04510286e-02  2.00994147e-02
  2.08936594e-02  2.10963536e-02  2.11091828e-02  2.21361531e-02
  2.70294292e-02  2.76347989e-02  2.94859472e-02  3.02914946e-02
  3.12935476e-02  3.13581519e-02  3.24732444e-02  3.43543862e-02
  3.48155540e-02  3.64898033e-02  3.79243168e-02  3.91456549e-02
  4.09129045e-02  4.09775934e-02  4.20045487e-02  4.26542435e-02
  4.73489886e-02  4.84663442e-02  4.91197403e-02  5.24057005e-02
  5.47165033e-02  5.54034856e-02  5.71060289e-02  5.77064599e-02
  5.94476548e-02  5.97530493e-02  6.05337892e-02  6.33792394e-02
  6.61212095e-02  6.65641182e-02  6.69028977e-02  6.83130576e-02
  6.84685586e-02  6.85714852e-02  7.09178601e-02  7.17167882e-02
  7.63956746e-02  7.70585196e-02  7.93785708e-02  8.18951791e-02
  8.27921469e-02  8.40313354e-02  9.14757524e-02  9.17496285e-02
  9.19602769e-02  9.62260920e-02  9.91101952e-02  9.93495933e-02
  1.00828545e-01  1.01775900e-01  1.03608069e-01  1.03651973e-01
  1.05147773e-01  1.05894018e-01  1.08886375e-01  1.12213278e-01
  1.12719848e-01  1.16141000e-01  1.17206507e-01  1.18349666e-01
  1.19100314e-01  1.20058879e-01  1.20614912e-01  1.22719138e-01
  1.25039035e-01  1.26383969e-01  1.26948408e-01  1.27025164e-01
  1.28000158e-01  1.30415989e-01  1.31137466e-01  1.33192228e-01
  1.33408312e-01  1.33507087e-01  1.37176670e-01  1.38294168e-01
  1.40818776e-01  1.41242629e-01  1.42158674e-01  1.43161697e-01
  1.43245271e-01  1.43958963e-01  1.45138818e-01  1.48467920e-01
  1.51345103e-01  1.51512602e-01  1.54130883e-01  1.56234720e-01
  1.58744424e-01  1.58846398e-01  1.60969175e-01  1.61567933e-01
  1.61661569e-01  1.61747228e-01  1.62586310e-01  1.63108814e-01
  1.63538063e-01  1.63552633e-01  1.64043132e-01  1.64446052e-01
  1.65101453e-01  1.66775580e-01  1.66972373e-01  1.70403636e-01
  1.70435043e-01  1.75787594e-01  1.76835768e-01  1.78574042e-01
  1.79221174e-01  1.79703223e-01  1.79922373e-01  1.83370532e-01
  1.84991518e-01  1.89180863e-01  1.89982551e-01  1.90301442e-01
  1.91461686e-01  1.91527375e-01  1.93648357e-01  1.93836648e-01
  1.95871901e-01  1.96793778e-01  1.98576664e-01  1.98660504e-01
  2.00533306e-01  2.02476918e-01  2.04709370e-01  2.07143055e-01
  2.11647216e-01  2.11692583e-01  2.12079570e-01  2.12322905e-01
  2.12856854e-01  2.16898405e-01  2.17165559e-01  2.17697251e-01
  2.18692699e-01  2.18787521e-01  2.20028052e-01  2.22393065e-01
  2.23582531e-01  2.25880592e-01  2.27012645e-01  2.27277640e-01
  2.31205819e-01  2.31622764e-01  2.32726465e-01  2.37413119e-01
  2.41274633e-01  2.41877594e-01  2.41945433e-01  2.44910773e-01
  2.45512359e-01  2.47503643e-01  2.49174932e-01  2.51050436e-01
  2.51709938e-01  2.54775059e-01  2.54969560e-01  2.56615302e-01
  2.58952564e-01  2.59990898e-01  2.60952556e-01  2.61514434e-01
  2.61900818e-01  2.64349536e-01  2.65821207e-01  2.68014074e-01
  2.69325018e-01  2.77179933e-01  2.78069440e-01  2.78763932e-01
  2.79571529e-01  2.80084492e-01  2.82652454e-01  2.83236529e-01
  2.86614532e-01  2.90282417e-01  2.98424887e-01  2.98731174e-01
  2.99723056e-01  3.02705473e-01  3.03957206e-01  3.11260165e-01
  3.11599002e-01  3.13584598e-01  3.14167658e-01  3.15599045e-01
  3.16862258e-01  3.22630922e-01  3.24489622e-01  3.24850151e-01
  3.26619350e-01  3.26946666e-01  3.30851700e-01  3.30876534e-01
  3.32676011e-01  3.33583875e-01  3.37158134e-01  3.41818029e-01
  3.41924677e-01  3.42201580e-01  3.42227665e-01  3.44168420e-01
  3.48985329e-01  3.50466549e-01  3.50741864e-01  3.52372643e-01
  3.52742256e-01  3.55380364e-01  3.56474234e-01  3.59607566e-01
  3.62514562e-01  3.64347463e-01  3.65329354e-01  3.67540601e-01
  3.70273049e-01  3.71416331e-01  3.71434427e-01  3.76381467e-01
  3.79176821e-01  3.80211902e-01  3.81874197e-01  3.82178535e-01
  3.85433937e-01  3.85870952e-01  3.93308997e-01  3.93532140e-01
  3.93736966e-01  3.94538431e-01  3.95973191e-01  3.98033899e-01
  3.98094357e-01  3.98108755e-01  4.01248287e-01  4.03920691e-01
  4.06356776e-01  4.06858637e-01  4.07590118e-01  4.09669724e-01
  4.15119757e-01  4.19512558e-01  4.19618430e-01  4.20966547e-01
  4.26407912e-01  4.28269102e-01  4.28957450e-01  4.30061115e-01
  4.30831156e-01  4.31781771e-01  4.32584197e-01  4.33523502e-01
  4.35232843e-01  4.36262235e-01  4.37711000e-01  4.38328832e-01
  4.39340866e-01  4.40357516e-01  4.40841310e-01  4.42374534e-01
  4.42917506e-01  4.44129054e-01  4.45163818e-01  4.47874273e-01
  4.48026298e-01  4.50864512e-01  4.54480902e-01  4.55526093e-01
  4.56120028e-01  4.56874392e-01  4.57798794e-01  4.62059886e-01
  4.63103449e-01  4.64818529e-01  4.66637968e-01  4.68587963e-01
  4.69344322e-01  4.72129364e-01  4.72696067e-01  4.73231436e-01
  4.74200958e-01  4.74243080e-01  4.74599944e-01  4.75572735e-01
  4.76763100e-01  4.76951190e-01  4.80489528e-01  4.81878219e-01
  4.86524971e-01  4.88722192e-01  4.89144035e-01  4.90136945e-01
  4.90923619e-01  4.96369069e-01  4.98094177e-01  4.99480826e-01
  5.00651560e-01  5.05540388e-01  5.05639573e-01  5.05685233e-01
  5.06098778e-01  5.06239437e-01  5.11154296e-01  5.17055936e-01
  5.21085298e-01  5.23569596e-01  5.24363026e-01  5.25583556e-01
  5.27619435e-01  5.31362577e-01  5.31426996e-01  5.33732739e-01
  5.34136792e-01  5.35656287e-01  5.40202116e-01  5.40753646e-01
  5.43228897e-01  5.51621930e-01  5.53227630e-01  5.53585598e-01
  5.54284892e-01  5.54429767e-01  5.55678074e-01  5.56108209e-01
  5.56838660e-01  5.57015731e-01  5.59792622e-01  5.60404115e-01
  5.60421495e-01  5.60871878e-01  5.64127456e-01  5.66071219e-01
  5.68547211e-01  5.69393878e-01  5.69956533e-01  5.70334329e-01
  5.74668826e-01  5.75559078e-01  5.75630174e-01  5.75714462e-01
  5.76130333e-01  5.76206475e-01  5.77583699e-01  5.79698807e-01
  5.79811865e-01  5.85880168e-01  5.89544830e-01  5.91914875e-01
  5.92437699e-01  6.07017452e-01  6.07918891e-01  6.09256391e-01
  6.10704552e-01  6.12244325e-01  6.15720098e-01  6.16198873e-01
  6.16612515e-01  6.22852063e-01  6.27184999e-01  6.27389391e-01
  6.31008719e-01  6.33559077e-01  6.34504263e-01  6.36979582e-01
  6.38641839e-01  6.40299705e-01  6.47600730e-01  6.48037639e-01
  6.50575548e-01  6.50668424e-01  6.55159508e-01  6.56796814e-01
  6.59114071e-01  6.63057602e-01  6.69459020e-01  6.71326068e-01
  6.78791750e-01  6.83756634e-01  6.87845241e-01  6.92638561e-01
  6.94242623e-01  6.94265206e-01  6.97263983e-01  6.97767292e-01
  7.09144843e-01  7.16116857e-01  7.19600921e-01  7.25472248e-01
  7.34536799e-01]

  warnings.warn(

2022-11-03 10:49:13,221:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.17859909 -0.17137604 -0.16883698 -0.16142804 -0.15823029 -0.15433136
 -0.14520169 -0.14441111 -0.14382384 -0.14100634 -0.14087518 -0.13970847
 -0.13939833 -0.13249522 -0.13246282 -0.13130347 -0.12968202 -0.12649065
 -0.12527451 -0.12306027 -0.11891008 -0.10731138 -0.09906699 -0.09631797
 -0.09589221 -0.09027215 -0.08319542 -0.08295828 -0.08277739 -0.08250432
 -0.08228986 -0.07808097 -0.07650528 -0.07380014 -0.07271407 -0.07262291
 -0.07233932 -0.07211808 -0.06746815 -0.06702841 -0.06594035 -0.06342977
 -0.06150551 -0.05364698 -0.05151769 -0.0501369  -0.04992765 -0.04987424
 -0.04494082 -0.04027569 -0.03818179 -0.03732028 -0.03453102 -0.03381415
 -0.02955203 -0.02864871 -0.02553229 -0.02487619 -0.02268468 -0.02252069
 -0.02037185 -0.01877173 -0.01822087 -0.0182084  -0.01770274 -0.01543315
 -0.01412414 -0.0114035  -0.00778528 -0.00710542 -0.00449067 -0.00204671
  0.00162219  0.00443329  0.00625776  0.00653145  0.00771633  0.00801741
  0.01023331  0.01061632  0.0147856   0.01687977  0.01835336  0.01943637
  0.02416442  0.02781641  0.02928354  0.02986251  0.03016783  0.03094979
  0.03379028  0.0356441   0.03925998  0.03962373  0.03985412  0.0399344
  0.04082751  0.05174638  0.05261206  0.05682378  0.06012914  0.06078503
  0.06159185  0.06564717  0.06577092  0.06666856  0.07102967  0.07256755
  0.07474553  0.07520501  0.07544624  0.0761721   0.07779038  0.08238989
  0.08386085  0.0849662   0.08584834  0.08643958  0.08680819  0.08706165
  0.09031636  0.09085284  0.09183286  0.09228593  0.09255045  0.0950769
  0.09526852  0.09544835  0.09714589  0.09834657  0.10211893  0.10374185
  0.10443462  0.10474312  0.10564904  0.10602678  0.10996826  0.11186988
  0.11197397  0.11202978  0.11359033  0.11478966  0.11549043  0.11689716
  0.11987762  0.12011251  0.120418    0.12100201  0.12263108  0.12330828
  0.12556199  0.12646656  0.12895193  0.13195149  0.13256507  0.13548904
  0.13652015  0.13769934  0.13859216  0.13906358  0.14001971  0.14274009
  0.14311831  0.14466241  0.14576557  0.14609819  0.14691511  0.14735177
  0.15345267  0.15387854  0.15460962  0.15594909  0.15669579  0.15758231
  0.1581733   0.15844873  0.16242785  0.16278127  0.16348791  0.16398071
  0.16564139  0.16606266  0.169004    0.17093509  0.17124833  0.17288807
  0.17335896  0.17355232  0.17437053  0.17473283  0.17526983  0.17527054
  0.17556138  0.17615514  0.17648087  0.17742806  0.17818027  0.17841982
  0.18097947  0.18112012  0.18208847  0.18282333  0.18437182  0.18544378
  0.18639886  0.19001275  0.1968446   0.19817807  0.19827033  0.19854711
  0.1987812   0.20017674  0.20150764  0.20176155  0.2030076   0.20403427
  0.20443419  0.20550196  0.20724564  0.20821553  0.20852778  0.20862954
  0.20890153  0.2092309   0.21137079  0.21314535  0.21396375  0.21419139
  0.2151643   0.21852511  0.21897764  0.22006489  0.22109162  0.22195463
  0.22231064  0.224986    0.22541104  0.22575761  0.2287525   0.23075427
  0.23160618  0.23546496  0.23548466  0.23573909  0.23750414  0.23869479
  0.24245129  0.24427656  0.24890434  0.25108094  0.25133329  0.25228159
  0.25435941  0.25461977  0.25813919  0.25989824  0.26381099  0.2644135
  0.2654833   0.26568439  0.26632215  0.26758752  0.27389795  0.27584366
  0.27982775  0.28013009  0.2802278   0.28081642  0.28565539  0.28809932
  0.28872465  0.28976764  0.29438848  0.29439729  0.2983794   0.298952
  0.30056516  0.30396865  0.30469516  0.30497693  0.30510832  0.3081813
  0.3087887   0.31129456  0.31191352  0.31202232  0.31205255  0.31214926
  0.313166    0.3141328   0.31855322  0.31918478  0.31972045  0.31984586
  0.32020819  0.32424274  0.3243946   0.32924798  0.33138723  0.33226646
  0.33506402  0.34231866  0.34290593  0.344175    0.34521683  0.34669929
  0.34687919  0.34791121  0.34799461  0.35228108  0.35248076  0.35273546
  0.35314949  0.35368953  0.35470757  0.35701237  0.35812474  0.3583853
  0.3596053   0.36101767  0.36270634  0.36839226  0.37608674  0.37659653
  0.37719176  0.37730064  0.37743132  0.38113479  0.38170251  0.38194438
  0.38312869  0.38468645  0.38689591  0.3903517   0.39379965  0.39383144
  0.39476262  0.39510278  0.39548872  0.39557743  0.39954061  0.40436677
  0.40909104  0.40975384  0.41720849  0.41763515  0.41920264  0.41936779
  0.42128908  0.42320586  0.42370236  0.4245139   0.43250952  0.43255183
  0.43431112  0.43611767  0.43903664  0.44193085  0.44444533  0.44481374
  0.44646204  0.44807096  0.44877735  0.45180219  0.45188271  0.45317501
  0.45476071  0.45576707  0.455824    0.45782896  0.45955087  0.45955986
  0.45970005  0.46424165  0.46591635  0.46655379  0.47076474  0.47498066
  0.47525658  0.47712874  0.47722008  0.48031336  0.48348754  0.48348842
  0.48663683  0.48929393  0.49136601  0.4921309   0.49815352  0.49820391
  0.49837914  0.50112939  0.50143085  0.50150219  0.50165706  0.50192396
  0.50549932  0.5062357   0.50665115  0.51135318  0.51287801  0.51404285
  0.51632054  0.51890148  0.51918744  0.52057425  0.52158057  0.52888547
  0.53218272  0.53260741  0.532886    0.5341401   0.53905577  0.5398203
  0.54003297  0.54101899  0.54191458  0.54329125  0.54891503  0.55005171
  0.55057397  0.55115858  0.55142052  0.55145255  0.55158167  0.55173629
  0.55192204  0.55280502  0.55319444  0.5541294   0.55763616  0.55763957
  0.56216761  0.5628237   0.56291041  0.56939642  0.56980925  0.57118016
  0.5862847   0.5877238   0.588692    0.59615516  0.59779372  0.6039677
  0.60609727  0.60803595  0.60876772  0.60899448  0.60904007  0.61031502
  0.61039874  0.61094132  0.61565771  0.61801732  0.62576125  0.62897278
  0.62922915  0.63256296  0.63393807  0.63661445  0.63812846  0.64145848
  0.6450304   0.64578623  0.64631825  0.64980956  0.65322722  0.6575099
  0.65754569  0.6580613   0.66211529  0.66728319  0.66754401  0.66778824
  0.67265571  0.67359233  0.67695794  0.68081029  0.68150292  0.68302453
  0.69239014  0.69457139  0.69503269  0.70163067  0.71779953  0.71909945
  0.72530793  0.7340377   0.75742178  0.76269903  0.76518511  0.76941151
  0.77693273]

  warnings.warn(

2022-11-03 10:49:13,242:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.22759221 -0.18281955 -0.16490057 -0.15916288 -0.15481519 -0.15037475
 -0.13391122 -0.13320558 -0.1327074  -0.13256409 -0.13095249 -0.1273081
 -0.12681571 -0.11797125 -0.11663401 -0.11609342 -0.11249005 -0.11221214
 -0.11213424 -0.10815903 -0.10812711 -0.10807845 -0.10758513 -0.10469526
 -0.0964037  -0.0916023  -0.08400326 -0.08322505 -0.08234236 -0.08132376
 -0.0720365  -0.06928709 -0.06874596 -0.06701821 -0.06437475 -0.06393701
 -0.06095805 -0.06010752 -0.05762167 -0.05535918 -0.05518988 -0.05499943
 -0.05433063 -0.05096152 -0.04819062 -0.04507993 -0.04477371 -0.04432384
 -0.04179743 -0.0401627  -0.03346355 -0.03186098 -0.03166437 -0.03161845
 -0.03058267 -0.02874367 -0.0254831  -0.02315566 -0.0230948  -0.02199682
 -0.02141183 -0.02011469 -0.02000063 -0.01965465 -0.01419046 -0.00459904
 -0.00370943  0.00344583  0.00743552  0.00922258  0.01040018  0.0104983
  0.01109955  0.01173035  0.01186176  0.01589742  0.01788842  0.01920366
  0.0209784   0.0224974   0.02332723  0.024382    0.02537359  0.02565929
  0.02701675  0.02720935  0.03319138  0.03319866  0.03585579  0.03632771
  0.0379261   0.03922978  0.04357103  0.04404904  0.04595345  0.04675752
  0.04693946  0.04733717  0.04830229  0.04927382  0.05036855  0.05162026
  0.05168833  0.0517132   0.05349545  0.05500137  0.05801273  0.059001
  0.06008231  0.06542563  0.0662237   0.06654805  0.06689119  0.06689152
  0.06715118  0.07511242  0.07638561  0.07672291  0.07921667  0.08006163
  0.0804636   0.08244954  0.08311129  0.0844589   0.08631289  0.0864061
  0.0864236   0.08902445  0.09070019  0.09162993  0.09411949  0.09867068
  0.10100462  0.10298721  0.10305137  0.10331588  0.1035254   0.10391193
  0.10476436  0.10572539  0.10633997  0.10669534  0.10720733  0.10776001
  0.11027698  0.11124508  0.11234039  0.11433397  0.11704384  0.11960591
  0.12041164  0.120981    0.12404506  0.12456287  0.12544055  0.12613948
  0.12630957  0.12776268  0.12807984  0.12896698  0.12943014  0.12993136
  0.13685346  0.13805388  0.13995182  0.14010507  0.14119334  0.14462429
  0.14526182  0.14573409  0.14937214  0.14980869  0.15156332  0.15356078
  0.15398515  0.15449947  0.15494677  0.1580319   0.16153808  0.16399483
  0.16748496  0.16863745  0.17026985  0.17176277  0.17322491  0.17497194
  0.17587985  0.17809644  0.17903139  0.17982566  0.18071381  0.18201188
  0.18243059  0.18262188  0.18650394  0.18659539  0.18701319  0.18843688
  0.18873573  0.18893998  0.18909032  0.19056766  0.19591832  0.19707374
  0.1999712   0.20048607  0.20240882  0.20259527  0.20309472  0.20359552
  0.20568186  0.20571297  0.2057855   0.20628684  0.20747989  0.20809745
  0.20832389  0.20833996  0.20874108  0.20937992  0.20957316  0.20975356
  0.21412598  0.21517826  0.21625468  0.21640577  0.21790559  0.22074496
  0.22157666  0.22342703  0.22368264  0.2251784   0.2277293   0.22968501
  0.23295245  0.23347432  0.23499852  0.23538645  0.23539654  0.23582126
  0.23603424  0.23642982  0.23659726  0.23868069  0.23978823  0.24301063
  0.24355835  0.24448087  0.24456466  0.24678786  0.24839935  0.24983298
  0.25031738  0.25078023  0.25172539  0.2555624   0.25571729  0.25607812
  0.25821318  0.26038021  0.26167455  0.26273589  0.2639788   0.26477114
  0.26688956  0.26711272  0.26794053  0.27137969  0.27519268  0.28404647
  0.28613917  0.28725794  0.28843805  0.28940204  0.28990584  0.29073417
  0.29091754  0.2914618   0.29211427  0.29349374  0.29395974  0.29503095
  0.29539312  0.29616104  0.29861094  0.30004213  0.30040813  0.30077668
  0.30216647  0.30245036  0.30297473  0.30372261  0.30444101  0.30465111
  0.30495027  0.30612323  0.30745607  0.30750639  0.30890722  0.30983346
  0.31109311  0.31270066  0.31300694  0.31419388  0.31439174  0.31497002
  0.31939761  0.31993411  0.32317877  0.32413821  0.32471538  0.32759252
  0.32829213  0.33022821  0.33043179  0.33057407  0.33143016  0.33179264
  0.33257427  0.33307936  0.33441494  0.33585127  0.33759265  0.34426155
  0.34484972  0.34530559  0.34995965  0.35152731  0.35165376  0.35177651
  0.35224241  0.35560022  0.35636658  0.35756157  0.36069095  0.36074197
  0.36146014  0.36219325  0.36570567  0.3665477   0.36753312  0.36872521
  0.36970901  0.37142197  0.37416747  0.37427992  0.37692361  0.37709295
  0.37785455  0.38199079  0.38312878  0.38400479  0.3847381   0.38541466
  0.38662125  0.39010744  0.40350929  0.40379373  0.40463114  0.40647878
  0.40855606  0.41048669  0.41094187  0.41545822  0.41792328  0.41871622
  0.42240963  0.42330054  0.42399341  0.42484933  0.43007681  0.43067912
  0.43184134  0.43184597  0.43320167  0.43446911  0.4357637   0.43740654
  0.43767834  0.43769191  0.44033269  0.44194887  0.45035741  0.45098822
  0.45137831  0.45191732  0.45274253  0.45410296  0.46303508  0.46442587
  0.46516541  0.46795817  0.46945319  0.4736979   0.47629589  0.47694102
  0.48043288  0.48055402  0.48098343  0.48103068  0.48169626  0.48461644
  0.48699976  0.48811948  0.49014283  0.49596642  0.50214313  0.50358034
  0.50415208  0.5059043   0.5108498   0.51290831  0.51356367  0.51523019
  0.51537383  0.51634971  0.51821451  0.52017281  0.526025    0.52602647
  0.52897194  0.53028964  0.530424    0.53124961  0.53548438  0.53642586
  0.53971278  0.54121636  0.54300089  0.54558479  0.54776939  0.54787775
  0.54957297  0.55174319  0.55176146  0.55359277  0.55538556  0.56015626
  0.56598114  0.56725437  0.57363187  0.57373614  0.57565645  0.57967081
  0.58276696  0.58669196  0.58790188  0.59037985  0.59214094  0.59299113
  0.59657468  0.60081719  0.60090122  0.60145224  0.60398068  0.60590994
  0.60767836  0.60788558  0.60935213  0.61303197  0.61460339  0.61995438
  0.62228099  0.62256566  0.62413935  0.62679056  0.63050591  0.63286426
  0.64401532  0.64468527  0.64572663  0.64745305  0.65767456  0.66336743
  0.66341367  0.66477482  0.66571625  0.6687756   0.67402886  0.67773434
  0.68360734  0.68472522  0.68601512  0.68642374  0.69594331  0.6964306
  0.70121238  0.71192317  0.71772971  0.71823367  0.72196     0.7238786
  0.73099813]

  warnings.warn(

2022-11-03 10:49:13,294:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.57098522e-01 -1.90276567e-01 -1.84791237e-01 -1.81700163e-01
 -1.73283491e-01 -1.72072705e-01 -1.69474249e-01 -1.67413124e-01
 -1.62080317e-01 -1.60307348e-01 -1.58089089e-01 -1.54173531e-01
 -1.48806832e-01 -1.47473492e-01 -1.47383739e-01 -1.42274144e-01
 -1.34965359e-01 -1.34467337e-01 -1.29753666e-01 -1.29130748e-01
 -1.23284294e-01 -1.19709248e-01 -1.19666193e-01 -1.17022843e-01
 -1.14800802e-01 -1.08273609e-01 -1.06839435e-01 -1.02226160e-01
 -1.02076222e-01 -1.01560206e-01 -9.76948841e-02 -9.27560785e-02
 -9.16132010e-02 -9.04240472e-02 -8.60033288e-02 -8.21041070e-02
 -8.04125499e-02 -7.97534025e-02 -7.91426305e-02 -7.81334926e-02
 -6.97768516e-02 -6.65307697e-02 -6.47949780e-02 -6.20977157e-02
 -6.16713787e-02 -6.12899027e-02 -6.10213848e-02 -6.07583885e-02
 -5.84702709e-02 -5.80947032e-02 -5.76241856e-02 -5.73024145e-02
 -5.45768569e-02 -5.42866421e-02 -5.41226286e-02 -5.27850171e-02
 -5.13149061e-02 -4.96293095e-02 -4.95120057e-02 -4.90559553e-02
 -4.80499291e-02 -4.38216421e-02 -4.19838701e-02 -4.18050138e-02
 -3.68897975e-02 -3.20842058e-02 -3.03875759e-02 -2.94943417e-02
 -2.77258915e-02 -2.68047659e-02 -2.57874167e-02 -2.57256088e-02
 -2.19911237e-02 -2.03458876e-02 -1.73413192e-02 -1.63704047e-02
 -1.48965538e-02 -1.08185202e-02 -1.02286995e-02 -7.66147551e-03
 -7.46692561e-03 -3.19626726e-03 -1.21900889e-03 -3.84875387e-04
  4.68971821e-03  9.76086495e-03  1.23753543e-02  1.47307486e-02
  1.56033762e-02  1.68913557e-02  2.13327076e-02  2.48371499e-02
  2.57660925e-02  2.63663929e-02  2.68190936e-02  2.83016678e-02
  2.88055717e-02  3.22450710e-02  3.40416737e-02  3.43037652e-02
  3.50289859e-02  3.56156049e-02  3.93706224e-02  3.99754504e-02
  4.13935993e-02  4.27806414e-02  4.42238874e-02  4.57581928e-02
  4.65822941e-02  4.72979311e-02  4.73772284e-02  4.81873452e-02
  4.84494751e-02  4.88100143e-02  5.10536402e-02  5.11815542e-02
  5.37074770e-02  5.62526344e-02  5.76972412e-02  5.92101373e-02
  6.13238744e-02  6.14703161e-02  6.21760107e-02  6.24169709e-02
  6.26666406e-02  6.40789061e-02  6.41046598e-02  6.45024969e-02
  6.51649713e-02  6.53828001e-02  6.66338949e-02  6.70466791e-02
  6.95711930e-02  6.97856180e-02  7.01258338e-02  7.15713384e-02
  7.32799637e-02  7.35766151e-02  7.40297267e-02  7.74471223e-02
  7.79642726e-02  7.96424591e-02  8.10578585e-02  8.16240640e-02
  8.26467671e-02  8.39592988e-02  8.53595815e-02  8.60345643e-02
  8.64545644e-02  8.81574691e-02  8.81801691e-02  9.81310549e-02
  9.96679323e-02  9.99084597e-02  1.00662646e-01  1.05057615e-01
  1.10076251e-01  1.12864223e-01  1.19473486e-01  1.20301917e-01
  1.23037878e-01  1.23039701e-01  1.24250932e-01  1.25387531e-01
  1.25885490e-01  1.29722055e-01  1.30121237e-01  1.31071163e-01
  1.33570512e-01  1.35615323e-01  1.37216605e-01  1.37888972e-01
  1.38484390e-01  1.39316522e-01  1.40041507e-01  1.42655312e-01
  1.42995788e-01  1.43308429e-01  1.44416990e-01  1.45095771e-01
  1.45424960e-01  1.48050954e-01  1.48352059e-01  1.51404631e-01
  1.51522322e-01  1.52101878e-01  1.52687564e-01  1.55827518e-01
  1.56182960e-01  1.56735904e-01  1.57088157e-01  1.60111464e-01
  1.61213708e-01  1.62834379e-01  1.66448057e-01  1.68935842e-01
  1.71792748e-01  1.72282885e-01  1.75008190e-01  1.76794053e-01
  1.81990880e-01  1.82403503e-01  1.82609918e-01  1.84079355e-01
  1.85835347e-01  1.85863123e-01  1.86105934e-01  1.86988167e-01
  1.87518151e-01  1.87913576e-01  1.90122297e-01  1.97606573e-01
  1.97843148e-01  1.98087975e-01  1.98719827e-01  1.99397006e-01
  2.00102074e-01  2.00278967e-01  2.00529567e-01  2.01338425e-01
  2.03654485e-01  2.03959195e-01  2.04448377e-01  2.07881738e-01
  2.08123842e-01  2.08884957e-01  2.10569119e-01  2.10729070e-01
  2.11191703e-01  2.12245414e-01  2.15064662e-01  2.15276514e-01
  2.15506044e-01  2.20994510e-01  2.21380421e-01  2.22071023e-01
  2.23185654e-01  2.23300984e-01  2.24104397e-01  2.24408829e-01
  2.28060359e-01  2.28608373e-01  2.28819466e-01  2.29272015e-01
  2.35092842e-01  2.35404808e-01  2.37261991e-01  2.38232077e-01
  2.38805031e-01  2.39514229e-01  2.40225753e-01  2.40250601e-01
  2.40519188e-01  2.41198395e-01  2.43578065e-01  2.44495448e-01
  2.46360237e-01  2.49850073e-01  2.54581252e-01  2.56783587e-01
  2.56880774e-01  2.57685838e-01  2.57798347e-01  2.58084999e-01
  2.59902860e-01  2.65422292e-01  2.65908196e-01  2.66141727e-01
  2.67147273e-01  2.68838346e-01  2.70171687e-01  2.70966436e-01
  2.71216888e-01  2.72575763e-01  2.72948585e-01  2.73727274e-01
  2.74417932e-01  2.76304513e-01  2.76851082e-01  2.76861395e-01
  2.78282540e-01  2.78415648e-01  2.80407268e-01  2.83297386e-01
  2.83967197e-01  2.84303237e-01  2.86569143e-01  2.86891152e-01
  2.87396618e-01  2.89266839e-01  2.89863121e-01  2.91364711e-01
  2.92778576e-01  2.92928281e-01  2.95448076e-01  3.00106680e-01
  3.03546737e-01  3.05005683e-01  3.06532283e-01  3.06848656e-01
  3.07424500e-01  3.08567999e-01  3.08758509e-01  3.09061246e-01
  3.10145707e-01  3.11767568e-01  3.13826246e-01  3.15206504e-01
  3.15369299e-01  3.16405922e-01  3.21666672e-01  3.22156380e-01
  3.23736491e-01  3.24132312e-01  3.28752243e-01  3.34501219e-01
  3.34933960e-01  3.36218132e-01  3.40218632e-01  3.44284448e-01
  3.44705246e-01  3.45013767e-01  3.47943442e-01  3.48070998e-01
  3.49056462e-01  3.49090713e-01  3.52360436e-01  3.53447891e-01
  3.54410544e-01  3.56803465e-01  3.57436721e-01  3.59592085e-01
  3.59611388e-01  3.60329064e-01  3.60461129e-01  3.63828879e-01
  3.64202664e-01  3.64218832e-01  3.65580519e-01  3.67897916e-01
  3.71890817e-01  3.73360774e-01  3.75739501e-01  3.76677423e-01
  3.79713903e-01  3.80022315e-01  3.85546767e-01  3.86550593e-01
  3.90132587e-01  3.95275045e-01  3.97964106e-01  3.99488962e-01
  4.02592886e-01  4.04817904e-01  4.10327053e-01  4.11367859e-01
  4.15692022e-01  4.16830051e-01  4.16996118e-01  4.22214807e-01
  4.22616523e-01  4.26578408e-01  4.26707495e-01  4.27364719e-01
  4.29620381e-01  4.30578659e-01  4.32426289e-01  4.33509605e-01
  4.40014116e-01  4.42692498e-01  4.43378795e-01  4.46447549e-01
  4.48058060e-01  4.48516579e-01  4.49393394e-01  4.50803095e-01
  4.51007014e-01  4.59366911e-01  4.63792250e-01  4.67091210e-01
  4.71738216e-01  4.74585755e-01  4.79907825e-01  4.80327475e-01
  4.80635817e-01  4.81796272e-01  4.83789050e-01  4.83952202e-01
  4.84612210e-01  4.85867536e-01  4.87226434e-01  4.90308244e-01
  4.92958699e-01  4.93387339e-01  4.94394091e-01  4.95000518e-01
  4.98113036e-01  5.00029639e-01  5.02796802e-01  5.04754184e-01
  5.05678441e-01  5.09721432e-01  5.10572943e-01  5.11544412e-01
  5.17048611e-01  5.21589759e-01  5.22161410e-01  5.22306736e-01
  5.25792913e-01  5.26132583e-01  5.29098277e-01  5.30462875e-01
  5.30845847e-01  5.31785229e-01  5.32123073e-01  5.33833669e-01
  5.34596809e-01  5.35405135e-01  5.38975976e-01  5.40632938e-01
  5.41891364e-01  5.42015580e-01  5.42154713e-01  5.43465704e-01
  5.46994535e-01  5.47237655e-01  5.49341498e-01  5.51269743e-01
  5.51342977e-01  5.51606260e-01  5.53353063e-01  5.54878563e-01
  5.55530814e-01  5.55673018e-01  5.56703222e-01  5.59422040e-01
  5.60162298e-01  5.62152003e-01  5.71379589e-01  5.76848920e-01
  5.76950764e-01  5.79114725e-01  5.80415434e-01  5.80845394e-01
  5.82737276e-01  5.87672924e-01  5.87736020e-01  5.89152561e-01
  5.89462371e-01  5.91455857e-01  5.92079908e-01  5.97686862e-01
  6.01238704e-01  6.01475124e-01  6.01888424e-01  6.05526487e-01
  6.12175766e-01  6.14797291e-01  6.16048052e-01  6.24031462e-01
  6.25022928e-01  6.25705113e-01  6.26370827e-01  6.28140389e-01
  6.28286658e-01  6.29349863e-01  6.30434256e-01  6.34693271e-01
  6.39487003e-01  6.40769688e-01  6.47050756e-01  6.50850149e-01
  6.54544507e-01  6.56453451e-01  6.56798868e-01  6.68292444e-01
  6.70476313e-01  6.73863258e-01  6.75845863e-01  6.75860894e-01
  6.77110841e-01  6.78615531e-01  6.79057772e-01  6.79689543e-01
  6.79723671e-01  6.83777457e-01  6.88320732e-01  6.89230348e-01
  6.90767690e-01  7.13812092e-01  7.17168723e-01  7.23735694e-01
  7.29849648e-01]

  warnings.warn(

2022-11-03 10:49:13,373:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.91536643e-01 -1.67359420e-01 -1.50870734e-01 -1.44159664e-01
 -1.38960061e-01 -1.32436532e-01 -1.30889801e-01 -1.28575354e-01
 -1.25375597e-01 -1.19656661e-01 -1.18150440e-01 -1.16905858e-01
 -1.15855640e-01 -1.14869827e-01 -1.10000053e-01 -1.08742246e-01
 -1.02658434e-01 -1.00956932e-01 -9.73111792e-02 -9.45469244e-02
 -9.38658895e-02 -8.94110251e-02 -8.83384577e-02 -8.72819415e-02
 -8.61035589e-02 -8.50650186e-02 -8.43474356e-02 -7.10468026e-02
 -7.04816209e-02 -6.87108069e-02 -6.37387019e-02 -6.28943016e-02
 -5.85875125e-02 -5.80858280e-02 -5.60564666e-02 -4.94512165e-02
 -4.90296134e-02 -4.88748001e-02 -4.45132564e-02 -4.43538630e-02
 -4.40967398e-02 -4.26252005e-02 -4.22766377e-02 -4.20604365e-02
 -3.92637758e-02 -3.83042172e-02 -3.62594597e-02 -3.54494103e-02
 -3.39927225e-02 -3.22665592e-02 -2.88161229e-02 -2.84128299e-02
 -2.83729897e-02 -2.60828247e-02 -2.39400514e-02 -2.32741770e-02
 -2.30260760e-02 -2.20656206e-02 -2.12055933e-02 -2.07070292e-02
 -1.83971892e-02 -1.60854585e-02 -1.11865461e-02 -9.60151851e-03
 -8.56415259e-03 -7.73981651e-03 -5.00728721e-03 -4.55203496e-03
 -3.64030117e-03 -2.47441635e-03 -1.84942937e-03 -5.46357679e-04
  3.97419267e-04  1.80096009e-03  1.82125363e-03  3.06468205e-03
  4.00706690e-03  5.84919655e-03  7.08961715e-03  9.09800058e-03
  1.03393827e-02  1.05297693e-02  1.27093672e-02  1.39506779e-02
  1.81484029e-02  1.85039729e-02  2.05039796e-02  2.23952074e-02
  2.42888276e-02  2.52986602e-02  2.64413677e-02  2.92927366e-02
  3.07691202e-02  3.15242904e-02  3.34991029e-02  3.46064463e-02
  3.55142691e-02  3.75459441e-02  3.92452318e-02  4.03918660e-02
  4.09253172e-02  4.11507475e-02  4.31378714e-02  4.50620786e-02
  4.55038215e-02  5.03371588e-02  5.08658530e-02  5.19539264e-02
  5.29843303e-02  5.41493830e-02  5.70246293e-02  5.93864979e-02
  6.10636622e-02  6.13139407e-02  6.48214776e-02  6.66912225e-02
  6.80486759e-02  8.36861943e-02  8.36933788e-02  8.43881223e-02
  8.57264399e-02  8.61214207e-02  8.69781282e-02  8.76503042e-02
  9.02553172e-02  9.20395944e-02  9.21943250e-02  9.21977971e-02
  9.22061069e-02  9.22336508e-02  9.27073663e-02  9.31882115e-02
  9.60494080e-02  9.63813149e-02  9.71826886e-02  9.75017600e-02
  9.87076957e-02  1.01190009e-01  1.01393881e-01  1.01450635e-01
  1.01495022e-01  1.02324181e-01  1.05597463e-01  1.06451733e-01
  1.07266935e-01  1.07517864e-01  1.16184338e-01  1.16716781e-01
  1.16826367e-01  1.20516159e-01  1.20635318e-01  1.21128695e-01
  1.21694195e-01  1.21795334e-01  1.21834318e-01  1.24970041e-01
  1.25118558e-01  1.25189682e-01  1.27316463e-01  1.28369007e-01
  1.28692825e-01  1.29577824e-01  1.29755724e-01  1.30321635e-01
  1.31631805e-01  1.34477280e-01  1.34986612e-01  1.36280298e-01
  1.37193143e-01  1.37699771e-01  1.37755461e-01  1.37959923e-01
  1.38161964e-01  1.44475733e-01  1.46114891e-01  1.47220300e-01
  1.47758796e-01  1.48081184e-01  1.51093358e-01  1.54781716e-01
  1.55712602e-01  1.55997642e-01  1.58113010e-01  1.59763918e-01
  1.60571794e-01  1.60797117e-01  1.63693205e-01  1.66537112e-01
  1.68539896e-01  1.71726594e-01  1.73918319e-01  1.73964749e-01
  1.74545681e-01  1.77316540e-01  1.78326132e-01  1.83367154e-01
  1.85435065e-01  1.86867145e-01  1.87019805e-01  1.89839919e-01
  1.94422132e-01  1.99536814e-01  2.00489722e-01  2.03530992e-01
  2.04817151e-01  2.05491322e-01  2.05966375e-01  2.06335933e-01
  2.07720454e-01  2.08740852e-01  2.08886326e-01  2.11783441e-01
  2.12010797e-01  2.13235793e-01  2.13572110e-01  2.13855980e-01
  2.14921852e-01  2.17055062e-01  2.18418133e-01  2.20202868e-01
  2.22421645e-01  2.23203353e-01  2.23533429e-01  2.27028625e-01
  2.28473450e-01  2.29210831e-01  2.32129963e-01  2.32626870e-01
  2.34638819e-01  2.35020610e-01  2.35106903e-01  2.35314619e-01
  2.37640732e-01  2.38041173e-01  2.39601716e-01  2.39677194e-01
  2.41551143e-01  2.41884232e-01  2.42746266e-01  2.43491566e-01
  2.45788293e-01  2.46428161e-01  2.46551139e-01  2.46721802e-01
  2.47415341e-01  2.48045752e-01  2.48844830e-01  2.48992325e-01
  2.49067212e-01  2.51036084e-01  2.51925249e-01  2.52376578e-01
  2.54552439e-01  2.56807930e-01  2.58692842e-01  2.58757014e-01
  2.59946085e-01  2.61976636e-01  2.62379740e-01  2.62706582e-01
  2.62861226e-01  2.63108193e-01  2.63804758e-01  2.63823336e-01
  2.64367743e-01  2.66615005e-01  2.66951226e-01  2.70205988e-01
  2.71447682e-01  2.71783207e-01  2.72010905e-01  2.72827817e-01
  2.72907220e-01  2.74952794e-01  2.75491700e-01  2.79469991e-01
  2.81996056e-01  2.82294479e-01  2.83853631e-01  2.86007186e-01
  2.86289887e-01  2.89077274e-01  2.95115315e-01  2.99569027e-01
  3.01769414e-01  3.09999139e-01  3.12326758e-01  3.12736518e-01
  3.13670556e-01  3.14759692e-01  3.15406198e-01  3.15956362e-01
  3.17854405e-01  3.18409436e-01  3.19196082e-01  3.19772226e-01
  3.23459591e-01  3.26347833e-01  3.26486051e-01  3.29380107e-01
  3.30493962e-01  3.30914543e-01  3.31556717e-01  3.32286323e-01
  3.32563149e-01  3.33160609e-01  3.33593845e-01  3.34459624e-01
  3.37699055e-01  3.37843768e-01  3.39145169e-01  3.40315803e-01
  3.42680218e-01  3.46397073e-01  3.51344889e-01  3.56294430e-01
  3.58882193e-01  3.61634306e-01  3.63340967e-01  3.63956521e-01
  3.64008378e-01  3.65432756e-01  3.65883448e-01  3.67102814e-01
  3.69351246e-01  3.71370904e-01  3.73254827e-01  3.74506060e-01
  3.74676228e-01  3.75517968e-01  3.76805650e-01  3.78060316e-01
  3.81806214e-01  3.84192011e-01  3.86478058e-01  3.86995876e-01
  3.89424358e-01  3.92010317e-01  3.93962002e-01  3.95289046e-01
  3.96091013e-01  3.96895918e-01  3.97283908e-01  4.02794716e-01
  4.05305972e-01  4.10243379e-01  4.10622452e-01  4.12596019e-01
  4.20545426e-01  4.20863573e-01  4.22774009e-01  4.23131319e-01
  4.24839658e-01  4.26670200e-01  4.32282090e-01  4.32714529e-01
  4.32829690e-01  4.33279559e-01  4.34204057e-01  4.36694750e-01
  4.39460093e-01  4.39680338e-01  4.48311560e-01  4.49166113e-01
  4.49722148e-01  4.50418509e-01  4.51015568e-01  4.51565791e-01
  4.53510080e-01  4.57773749e-01  4.58220552e-01  4.59966294e-01
  4.60802966e-01  4.61139132e-01  4.61291091e-01  4.61805462e-01
  4.62594966e-01  4.65824252e-01  4.67167090e-01  4.67296781e-01
  4.67518541e-01  4.68380396e-01  4.69058356e-01  4.69131342e-01
  4.69619104e-01  4.69663813e-01  4.70717059e-01  4.73423934e-01
  4.74338783e-01  4.76293653e-01  4.77634827e-01  4.78615950e-01
  4.78647934e-01  4.78856741e-01  4.82089552e-01  4.82495921e-01
  4.86557177e-01  4.86757866e-01  4.88552614e-01  4.88752072e-01
  4.91410939e-01  4.91918905e-01  4.92848246e-01  4.96948599e-01
  4.97674764e-01  4.98651134e-01  5.01183617e-01  5.01354555e-01
  5.02641144e-01  5.03203411e-01  5.07567064e-01  5.08724173e-01
  5.09267201e-01  5.10605132e-01  5.11336496e-01  5.11890025e-01
  5.18909118e-01  5.20727852e-01  5.21601739e-01  5.23036185e-01
  5.28846937e-01  5.31834730e-01  5.33438523e-01  5.34981273e-01
  5.36705237e-01  5.38826779e-01  5.39623551e-01  5.39820240e-01
  5.46043864e-01  5.47859601e-01  5.53252308e-01  5.54452252e-01
  5.64841343e-01  5.65029514e-01  5.67129649e-01  5.68497847e-01
  5.69127893e-01  5.69420600e-01  5.70882558e-01  5.76068122e-01
  5.76512105e-01  5.77408426e-01  5.79633590e-01  5.83147773e-01
  5.85003364e-01  5.88695094e-01  5.88879332e-01  5.90208191e-01
  5.92126964e-01  5.92946725e-01  5.94383573e-01  6.05977211e-01
  6.07875684e-01  6.10498520e-01  6.11088281e-01  6.12979036e-01
  6.14828938e-01  6.15578813e-01  6.15786892e-01  6.16536838e-01
  6.25331307e-01  6.26666691e-01  6.30485604e-01  6.30858969e-01
  6.31475245e-01  6.32517642e-01  6.36848552e-01  6.37108377e-01
  6.41339326e-01  6.42807978e-01  6.45803629e-01  6.47796298e-01
  6.49530005e-01  6.51708312e-01  6.56994861e-01  6.57718430e-01
  6.58356301e-01  6.59537916e-01  6.63803527e-01  6.65764377e-01
  6.76686596e-01  6.82522271e-01  6.86354446e-01  6.89426665e-01
  6.89473322e-01  6.90933830e-01  6.97097964e-01  7.00935149e-01
  7.11969663e-01  7.15067857e-01  7.19851108e-01  7.20450391e-01
  7.35815757e-01]

  warnings.warn(

2022-11-03 10:49:13,420:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.21995375 -0.20156224 -0.19517494 -0.17532196 -0.17526044 -0.17190587
 -0.16542606 -0.16485416 -0.16473621 -0.16382731 -0.15718968 -0.1567626
 -0.15370434 -0.15249719 -0.14336911 -0.13846612 -0.13716038 -0.13617887
 -0.13612657 -0.12946446 -0.12688151 -0.11988778 -0.11883769 -0.11591722
 -0.11344735 -0.11168056 -0.11134392 -0.11102781 -0.10922339 -0.10921945
 -0.10552838 -0.09637341 -0.09617047 -0.09268386 -0.09246421 -0.09106299
 -0.09049436 -0.08483331 -0.08423254 -0.08395837 -0.08332911 -0.08278316
 -0.08254977 -0.08042081 -0.07882946 -0.07486019 -0.06887859 -0.06816356
 -0.06648795 -0.06282146 -0.06253692 -0.06240397 -0.06055665 -0.0561319
 -0.05561258 -0.05423256 -0.05390765 -0.0529403  -0.05242863 -0.04733807
 -0.04646914 -0.04635257 -0.04583403 -0.04499208 -0.04491378 -0.04151733
 -0.03846217 -0.03759608 -0.03595636 -0.03356973 -0.03300069 -0.03244074
 -0.02533164 -0.01986503 -0.01693559 -0.01556187 -0.01546881 -0.01449584
 -0.01336515 -0.01099972 -0.01066485 -0.00811392 -0.00693457 -0.00489688
 -0.00175242  0.00122484  0.00182201  0.00604517  0.0096298   0.01149551
  0.01206894  0.01391786  0.01686972  0.01817437  0.01833533  0.02119341
  0.02133919  0.02310854  0.02466553  0.02953944  0.03039273  0.03070166
  0.03143429  0.03695     0.03903229  0.03980566  0.04090561  0.04462778
  0.05041741  0.050648    0.05099537  0.05499355  0.05512309  0.05532132
  0.05673146  0.05906322  0.05919481  0.06075396  0.06105576  0.06563767
  0.06850039  0.06963128  0.07166113  0.07637359  0.07687042  0.07828668
  0.07857365  0.0807738   0.08452663  0.08629059  0.08664293  0.08746303
  0.09033173  0.09243359  0.09263014  0.09350479  0.09474971  0.0978683
  0.09975243  0.10090523  0.1027711   0.10537573  0.10595945  0.10804259
  0.10932383  0.11028343  0.11029149  0.11100749  0.11376965  0.1172224
  0.12107135  0.12406677  0.12412644  0.1243029   0.12777708  0.12947169
  0.13038808  0.13263488  0.13294959  0.13309386  0.13636565  0.1377309
  0.13850561  0.13903155  0.14279615  0.14347773  0.14434866  0.1462418
  0.14681006  0.14896265  0.1550299   0.15634256  0.15789115  0.15820876
  0.15889219  0.15946524  0.16078833  0.16362414  0.16471883  0.16584496
  0.16971421  0.1720216   0.17376434  0.17603084  0.18207719  0.18220787
  0.18256065  0.18358076  0.18529167  0.18637677  0.19150296  0.192734
  0.19293644  0.19298236  0.19393749  0.19637504  0.1967431   0.19709383
  0.19759569  0.19800886  0.19844034  0.19845084  0.20106684  0.20250968
  0.20418742  0.2055052   0.20589124  0.20734537  0.20737285  0.21048647
  0.21133808  0.21171044  0.21825507  0.21903192  0.21981598  0.22053948
  0.22075728  0.22134801  0.22257426  0.22345965  0.22424499  0.22493684
  0.2253667   0.22542378  0.22548906  0.23096236  0.23248797  0.23663138
  0.24005468  0.24022641  0.24166679  0.24249524  0.24364751  0.24532052
  0.24730132  0.2476877   0.24929943  0.25248203  0.25406676  0.25471994
  0.25474042  0.25522692  0.2554401   0.25686827  0.25690413  0.25717003
  0.26223076  0.26396139  0.26407468  0.26540716  0.26647102  0.26658703
  0.27034574  0.2706378   0.27387613  0.27402494  0.27479193  0.27557641
  0.27621449  0.27737494  0.2777618   0.27838998  0.28099577  0.28187683
  0.28401214  0.2844458   0.28529793  0.28580957  0.28663927  0.28682262
  0.28850114  0.28851251  0.28989243  0.28997739  0.29233565  0.29494311
  0.29846965  0.29876389  0.30701713  0.31155038  0.31236627  0.31319372
  0.31334198  0.31425054  0.31583074  0.31657161  0.31722067  0.31729027
  0.3173681   0.31820598  0.32144875  0.32197988  0.32299161  0.32400343
  0.32463548  0.32531887  0.32605303  0.32634561  0.32803443  0.32852128
  0.32980238  0.33104302  0.33282144  0.33466423  0.34188786  0.34218012
  0.34652675  0.34665565  0.35022167  0.35136931  0.35162105  0.3524659
  0.35280341  0.35389056  0.3555991   0.35628712  0.35736158  0.35760006
  0.35801337  0.36007729  0.3623172   0.36423307  0.3665527   0.36909796
  0.37117461  0.37315326  0.37334409  0.37745377  0.37860499  0.38003817
  0.38471284  0.38692133  0.38791566  0.38908373  0.38993828  0.39022611
  0.39029691  0.39032836  0.39141929  0.3915439   0.39312261  0.39569472
  0.39693782  0.39920439  0.40293068  0.40632054  0.40723917  0.4127757
  0.41464425  0.41635175  0.41636185  0.41972     0.42007464  0.42231677
  0.42249514  0.4239188   0.42465395  0.42979767  0.43118197  0.43225241
  0.43263666  0.4328125   0.43445567  0.44102219  0.44105451  0.44132504
  0.44151715  0.44247983  0.44289565  0.44375305  0.44558489  0.44572274
  0.44706832  0.44790358  0.44878909  0.44988428  0.45407443  0.45600448
  0.4630452   0.46444079  0.4660532   0.46811018  0.46973204  0.47142993
  0.4729564   0.47480479  0.47871436  0.4807098   0.48099816  0.48452403
  0.48531511  0.48765297  0.48803672  0.48902529  0.48996073  0.49022132
  0.49195976  0.49254325  0.494444    0.49510512  0.49814578  0.49938263
  0.50053628  0.50286795  0.50304959  0.50337797  0.50373531  0.50579938
  0.50614557  0.50715503  0.50971351  0.51410136  0.51556894  0.51611288
  0.51937683  0.52121574  0.52411329  0.53216051  0.53281377  0.53415622
  0.53609685  0.53611704  0.53653496  0.53725898  0.53925269  0.54315348
  0.54456024  0.54486036  0.549304    0.55417157  0.55764519  0.56153977
  0.56160231  0.56248144  0.5641628   0.56581901  0.56804971  0.57039778
  0.5735457   0.57504472  0.5758139   0.57661747  0.57937371  0.57949028
  0.58038332  0.58094584  0.58804753  0.59242457  0.59480299  0.59621177
  0.59741719  0.59807833  0.60348283  0.60895934  0.60994314  0.61016638
  0.61197567  0.61222734  0.61263175  0.61504288  0.61590109  0.62467272
  0.62727327  0.62780469  0.63030568  0.63690553  0.642505    0.64462021
  0.64474076  0.64492361  0.64642337  0.64751352  0.64930491  0.65005229
  0.65053256  0.65497884  0.6617082   0.66282176  0.66337642  0.66470706
  0.66494939  0.66512453  0.66537536  0.67329833  0.67536626  0.69493366
  0.7048751   0.71299944  0.73657167  0.74688423  0.75921636  0.76776268
  0.76948426]

  warnings.warn(

2022-11-03 10:49:13,470:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.76828970e-01 -1.71590919e-01 -1.60074520e-01 -1.54233416e-01
 -1.53754253e-01 -1.35760584e-01 -1.33551417e-01 -1.32461881e-01
 -1.25838367e-01 -1.17038122e-01 -1.16500262e-01 -1.15088990e-01
 -1.13429493e-01 -1.11939679e-01 -1.11383469e-01 -1.09295667e-01
 -1.08794800e-01 -1.05104520e-01 -1.02609051e-01 -1.01706842e-01
 -1.00838178e-01 -1.00060102e-01 -9.92363730e-02 -9.65399041e-02
 -9.46506116e-02 -9.17803428e-02 -9.06399124e-02 -8.03530817e-02
 -7.89968938e-02 -7.75164309e-02 -7.71067875e-02 -7.69268583e-02
 -7.57457605e-02 -6.90883016e-02 -6.83036690e-02 -6.17971968e-02
 -6.04433692e-02 -6.01744042e-02 -5.86243117e-02 -5.81042684e-02
 -5.63905720e-02 -5.55752302e-02 -5.30256916e-02 -5.25311828e-02
 -5.18352273e-02 -4.95095232e-02 -4.75784811e-02 -4.73616742e-02
 -4.57105651e-02 -4.46881460e-02 -4.37709708e-02 -4.34436666e-02
 -4.05282443e-02 -3.80974203e-02 -3.69345616e-02 -3.62506776e-02
 -3.53203570e-02 -3.46977167e-02 -3.38618021e-02 -3.31728175e-02
 -2.97655185e-02 -2.77641212e-02 -2.58381100e-02 -2.56689700e-02
 -2.51336614e-02 -2.50571707e-02 -2.48193959e-02 -2.05577396e-02
 -1.99662980e-02 -1.69089378e-02 -1.59841051e-02 -1.48097797e-02
 -1.46612823e-02 -9.85119430e-03 -8.24179322e-03 -3.09162107e-03
 -2.16165421e-03 -1.65463332e-03 -2.03126882e-04  8.15561744e-06
  4.98402147e-03  7.44600851e-03  1.02434234e-02  1.05811733e-02
  1.52127800e-02  1.58290733e-02  1.86435511e-02  1.86641337e-02
  2.14386405e-02  2.21812666e-02  2.32399765e-02  2.45357686e-02
  2.52718538e-02  2.70195193e-02  2.76674864e-02  2.77928925e-02
  3.04522416e-02  3.08062873e-02  3.09463859e-02  3.46253724e-02
  3.53403464e-02  3.58346150e-02  3.62356384e-02  3.74417980e-02
  3.82654510e-02  3.92358557e-02  4.14142745e-02  4.21686074e-02
  4.21755080e-02  4.22210887e-02  4.37041475e-02  4.45280409e-02
  4.52454903e-02  5.01641867e-02  5.14701976e-02  5.31305104e-02
  5.49214926e-02  5.56424330e-02  5.59853385e-02  5.60559178e-02
  5.71679815e-02  5.84685098e-02  5.98297980e-02  6.24660123e-02
  6.46230377e-02  6.59403227e-02  6.73136198e-02  6.79505493e-02
  6.87671957e-02  6.91527335e-02  6.93247992e-02  6.99943193e-02
  7.06540423e-02  7.40828575e-02  7.69243052e-02  7.74713345e-02
  7.75698252e-02  7.84758769e-02  8.07963095e-02  8.14315408e-02
  8.27171309e-02  8.30835894e-02  8.56577914e-02  8.61202990e-02
  8.74514238e-02  8.92051393e-02  9.11343239e-02  9.13090491e-02
  9.18111157e-02  9.21068953e-02  9.51817366e-02  9.57133179e-02
  9.71698666e-02  9.95515120e-02  1.00874940e-01  1.02105049e-01
  1.04753719e-01  1.05660935e-01  1.06040994e-01  1.06668271e-01
  1.06862947e-01  1.08689657e-01  1.10108758e-01  1.13412228e-01
  1.14168775e-01  1.14886257e-01  1.16700158e-01  1.17372249e-01
  1.21480659e-01  1.24704294e-01  1.27825327e-01  1.28326517e-01
  1.29905645e-01  1.31903976e-01  1.33956037e-01  1.39452550e-01
  1.40565854e-01  1.49006685e-01  1.52111030e-01  1.53052374e-01
  1.55638544e-01  1.55852230e-01  1.56153701e-01  1.58908051e-01
  1.58916757e-01  1.59320338e-01  1.59632253e-01  1.61553977e-01
  1.62308063e-01  1.62631600e-01  1.63239273e-01  1.63970966e-01
  1.65400834e-01  1.67744732e-01  1.69409182e-01  1.70200607e-01
  1.71577613e-01  1.72650654e-01  1.74332081e-01  1.76668793e-01
  1.78249035e-01  1.81248698e-01  1.81459875e-01  1.82512820e-01
  1.83982105e-01  1.88441880e-01  1.88655686e-01  1.89848852e-01
  1.90736604e-01  1.92298419e-01  1.94421113e-01  1.98386300e-01
  1.99683331e-01  2.00619640e-01  2.00989311e-01  2.02818361e-01
  2.04494302e-01  2.05432037e-01  2.06751453e-01  2.06929719e-01
  2.07346486e-01  2.07809537e-01  2.07992895e-01  2.10484206e-01
  2.11319619e-01  2.11459960e-01  2.15608003e-01  2.19177550e-01
  2.20660602e-01  2.23231563e-01  2.23328837e-01  2.27690324e-01
  2.28084914e-01  2.28258642e-01  2.28906586e-01  2.29709847e-01
  2.29876168e-01  2.34117328e-01  2.36648358e-01  2.37783622e-01
  2.39450639e-01  2.40454788e-01  2.41435331e-01  2.43926043e-01
  2.44454053e-01  2.47078210e-01  2.47522202e-01  2.49226023e-01
  2.49489784e-01  2.53536602e-01  2.56838799e-01  2.57268689e-01
  2.59456426e-01  2.60866806e-01  2.61045491e-01  2.63134590e-01
  2.64950540e-01  2.66969778e-01  2.67653795e-01  2.68719169e-01
  2.69415381e-01  2.69599170e-01  2.70587445e-01  2.72027198e-01
  2.75375763e-01  2.77384029e-01  2.78114262e-01  2.79386590e-01
  2.80013012e-01  2.80238803e-01  2.84934635e-01  2.86328533e-01
  2.87644820e-01  2.88524457e-01  2.92390551e-01  2.92742897e-01
  2.93208408e-01  2.93312143e-01  2.96445530e-01  2.98256748e-01
  2.98365610e-01  2.98767273e-01  2.99033117e-01  2.99419398e-01
  2.99778438e-01  3.03531510e-01  3.04870281e-01  3.06160536e-01
  3.06819081e-01  3.07362572e-01  3.07481063e-01  3.07593984e-01
  3.07769271e-01  3.10477076e-01  3.13067822e-01  3.14376786e-01
  3.14624999e-01  3.15849516e-01  3.18379503e-01  3.18554257e-01
  3.19027010e-01  3.20464702e-01  3.22056721e-01  3.22808893e-01
  3.22809392e-01  3.28589192e-01  3.29107710e-01  3.30089319e-01
  3.31001306e-01  3.31920809e-01  3.33944077e-01  3.36306619e-01
  3.39043529e-01  3.41649017e-01  3.45053127e-01  3.47811905e-01
  3.49203531e-01  3.50015594e-01  3.51526776e-01  3.53123103e-01
  3.55200558e-01  3.55388683e-01  3.57168020e-01  3.58612946e-01
  3.58989094e-01  3.60087481e-01  3.60592265e-01  3.63474439e-01
  3.63763798e-01  3.66045287e-01  3.66621314e-01  3.67057851e-01
  3.68296817e-01  3.69975278e-01  3.70732555e-01  3.72438024e-01
  3.73466860e-01  3.75312022e-01  3.75977138e-01  3.81285658e-01
  3.83788074e-01  3.84157003e-01  3.84493599e-01  3.88150338e-01
  3.90226797e-01  3.93561955e-01  3.96175629e-01  4.01164172e-01
  4.04266362e-01  4.05352072e-01  4.12868229e-01  4.12932899e-01
  4.13247697e-01  4.13609466e-01  4.14089768e-01  4.14667631e-01
  4.15573631e-01  4.19887153e-01  4.21804021e-01  4.23553221e-01
  4.24276146e-01  4.26017007e-01  4.26203539e-01  4.26262530e-01
  4.34462828e-01  4.36573217e-01  4.37353302e-01  4.38545434e-01
  4.41785737e-01  4.44099947e-01  4.46395743e-01  4.49317434e-01
  4.49329431e-01  4.49447897e-01  4.50491964e-01  4.51289664e-01
  4.55347155e-01  4.56609081e-01  4.58432099e-01  4.59064721e-01
  4.59356465e-01  4.60865732e-01  4.60949787e-01  4.63882765e-01
  4.68299103e-01  4.68738903e-01  4.69909047e-01  4.73102315e-01
  4.76480006e-01  4.76760631e-01  4.77177494e-01  4.78797523e-01
  4.81139207e-01  4.85327175e-01  4.89884066e-01  4.91808739e-01
  4.92500030e-01  4.93823502e-01  4.96253001e-01  4.97267539e-01
  4.99699543e-01  5.01555007e-01  5.03398937e-01  5.06210835e-01
  5.09613397e-01  5.13528798e-01  5.13587699e-01  5.13869720e-01
  5.19318546e-01  5.22035845e-01  5.22507502e-01  5.25244696e-01
  5.28032313e-01  5.28759864e-01  5.30013458e-01  5.30127594e-01
  5.30508558e-01  5.35903881e-01  5.39752239e-01  5.45661764e-01
  5.45890323e-01  5.47712762e-01  5.47891902e-01  5.50044906e-01
  5.52096380e-01  5.53671158e-01  5.60032958e-01  5.61498711e-01
  5.68278403e-01  5.74696319e-01  5.78761926e-01  5.83139858e-01
  5.85206103e-01  5.85641166e-01  5.86640733e-01  5.93164100e-01
  5.94548245e-01  5.95417966e-01  5.96593657e-01  5.97208020e-01
  5.98201852e-01  5.98325363e-01  6.00104142e-01  6.02111061e-01
  6.03521359e-01  6.03553934e-01  6.05935122e-01  6.06261823e-01
  6.06507172e-01  6.06953774e-01  6.12001248e-01  6.12560296e-01
  6.12849245e-01  6.13269654e-01  6.15091948e-01  6.17064177e-01
  6.18273199e-01  6.19377878e-01  6.24037071e-01  6.27114018e-01
  6.27962170e-01  6.29657402e-01  6.32783546e-01  6.35194715e-01
  6.38056403e-01  6.41347368e-01  6.42440633e-01  6.46976870e-01
  6.49446479e-01  6.53948993e-01  6.54646347e-01  6.57946539e-01
  6.58463372e-01  6.58914496e-01  6.61215993e-01  6.61245463e-01
  6.65931179e-01  6.66746575e-01  6.66840200e-01  6.73361811e-01
  6.84028019e-01  6.84917464e-01  7.02234432e-01  7.08033739e-01
  7.09314070e-01  7.12785068e-01  7.21896567e-01  7.22050583e-01
  7.22933656e-01  7.22964407e-01  7.26397217e-01  7.64551607e-01]

  warnings.warn(

2022-11-03 10:49:16,769:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.13808844e-01 -2.09173753e-01 -2.07443349e-01 -1.90821735e-01
 -1.86030049e-01 -1.71647435e-01 -1.68266034e-01 -1.51747971e-01
 -1.51216949e-01 -1.51137418e-01 -1.49946208e-01 -1.49577891e-01
 -1.39052724e-01 -1.32655682e-01 -1.32163734e-01 -1.31465927e-01
 -1.28257956e-01 -1.27042841e-01 -1.26413752e-01 -1.25099816e-01
 -1.24888447e-01 -1.24662051e-01 -1.23411706e-01 -1.21309139e-01
 -1.20499269e-01 -1.18221138e-01 -1.06674896e-01 -1.05730834e-01
 -1.01341223e-01 -9.91349154e-02 -9.71267462e-02 -9.55788703e-02
 -9.35183332e-02 -9.26751554e-02 -9.25446485e-02 -9.21121974e-02
 -9.10499746e-02 -8.91215239e-02 -8.37524877e-02 -7.67716699e-02
 -7.62168318e-02 -7.38828992e-02 -7.28941467e-02 -7.08876713e-02
 -7.07886706e-02 -6.48864923e-02 -6.00769827e-02 -5.93176616e-02
 -5.84790659e-02 -5.73694146e-02 -5.41268509e-02 -5.28297489e-02
 -4.98184211e-02 -4.88872160e-02 -4.70962935e-02 -4.70477157e-02
 -4.42225112e-02 -4.30382081e-02 -4.27737501e-02 -4.06245406e-02
 -3.98367651e-02 -3.90618496e-02 -3.76747594e-02 -3.53186775e-02
 -3.40138254e-02 -3.37692003e-02 -2.97115328e-02 -2.76807581e-02
 -2.71841268e-02 -2.26024178e-02 -2.14616189e-02 -2.03326902e-02
 -1.89302134e-02 -1.82186680e-02 -1.58550346e-02 -1.51932204e-02
 -1.42069042e-02 -1.24354957e-02 -1.04429323e-02 -9.84225601e-03
 -9.47102271e-03 -5.57915398e-03 -4.06564158e-03 -4.06219675e-03
 -1.71271546e-03 -1.35901474e-03 -1.64804897e-04  1.13630779e-03
  2.81053167e-03  2.98279770e-03  5.34608793e-03  6.23609957e-03
  6.99276014e-03  8.66732583e-03  9.12070687e-03  1.55780243e-02
  2.04126261e-02  2.20247119e-02  2.26399563e-02  2.38025114e-02
  2.42864367e-02  2.98602848e-02  2.99523028e-02  3.21878503e-02
  3.23974064e-02  3.34262185e-02  3.52450004e-02  3.64047101e-02
  4.08066266e-02  4.34728324e-02  4.76591241e-02  4.85309113e-02
  4.91113118e-02  4.95568937e-02  5.32121522e-02  5.60030061e-02
  5.86330113e-02  5.95480323e-02  6.01138896e-02  6.11387418e-02
  6.28092944e-02  6.44947334e-02  6.49225071e-02  6.54623712e-02
  6.62700548e-02  6.84301360e-02  6.85575174e-02  7.02576278e-02
  7.14701277e-02  7.20266155e-02  7.37930795e-02  7.47211379e-02
  7.49301379e-02  7.54888294e-02  8.03495252e-02  8.41906933e-02
  8.52403699e-02  8.54353497e-02  8.71562391e-02  8.72230915e-02
  8.72522669e-02  8.77712202e-02  8.79049848e-02  9.19845458e-02
  9.26359475e-02  9.35275423e-02  9.55800594e-02  9.61981687e-02
  9.80993610e-02  1.01326888e-01  1.01379761e-01  1.02150401e-01
  1.03862949e-01  1.05245777e-01  1.09135275e-01  1.11602020e-01
  1.12327002e-01  1.12375769e-01  1.12690676e-01  1.13478792e-01
  1.13647890e-01  1.16199187e-01  1.16646952e-01  1.21482893e-01
  1.21566378e-01  1.22385368e-01  1.22586609e-01  1.22908380e-01
  1.23641196e-01  1.25467003e-01  1.25939344e-01  1.26463328e-01
  1.28662109e-01  1.31436952e-01  1.31971491e-01  1.33557354e-01
  1.35755289e-01  1.39277032e-01  1.39380372e-01  1.40015679e-01
  1.40442969e-01  1.41247015e-01  1.41321471e-01  1.42406469e-01
  1.42553363e-01  1.45331022e-01  1.48157813e-01  1.53653170e-01
  1.53914943e-01  1.54648976e-01  1.55411547e-01  1.55493257e-01
  1.56873935e-01  1.58485337e-01  1.59468259e-01  1.61962540e-01
  1.63196848e-01  1.64283153e-01  1.64836651e-01  1.65822618e-01
  1.65828972e-01  1.67998539e-01  1.70842505e-01  1.73517748e-01
  1.75996485e-01  1.76553807e-01  1.78440602e-01  1.78522057e-01
  1.81072047e-01  1.81362642e-01  1.83209427e-01  1.84015134e-01
  1.84492719e-01  1.89159068e-01  1.89449419e-01  1.91881875e-01
  1.93597891e-01  1.94607487e-01  1.95508371e-01  1.96911187e-01
  2.01110297e-01  2.02157095e-01  2.02283618e-01  2.02754127e-01
  2.02967545e-01  2.03276532e-01  2.03431101e-01  2.04808617e-01
  2.04944351e-01  2.10152747e-01  2.13086500e-01  2.14789513e-01
  2.20916272e-01  2.24292164e-01  2.25903939e-01  2.26346976e-01
  2.26590945e-01  2.30847592e-01  2.31407937e-01  2.32745969e-01
  2.32872781e-01  2.33180475e-01  2.34352402e-01  2.34510388e-01
  2.36650533e-01  2.38437111e-01  2.38814057e-01  2.40450276e-01
  2.43376916e-01  2.44331706e-01  2.46421728e-01  2.48293216e-01
  2.49406172e-01  2.50916534e-01  2.54196815e-01  2.59047286e-01
  2.59170262e-01  2.63747523e-01  2.64566535e-01  2.65438429e-01
  2.65456883e-01  2.67219232e-01  2.71140403e-01  2.73352796e-01
  2.80369657e-01  2.81218396e-01  2.81890963e-01  2.82532060e-01
  2.85913129e-01  2.87387377e-01  2.89071082e-01  2.92742157e-01
  2.93385913e-01  2.94401844e-01  2.95786774e-01  2.96022781e-01
  2.96373416e-01  2.96708133e-01  2.97156914e-01  2.97603129e-01
  2.98665260e-01  2.98782636e-01  2.99539381e-01  3.01233927e-01
  3.01786419e-01  3.02490253e-01  3.02665091e-01  3.06900083e-01
  3.07321522e-01  3.09579416e-01  3.12008884e-01  3.14856474e-01
  3.20370569e-01  3.23071474e-01  3.26139729e-01  3.26681600e-01
  3.27542361e-01  3.27734089e-01  3.28185879e-01  3.28211300e-01
  3.28496129e-01  3.29135310e-01  3.29726159e-01  3.30192559e-01
  3.33955802e-01  3.39569255e-01  3.40340182e-01  3.41220302e-01
  3.42363645e-01  3.45405152e-01  3.47015863e-01  3.52102259e-01
  3.53883476e-01  3.54112185e-01  3.55651029e-01  3.56076815e-01
  3.59260748e-01  3.60720008e-01  3.60878626e-01  3.62679372e-01
  3.62865799e-01  3.64790794e-01  3.65054422e-01  3.67225841e-01
  3.67709328e-01  3.68547060e-01  3.70889601e-01  3.71235160e-01
  3.72822835e-01  3.72948267e-01  3.78279150e-01  3.81116836e-01
  3.83182599e-01  3.86533218e-01  3.92319014e-01  3.93500952e-01
  3.95810935e-01  3.96484314e-01  3.97800334e-01  3.99712586e-01
  4.00830665e-01  4.01555282e-01  4.02176136e-01  4.03412045e-01
  4.03560211e-01  4.05581825e-01  4.05645234e-01  4.05886903e-01
  4.08174661e-01  4.09063228e-01  4.11614884e-01  4.15675585e-01
  4.16746175e-01  4.16877740e-01  4.18935483e-01  4.19832094e-01
  4.21344981e-01  4.22765367e-01  4.28365472e-01  4.28468383e-01
  4.29070004e-01  4.30957046e-01  4.34426134e-01  4.34801624e-01
  4.36953329e-01  4.37502610e-01  4.38630548e-01  4.38886638e-01
  4.40077151e-01  4.40494529e-01  4.47110453e-01  4.47782274e-01
  4.49100743e-01  4.52154928e-01  4.52976624e-01  4.53226038e-01
  4.55844351e-01  4.55900398e-01  4.58553743e-01  4.59575991e-01
  4.60726428e-01  4.63824396e-01  4.64081750e-01  4.67481095e-01
  4.74954071e-01  4.79180975e-01  4.81097062e-01  4.82182753e-01
  4.82436848e-01  4.83125116e-01  4.84771358e-01  4.87865412e-01
  4.88593489e-01  4.88945050e-01  4.90300419e-01  4.92203269e-01
  4.96293917e-01  4.96711286e-01  4.98616414e-01  4.99202896e-01
  5.00534115e-01  5.01877557e-01  5.03415304e-01  5.07635951e-01
  5.08117992e-01  5.09927175e-01  5.10277890e-01  5.14009217e-01
  5.14856356e-01  5.15529418e-01  5.17128279e-01  5.18926774e-01
  5.21676836e-01  5.23349949e-01  5.24210273e-01  5.24719021e-01
  5.27072811e-01  5.29270402e-01  5.29454778e-01  5.29555710e-01
  5.29986320e-01  5.30952820e-01  5.31748171e-01  5.32716472e-01
  5.33411574e-01  5.33418829e-01  5.35428162e-01  5.41209878e-01
  5.41835609e-01  5.47198486e-01  5.48030342e-01  5.51476415e-01
  5.52107598e-01  5.54393060e-01  5.54561878e-01  5.59259888e-01
  5.64725473e-01  5.64895775e-01  5.74208127e-01  5.74941720e-01
  5.76497442e-01  5.77721314e-01  5.86299935e-01  5.87372260e-01
  5.88078149e-01  5.89483849e-01  5.92350769e-01  5.92693255e-01
  5.93175017e-01  5.94906585e-01  5.97409262e-01  6.02261132e-01
  6.02651612e-01  6.02988262e-01  6.03190021e-01  6.14590807e-01
  6.18052881e-01  6.19551354e-01  6.19935930e-01  6.20634446e-01
  6.22975487e-01  6.25623671e-01  6.26173328e-01  6.26444585e-01
  6.27638656e-01  6.28177573e-01  6.28229711e-01  6.30938548e-01
  6.32527958e-01  6.37082199e-01  6.41447573e-01  6.42862080e-01
  6.50009861e-01  6.51269545e-01  6.51723063e-01  6.58075050e-01
  6.60717942e-01  6.63822374e-01  6.67209940e-01  6.80513386e-01
  6.87751210e-01  6.88374859e-01  6.94661092e-01  7.02351090e-01
  7.06838203e-01  7.11749803e-01  7.13185848e-01  7.13612622e-01
  7.16448531e-01  7.20580983e-01  7.21338357e-01  7.27856171e-01
  7.60407830e-01]

  warnings.warn(

2022-11-03 10:49:16,785:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16455598 -0.15778297 -0.15584259 -0.15453334 -0.14849395 -0.14315033
 -0.13549726 -0.13391493 -0.13092513 -0.12850628 -0.1265135  -0.12535711
 -0.12224866 -0.1193439  -0.11881186 -0.11873383 -0.11732448 -0.11569529
 -0.11314379 -0.10904988 -0.10743556 -0.1025979  -0.10251461 -0.10217005
 -0.10074024 -0.09667925 -0.09355669 -0.09277183 -0.09155345 -0.08912912
 -0.08654163 -0.08557394 -0.08167376 -0.07992157 -0.07899935 -0.0783477
 -0.0777423  -0.07165881 -0.07141679 -0.06979461 -0.0695772  -0.0670305
 -0.06532054 -0.06284141 -0.0558495  -0.05387225 -0.05320725 -0.05211233
 -0.05154854 -0.05042014 -0.05007438 -0.04802195 -0.042985   -0.03848683
 -0.0358297  -0.03378194 -0.03255749 -0.03067341 -0.02655675 -0.02367425
 -0.02304337 -0.0213852  -0.01833675 -0.01681467 -0.01595889 -0.0131883
 -0.01084359 -0.0107814  -0.00998209 -0.00943123 -0.00814051 -0.00666967
 -0.00637135 -0.00531709 -0.0050711  -0.00162262  0.004776    0.0064463
  0.00650589  0.00685774  0.00777641  0.00866011  0.01048657  0.01187997
  0.01264618  0.01423359  0.0153113   0.01689186  0.0182379   0.02046429
  0.0235247   0.03014092  0.03029067  0.03151567  0.03313989  0.03670066
  0.0375063   0.03884322  0.0397251   0.04036364  0.04624663  0.04719934
  0.05010679  0.05212244  0.05318683  0.0566189   0.05671034  0.05717231
  0.06067153  0.06337224  0.06412842  0.06708053  0.06746839  0.06899463
  0.07071534  0.07401867  0.07404729  0.07431167  0.07577905  0.07618448
  0.07784181  0.08010258  0.08220794  0.08267931  0.0829428   0.08414403
  0.08452838  0.08579855  0.0861617   0.08628654  0.08669903  0.08700396
  0.08898464  0.09011807  0.09093103  0.09296087  0.09354578  0.09473868
  0.09610742  0.09821659  0.10454076  0.10487273  0.10515482  0.1080247
  0.10946941  0.10977978  0.11027147  0.11407098  0.11588676  0.12051609
  0.12083601  0.12265716  0.12311345  0.12354525  0.12659852  0.12762
  0.13059643  0.13081086  0.13606004  0.13613556  0.13801754  0.13906093
  0.14126262  0.14356596  0.1444361   0.14672869  0.14720568  0.14817836
  0.14892235  0.15017333  0.15639726  0.1565849   0.15680308  0.15692252
  0.1570772   0.15899642  0.16359925  0.16369203  0.16557286  0.16652421
  0.16735154  0.17359178  0.17737772  0.17785204  0.17962142  0.18054704
  0.18138746  0.18235075  0.18256157  0.18339173  0.18526015  0.18661709
  0.18867369  0.18888297  0.18933407  0.19027121  0.19096162  0.19117123
  0.19145938  0.19324049  0.19490223  0.19502955  0.19883532  0.202083
  0.2030645   0.20404851  0.20568153  0.20629976  0.20686797  0.20692162
  0.2084808   0.20917771  0.20947736  0.20976179  0.20988336  0.21113262
  0.21179101  0.21248435  0.21323655  0.21371093  0.21531245  0.21599969
  0.21614883  0.21654662  0.21854467  0.21930623  0.21993208  0.22073415
  0.22156349  0.22274246  0.22278041  0.22720704  0.22830442  0.2332163
  0.23489719  0.23793795  0.23798222  0.23810334  0.23813025  0.23919514
  0.24121704  0.24121897  0.24382049  0.24426308  0.24438422  0.24579294
  0.24950079  0.25058249  0.25127984  0.25391784  0.25689539  0.25752722
  0.25919776  0.25975673  0.26054113  0.26199368  0.2627892   0.266113
  0.267596    0.2677555   0.26855543  0.26994063  0.27149926  0.2749072
  0.27572721  0.27668083  0.27723246  0.27891908  0.2795171   0.2852167
  0.28718837  0.28816986  0.288233    0.28957725  0.2926715   0.29343022
  0.29858306  0.29933116  0.30096634  0.30673595  0.30748607  0.30839589
  0.30881805  0.31003286  0.31204793  0.31354719  0.31468601  0.31478193
  0.31534462  0.31630826  0.31669164  0.31741234  0.32035988  0.32275089
  0.32295846  0.32296774  0.32610306  0.32690321  0.33016479  0.33063453
  0.3365534   0.33711034  0.33758261  0.3398276   0.3404029   0.34175345
  0.34317305  0.34431955  0.34441871  0.34906417  0.34943426  0.35069074
  0.35178318  0.35386561  0.35507485  0.35520169  0.36000921  0.36127103
  0.3634267   0.36498604  0.36632394  0.36904074  0.37077548  0.37127057
  0.37234057  0.37614403  0.37671164  0.37869298  0.38032491  0.38262657
  0.3832484   0.38357193  0.38882382  0.39017232  0.39029141  0.39819147
  0.40211042  0.40405046  0.40446996  0.40537587  0.40637906  0.40858866
  0.40871353  0.41061641  0.41317601  0.41370397  0.41378037  0.414047
  0.41418434  0.41476985  0.41503963  0.41568815  0.41599707  0.417041
  0.4223422   0.42328665  0.42580445  0.42595621  0.42666557  0.43313382
  0.43768636  0.43909392  0.44396685  0.44651809  0.44972773  0.45021357
  0.4506329   0.45230057  0.4542367   0.45534688  0.45553455  0.45820875
  0.45889525  0.46279305  0.47159525  0.47180697  0.47200251  0.4739427
  0.47546551  0.47595654  0.47685123  0.4769996   0.47979505  0.48010894
  0.48079795  0.48266795  0.48307097  0.4893361   0.4914372   0.49191037
  0.49506786  0.49519648  0.49839749  0.4984186   0.49932992  0.49953322
  0.49953608  0.50192336  0.50234478  0.5031838   0.50570424  0.50571781
  0.50651754  0.51438328  0.51494117  0.51691389  0.52289214  0.52718604
  0.52880228  0.52916953  0.52934278  0.53108128  0.53381469  0.53774012
  0.538041    0.53940862  0.54091367  0.5427898   0.54846781  0.5507628
  0.5509917   0.55229711  0.55420798  0.55969835  0.56250878  0.56257692
  0.56338437  0.56527307  0.5654313   0.56582798  0.5673238   0.56846705
  0.57371191  0.57424081  0.57468846  0.57498406  0.57529896  0.5771116
  0.58227578  0.58331974  0.58434484  0.5845451   0.5862193   0.58790336
  0.58892423  0.59106725  0.59125056  0.59128576  0.59148063  0.59328897
  0.59769325  0.59789754  0.60243083  0.60455956  0.60617333  0.60658932
  0.60880277  0.6094485   0.60958097  0.6109957   0.61345492  0.6214292
  0.62197663  0.62342066  0.6243093   0.62498979  0.62772984  0.62935509
  0.6302732   0.64229785  0.64618179  0.64751414  0.64967769  0.65241776
  0.6552164   0.65620149  0.65638687  0.66185991  0.66244199  0.66724164
  0.66881012  0.67133324  0.67631782  0.67723411  0.68396019  0.6849062
  0.69769604  0.69861703  0.69937474  0.70666365  0.72730881  0.73101792
  0.73268752]

  warnings.warn(

2022-11-03 10:49:16,802:INFO:Calculating mean and std
2022-11-03 10:49:16,802:INFO:Creating metrics dataframe
2022-11-03 10:49:16,802:INFO:Uploading results into container
2022-11-03 10:49:16,802:INFO:Uploading model into container now
2022-11-03 10:49:16,802:INFO:master_model_container: 2
2022-11-03 10:49:16,802:INFO:display_container: 2
2022-11-03 10:49:16,802:INFO:LinearRegression(n_jobs=-1)
2022-11-03 10:49:16,802:INFO:create_model() successfully completed......................................
2022-11-03 10:49:17,063:ERROR:create_model() for LinearRegression(n_jobs=-1) raised an exception or returned all 0.0:
2022-11-03 10:49:17,063:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:49:17,063:INFO:Initializing Lasso Regression
2022-11-03 10:49:17,063:INFO:Total runtime is 0.6858992377916971 minutes
2022-11-03 10:49:17,063:INFO:SubProcess create_model() called ==================================
2022-11-03 10:49:17,072:INFO:Initializing create_model()
2022-11-03 10:49:17,072:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:49:17,072:INFO:Checking exceptions
2022-11-03 10:49:17,073:INFO:Importing libraries
2022-11-03 10:49:17,073:INFO:Copying training dataset
2022-11-03 10:49:17,090:INFO:Defining folds
2022-11-03 10:49:17,090:INFO:Declaring metric variables
2022-11-03 10:49:17,093:INFO:Importing untrained model
2022-11-03 10:49:17,093:INFO:Lasso Regression Imported successfully
2022-11-03 10:49:17,093:INFO:Starting cross validation
2022-11-03 10:49:17,093:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:49:21,151:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.08810503 -0.08202341 -0.08130011 -0.07682153 -0.0759418  -0.0752185
 -0.07492918 -0.07014951 -0.06784671 -0.06102414 -0.05278439 -0.04757663
 -0.04366491 -0.03872293 -0.03859592 -0.03801728 -0.02742767 -0.02424514
 -0.02242927 -0.02134605 -0.01860928 -0.01425182 -0.01368494 -0.01062943
 -0.00788088 -0.00353519 -0.00275547  0.00427058  0.01447086  0.01619502
  0.02803604  0.03111856  0.03531371  0.04032628  0.04556105  0.04758282
  0.04991852  0.05439362  0.05598488  0.05716917  0.05828768  0.0624863
  0.06395644  0.06401526  0.06573942  0.06697077  0.06697665  0.06730127
  0.06877729  0.06958295  0.06970996  0.07293127  0.07351578  0.07688415
  0.07872008  0.08418534  0.08447468  0.08482282  0.08592128  0.08610363
  0.08840991  0.08940487  0.09096914  0.09143601  0.09266388  0.09597342
  0.0982551   0.09908535  0.09993219  0.09994396  0.10012391  0.10087663
  0.10171865  0.10183043  0.10360512  0.10461773  0.1056563   0.10567742
  0.10807782  0.10849414  0.11082047  0.11258583  0.11497686  0.11507098
  0.11572716  0.11660688  0.11682803  0.11789118  0.11812997  0.11980706
  0.12154298  0.12303318  0.12606863  0.12694835  0.12853615  0.12868081
  0.13220319  0.13238074  0.1348635   0.13517876  0.13519641  0.13601731
  0.13617133  0.13915624  0.14123444  0.1427305   0.14514614  0.14636814
  0.14827572  0.14832279  0.15020337  0.15069858  0.1509208   0.15192164
  0.15275431  0.15327654  0.15494427  0.1565743   0.15694945  0.15709412
  0.15880544  0.16000496  0.1600285   0.16016381  0.16191496  0.16248772
  0.16300059  0.16311584  0.16473411  0.16503172  0.16641708  0.16664169
  0.16722622  0.16740618  0.16967956  0.1705887   0.1717048   0.17241527
  0.1731514   0.17373005  0.1748991   0.17580476  0.17633394  0.17662326
  0.17705028  0.17770888  0.17820756  0.17898729  0.17907553  0.18014217
  0.18036918  0.18113713  0.18171578  0.18313885  0.18404211  0.18691178
  0.18703879  0.18778215  0.18822549  0.18849021  0.18925577  0.18997907
  0.19008147  0.19022131  0.19037079  0.1906553   0.19108233  0.19272173
  0.19306987  0.19330277  0.19432475  0.19529513  0.19558552  0.19654171
  0.19716394  0.20053337  0.20184121  0.20303378  0.20545771  0.20788752
  0.21054436  0.21093608  0.211323    0.21262254  0.21485475  0.21531466
  0.21817498  0.22018845  0.2221054   0.22353329  0.2238178   0.22405069
  0.22519274  0.22589251  0.22628772  0.22734847  0.22808943  0.22848222
  0.22936783  0.2293796   0.23084384  0.23249394  0.232641    0.23516601
  0.23541656  0.23558234  0.23561763  0.2361551   0.23863785  0.23960823
  0.24057727  0.24117943  0.24180862  0.2429413   0.24333758  0.24432907
  0.24582622  0.24811135  0.24848063  0.2493427   0.24960155  0.25183136
  0.25224421  0.25352851  0.25495986  0.25519384  0.25677333  0.25681103
  0.25732255  0.25741909  0.25887157  0.25918548  0.26198109  0.26241748
  0.2645192   0.26480852  0.2659987   0.26784639  0.26920476  0.2695882
  0.27057489  0.2732047   0.27436787  0.27498528  0.27552515  0.27581447
  0.27601903  0.27730333  0.27940505  0.28185491  0.28258997  0.28280282
  0.28363548  0.28513263  0.28667576  0.28696508  0.28768838  0.28845045
  0.2888256   0.28899032  0.28913498  0.28927964  0.29089551  0.29262795
  0.29304669  0.29465666  0.29579524  0.2961268   0.29631852  0.29680786
  0.29710306  0.2976817   0.29815445  0.298405    0.29854966  0.29886011
  0.29945987  0.30202739  0.30273304  0.3027507   0.30332933  0.30361866
  0.30376331  0.30390798  0.30395851  0.30405263  0.30448661  0.30535458
  0.3056439   0.30700226  0.3075162   0.30791487  0.30835474  0.30844887
  0.30867     0.30903927  0.31081986  0.31127976  0.31223115  0.31270872
  0.31340261  0.31523961  0.31667445  0.31772236  0.31895264  0.31900318
  0.31953128  0.31987703  0.32083323  0.32097789  0.32262556  0.32267612
  0.32408153  0.32499308  0.32671135  0.32733706  0.32892832  0.32937167
  0.33108993  0.33199319  0.33214722  0.33257182  0.33284003  0.33292935
  0.33551558  0.3364953   0.33666696  0.33781837  0.33854167  0.34018587
  0.34040703  0.34240285  0.34252638  0.34275688  0.34591     0.3461052
  0.3461699   0.34641805  0.34665441  0.34942889  0.34984523  0.35408155
  0.35463078  0.35498828  0.35685122  0.3606418   0.36074769  0.36667636
  0.36868048  0.369165    0.3708798   0.37191943  0.37201943  0.37219109
  0.37257213  0.37412463  0.37421287  0.3747915   0.37511959  0.375149
  0.37540303  0.37736946  0.37737774  0.37739539  0.37754005  0.37969231
  0.38055198  0.3825537   0.38357807  0.38594559  0.38645604  0.38668065
  0.38708762  0.38822726  0.38898345  0.38944097  0.38996318  0.39015839
  0.3913039   0.39133331  0.39244006  0.39272697  0.39330802  0.39360669
  0.39378665  0.39460996  0.39755371  0.39792539  0.39850991  0.40113492
  0.4017018   0.40288848  0.40512311  0.4057488   0.4068002   0.40766815
  0.40769756  0.40948644  0.41167399  0.4141038   0.41426022  0.41609964
  0.41768501  0.41807192  0.41872116  0.41939739  0.42035948  0.42090872
  0.42105925  0.42129564  0.42306097  0.42479689  0.42663631  0.42691384
  0.42704676  0.43247564  0.43421746  0.43687776  0.43698366  0.43890302
  0.44060604  0.44349925  0.4438744   0.44465653  0.44504933  0.44552448
  0.44668765  0.44738153  0.44767087  0.44784493  0.44841181  0.44853294
  0.44885755  0.44982312  0.45073814  0.45088281  0.45121089  0.45152962
  0.45160611  0.45182484  0.45210828  0.45243289  0.45377013  0.45563306
  0.45592238  0.45615287  0.45676094  0.45768183  0.459406    0.46007047
  0.46201576  0.46258852  0.46332358  0.46373993  0.46375756  0.4638787
  0.46433622  0.46445145  0.46457258  0.46790565  0.46968864  0.47255831
  0.47281821  0.47312519  0.47402257  0.47416722  0.47648767  0.48097215
  0.48193181  0.48196711  0.48226232  0.4830891   0.48443223  0.48703022
  0.48800754  0.49138768  0.49471487  0.49772919  0.50060476  0.50237597
  0.50366027  0.50860224  0.51132727  0.5135148   0.51495553  0.51568471
  0.51727598  0.5187167 ]

  warnings.warn(

2022-11-03 10:49:21,167:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.17171363e-02 -8.06973340e-02 -7.36368784e-02 -7.30541350e-02
 -6.68570992e-02 -6.38544625e-02 -5.88229246e-02 -5.71635809e-02
 -5.67833089e-02 -5.62112635e-02 -4.86141533e-02 -4.69548096e-02
 -4.34369199e-02 -3.98836560e-02 -2.78484494e-02 -2.53717747e-02
 -2.09904707e-02 -1.65309727e-02 -1.62256255e-02 -8.14863669e-03
 -2.96813971e-03 -5.02163009e-04  7.73112793e-05  5.98297256e-03
  8.10405700e-03  1.13980627e-02  1.23289619e-02  1.40171472e-02
  1.78831982e-02  2.32982772e-02  2.39913364e-02  2.62655342e-02
  3.28922000e-02  3.78669454e-02  4.26606377e-02  4.49520884e-02
  4.51405760e-02  4.52829969e-02  4.55175845e-02  4.99054267e-02
  5.12775557e-02  5.14404931e-02  5.62481580e-02  5.67955105e-02
  5.89207547e-02  6.32345683e-02  6.62264959e-02  6.72463038e-02
  6.75376783e-02  6.84858304e-02  6.87031706e-02  7.03550799e-02
  7.06752960e-02  7.43207266e-02  7.47717693e-02  7.72698455e-02
  8.03860450e-02  8.32150519e-02  8.35664880e-02  8.46002685e-02
  8.78663233e-02  9.05380349e-02  9.17174999e-02  9.26344431e-02
  9.47843580e-02  9.49407460e-02  9.67490378e-02  9.92405869e-02
  9.95845941e-02  1.02306543e-01  1.02352632e-01  1.04054779e-01
  1.04652402e-01  1.07540569e-01  1.08400709e-01  1.08617164e-01
  1.10092163e-01  1.12266760e-01  1.12696395e-01  1.14153262e-01
  1.14590305e-01  1.14832332e-01  1.14942642e-01  1.16527050e-01
  1.17941103e-01  1.18940400e-01  1.18971609e-01  1.19960208e-01
  1.20304209e-01  1.20595578e-01  1.22329847e-01  1.23243510e-01
  1.23409705e-01  1.23555401e-01  1.24554686e-01  1.26089753e-01
  1.27801713e-01  1.28586938e-01  1.28712112e-01  1.28960672e-01
  1.29461067e-01  1.29752436e-01  1.34180714e-01  1.34503309e-01
  1.35725577e-01  1.35918252e-01  1.36901778e-01  1.36955296e-01
  1.38458252e-01  1.38735654e-01  1.39243478e-01  1.39901140e-01
  1.39997478e-01  1.42349870e-01  1.42707839e-01  1.43223993e-01
  1.43237960e-01  1.43884040e-01  1.45038834e-01  1.45963195e-01
  1.46944354e-01  1.48151759e-01  1.51303366e-01  1.51605439e-01
  1.51950325e-01  1.54295277e-01  1.54586668e-01  1.55383477e-01
  1.56015579e-01  1.56506161e-01  1.56687220e-01  1.57038656e-01
  1.57667482e-01  1.58150636e-01  1.60212544e-01  1.60481609e-01
  1.61378036e-01  1.63541038e-01  1.63672756e-01  1.65872034e-01
  1.67478740e-01  1.67681217e-01  1.69222810e-01  1.69226987e-01
  1.69802296e-01  1.70431122e-01  1.70441826e-01  1.71116741e-01
  1.71280570e-01  1.71418814e-01  1.72651808e-01  1.75721941e-01
  1.75924406e-01  1.77201100e-01  1.78753402e-01  1.78859547e-01
  1.79037353e-01  1.79095025e-01  1.80081838e-01  1.82743742e-01
  1.83501011e-01  1.84381667e-01  1.87287972e-01  1.87701245e-01
  1.87963778e-01  1.87992614e-01  1.88261685e-01  1.89147409e-01
  1.89502108e-01  1.89910337e-01  1.90081583e-01  1.90604276e-01
  1.92150040e-01  1.93730288e-01  1.93787080e-01  1.94661209e-01
  1.97021019e-01  1.97327268e-01  1.97486040e-01  1.97955215e-01
  1.98695237e-01  2.00332277e-01  2.00496105e-01  2.03144032e-01
  2.04948158e-01  2.07165569e-01  2.10718855e-01  2.10943619e-01
  2.14955354e-01  2.15299350e-01  2.18530031e-01  2.18725073e-01
  2.18735777e-01  2.19467485e-01  2.19861735e-01  2.20949918e-01
  2.21095603e-01  2.21812437e-01  2.22630664e-01  2.23270200e-01
  2.23841360e-01  2.24233221e-01  2.25544397e-01  2.27693422e-01
  2.28507483e-01  2.28653179e-01  2.30195669e-01  2.30284566e-01
  2.30867315e-01  2.30933909e-01  2.33936546e-01  2.34281433e-01
  2.35024735e-01  2.40113066e-01  2.41278563e-01  2.41469435e-01
  2.46690373e-01  2.47127438e-01  2.47766968e-01  2.48181735e-01
  2.49167048e-01  2.50119366e-01  2.55864473e-01  2.55879342e-01
  2.55945948e-01  2.56550093e-01  2.57459585e-01  2.59495937e-01
  2.60104253e-01  2.61639314e-01  2.63010552e-01  2.65135796e-01
  2.65433714e-01  2.66076519e-01  2.66108630e-01  2.66623872e-01
  2.68273414e-01  2.69168037e-01  2.69260215e-01  2.69370525e-01
  2.70443845e-01  2.70885069e-01  2.71293292e-01  2.72341936e-01
  2.75024351e-01  2.76278741e-01  2.76510060e-01  2.77384183e-01
  2.78161969e-01  2.78292785e-01  2.80304455e-01  2.81104544e-01
  2.81150628e-01  2.81195815e-01  2.81882336e-01  2.81935853e-01
  2.82791828e-01  2.84550767e-01  2.84824905e-01  2.86359070e-01
  2.88321388e-01  2.88889257e-01  2.90118976e-01  2.91724813e-01
  2.91750375e-01  2.92094382e-01  2.92737187e-01  2.93601492e-01
  2.94038545e-01  2.96176866e-01  2.96535731e-01  2.98093095e-01
  3.00099698e-01  3.00210904e-01  3.00323593e-01  3.00939338e-01
  3.01230712e-01  3.01376397e-01  3.01667771e-01  3.02833264e-01
  3.03270328e-01  3.03273592e-01  3.03280141e-01  3.04144446e-01
  3.04890132e-01  3.05153550e-01  3.05309938e-01  3.06104972e-01
  3.08156182e-01  3.10916791e-01  3.11360405e-01  3.13002495e-01
  3.13016462e-01  3.15508006e-01  3.15511275e-01  3.15844568e-01
  3.16382130e-01  3.16673498e-01  3.16804325e-01  3.17120376e-01
  3.19398733e-01  3.20016857e-01  3.20159277e-01  3.20187234e-01
  3.20596336e-01  3.20834193e-01  3.20929623e-01  3.20993845e-01
  3.23073011e-01  3.23232674e-01  3.23470536e-01  3.23801445e-01
  3.24671397e-01  3.24784982e-01  3.25747992e-01  3.26298641e-01
  3.26724980e-01  3.29233782e-01  3.29987777e-01  3.30033866e-01
  3.30232177e-01  3.32713012e-01  3.32918763e-01  3.33153340e-01
  3.33437280e-01  3.33490797e-01  3.33576426e-01  3.35893444e-01
  3.35956763e-01  3.40370178e-01  3.40767703e-01  3.41265714e-01
  3.42725855e-01  3.44019785e-01  3.44381913e-01  3.44630468e-01
  3.44687260e-01  3.46094775e-01  3.46478310e-01  3.47085752e-01
  3.47996135e-01  3.49591257e-01  3.49644775e-01  3.50888462e-01
  3.51364175e-01  3.53663060e-01  3.54822009e-01  3.54878790e-01
  3.55468088e-01  3.55702665e-01  3.56225358e-01  3.56268172e-01
  3.59511934e-01  3.60251071e-01  3.60336689e-01  3.60339952e-01
  3.61896426e-01  3.62670948e-01  3.62908799e-01  3.64948426e-01
  3.65751784e-01  3.67375732e-01  3.67457206e-01  3.68459766e-01
  3.69188200e-01  3.69916633e-01  3.70022778e-01  3.70111681e-01
  3.72787553e-01  3.73054251e-01  3.73977721e-01  3.75931697e-01
  3.76340828e-01  3.76685714e-01  3.77530985e-01  3.77865168e-01
  3.78007589e-01  3.78828200e-01  3.80423312e-01  3.80984665e-01
  3.81780572e-01  3.83067084e-01  3.84779934e-01  3.88788406e-01
  3.89296230e-01  3.89576895e-01  3.90006531e-01  3.90884814e-01
  3.92903012e-01  3.93805075e-01  3.94516278e-01  3.98460517e-01
  3.99156840e-01  3.99448209e-01  4.00073772e-01  4.00222731e-01
  4.00357723e-01  4.00816172e-01  4.00997253e-01  4.01132223e-01
  4.01800612e-01  4.03082947e-01  4.04668256e-01  4.05421372e-01
  4.06668322e-01  4.09219916e-01  4.13444843e-01  4.13960085e-01
  4.14691793e-01  4.14951051e-01  4.15697639e-01  4.16244991e-01
  4.17925742e-01  4.20103603e-01  4.20942346e-01  4.21120152e-01
  4.22558887e-01  4.22903773e-01  4.26581326e-01  4.27544336e-01
  4.28013511e-01  4.28429168e-01  4.29562533e-01  4.30241625e-01
  4.31310791e-01  4.32362704e-01  4.32416222e-01  4.33943837e-01
  4.34124918e-01  4.35411408e-01  4.35543125e-01  4.35613896e-01
  4.37205732e-01  4.37586004e-01  4.41071794e-01  4.41740161e-01
  4.43591278e-01  4.43690885e-01  4.43779788e-01  4.44465407e-01
  4.44788886e-01  4.45751896e-01  4.46188961e-01  4.46607893e-01
  4.47354459e-01  4.47937197e-01  4.48374261e-01  4.48431053e-01
  4.49294457e-01  4.50694553e-01  4.51344787e-01  4.51579364e-01
  4.54382787e-01  4.54482394e-01  4.55928557e-01  4.55974646e-01
  4.56589494e-01  4.57182958e-01  4.58550922e-01  4.60526305e-01
  4.60903325e-01  4.61397159e-01  4.61777431e-01  4.64399796e-01
  4.64513381e-01  4.64545492e-01  4.64826157e-01  4.68031259e-01
  4.71563116e-01  4.72291550e-01  4.72380453e-01  4.75180602e-01
  4.75617666e-01  4.77511597e-01  4.79114160e-01  4.83474057e-01
  4.86131784e-01  4.88651268e-01  4.89649685e-01  4.90040661e-01
  4.92058858e-01  4.92876195e-01  4.96138090e-01  5.03859489e-01
  5.04026569e-01  5.05405236e-01  5.11445895e-01  5.13364486e-01
  5.42040098e-01]

  warnings.warn(

2022-11-03 10:49:21,182:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.08401663 -0.08087889 -0.07214788 -0.06800965 -0.0551152  -0.04933996
 -0.03720336 -0.02992739 -0.02864409 -0.02042829 -0.01852343 -0.01717948
 -0.01374867 -0.00875176 -0.0058212  -0.00193034 -0.0017586   0.00179351
  0.00372359  0.00449167  0.00859445  0.01005973  0.01662827  0.02051834
  0.02368681  0.02474796  0.03435813  0.0374297   0.03837504  0.04361932
  0.04419518  0.04610004  0.04785837  0.05025404  0.05137033  0.05259848
  0.05309321  0.05503825  0.05851001  0.05918671  0.06630592  0.07017629
  0.07097512  0.08119109  0.0861628   0.08880581  0.09118019  0.09210978
  0.09227206  0.09269589  0.0944046   0.09455112  0.09475751  0.09554057
  0.09606603  0.09616293  0.09840574  0.10176564  0.10190193  0.10474741
  0.10536897  0.10837987  0.10995151  0.11121904  0.11290176  0.11359895
  0.11421026  0.11450331  0.11488223  0.11560071  0.11628687  0.1190512
  0.12532117  0.1257253   0.12634686  0.12683687  0.12788383  0.13146116
  0.13274446  0.13676137  0.13738293  0.13744832  0.13866543  0.13982741
  0.14067113  0.142253    0.14262168  0.1477612   0.14916501  0.14953922
  0.15150946  0.15157092  0.15210031  0.1527329   0.15281798  0.15602741
  0.15607231  0.15611721  0.15873974  0.15876494  0.16237301  0.16374216
  0.16387925  0.16500578  0.16587391  0.16603147  0.16748572  0.16880446
  0.16891161  0.16972932  0.17100869  0.17185241  0.17188786  0.17194222
  0.17203439  0.17299312  0.17372576  0.17548409  0.1763987   0.17651531
  0.17707069  0.18050779  0.18096628  0.18182575  0.18299877  0.18343835
  0.1835384   0.18387162  0.18411426  0.184286    0.18525734  0.18530145
  0.18686915  0.1876459   0.18848095  0.19003603  0.19013766  0.19121851
  0.19153677  0.19176915  0.19216936  0.19249154  0.19277437  0.19384182
  0.19389144  0.1939466   0.19403798  0.19412385  0.19427589  0.1948163
  0.19488642  0.19538193  0.1956293   0.19601453  0.19701895  0.19718596
  0.19760979  0.19838811  0.19870637  0.1994138   0.20003144  0.20035915
  0.20171336  0.20212693  0.20225457  0.20280915  0.20367415  0.20382067
  0.20410271  0.20506773  0.20522057  0.20595713  0.20618954  0.2085135
  0.20938323  0.20938401  0.21229329  0.21507731  0.21515925  0.21599194
  0.2171996   0.21818512  0.21835136  0.21867592  0.21888232  0.21902964
  0.22015065  0.22292995  0.22326238  0.22670974  0.22708236  0.22802218
  0.22889111  0.22976554  0.23096298  0.23334287  0.23465138  0.2346821
  0.23663738  0.23766781  0.23993269  0.24227714  0.24407957  0.24445378
  0.24596554  0.24670762  0.24703062  0.24718738  0.25153833  0.25412068
  0.25733327  0.2573695   0.25813759  0.25974939  0.26080975  0.26120364
  0.26131551  0.26244756  0.26263898  0.26288714  0.26369461  0.2637663
  0.26411451  0.26420589  0.26449894  0.264792    0.26493853  0.26635811
  0.2664684   0.266752    0.26697888  0.26728218  0.26792343  0.26916735
  0.27011742  0.27067753  0.27086424  0.27100683  0.27129988  0.27244138
  0.27282582  0.27500798  0.2767151   0.2770184   0.27731224  0.27768644
  0.27775183  0.27854041  0.27900914  0.27901464  0.27933764  0.27944399
  0.28132915  0.28138509  0.28151113  0.28285037  0.28314342  0.2837902
  0.28538624  0.28774645  0.28795285  0.28807415  0.28861458  0.28891237
  0.28952368  0.29381474  0.29427402  0.29440086  0.29548721  0.29612847
  0.29691624  0.29739207  0.29980112  0.29998388  0.30061489  0.30243468
  0.30275294  0.30287426  0.30316731  0.30345013  0.30404648  0.30433954
  0.30434346  0.30463259  0.30477912  0.30492565  0.30582924  0.30704714
  0.30863375  0.31152412  0.31189832  0.31376694  0.31431761  0.31488954
  0.31532361  0.31562218  0.31603576  0.31607122  0.31643517  0.31729937
  0.31892062  0.31909236  0.31971944  0.32031107  0.3211343   0.32154788
  0.32301315  0.32561049  0.32772332  0.32882385  0.32902159  0.32914211
  0.33235628  0.33351354  0.33408468  0.33745955  0.33748003  0.33796923
  0.33833871  0.33838913  0.33849548  0.33878774  0.33919267  0.34027351
  0.34056184  0.3411322   0.34124325  0.34178921  0.34261323  0.34354754
  0.34902027  0.3499593   0.35339561  0.35344052  0.35352166  0.35419916
  0.35503264  0.35509801  0.35591181  0.35718485  0.35766541  0.35987829
  0.36024225  0.3605802   0.36147513  0.36160589  0.3621416   0.36381406
  0.36560312  0.36572917  0.36673437  0.36697702  0.36739691  0.36743157
  0.36955937  0.37027705  0.37038812  0.37097423  0.37208579  0.37213621
  0.3728791   0.37467288  0.37555205  0.37569857  0.37643121  0.37790594
  0.37832583  0.37833608  0.37862913  0.37953272  0.37994708  0.38029135
  0.38042212  0.3805592   0.38066004  0.38278153  0.38340388  0.38589406
  0.38648097  0.38741529  0.38815343  0.38862295  0.38961871  0.39054277
  0.39103829  0.39118482  0.39166458  0.39254927  0.39294315  0.39298886
  0.39328191  0.39377663  0.39489844  0.39633851  0.39688444  0.39711684
  0.39816775  0.40033492  0.4038319   0.4077527   0.40808121  0.40819229
  0.4087327   0.41117325  0.41424484  0.41433622  0.41467969  0.41493257
  0.41508382  0.41510903  0.41518466  0.41610951  0.41716041  0.41778199
  0.41804511  0.41855006  0.4189999   0.42005079  0.42138924  0.42261267
  0.4226883   0.42337524  0.42471921  0.42496184  0.42560861  0.42639718
  0.42806414  0.42812479  0.42821066  0.43042907  0.4318841   0.43217715
  0.43348567  0.4335668   0.43382442  0.43741199  0.43789175  0.44171642
  0.44481401  0.44523861  0.44573886  0.44668342  0.44749719  0.44837636
  0.44940205  0.44954857  0.44969511  0.45059396  0.45189302  0.453055
  0.45718299  0.45918396  0.46121015  0.46171038  0.46229651  0.46283691
  0.46346871  0.46389805  0.46522706  0.46537358  0.46795515  0.46857671
  0.46859718  0.47219974  0.47278584  0.47307889  0.47442286  0.47869737
  0.47899042  0.4791472   0.48195643  0.48222427  0.48447261  0.48728186
  0.48865101  0.48896456  0.49168791  0.49204637  0.49573003  0.49774598
  0.50276337  0.50425386  0.50477932  0.51228768  0.5179164   0.52178677
  0.52453536  0.54130018  0.54186108  0.54994532]

  warnings.warn(

2022-11-03 10:49:21,246:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.0821481  -0.081414   -0.06731932 -0.06702568 -0.06541067 -0.06507759
 -0.06463713 -0.06222593 -0.05335467 -0.05262058 -0.0496221  -0.04922107
 -0.04804651 -0.04490122 -0.04471496 -0.04058136 -0.04024829 -0.02808492
 -0.02552689 -0.02170959 -0.01478054 -0.0093373  -0.00596045  0.00468438
  0.00670547  0.00872151  0.0179485   0.01945613  0.02421644  0.03126965
  0.0344376   0.03591167  0.0369671   0.04143377  0.04485088  0.04512691
  0.04914808  0.04992665  0.05112973  0.0512086   0.05413995  0.05747654
  0.06041377  0.0640616   0.06467153  0.06726312  0.07234476  0.07345805
  0.07593051  0.07663692  0.07867561  0.08096852  0.08213133  0.08738245
  0.08994129  0.09882934  0.10025727  0.10359552  0.10399654  0.1056963
  0.10640774  0.10676347  0.10693293  0.10722657  0.10981228  0.11180064
  0.11194745  0.11391821  0.11392324  0.11585958  0.11612553  0.11644686
  0.11670777  0.11800567  0.11811389  0.12164763  0.12165937  0.12310575
  0.12486758  0.12618897  0.12662941  0.12714204  0.13058182  0.1312144
  0.13200974  0.13235455  0.13674237  0.13783723  0.13813758  0.13903443
  0.13911413  0.14197753  0.14244065  0.14325277  0.14656167  0.14666401
  0.14923965  0.14988315  0.15233963  0.15329272  0.15434898  0.15504364
  0.15563176  0.15647324  0.15786678  0.15860088  0.16040884  0.16083252
  0.16260022  0.16423203  0.16480252  0.16513477  0.16532689  0.1658353
  0.16786226  0.16788574  0.16933549  0.16935394  0.17111577  0.17125085
  0.17409581  0.17611185  0.17684175  0.17787452  0.17820676  0.17822519
  0.17855827  0.17910025  0.17929658  0.17937122  0.18002564  0.18149887
  0.18173126  0.18195778  0.18296707  0.186406    0.18674073  0.18726593
  0.18829366  0.19248769  0.1933921   0.19348857  0.19365804  0.193726
  0.19431327  0.1944601   0.19499619  0.19713641  0.19755422  0.19769013
  0.19784198  0.19871786  0.20036057  0.20055855  0.20064331  0.20093107
  0.2016979   0.20194789  0.20201501  0.20207206  0.20215092  0.20267612
  0.20325754  0.20475928  0.2054657   0.20549843  0.20568048  0.20712015
  0.20720909  0.21052973  0.21064802  0.21101633  0.21138212  0.21138378
  0.21196436  0.21255668  0.21320771  0.21382184  0.21443847  0.2148328
  0.21518936  0.21570197  0.21714918  0.2176341   0.21946977  0.21966107
  0.2206888   0.22148582  0.22163685  0.22193049  0.22237094  0.22362602
  0.22629143  0.22681663  0.22794505  0.22882093  0.22917664  0.23026565
  0.2343875   0.23468114  0.23478432  0.23699166  0.2375328   0.24002873
  0.24032237  0.24038026  0.24138451  0.24229311  0.24297183  0.24333928
  0.24599296  0.24701399  0.24869192  0.2491089   0.24933037  0.25299499
  0.25372239  0.25956162  0.25962453  0.26003647  0.2609736   0.26146522
  0.26389321  0.26544531  0.26624736  0.26628176  0.26658044  0.26680108
  0.26746805  0.26754189  0.26808806  0.26810568  0.26912249  0.26957974
  0.2701385   0.27035244  0.2707929   0.27317641  0.27356065  0.27385512
  0.27483086  0.27595424  0.27614049  0.27653144  0.27668834  0.27966416
  0.27968682  0.28178172  0.28215588  0.282369    0.28238075  0.28244952
  0.28290677  0.28375246  0.2839622   0.28420466  0.28458304  0.28463338
  0.28464512  0.28511998  0.28628279  0.28629957  0.28654204  0.28706301
  0.28712932  0.28742295  0.28769981  0.2882938   0.28888694  0.28909584
  0.28977207  0.29027461  0.29095919  0.29140973  0.29218327  0.29262372
  0.29277054  0.29291736  0.29365146  0.29390567  0.29429661  0.29499549
  0.29507602  0.29532856  0.29591584  0.2963563   0.2973446   0.29753086
  0.29770116  0.2985586   0.29885223  0.29929269  0.29943951  0.29991436
  0.30017361  0.30139347  0.30149078  0.30212841  0.3026922   0.3040035
  0.30582658  0.30781997  0.30846346  0.30959354  0.3107681   0.31100553
  0.31135539  0.31191918  0.31203327  0.31276736  0.31332026  0.31355181
  0.31400904  0.31451243  0.31532539  0.31690097  0.31767953  0.31842455
  0.32004627  0.32054798  0.32350702  0.32563631  0.32585109  0.32600883
  0.3261053   0.32718843  0.32931267  0.32975901  0.33103509  0.33296053
  0.33334477  0.33345133  0.3342022   0.33589105  0.33632647  0.3364498
  0.33800357  0.33813864  0.33894574  0.34359949  0.34560966  0.34686813
  0.34886235  0.34961323  0.35143714  0.35183818  0.35244224  0.35262848
  0.35630402  0.35700455  0.3592295   0.36348224  0.36485898  0.3652038
  0.36676764  0.36698829  0.36746231  0.36764856  0.36823079  0.36845648
  0.37005472  0.37020657  0.37083998  0.3720758   0.37217816  0.37343661
  0.3740415   0.37580838  0.37664986  0.37794942  0.38069872  0.38106032
  0.38149574  0.38170381  0.38297989  0.38486086  0.38515449  0.38532395
  0.38562263  0.38617048  0.38635673  0.38643644  0.38709083  0.38770077
  0.38910688  0.38940638  0.39020258  0.39034351  0.3904853   0.39091402
  0.39256342  0.39427409  0.39575907  0.39648729  0.39851507  0.40236007
  0.40345576  0.40499193  0.40503135  0.4066405   0.40727895  0.40783268
  0.40788888  0.40917586  0.40965071  0.41035628  0.41141841  0.41224816
  0.41255858  0.41298226  0.41782731  0.41873089  0.41921077  0.42114796
  0.42251966  0.42338967  0.42349119  0.42380161  0.42413469  0.42676066
  0.42683952  0.42702075  0.42712813  0.43067445  0.43087748  0.43108052
  0.43158893  0.43409073  0.43415282  0.44039223  0.44081591  0.44209786
  0.44229502  0.44293347  0.44295026  0.44302912  0.44384291  0.44523142
  0.44537822  0.44552505  0.4470159   0.44846731  0.44909402  0.44976605
  0.45501212  0.45521516  0.45526044  0.45603986  0.45653652  0.45859199
  0.46035384  0.46168695  0.46231366  0.46240931  0.46577525  0.46645817
  0.46939456  0.46954139  0.47106077  0.47107166  0.47151212  0.47283351
  0.4739124   0.47472539  0.47707449  0.48273837  0.48347247  0.48447757
  0.48715974  0.48766228  0.4886506   0.48867325  0.48938469  0.49177324
  0.49232109  0.49721143  0.49867375  0.49869641  0.50102288  0.50219744
  0.50347937  0.50599795  0.50947632  0.51526438  0.51849441  0.51952214
  0.53523186]

  warnings.warn(

2022-11-03 10:49:21,246:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.08327158 -0.08297938 -0.07725655 -0.07211814 -0.07056055 -0.06939172
 -0.0685151  -0.06822289 -0.06720016 -0.06697972 -0.06668752 -0.06661575
 -0.06559302 -0.06308448 -0.05648503 -0.04489327 -0.03887823 -0.03176613
 -0.02833655 -0.02273247 -0.02219761 -0.02192762 -0.01445154 -0.0122352
 -0.0105563  -0.00861253 -0.00210448  0.00434887  0.00800146  0.01060141
  0.01749821  0.0243181   0.0274307   0.03262124  0.03512978  0.03568942
  0.03649427  0.037517    0.03892077  0.0394582   0.03958724  0.04611235
  0.04851407  0.05139687  0.05234783  0.05421211  0.05763234  0.05938559
  0.05993494  0.06087141  0.06405577  0.07162324  0.0737781   0.07437701
  0.07493666  0.07569966  0.07936675  0.0845795   0.08859608  0.08875503
  0.09103027  0.09174887  0.09351403  0.09528014  0.09707781  0.09883362
  0.09952908  0.10020068  0.10050739  0.10130711  0.10283056  0.10369689
  0.10520234  0.10577741  0.10578677  0.10581155  0.10621572  0.10723845
  0.11012288  0.11022458  0.1114259   0.11221533  0.11238623  0.11439497
  0.11567483  0.11894382  0.1192061   0.12072956  0.12181119  0.12277501
  0.12447099  0.12492636  0.1271205   0.12728881  0.12787322  0.12916338
  0.13410871  0.13456922  0.13707776  0.13711026  0.1390788   0.14033225
  0.14143609  0.14157542  0.1417061   0.14181807  0.14291513  0.14437617
  0.14566119  0.14772628  0.14861996  0.1488404   0.15068341  0.15091321
  0.15112173  0.15262461  0.15487087  0.15651985  0.15847133  0.15860972
  0.16066803  0.16087304  0.16232895  0.16349778  0.16402071  0.16517926
  0.16583543  0.16602432  0.16769294  0.16837391  0.16886692  0.17085511
  0.17138317  0.17167538  0.17299031  0.17346019  0.17475549  0.1756244
  0.17633527  0.17752373  0.17810816  0.179277    0.18133273  0.18152326
  0.18153095  0.18224441  0.18384898  0.18405143  0.184101    0.18420524
  0.184864    0.18531681  0.18571069  0.1871666   0.18760491  0.18851403
  0.19096787  0.19132763  0.19161984  0.19201794  0.19314493  0.19455127
  0.195034    0.19534842  0.19824406  0.19864988  0.19912069  0.19999731
  0.20046462  0.20484351  0.20492463  0.20513572  0.20545692  0.21019723
  0.21081322  0.21115496  0.21290823  0.21308682  0.21311416  0.21326026
  0.21425821  0.2146119   0.21497424  0.21563978  0.21614307  0.21799964
  0.21849009  0.21909672  0.2196      0.22050819  0.22171979  0.22174879
  0.22576114  0.22590725  0.22636776  0.2264199   0.22857125  0.22993831
  0.23119176  0.23446238  0.23480415  0.23502203  0.2359662   0.23663174
  0.23864727  0.24082433  0.24110462  0.24256566  0.24280316  0.24285786
  0.24420435  0.24476657  0.24496737  0.2457056   0.24661635  0.24672319
  0.24680266  0.24870623  0.249019    0.25078418  0.25111566  0.25214095
  0.25440427  0.25441713  0.25462471  0.25555604  0.25885752  0.25887201
  0.26017245  0.26035526  0.26320557  0.26560307  0.26745286  0.26767073
  0.27038943  0.27044928  0.27194189  0.27212819  0.27237761  0.27460168
  0.27514938  0.27623102  0.27639676  0.27761258  0.27792535  0.27888566
  0.27975715  0.28213501  0.28277156  0.28282789  0.28435135  0.28445982
  0.2855723   0.28649077  0.28659503  0.28703334  0.28706491  0.28765961
  0.28896683  0.28938293  0.2894325   0.28948719  0.29198289  0.29203243
  0.29409589  0.2948554   0.29578157  0.29584913  0.29599524  0.29672575
  0.29731017  0.29804069  0.29853791  0.29891731  0.29934278  0.30025703
  0.30041248  0.30178726  0.30197522  0.30268867  0.30288689  0.303033
  0.30324316  0.3033252   0.30340632  0.30347131  0.30382501  0.30420183
  0.30551676  0.30618744  0.30814149  0.31046631  0.31124894  0.31271257
  0.31276211  0.31362424  0.31371565  0.31448544  0.3147905   0.31504251
  0.31802699  0.31912406  0.32053553  0.32067136  0.32106267  0.32237247
  0.32323204  0.32684021  0.32756302  0.32895229  0.32942309  0.32946494
  0.32959912  0.32975714  0.33136173  0.33204781  0.33238958  0.33703242
  0.33910521  0.33923169  0.33953161  0.33966999  0.3407645   0.34076706
  0.3409328   0.34244341  0.34291677  0.34438974  0.34667784  0.34709138
  0.34855498  0.34952816  0.34988019  0.34997417  0.34997674  0.35070213
  0.35089265  0.35091741  0.35124726  0.35355501  0.35411978  0.35466399
  0.35639503  0.3565215   0.3582918   0.36002285  0.36069188  0.36089948
  0.36209051  0.3628014   0.3630936   0.36471102  0.36524588  0.36646427
  0.36670692  0.36678126  0.36767751  0.36883864  0.36916333  0.36933936
  0.371771    0.37191453  0.37272452  0.37593881  0.37732037  0.37873699
  0.38150526  0.38151039  0.38526468  0.38584909  0.38631218  0.38656933
  0.38868912  0.38981352  0.38993999  0.39112331  0.39215634  0.39347127
  0.39449399  0.39558849  0.39811924  0.39826792  0.39865154  0.39996133
  0.40084566  0.40204183  0.40264075  0.4032525   0.40340633  0.40442906
  0.40493914  0.40545178  0.4061823   0.40622671  0.40662062  0.40726652
  0.40749724  0.40753907  0.40805174  0.40831403  0.41101823  0.41253396
  0.41275184  0.41321492  0.41516384  0.416407    0.41662744  0.41913342
  0.41993827  0.41998784  0.42171373  0.42264247  0.42293211  0.42373698
  0.42587897  0.42643089  0.42680001  0.42743655  0.42794919  0.42807052
  0.43167355  0.43208452  0.4344521   0.43554659  0.43710676  0.43841912
  0.43929832  0.43978105  0.44080377  0.44297568  0.4432679   0.44489725
  0.44489981  0.44621474  0.44662827  0.44730925  0.45015956  0.45059785
  0.45086273  0.45103617  0.45186323  0.45249721  0.45312862  0.45731351
  0.45962896  0.46057992  0.46128309  0.46172397  0.4625536   0.46301668
  0.46486133  0.46491347  0.46513391  0.46593362  0.46608487  0.46948711
  0.470151    0.47051496  0.47146592  0.471976    0.47219644  0.47226823
  0.47234255  0.47404366  0.47458109  0.47565338  0.47611391  0.47645566
  0.47718877  0.47733486  0.47911032  0.48051922  0.48151975  0.48351822
  0.48420173  0.48887449  0.49189311  0.49269541  0.49308929  0.49790816
  0.50229384  0.50346267  0.50426495  0.50553034  0.51398214  0.51719642
  0.52408808  0.53061321  0.53209901  0.5371631 ]

  warnings.warn(

2022-11-03 10:49:21,313:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.09152929 -0.091378   -0.08895749 -0.08109082 -0.07755781 -0.07700621
 -0.0758937  -0.07534211 -0.07398056 -0.07032885 -0.06512009 -0.05995556
 -0.05054349 -0.04882585 -0.04609114 -0.04462254 -0.043261   -0.04235331
 -0.03482179 -0.03309252 -0.02414593 -0.01827852 -0.01518771 -0.0149829
 -0.01077959 -0.00926676 -0.00490982 -0.0032783   0.00915707  0.00961091
  0.01218271  0.0199749   0.02306337  0.02397107  0.02418986  0.02639158
  0.02782063  0.02885867  0.03146305  0.03221946  0.03262673  0.03691616
  0.03919703  0.04096819  0.04157331  0.04205279  0.04359585  0.04543453
  0.04603966  0.05004746  0.05143924  0.05214447  0.0528543   0.0528683
  0.05291251  0.05469531  0.05656423  0.05783967  0.05863796  0.06201968
  0.06216862  0.066223    0.07200665  0.0725885   0.07261179  0.07390345
  0.07662653  0.07795783  0.08108357  0.08240784  0.08993936  0.09003015
  0.09179203  0.0919433   0.09286959  0.09342119  0.09523892  0.09797825
  0.09955624  0.10172771  0.10205592  0.10270058  0.10324053  0.10337556
  0.10513744  0.10613819  0.10789769  0.1113493   0.11267825  0.11346724
  0.11397227  0.11473798  0.11475432  0.11550603  0.11663486  0.11676286
  0.1176682   0.12177847  0.12213455  0.12319118  0.13169556  0.14149632
  0.14201768  0.14294396  0.14410069  0.14413562  0.14459876  0.14503627
  0.14564609  0.14779898  0.14943744  0.14972142  0.14977494  0.15178583
  0.1530054   0.15412025  0.15904034  0.15946865  0.16143532  0.16156564
  0.16320411  0.16389536  0.16653927  0.1703772   0.1713803   0.17198543
  0.17230668  0.1729118   0.17305839  0.17416395  0.17518799  0.17785284
  0.17823693  0.1790445   0.18032463  0.18119973  0.18170007  0.18505625
  0.18514697  0.18546587  0.18601514  0.18660166  0.18710903  0.18839377
  0.18925027  0.18936898  0.19212464  0.19288105  0.19333488  0.19360712
  0.19438221  0.19637677  0.19641405  0.19677016  0.19708661  0.19726822
  0.19741951  0.19813168  0.19886941  0.19906031  0.19935358  0.20136214
  0.20187881  0.2023489   0.20273531  0.2034708   0.20431557  0.20471589
  0.20507198  0.20700606  0.20742506  0.20748788  0.20751117  0.20757165
  0.20898906  0.21028778  0.21081845  0.21105809  0.21118148  0.21167956
  0.21176097  0.21330179  0.21490531  0.21519399  0.21708843  0.21843833
  0.21868737  0.21987208  0.22070756  0.22121266  0.22185968  0.22210873
  0.2248411   0.22491087  0.2249505   0.22495744  0.22581856  0.22603736
  0.2261724   0.22669367  0.22770147  0.22786909  0.22880703  0.22886055
  0.22912122  0.22958671  0.23002188  0.23378768  0.2349956   0.23513288
  0.23640136  0.23702049  0.23709955  0.23727881  0.23751386  0.23846109
  0.23855423  0.23887774  0.24092818  0.24105617  0.24160542  0.24208725
  0.24281569  0.24542476  0.24550618  0.24768929  0.24773353  0.24809196
  0.24838054  0.24980959  0.24989336  0.25050553  0.25144347  0.252016
  0.25237444  0.25458545  0.25527434  0.25609133  0.2563473   0.25692219
  0.25779261  0.25903084  0.25974538  0.26115809  0.26130938  0.26220073
  0.26275234  0.26345062  0.26475627  0.26570351  0.26717679  0.26749101
  0.26871751  0.27042124  0.27252519  0.27261365  0.27300928  0.27344218
  0.27348172  0.27438941  0.27668193  0.27734293  0.27867659  0.2792142
  0.27936312  0.27992872  0.28054315  0.28093182  0.28101324  0.28133214
  0.28139965  0.28435544  0.28701803  0.28835159  0.28881474  0.28888696
  0.29082102  0.29108397  0.29149593  0.29167051  0.29205917  0.29305063
  0.29341837  0.29390019  0.29475202  0.29494988  0.29581099  0.29688397
  0.29689786  0.29710267  0.29732616  0.29816164  0.29846421  0.29906933
  0.29993053  0.30123856  0.30130598  0.30191111  0.30236495  0.30251624
  0.3028188   0.30312136  0.30327265  0.30342393  0.30364507  0.30372649
  0.30426185  0.30433162  0.30583983  0.30794849  0.30874209  0.30891666
  0.31192831  0.31224252  0.3151704   0.31537522  0.31609209  0.31688804
  0.31724647  0.31775853  0.31840086  0.3188734   0.31892458  0.31900598
  0.32169187  0.32271592  0.32283695  0.32285555  0.32326517  0.32441955
  0.32534123  0.32599989  0.33001004  0.33064541  0.33097362  0.33125054
  0.33215127  0.33465321  0.33496742  0.33543761  0.33709701  0.34118633
  0.34148654  0.34179145  0.34209861  0.34386048  0.34437255  0.34453548
  0.34523366  0.34663708  0.34718868  0.34834542  0.35261859  0.35383814
  0.35384743  0.3552532   0.35793441  0.36055973  0.36444649  0.36520525
  0.36548217  0.36576379  0.3685404   0.37017192  0.37050706  0.37119369
  0.37132169  0.37144734  0.37380268  0.37387021  0.37395397  0.37528292
  0.37592299  0.37629071  0.37639082  0.37649318  0.3768935   0.37719841
  0.37845984  0.37936753  0.38001924  0.38041721  0.38088035  0.38282608
  0.38286101  0.38353131  0.38420161  0.38442735  0.38463217  0.38473923
  0.3850325   0.38560269  0.38602865  0.3868153   0.38762523  0.38819083
  0.39056241  0.39121413  0.39210782  0.39268036  0.39283164  0.39328549
  0.3946168   0.39525215  0.39721882  0.39826154  0.40006294  0.40272086
  0.4046247   0.40603046  0.4064075   0.40761541  0.40838346  0.41084819
  0.41234703  0.41545184  0.41611049  0.42010664  0.42090729  0.42183826
  0.42214082  0.42249693  0.422995    0.42317652  0.42464511  0.42780344
  0.42861103  0.4308919   0.43093379  0.43194158  0.43296797  0.43359403
  0.43537449  0.43607737  0.43614256  0.4399479   0.44642974  0.44848252
  0.44990921  0.45062375  0.45122653  0.45198294  0.45229715  0.45300004
  0.45425217  0.45613273  0.456284    0.4563934   0.45697525  0.4571917
  0.46007771  0.46397845  0.4656309   0.46623605  0.46711114  0.467309
  0.467565    0.46922911  0.47119576  0.4723432   0.47394213  0.47404918
  0.47519426  0.47899961  0.48026339  0.48184138  0.48223935  0.48325646
  0.48460635  0.48465988  0.48481114  0.48581661  0.48755752  0.4875901
  0.48770881  0.49024802  0.49032247  0.49380196  0.49791918  0.49933423
  0.50042813  0.50984951  0.51213969  0.51612656  0.52599247  0.52681404
  0.53056352  0.53486458]

  warnings.warn(

2022-11-03 10:49:21,345:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.08518249 -0.08399202 -0.08265273 -0.0742899  -0.07399229 -0.06641776
 -0.05400758 -0.04772808 -0.04205858 -0.04027287 -0.03583809 -0.03539166
 -0.03356169 -0.03165668 -0.02793644 -0.00405319 -0.00400893 -0.00107571
 -0.00058632  0.00520249  0.01017221  0.0119002   0.01286682  0.01719833
  0.01732981  0.02253942  0.02285051  0.03490665  0.03642166  0.03939913
  0.04354975  0.04552853  0.04666259  0.04985875  0.05164447  0.05390869
  0.05753783  0.0587591   0.06164419  0.06268586  0.06417395  0.0643709
  0.06621432  0.06713797  0.06899744  0.07548346  0.07642057  0.0785039
  0.07968221  0.08061804  0.08088614  0.08188355  0.08357817  0.08503676
  0.08927848  0.09056133  0.09207893  0.0929718   0.09313147  0.09392365
  0.09423603  0.09729943  0.09770418  0.09951811  0.09984265  0.09987474
  0.10201967  0.10212163  0.10368606  0.10402406  0.10615552  0.10775905
  0.1086519   0.11009574  0.11098989  0.11258254  0.11414568  0.11427715
  0.11481468  0.11595831  0.11630535  0.11770364  0.11932451  0.1199332
  0.12076963  0.12185426  0.1232981   0.12448728  0.12453283  0.12538273
  0.12640447  0.13032165  0.1308835   0.13088867  0.1346089   0.13584105
  0.1358571   0.13600719  0.13786537  0.14015522  0.14110967  0.14162727
  0.14441428  0.14527762  0.14538088  0.14563555  0.14627503  0.14651365
  0.14703254  0.14822432  0.14829936  0.15045646  0.1505597   0.15164305
  0.15366997  0.15518496  0.15538192  0.15655246  0.1571477   0.15880453
  0.15884619  0.16160239  0.16222713  0.16226881  0.16282108  0.16473827
  0.16574914  0.16835653  0.16871574  0.16928148  0.1713648   0.1764352
  0.17805995  0.17972764  0.17997453  0.18195978  0.182106    0.18296548
  0.18380321  0.18423617  0.18471081  0.18490646  0.18545098  0.18648178
  0.1867191   0.1871386   0.18986143  0.19071002  0.19213524  0.19503894
  0.19622942  0.19889453  0.20042043  0.20172192  0.20175971  0.20200349
  0.20246597  0.20337228  0.20474106  0.20509899  0.206335    0.20679359
  0.20683397  0.20698279  0.20772683  0.20781922  0.21042015  0.21093075
  0.2111642   0.21172165  0.21299933  0.21320586  0.21413079  0.21549442
  0.21585493  0.21615254  0.21718204  0.21721155  0.21765668  0.21772656
  0.21784587  0.21817299  0.21970792  0.21975347  0.21981118  0.22039555
  0.22121465  0.22220559  0.22245895  0.22289579  0.22304461  0.2317506
  0.23190771  0.23212069  0.23246255  0.23253631  0.23277364  0.23310464
  0.23550992  0.23649517  0.23706219  0.24035075  0.24319159  0.24375474
  0.24428838  0.24522808  0.24548274  0.24592528  0.24659686  0.24827931
  0.24842942  0.2484698   0.24866932  0.24921384  0.25090016  0.25150757
  0.25297004  0.25375706  0.25379743  0.2544703   0.25532978  0.2557762
  0.25686858  0.25698789  0.25789163  0.25796409  0.25872934  0.25967862
  0.26077671  0.26156502  0.26241879  0.26359451  0.263844    0.26418976
  0.26420192  0.26592474  0.26780284  0.26802283  0.2688176   0.26923064
  0.26965973  0.27023063  0.27025497  0.27174306  0.27328188  0.273786
  0.27466282  0.27626893  0.27716567  0.27744852  0.27906681  0.27939263
  0.28080567  0.28233802  0.28332456  0.28464522  0.28488512  0.28726348
  0.28768557  0.28774589  0.28790945  0.28917108  0.28921922  0.29004661
  0.29114728  0.29125184  0.29278418  0.29323061  0.29352823  0.29443583
  0.29454039  0.29463148  0.29494256  0.29511129  0.29512734  0.29552434
  0.29562631  0.29564623  0.29592393  0.29637036  0.29696559  0.29766538
  0.29830488  0.29845369  0.29884627  0.29904893  0.29919773  0.29933695
  0.29949535  0.29979297  0.30068583  0.30083464  0.3015999   0.3021483
  0.3041232   0.30418737  0.30851501  0.30911025  0.31036102  0.31125129
  0.31131159  0.31344305  0.31613637  0.31765527  0.31821583  0.3187386
  0.32159678  0.32173211  0.32251655  0.32351654  0.32988326  0.33019692
  0.3322039   0.33281778  0.33295313  0.33360737  0.33473884  0.33521476
  0.33542129  0.33637445  0.3371159   0.34006517  0.34088168  0.34175721
  0.34196889  0.34263918  0.34436459  0.34532863  0.34595853  0.34835036
  0.35186407  0.35204368  0.35546371  0.35610578  0.3562828   0.35648933
  0.35736872  0.35790367  0.35826416  0.35903513  0.35954316  0.36120826
  0.36178746  0.36510552  0.3659875   0.36784568  0.36894506  0.36949604
  0.37138889  0.37156592  0.37211432  0.37270954  0.3728141   0.37360241
  0.37400845  0.37449526  0.37458766  0.37479288  0.37523931  0.37620721
  0.37625147  0.37848361  0.37903459  0.37911223  0.37989923  0.38037517
  0.38094219  0.38223463  0.38234048  0.38333531  0.38357263  0.38442382
  0.38470539  0.3872794   0.38732625  0.39077708  0.39129855  0.39171417
  0.39218882  0.39251981  0.39274239  0.39410859  0.39567172  0.39732338
  0.39774161  0.39924316  0.4006857   0.40101413  0.40147658  0.40172867
  0.40186143  0.40186272  0.40192301  0.4023534   0.40244191  0.40425713
  0.40802548  0.4087227   0.40876825  0.40882855  0.40903508  0.41044941
  0.41058347  0.41071752  0.41073226  0.41155137  0.41254748  0.41568853
  0.41589506  0.41689247  0.41750116  0.41760571  0.41803868  0.41860442
  0.42190901  0.42339452  0.42547787  0.4255247   0.42842454  0.43037511
  0.43084975  0.43116469  0.43129616  0.43251745  0.43474829  0.43784507
  0.43875267  0.43927287  0.44137095  0.44221954  0.44226382  0.44337924
  0.44443564  0.44651898  0.44662353  0.44666778  0.44726303  0.44815587
  0.44863312  0.44946566  0.44952596  0.45110258  0.45111862  0.45244185
  0.45408004  0.45553735  0.45602803  0.45753087  0.45767968  0.45844105
  0.45974954  0.45991182  0.46139992  0.46166933  0.46215871  0.46224851
  0.46226455  0.46361859  0.46446591  0.4661041   0.46689112  0.46894493
  0.46948117  0.4756856   0.47747133  0.47806656  0.4786323   0.47973426
  0.48123709  0.48163797  0.48171303  0.4825161   0.4827547   0.48326014
  0.48784371  0.48802204  0.49101297  0.49443689  0.49515013  0.50276892
  0.50604272  0.50887009  0.51020939  0.51107272  0.51616176  0.51631186
  0.53038845]

  warnings.warn(

2022-11-03 10:49:21,391:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.09020317 -0.08723064 -0.08473821 -0.08131981 -0.07734112 -0.07415154
 -0.0732138  -0.07311115 -0.07225361 -0.07013863 -0.0676462  -0.06734895
 -0.06596533 -0.06574828 -0.06302703 -0.06028332 -0.0475699  -0.04385424
 -0.03510775 -0.03481049 -0.01614134 -0.01042403 -0.00808023 -0.00570221
 -0.00552045  0.00359281  0.00935502  0.01009815  0.01273921  0.01561869
  0.0229602   0.0271089   0.02986436  0.03047171  0.03136346  0.03663381
  0.03682842  0.03846331  0.03967369  0.04199612  0.04370905  0.04430354
  0.04775833  0.05099172  0.05262878  0.05597658  0.05685874  0.05840059
  0.06124695  0.0655571   0.06712355  0.06766248  0.07473771  0.07567761
  0.07614485  0.07637368  0.0832447   0.08334842  0.08601301  0.086708
  0.08834505  0.08909995  0.09037767  0.09818644  0.09841527  0.09933055
  0.09983417  0.10182191  0.1046458   0.10464687  0.10663676  0.10765469
  0.10922007  0.10989369  0.11037486  0.11148261  0.11210065  0.1126727
  0.1152111   0.1174651   0.120289    0.12187575  0.12258682  0.12270123
  0.12499691  0.1279459   0.12953804  0.13007263  0.13093126  0.13197057
  0.13476133  0.13594073  0.13719817  0.13733287  0.13782473  0.13791669
  0.13909394  0.14164304  0.14229745  0.14405636  0.14475351  0.1448786
  0.14508712  0.14627614  0.14854935  0.1504815   0.15116581  0.15170365
  0.15812987  0.15999573  0.16143385  0.16162739  0.16182198  0.16200699
  0.16222512  0.16239296  0.16253198  0.16335315  0.16339913  0.16409628
  0.16455608  0.16492176  0.16676086  0.1672442   0.16757244  0.17048721
  0.17094484  0.17179063  0.17226327  0.17322022  0.17432051  0.17650176
  0.17655843  0.17672951  0.1772802   0.17747264  0.17793244  0.17801263
  0.17880067  0.179809    0.18044517  0.180998    0.18364652  0.18472326
  0.18592296  0.18639021  0.18818873  0.18856618  0.1889746   0.19077096
  0.19210861  0.19304418  0.19308271  0.19386858  0.19653209  0.19679836
  0.19723993  0.19757356  0.19796384  0.19937844  0.1998949   0.20007559
  0.20033118  0.20058245  0.20235096  0.20251352  0.20374634  0.20445849
  0.20578221  0.20603348  0.20655848  0.20713377  0.20778171  0.20879002
  0.2092562   0.21143109  0.2126297   0.21299754  0.21349048  0.21430204
  0.21536487  0.21539801  0.21653997  0.21996905  0.22099876  0.22159649
  0.22173012  0.22201884  0.2227705   0.22489724  0.22502233  0.22517313
  0.22572381  0.22582537  0.22618891  0.22646694  0.22669144  0.22748055
  0.22884496  0.22955387  0.23036112  0.23173405  0.2331209   0.23509471
  0.23524335  0.23535991  0.23905096  0.23985399  0.24028593  0.24301043
  0.24379523  0.24485914  0.24538845  0.24539699  0.24539806  0.24541943
  0.24563649  0.24936714  0.2494548   0.25115065  0.25224447  0.25263584
  0.25310417  0.25321643  0.25340143  0.25453376  0.25468347  0.25590345
  0.25614298  0.25631836  0.25881079  0.25955392  0.26040715  0.26051088
  0.26086695  0.26104018  0.26118881  0.26240988  0.26376888  0.26378388
  0.26471839  0.26537171  0.26570318  0.26629662  0.26665376  0.26784277
  0.26905423  0.26925845  0.26969472  0.27101734  0.27136059  0.27137558
  0.27182146  0.27249401  0.27265224  0.27290675  0.27451811  0.27580015
  0.27591134  0.27651653  0.27848396  0.27882612  0.27892875  0.28046209
  0.28062247  0.28096462  0.28318225  0.28422263  0.28438626  0.28539987
  0.28640604  0.28653546  0.28673006  0.28762182  0.28962239  0.29083386
  0.29090013  0.29480931  0.29528196  0.29586361  0.29632234  0.29649018
  0.29655968  0.29736272  0.29751135  0.29765997  0.29772518  0.29914623
  0.29929486  0.29944349  0.30060682  0.30078113  0.30197014  0.30241386
  0.30296238  0.30325963  0.30327356  0.30372688  0.30380707  0.30489345
  0.30634765  0.30822097  0.30845086  0.30946664  0.31137525  0.31177946
  0.31213122  0.313517    0.31518502  0.31554002  0.31633981  0.31815646
  0.32052272  0.32099105  0.32163262  0.32250085  0.32271898  0.32394113
  0.32556533  0.32589895  0.32655866  0.32789628  0.32891638  0.32916658
  0.33017166  0.33072019  0.33151998  0.33471173  0.33726081  0.33735386
  0.33825631  0.33892885  0.33999491  0.34044079  0.34048463  0.34137637
  0.34479694  0.34541174  0.34607575  0.34624683  0.34640721  0.34660183
  0.34844201  0.35031748  0.35334666  0.35434323  0.35691477  0.35749643
  0.35751036  0.35767927  0.35796585  0.35805888  0.35846946  0.35886936
  0.35892604  0.35979534  0.36005945  0.36273365  0.36292933  0.36428834
  0.3653747   0.36758057  0.37002917  0.37033819  0.37086211  0.37104603
  0.37122995  0.37237298  0.37240503  0.37241896  0.37277072  0.37316209
  0.37331071  0.37386885  0.37390522  0.37509423  0.37551442  0.37593893
  0.37598599  0.37679647  0.37703599  0.37751716  0.37872969  0.38133437
  0.38295966  0.38298211  0.38356592  0.38595464  0.38705276  0.38757884
  0.38998891  0.39105175  0.39349927  0.39522504  0.39664288  0.39682787
  0.397571    0.39759346  0.39840502  0.40288625  0.40321665  0.406727
  0.40704778  0.40734503  0.41170119  0.41192892  0.4120102   0.41226147
  0.41319922  0.41347293  0.41364508  0.41417008  0.41624016  0.41681114
  0.41742918  0.41794348  0.41852731  0.41942974  0.42098443  0.42111062
  0.42264286  0.42314433  0.42388853  0.42462097  0.42467764  0.42594577
  0.42801586  0.42888409  0.42897605  0.43155934  0.43212072  0.43585882
  0.43624803  0.43758568  0.43862498  0.43879713  0.43910615  0.44154083
  0.44394237  0.44446737  0.44598894  0.44676629  0.44704     0.44763451
  0.44891549  0.44911008  0.44940733  0.45081233  0.45123575  0.45200241
  0.45362447  0.45434621  0.45583247  0.4570557   0.45773041  0.46169733
  0.46189194  0.46352682  0.4640978   0.46508153  0.46563004  0.46575621
  0.47031765  0.47074     0.47124363  0.47322176  0.47381625  0.47441075
  0.47482242  0.47515281  0.47942984  0.48772938  0.49072542  0.49282864
  0.49703616  0.49982584  0.50051122  0.50078603  0.50199748  0.50311808
  0.50316406  0.50494757  0.50950899  0.50952075  0.51026389  0.53860552
  0.54378498  0.54411646]

  warnings.warn(

2022-11-03 10:49:23,524:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.09006549 -0.08809969 -0.07766735 -0.07721225 -0.07494403 -0.07456818
 -0.07282703 -0.07207095 -0.06942542 -0.06836692 -0.06126275 -0.06096033
 -0.05408079 -0.04916705 -0.04417551 -0.04221263 -0.03850569 -0.03586017
 -0.03427459 -0.03298709 -0.0307952  -0.02988792 -0.02966328 -0.02399346
 -0.02376446 -0.02323593 -0.0230113  -0.01159387 -0.00077402  0.01289998
  0.01548228  0.01645133  0.0198675   0.02136508  0.02712143  0.02794364
  0.0377079   0.0404152   0.04231922  0.0459338   0.04980799  0.05063894
  0.05145241  0.0523597   0.05373955  0.05493034  0.05751847  0.05872817
  0.05978669  0.06248961  0.064627    0.06734013  0.07118518  0.07360461
  0.07520765  0.08040635  0.08259824  0.08274945  0.0829225   0.08411475
  0.08562253  0.08640044  0.08790094  0.08897255  0.090862    0.09129235
  0.09146687  0.09176783  0.09206589  0.09244902  0.09439152  0.09484516
  0.09545002  0.10152336  0.10514959  0.10633601  0.10696707  0.10704195
  0.10921054  0.10958639  0.11126432  0.11157839  0.11245947  0.11664335
  0.11732454  0.12283296  0.1228446   0.12450797  0.12463006  0.12508371
  0.12601721  0.12828542  0.12926614  0.12955254  0.13128495  0.13146238
  0.13168848  0.13297599  0.13516787  0.13657455  0.14037968  0.14241014
  0.14445663  0.14492193  0.14574998  0.14628141  0.14725047  0.14755436
  0.150125    0.15056845  0.15088836  0.15156955  0.15163861  0.15267672
  0.15413582  0.1546527   0.15572577  0.1559606   0.1563379   0.15653196
  0.15799834  0.16064096  0.16167326  0.16245846  0.16673821  0.16813118
  0.16877889  0.16881384  0.16938374  0.17024382  0.17044224  0.17077963
  0.17084139  0.17190135  0.17484348  0.1749373   0.17521933  0.17529276
  0.17682675  0.17732178  0.17763876  0.17770346  0.17939886  0.1802945
  0.18088916  0.18186987  0.18333042  0.18431113  0.18488396  0.18605229
  0.18861858  0.18869781  0.18964358  0.18966833  0.18974758  0.19103363
  0.19115573  0.19154759  0.19193655  0.19346035  0.19562602  0.19648464
  0.19663731  0.19753148  0.19843731  0.19875139  0.19886621  0.1989542
  0.19962228  0.19964122  0.19992471  0.20010068  0.20049111  0.20064377
  0.20264658  0.2028342   0.20518021  0.20683921  0.2069199   0.20722233
  0.20812671  0.2090369   0.20944978  0.20955961  0.2102408   0.21136836
  0.21197175  0.21439265  0.2147038   0.21617809  0.21731876  0.21733478
  0.21769024  0.21892616  0.21893637  0.21965749  0.22020348  0.22180944
  0.22245277  0.22475738  0.22731056  0.22797137  0.22934978  0.22936288
  0.22944651  0.22974456  0.23056823  0.23171617  0.23212905  0.23439726
  0.23624679  0.23940336  0.23946222  0.23955457  0.24016235  0.24136478
  0.24266393  0.24482961  0.24528325  0.24577185  0.24818254  0.24944093
  0.24955075  0.24974336  0.25013959  0.25098366  0.25204216  0.25318428
  0.25446304  0.25480832  0.25514133  0.25567858  0.25601803  0.25775627
  0.25813504  0.25860906  0.25873989  0.25907289  0.25950762  0.26293398
  0.26382962  0.26413786  0.26463581  0.26478702  0.26512294  0.26539188
  0.26722684  0.26811374  0.26849542  0.2694129   0.2704714   0.27056668
  0.27093233  0.27123912  0.27272506  0.2731787   0.27320929  0.27431645
  0.27433683  0.27521937  0.27538515  0.27569631  0.27585481  0.27637897
  0.27688273  0.27748759  0.27854609  0.27863553  0.27917278  0.27993323
  0.2805381   0.28137196  0.28300619  0.2831574   0.28407634  0.28560451
  0.28565171  0.28566773  0.2877215   0.28776872  0.28882722  0.28943354
  0.28946849  0.28990612  0.29026303  0.29058584  0.29086789  0.29132153
  0.29207761  0.29253125  0.29283368  0.29429424  0.29449704  0.2960252
  0.29676671  0.29747557  0.29755482  0.29805422  0.30063652  0.30122682
  0.30164054  0.30372986  0.30390148  0.30524346  0.30570002  0.30669238
  0.3068538   0.30795513  0.30834118  0.30901364  0.30916485  0.30931607
  0.30932189  0.31037457  0.31040806  0.31098962  0.31138439  0.31199217
  0.31212737  0.3125883   0.31267337  0.31319606  0.3133531   0.31402119
  0.3144894   0.3168325   0.31788081  0.31802329  0.31970703  0.32075097
  0.32075534  0.32128678  0.32172002  0.3221897   0.32423472  0.32444772
  0.32460912  0.32489117  0.32746181  0.32824701  0.32860393  0.33194377
  0.33336063  0.3342178   0.3351251   0.3372319   0.33865169  0.34017841
  0.34078616  0.34123835  0.34214418  0.34259929  0.34275924  0.3437312
  0.3453226   0.34546362  0.3454782   0.34630187  0.34690962  0.34873003
  0.35189971  0.35378916  0.35445726  0.35725836  0.35750192  0.35854733
  0.35878799  0.36081408  0.36151421  0.36203255  0.363376    0.36361957
  0.36413207  0.36526545  0.36821486  0.3689724   0.36919703  0.36934825
  0.36965067  0.37040674  0.37110251  0.37268226  0.37344852  0.37413553
  0.37433979  0.37450701  0.37464221  0.37728773  0.37851783  0.37910231
  0.37950291  0.37964539  0.38431849  0.38598185  0.38615637  0.38629447
  0.38870517  0.39045942  0.39128163  0.39325761  0.3959046   0.39703944
  0.39733896  0.39755778  0.39763556  0.39763994  0.39830656  0.4012618
  0.40383244  0.40489531  0.40527554  0.40860079  0.40959025  0.41018491
  0.41033467  0.41041101  0.41410775  0.42145259  0.42348742  0.42454739
  0.42545322  0.42569242  0.42681414  0.42772144  0.42946403  0.42969014
  0.43021429  0.43316224  0.43490193  0.4354363   0.43686628  0.43883205
  0.43958812  0.44004177  0.44005052  0.440193    0.44103851  0.44110027
  0.44126168  0.44140416  0.44283852  0.4432231   0.44344337  0.44450189
  0.44465309  0.44525796  0.44692131  0.4478286   0.44919099  0.45070167
  0.45206406  0.45395351  0.45456857  0.45472123  0.45493859  0.45516469
  0.45524101  0.45668556  0.45765755  0.45902866  0.46037942  0.46159786
  0.46431682  0.46816769  0.46990591  0.47006151  0.47058711  0.47066636
  0.47096443  0.47271141  0.47293895  0.47534965  0.47557866  0.47565354
  0.47626276  0.47936193  0.48147892  0.48185479  0.48404666  0.48760094
  0.48775361  0.48971503  0.49002329  0.49138275  0.49455389  0.49727722
  0.50294994  0.50763905  0.50997777  0.51277742  0.52411852  0.54838772]

  warnings.warn(

2022-11-03 10:49:23,524:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.09117087 -0.08840855 -0.08571687 -0.08448917 -0.07726428 -0.07674541
 -0.0735227  -0.07306231 -0.072295   -0.07088951 -0.06614434 -0.0658959
 -0.06099727 -0.05890728 -0.05278096 -0.043773   -0.04254529 -0.03939323
 -0.02618331 -0.02389354 -0.00736593 -0.00559503 -0.00513464 -0.00075488
 -0.00060142  0.0021609   0.00561989  0.00691589  0.0102579   0.01107388
  0.01491044  0.01790904  0.01966542  0.02127304  0.02535569  0.02955766
  0.03129207  0.03215436  0.03499949  0.03763268  0.04370288  0.04956859
  0.05031392  0.05063301  0.05875435  0.05883716  0.05923907  0.06604989
  0.06815952  0.07290938  0.07413708  0.074444    0.07583263  0.07606144
  0.07633935  0.07759606  0.08066296  0.08273844  0.08600748  0.08688192
  0.08914736  0.09273035  0.09299843  0.09500561  0.09668153  0.09878135
  0.09987243  0.09990424  0.10139489  0.1026642   0.10307827  0.10380909
  0.1040332   0.10446414  0.10510745  0.1052609   0.10633513  0.10710245
  0.10724609  0.10735089  0.1074749   0.10768214  0.10795257  0.10825951
  0.10965752  0.11036166  0.11131893  0.1141884   0.1146951   0.11790564
  0.12184699  0.12198831  0.12268731  0.12290907  0.12305741  0.12306957
  0.12370072  0.12974424  0.12982705  0.13021679  0.13137383  0.13204617
  0.13477669  0.1350518   0.13520526  0.13626733  0.13651577  0.13767795
  0.13950734  0.14304401  0.1452023   0.14633033  0.14711684  0.14850313
  0.14999378  0.1506198   0.1516014   0.15349161  0.15543327  0.15615191
  0.15644667  0.15737961  0.15774971  0.15879214  0.15926983  0.15955476
  0.15974473  0.15981303  0.16097989  0.16105054  0.1619442   0.16311854
  0.16538867  0.16641425  0.17123475  0.17154168  0.17278432  0.17278902
  0.17508626  0.17575112  0.17848911  0.17884704  0.17910062  0.18253527
  0.18337324  0.18350705  0.18443252  0.18524614  0.18567517  0.18577014
  0.18613554  0.18683691  0.18851048  0.18981865  0.19107772  0.19310217
  0.1932042   0.19360889  0.19365521  0.19383769  0.19391581  0.1953213
  0.19630056  0.19650972  0.19683161  0.19729902  0.19983019  0.19984705
  0.19987652  0.20104807  0.20158892  0.20436576  0.20782944  0.20814619
  0.20818781  0.20864072  0.20879888  0.20987823  0.21143719  0.211469
  0.21198039  0.21317863  0.21374895  0.21418736  0.21727859  0.21817737
  0.22007227  0.22030152  0.22033801  0.22109081  0.22169485  0.22366553
  0.22376285  0.22436687  0.22936048  0.22984754  0.23010111  0.23015447
  0.23149209  0.23196464  0.23286109  0.23407384  0.23416177  0.23470263
  0.23477798  0.23479762  0.2361203   0.23628592  0.23676082  0.23834177
  0.23866832  0.23898463  0.23968878  0.24055854  0.24106246  0.24224663
  0.24229016  0.24332086  0.24332555  0.24363994  0.2439249   0.24424398
  0.24565416  0.24605606  0.2466774   0.24759583  0.24907665  0.24956651
  0.25003906  0.25021449  0.25088405  0.25204858  0.25238731  0.25265774
  0.2526891   0.25335161  0.25389012  0.25596047  0.25624072  0.25668659
  0.25742956  0.25758069  0.25769252  0.25892257  0.25895907  0.25967537
  0.25979981  0.26040149  0.2630319   0.26343895  0.26445938  0.26490526
  0.26686611  0.26877128  0.26909975  0.26952129  0.27018893  0.27022777
  0.27117054  0.27163796  0.27409335  0.27536269  0.27551379  0.27591805
  0.28034411  0.28048777  0.28062159  0.28232931  0.28377365  0.28546174
  0.2856152   0.28607558  0.2861902   0.28658741  0.28994395  0.29024853
  0.29069674  0.29122265  0.29134475  0.29311798  0.29338094  0.29406779
  0.29422125  0.294465    0.2948351   0.29518366  0.29535397  0.29590933
  0.2960628   0.29636973  0.29652319  0.29667665  0.29698357  0.29713704
  0.29719831  0.2975684   0.29761706  0.29897858  0.30012112  0.30107136
  0.30224058  0.30359975  0.30380656  0.30411349  0.30722436  0.30791868
  0.30868834  0.31060522  0.31074184  0.3124201   0.31249309  0.31252255
  0.3149265   0.31754518  0.31982044  0.32337439  0.32397843  0.32416839
  0.32492353  0.32508916  0.32764936  0.32955921  0.33124495  0.33155891
  0.33169316  0.33381218  0.33599481  0.33666948  0.33685242  0.33847218
  0.33904207  0.34141232  0.34255718  0.34409651  0.34497845  0.34549732
  0.34593571  0.35037629  0.35213034  0.35394008  0.35410102  0.35470739
  0.35603007  0.35662706  0.35689236  0.35824685  0.35875354  0.35909931
  0.35948906  0.36265328  0.36486772  0.36676774  0.36709152  0.36726931
  0.36915484  0.36957125  0.37033856  0.37079895  0.37092573  0.37110587
  0.37340781  0.37371474  0.37408014  0.37423361  0.37448205  0.37540282
  0.37562693  0.37636522  0.37711054  0.37769259  0.37878646  0.379354
  0.38027243  0.38060604  0.38161944  0.38193852  0.38353632  0.38378008
  0.38606984  0.38639874  0.3864521   0.38721942  0.38743137  0.38752635
  0.38790158  0.38829366  0.3886469   0.38952135  0.39028866  0.39223267
  0.39236415  0.3923983   0.39605707  0.3963738   0.39857563  0.39911649
  0.40276308  0.40317715  0.40421489  0.40456066  0.40717934  0.40838505
  0.40874813  0.41004646  0.41021444  0.41063832  0.41116702  0.41134715
  0.41153711  0.4127868   0.41337631  0.41360044  0.41639926  0.41680115
  0.41923222  0.42129787  0.42154631  0.4274242   0.42762397  0.43036431
  0.43246413  0.43272473  0.43923099  0.44009092  0.44246351  0.44299689
  0.44312368  0.44315036  0.44320884  0.44407112  0.44422459  0.44452916
  0.44482628  0.44606614  0.4463609   0.44645587  0.44651435  0.44781036
  0.44799049  0.44814397  0.44860436  0.44875781  0.44908671  0.44939365
  0.4501268   0.45056286  0.45089411  0.45228744  0.45269916  0.45288913
  0.4532522   0.45365644  0.45419729  0.45428246  0.45457722  0.45726889
  0.45870852  0.46093982  0.46226249  0.46241597  0.46287633  0.46400907
  0.46810387  0.46834016  0.46844731  0.46869575  0.46914396  0.47480991
  0.47507051  0.47828107  0.47883643  0.48051235  0.4860833   0.48696994
  0.48986138  0.49241175  0.49287215  0.49506693  0.49865505  0.50745107
  0.50845466  0.50860811  0.51473445  0.51910203  0.52356461  0.52554745
  0.52855821]

  warnings.warn(

2022-11-03 10:49:23,524:INFO:Calculating mean and std
2022-11-03 10:49:23,539:INFO:Creating metrics dataframe
2022-11-03 10:49:23,539:INFO:Uploading results into container
2022-11-03 10:49:23,539:INFO:Uploading model into container now
2022-11-03 10:49:23,539:INFO:master_model_container: 3
2022-11-03 10:49:23,539:INFO:display_container: 2
2022-11-03 10:49:23,539:INFO:Lasso(random_state=4411)
2022-11-03 10:49:23,539:INFO:create_model() successfully completed......................................
2022-11-03 10:49:23,804:WARNING:create_model() for Lasso(random_state=4411) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:49:23,804:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:49:23,804:INFO:Initializing create_model()
2022-11-03 10:49:23,804:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:49:23,804:INFO:Checking exceptions
2022-11-03 10:49:23,804:INFO:Importing libraries
2022-11-03 10:49:23,804:INFO:Copying training dataset
2022-11-03 10:49:23,820:INFO:Defining folds
2022-11-03 10:49:23,820:INFO:Declaring metric variables
2022-11-03 10:49:23,820:INFO:Importing untrained model
2022-11-03 10:49:23,820:INFO:Lasso Regression Imported successfully
2022-11-03 10:49:23,820:INFO:Starting cross validation
2022-11-03 10:49:23,820:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:49:27,788:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.0821481  -0.081414   -0.06731932 -0.06702568 -0.06541067 -0.06507759
 -0.06463713 -0.06222593 -0.05335467 -0.05262058 -0.0496221  -0.04922107
 -0.04804651 -0.04490122 -0.04471496 -0.04058136 -0.04024829 -0.02808492
 -0.02552689 -0.02170959 -0.01478054 -0.0093373  -0.00596045  0.00468438
  0.00670547  0.00872151  0.0179485   0.01945613  0.02421644  0.03126965
  0.0344376   0.03591167  0.0369671   0.04143377  0.04485088  0.04512691
  0.04914808  0.04992665  0.05112973  0.0512086   0.05413995  0.05747654
  0.06041377  0.0640616   0.06467153  0.06726312  0.07234476  0.07345805
  0.07593051  0.07663692  0.07867561  0.08096852  0.08213133  0.08738245
  0.08994129  0.09882934  0.10025727  0.10359552  0.10399654  0.1056963
  0.10640774  0.10676347  0.10693293  0.10722657  0.10981228  0.11180064
  0.11194745  0.11391821  0.11392324  0.11585958  0.11612553  0.11644686
  0.11670777  0.11800567  0.11811389  0.12164763  0.12165937  0.12310575
  0.12486758  0.12618897  0.12662941  0.12714204  0.13058182  0.1312144
  0.13200974  0.13235455  0.13674237  0.13783723  0.13813758  0.13903443
  0.13911413  0.14197753  0.14244065  0.14325277  0.14656167  0.14666401
  0.14923965  0.14988315  0.15233963  0.15329272  0.15434898  0.15504364
  0.15563176  0.15647324  0.15786678  0.15860088  0.16040884  0.16083252
  0.16260022  0.16423203  0.16480252  0.16513477  0.16532689  0.1658353
  0.16786226  0.16788574  0.16933549  0.16935394  0.17111577  0.17125085
  0.17409581  0.17611185  0.17684175  0.17787452  0.17820676  0.17822519
  0.17855827  0.17910025  0.17929658  0.17937122  0.18002564  0.18149887
  0.18173126  0.18195778  0.18296707  0.186406    0.18674073  0.18726593
  0.18829366  0.19248769  0.1933921   0.19348857  0.19365804  0.193726
  0.19431327  0.1944601   0.19499619  0.19713641  0.19755422  0.19769013
  0.19784198  0.19871786  0.20036057  0.20055855  0.20064331  0.20093107
  0.2016979   0.20194789  0.20201501  0.20207206  0.20215092  0.20267612
  0.20325754  0.20475928  0.2054657   0.20549843  0.20568048  0.20712015
  0.20720909  0.21052973  0.21064802  0.21101633  0.21138212  0.21138378
  0.21196436  0.21255668  0.21320771  0.21382184  0.21443847  0.2148328
  0.21518936  0.21570197  0.21714918  0.2176341   0.21946977  0.21966107
  0.2206888   0.22148582  0.22163685  0.22193049  0.22237094  0.22362602
  0.22629143  0.22681663  0.22794505  0.22882093  0.22917664  0.23026565
  0.2343875   0.23468114  0.23478432  0.23699166  0.2375328   0.24002873
  0.24032237  0.24038026  0.24138451  0.24229311  0.24297183  0.24333928
  0.24599296  0.24701399  0.24869192  0.2491089   0.24933037  0.25299499
  0.25372239  0.25956162  0.25962453  0.26003647  0.2609736   0.26146522
  0.26389321  0.26544531  0.26624736  0.26628176  0.26658044  0.26680108
  0.26746805  0.26754189  0.26808806  0.26810568  0.26912249  0.26957974
  0.2701385   0.27035244  0.2707929   0.27317641  0.27356065  0.27385512
  0.27483086  0.27595424  0.27614049  0.27653144  0.27668834  0.27966416
  0.27968682  0.28178172  0.28215588  0.282369    0.28238075  0.28244952
  0.28290677  0.28375246  0.2839622   0.28420466  0.28458304  0.28463338
  0.28464512  0.28511998  0.28628279  0.28629957  0.28654204  0.28706301
  0.28712932  0.28742295  0.28769981  0.2882938   0.28888694  0.28909584
  0.28977207  0.29027461  0.29095919  0.29140973  0.29218327  0.29262372
  0.29277054  0.29291736  0.29365146  0.29390567  0.29429661  0.29499549
  0.29507602  0.29532856  0.29591584  0.2963563   0.2973446   0.29753086
  0.29770116  0.2985586   0.29885223  0.29929269  0.29943951  0.29991436
  0.30017361  0.30139347  0.30149078  0.30212841  0.3026922   0.3040035
  0.30582658  0.30781997  0.30846346  0.30959354  0.3107681   0.31100553
  0.31135539  0.31191918  0.31203327  0.31276736  0.31332026  0.31355181
  0.31400904  0.31451243  0.31532539  0.31690097  0.31767953  0.31842455
  0.32004627  0.32054798  0.32350702  0.32563631  0.32585109  0.32600883
  0.3261053   0.32718843  0.32931267  0.32975901  0.33103509  0.33296053
  0.33334477  0.33345133  0.3342022   0.33589105  0.33632647  0.3364498
  0.33800357  0.33813864  0.33894574  0.34359949  0.34560966  0.34686813
  0.34886235  0.34961323  0.35143714  0.35183818  0.35244224  0.35262848
  0.35630402  0.35700455  0.3592295   0.36348224  0.36485898  0.3652038
  0.36676764  0.36698829  0.36746231  0.36764856  0.36823079  0.36845648
  0.37005472  0.37020657  0.37083998  0.3720758   0.37217816  0.37343661
  0.3740415   0.37580838  0.37664986  0.37794942  0.38069872  0.38106032
  0.38149574  0.38170381  0.38297989  0.38486086  0.38515449  0.38532395
  0.38562263  0.38617048  0.38635673  0.38643644  0.38709083  0.38770077
  0.38910688  0.38940638  0.39020258  0.39034351  0.3904853   0.39091402
  0.39256342  0.39427409  0.39575907  0.39648729  0.39851507  0.40236007
  0.40345576  0.40499193  0.40503135  0.4066405   0.40727895  0.40783268
  0.40788888  0.40917586  0.40965071  0.41035628  0.41141841  0.41224816
  0.41255858  0.41298226  0.41782731  0.41873089  0.41921077  0.42114796
  0.42251966  0.42338967  0.42349119  0.42380161  0.42413469  0.42676066
  0.42683952  0.42702075  0.42712813  0.43067445  0.43087748  0.43108052
  0.43158893  0.43409073  0.43415282  0.44039223  0.44081591  0.44209786
  0.44229502  0.44293347  0.44295026  0.44302912  0.44384291  0.44523142
  0.44537822  0.44552505  0.4470159   0.44846731  0.44909402  0.44976605
  0.45501212  0.45521516  0.45526044  0.45603986  0.45653652  0.45859199
  0.46035384  0.46168695  0.46231366  0.46240931  0.46577525  0.46645817
  0.46939456  0.46954139  0.47106077  0.47107166  0.47151212  0.47283351
  0.4739124   0.47472539  0.47707449  0.48273837  0.48347247  0.48447757
  0.48715974  0.48766228  0.4886506   0.48867325  0.48938469  0.49177324
  0.49232109  0.49721143  0.49867375  0.49869641  0.50102288  0.50219744
  0.50347937  0.50599795  0.50947632  0.51526438  0.51849441  0.51952214
  0.53523186]

  warnings.warn(

2022-11-03 10:49:27,875:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.08401663 -0.08087889 -0.07214788 -0.06800965 -0.0551152  -0.04933996
 -0.03720336 -0.02992739 -0.02864409 -0.02042829 -0.01852343 -0.01717948
 -0.01374867 -0.00875176 -0.0058212  -0.00193034 -0.0017586   0.00179351
  0.00372359  0.00449167  0.00859445  0.01005973  0.01662827  0.02051834
  0.02368681  0.02474796  0.03435813  0.0374297   0.03837504  0.04361932
  0.04419518  0.04610004  0.04785837  0.05025404  0.05137033  0.05259848
  0.05309321  0.05503825  0.05851001  0.05918671  0.06630592  0.07017629
  0.07097512  0.08119109  0.0861628   0.08880581  0.09118019  0.09210978
  0.09227206  0.09269589  0.0944046   0.09455112  0.09475751  0.09554057
  0.09606603  0.09616293  0.09840574  0.10176564  0.10190193  0.10474741
  0.10536897  0.10837987  0.10995151  0.11121904  0.11290176  0.11359895
  0.11421026  0.11450331  0.11488223  0.11560071  0.11628687  0.1190512
  0.12532117  0.1257253   0.12634686  0.12683687  0.12788383  0.13146116
  0.13274446  0.13676137  0.13738293  0.13744832  0.13866543  0.13982741
  0.14067113  0.142253    0.14262168  0.1477612   0.14916501  0.14953922
  0.15150946  0.15157092  0.15210031  0.1527329   0.15281798  0.15602741
  0.15607231  0.15611721  0.15873974  0.15876494  0.16237301  0.16374216
  0.16387925  0.16500578  0.16587391  0.16603147  0.16748572  0.16880446
  0.16891161  0.16972932  0.17100869  0.17185241  0.17188786  0.17194222
  0.17203439  0.17299312  0.17372576  0.17548409  0.1763987   0.17651531
  0.17707069  0.18050779  0.18096628  0.18182575  0.18299877  0.18343835
  0.1835384   0.18387162  0.18411426  0.184286    0.18525734  0.18530145
  0.18686915  0.1876459   0.18848095  0.19003603  0.19013766  0.19121851
  0.19153677  0.19176915  0.19216936  0.19249154  0.19277437  0.19384182
  0.19389144  0.1939466   0.19403798  0.19412385  0.19427589  0.1948163
  0.19488642  0.19538193  0.1956293   0.19601453  0.19701895  0.19718596
  0.19760979  0.19838811  0.19870637  0.1994138   0.20003144  0.20035915
  0.20171336  0.20212693  0.20225457  0.20280915  0.20367415  0.20382067
  0.20410271  0.20506773  0.20522057  0.20595713  0.20618954  0.2085135
  0.20938323  0.20938401  0.21229329  0.21507731  0.21515925  0.21599194
  0.2171996   0.21818512  0.21835136  0.21867592  0.21888232  0.21902964
  0.22015065  0.22292995  0.22326238  0.22670974  0.22708236  0.22802218
  0.22889111  0.22976554  0.23096298  0.23334287  0.23465138  0.2346821
  0.23663738  0.23766781  0.23993269  0.24227714  0.24407957  0.24445378
  0.24596554  0.24670762  0.24703062  0.24718738  0.25153833  0.25412068
  0.25733327  0.2573695   0.25813759  0.25974939  0.26080975  0.26120364
  0.26131551  0.26244756  0.26263898  0.26288714  0.26369461  0.2637663
  0.26411451  0.26420589  0.26449894  0.264792    0.26493853  0.26635811
  0.2664684   0.266752    0.26697888  0.26728218  0.26792343  0.26916735
  0.27011742  0.27067753  0.27086424  0.27100683  0.27129988  0.27244138
  0.27282582  0.27500798  0.2767151   0.2770184   0.27731224  0.27768644
  0.27775183  0.27854041  0.27900914  0.27901464  0.27933764  0.27944399
  0.28132915  0.28138509  0.28151113  0.28285037  0.28314342  0.2837902
  0.28538624  0.28774645  0.28795285  0.28807415  0.28861458  0.28891237
  0.28952368  0.29381474  0.29427402  0.29440086  0.29548721  0.29612847
  0.29691624  0.29739207  0.29980112  0.29998388  0.30061489  0.30243468
  0.30275294  0.30287426  0.30316731  0.30345013  0.30404648  0.30433954
  0.30434346  0.30463259  0.30477912  0.30492565  0.30582924  0.30704714
  0.30863375  0.31152412  0.31189832  0.31376694  0.31431761  0.31488954
  0.31532361  0.31562218  0.31603576  0.31607122  0.31643517  0.31729937
  0.31892062  0.31909236  0.31971944  0.32031107  0.3211343   0.32154788
  0.32301315  0.32561049  0.32772332  0.32882385  0.32902159  0.32914211
  0.33235628  0.33351354  0.33408468  0.33745955  0.33748003  0.33796923
  0.33833871  0.33838913  0.33849548  0.33878774  0.33919267  0.34027351
  0.34056184  0.3411322   0.34124325  0.34178921  0.34261323  0.34354754
  0.34902027  0.3499593   0.35339561  0.35344052  0.35352166  0.35419916
  0.35503264  0.35509801  0.35591181  0.35718485  0.35766541  0.35987829
  0.36024225  0.3605802   0.36147513  0.36160589  0.3621416   0.36381406
  0.36560312  0.36572917  0.36673437  0.36697702  0.36739691  0.36743157
  0.36955937  0.37027705  0.37038812  0.37097423  0.37208579  0.37213621
  0.3728791   0.37467288  0.37555205  0.37569857  0.37643121  0.37790594
  0.37832583  0.37833608  0.37862913  0.37953272  0.37994708  0.38029135
  0.38042212  0.3805592   0.38066004  0.38278153  0.38340388  0.38589406
  0.38648097  0.38741529  0.38815343  0.38862295  0.38961871  0.39054277
  0.39103829  0.39118482  0.39166458  0.39254927  0.39294315  0.39298886
  0.39328191  0.39377663  0.39489844  0.39633851  0.39688444  0.39711684
  0.39816775  0.40033492  0.4038319   0.4077527   0.40808121  0.40819229
  0.4087327   0.41117325  0.41424484  0.41433622  0.41467969  0.41493257
  0.41508382  0.41510903  0.41518466  0.41610951  0.41716041  0.41778199
  0.41804511  0.41855006  0.4189999   0.42005079  0.42138924  0.42261267
  0.4226883   0.42337524  0.42471921  0.42496184  0.42560861  0.42639718
  0.42806414  0.42812479  0.42821066  0.43042907  0.4318841   0.43217715
  0.43348567  0.4335668   0.43382442  0.43741199  0.43789175  0.44171642
  0.44481401  0.44523861  0.44573886  0.44668342  0.44749719  0.44837636
  0.44940205  0.44954857  0.44969511  0.45059396  0.45189302  0.453055
  0.45718299  0.45918396  0.46121015  0.46171038  0.46229651  0.46283691
  0.46346871  0.46389805  0.46522706  0.46537358  0.46795515  0.46857671
  0.46859718  0.47219974  0.47278584  0.47307889  0.47442286  0.47869737
  0.47899042  0.4791472   0.48195643  0.48222427  0.48447261  0.48728186
  0.48865101  0.48896456  0.49168791  0.49204637  0.49573003  0.49774598
  0.50276337  0.50425386  0.50477932  0.51228768  0.5179164   0.52178677
  0.52453536  0.54130018  0.54186108  0.54994532]

  warnings.warn(

2022-11-03 10:49:27,892:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.08327158 -0.08297938 -0.07725655 -0.07211814 -0.07056055 -0.06939172
 -0.0685151  -0.06822289 -0.06720016 -0.06697972 -0.06668752 -0.06661575
 -0.06559302 -0.06308448 -0.05648503 -0.04489327 -0.03887823 -0.03176613
 -0.02833655 -0.02273247 -0.02219761 -0.02192762 -0.01445154 -0.0122352
 -0.0105563  -0.00861253 -0.00210448  0.00434887  0.00800146  0.01060141
  0.01749821  0.0243181   0.0274307   0.03262124  0.03512978  0.03568942
  0.03649427  0.037517    0.03892077  0.0394582   0.03958724  0.04611235
  0.04851407  0.05139687  0.05234783  0.05421211  0.05763234  0.05938559
  0.05993494  0.06087141  0.06405577  0.07162324  0.0737781   0.07437701
  0.07493666  0.07569966  0.07936675  0.0845795   0.08859608  0.08875503
  0.09103027  0.09174887  0.09351403  0.09528014  0.09707781  0.09883362
  0.09952908  0.10020068  0.10050739  0.10130711  0.10283056  0.10369689
  0.10520234  0.10577741  0.10578677  0.10581155  0.10621572  0.10723845
  0.11012288  0.11022458  0.1114259   0.11221533  0.11238623  0.11439497
  0.11567483  0.11894382  0.1192061   0.12072956  0.12181119  0.12277501
  0.12447099  0.12492636  0.1271205   0.12728881  0.12787322  0.12916338
  0.13410871  0.13456922  0.13707776  0.13711026  0.1390788   0.14033225
  0.14143609  0.14157542  0.1417061   0.14181807  0.14291513  0.14437617
  0.14566119  0.14772628  0.14861996  0.1488404   0.15068341  0.15091321
  0.15112173  0.15262461  0.15487087  0.15651985  0.15847133  0.15860972
  0.16066803  0.16087304  0.16232895  0.16349778  0.16402071  0.16517926
  0.16583543  0.16602432  0.16769294  0.16837391  0.16886692  0.17085511
  0.17138317  0.17167538  0.17299031  0.17346019  0.17475549  0.1756244
  0.17633527  0.17752373  0.17810816  0.179277    0.18133273  0.18152326
  0.18153095  0.18224441  0.18384898  0.18405143  0.184101    0.18420524
  0.184864    0.18531681  0.18571069  0.1871666   0.18760491  0.18851403
  0.19096787  0.19132763  0.19161984  0.19201794  0.19314493  0.19455127
  0.195034    0.19534842  0.19824406  0.19864988  0.19912069  0.19999731
  0.20046462  0.20484351  0.20492463  0.20513572  0.20545692  0.21019723
  0.21081322  0.21115496  0.21290823  0.21308682  0.21311416  0.21326026
  0.21425821  0.2146119   0.21497424  0.21563978  0.21614307  0.21799964
  0.21849009  0.21909672  0.2196      0.22050819  0.22171979  0.22174879
  0.22576114  0.22590725  0.22636776  0.2264199   0.22857125  0.22993831
  0.23119176  0.23446238  0.23480415  0.23502203  0.2359662   0.23663174
  0.23864727  0.24082433  0.24110462  0.24256566  0.24280316  0.24285786
  0.24420435  0.24476657  0.24496737  0.2457056   0.24661635  0.24672319
  0.24680266  0.24870623  0.249019    0.25078418  0.25111566  0.25214095
  0.25440427  0.25441713  0.25462471  0.25555604  0.25885752  0.25887201
  0.26017245  0.26035526  0.26320557  0.26560307  0.26745286  0.26767073
  0.27038943  0.27044928  0.27194189  0.27212819  0.27237761  0.27460168
  0.27514938  0.27623102  0.27639676  0.27761258  0.27792535  0.27888566
  0.27975715  0.28213501  0.28277156  0.28282789  0.28435135  0.28445982
  0.2855723   0.28649077  0.28659503  0.28703334  0.28706491  0.28765961
  0.28896683  0.28938293  0.2894325   0.28948719  0.29198289  0.29203243
  0.29409589  0.2948554   0.29578157  0.29584913  0.29599524  0.29672575
  0.29731017  0.29804069  0.29853791  0.29891731  0.29934278  0.30025703
  0.30041248  0.30178726  0.30197522  0.30268867  0.30288689  0.303033
  0.30324316  0.3033252   0.30340632  0.30347131  0.30382501  0.30420183
  0.30551676  0.30618744  0.30814149  0.31046631  0.31124894  0.31271257
  0.31276211  0.31362424  0.31371565  0.31448544  0.3147905   0.31504251
  0.31802699  0.31912406  0.32053553  0.32067136  0.32106267  0.32237247
  0.32323204  0.32684021  0.32756302  0.32895229  0.32942309  0.32946494
  0.32959912  0.32975714  0.33136173  0.33204781  0.33238958  0.33703242
  0.33910521  0.33923169  0.33953161  0.33966999  0.3407645   0.34076706
  0.3409328   0.34244341  0.34291677  0.34438974  0.34667784  0.34709138
  0.34855498  0.34952816  0.34988019  0.34997417  0.34997674  0.35070213
  0.35089265  0.35091741  0.35124726  0.35355501  0.35411978  0.35466399
  0.35639503  0.3565215   0.3582918   0.36002285  0.36069188  0.36089948
  0.36209051  0.3628014   0.3630936   0.36471102  0.36524588  0.36646427
  0.36670692  0.36678126  0.36767751  0.36883864  0.36916333  0.36933936
  0.371771    0.37191453  0.37272452  0.37593881  0.37732037  0.37873699
  0.38150526  0.38151039  0.38526468  0.38584909  0.38631218  0.38656933
  0.38868912  0.38981352  0.38993999  0.39112331  0.39215634  0.39347127
  0.39449399  0.39558849  0.39811924  0.39826792  0.39865154  0.39996133
  0.40084566  0.40204183  0.40264075  0.4032525   0.40340633  0.40442906
  0.40493914  0.40545178  0.4061823   0.40622671  0.40662062  0.40726652
  0.40749724  0.40753907  0.40805174  0.40831403  0.41101823  0.41253396
  0.41275184  0.41321492  0.41516384  0.416407    0.41662744  0.41913342
  0.41993827  0.41998784  0.42171373  0.42264247  0.42293211  0.42373698
  0.42587897  0.42643089  0.42680001  0.42743655  0.42794919  0.42807052
  0.43167355  0.43208452  0.4344521   0.43554659  0.43710676  0.43841912
  0.43929832  0.43978105  0.44080377  0.44297568  0.4432679   0.44489725
  0.44489981  0.44621474  0.44662827  0.44730925  0.45015956  0.45059785
  0.45086273  0.45103617  0.45186323  0.45249721  0.45312862  0.45731351
  0.45962896  0.46057992  0.46128309  0.46172397  0.4625536   0.46301668
  0.46486133  0.46491347  0.46513391  0.46593362  0.46608487  0.46948711
  0.470151    0.47051496  0.47146592  0.471976    0.47219644  0.47226823
  0.47234255  0.47404366  0.47458109  0.47565338  0.47611391  0.47645566
  0.47718877  0.47733486  0.47911032  0.48051922  0.48151975  0.48351822
  0.48420173  0.48887449  0.49189311  0.49269541  0.49308929  0.49790816
  0.50229384  0.50346267  0.50426495  0.50553034  0.51398214  0.51719642
  0.52408808  0.53061321  0.53209901  0.5371631 ]

  warnings.warn(

2022-11-03 10:49:28,075:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.09152929 -0.091378   -0.08895749 -0.08109082 -0.07755781 -0.07700621
 -0.0758937  -0.07534211 -0.07398056 -0.07032885 -0.06512009 -0.05995556
 -0.05054349 -0.04882585 -0.04609114 -0.04462254 -0.043261   -0.04235331
 -0.03482179 -0.03309252 -0.02414593 -0.01827852 -0.01518771 -0.0149829
 -0.01077959 -0.00926676 -0.00490982 -0.0032783   0.00915707  0.00961091
  0.01218271  0.0199749   0.02306337  0.02397107  0.02418986  0.02639158
  0.02782063  0.02885867  0.03146305  0.03221946  0.03262673  0.03691616
  0.03919703  0.04096819  0.04157331  0.04205279  0.04359585  0.04543453
  0.04603966  0.05004746  0.05143924  0.05214447  0.0528543   0.0528683
  0.05291251  0.05469531  0.05656423  0.05783967  0.05863796  0.06201968
  0.06216862  0.066223    0.07200665  0.0725885   0.07261179  0.07390345
  0.07662653  0.07795783  0.08108357  0.08240784  0.08993936  0.09003015
  0.09179203  0.0919433   0.09286959  0.09342119  0.09523892  0.09797825
  0.09955624  0.10172771  0.10205592  0.10270058  0.10324053  0.10337556
  0.10513744  0.10613819  0.10789769  0.1113493   0.11267825  0.11346724
  0.11397227  0.11473798  0.11475432  0.11550603  0.11663486  0.11676286
  0.1176682   0.12177847  0.12213455  0.12319118  0.13169556  0.14149632
  0.14201768  0.14294396  0.14410069  0.14413562  0.14459876  0.14503627
  0.14564609  0.14779898  0.14943744  0.14972142  0.14977494  0.15178583
  0.1530054   0.15412025  0.15904034  0.15946865  0.16143532  0.16156564
  0.16320411  0.16389536  0.16653927  0.1703772   0.1713803   0.17198543
  0.17230668  0.1729118   0.17305839  0.17416395  0.17518799  0.17785284
  0.17823693  0.1790445   0.18032463  0.18119973  0.18170007  0.18505625
  0.18514697  0.18546587  0.18601514  0.18660166  0.18710903  0.18839377
  0.18925027  0.18936898  0.19212464  0.19288105  0.19333488  0.19360712
  0.19438221  0.19637677  0.19641405  0.19677016  0.19708661  0.19726822
  0.19741951  0.19813168  0.19886941  0.19906031  0.19935358  0.20136214
  0.20187881  0.2023489   0.20273531  0.2034708   0.20431557  0.20471589
  0.20507198  0.20700606  0.20742506  0.20748788  0.20751117  0.20757165
  0.20898906  0.21028778  0.21081845  0.21105809  0.21118148  0.21167956
  0.21176097  0.21330179  0.21490531  0.21519399  0.21708843  0.21843833
  0.21868737  0.21987208  0.22070756  0.22121266  0.22185968  0.22210873
  0.2248411   0.22491087  0.2249505   0.22495744  0.22581856  0.22603736
  0.2261724   0.22669367  0.22770147  0.22786909  0.22880703  0.22886055
  0.22912122  0.22958671  0.23002188  0.23378768  0.2349956   0.23513288
  0.23640136  0.23702049  0.23709955  0.23727881  0.23751386  0.23846109
  0.23855423  0.23887774  0.24092818  0.24105617  0.24160542  0.24208725
  0.24281569  0.24542476  0.24550618  0.24768929  0.24773353  0.24809196
  0.24838054  0.24980959  0.24989336  0.25050553  0.25144347  0.252016
  0.25237444  0.25458545  0.25527434  0.25609133  0.2563473   0.25692219
  0.25779261  0.25903084  0.25974538  0.26115809  0.26130938  0.26220073
  0.26275234  0.26345062  0.26475627  0.26570351  0.26717679  0.26749101
  0.26871751  0.27042124  0.27252519  0.27261365  0.27300928  0.27344218
  0.27348172  0.27438941  0.27668193  0.27734293  0.27867659  0.2792142
  0.27936312  0.27992872  0.28054315  0.28093182  0.28101324  0.28133214
  0.28139965  0.28435544  0.28701803  0.28835159  0.28881474  0.28888696
  0.29082102  0.29108397  0.29149593  0.29167051  0.29205917  0.29305063
  0.29341837  0.29390019  0.29475202  0.29494988  0.29581099  0.29688397
  0.29689786  0.29710267  0.29732616  0.29816164  0.29846421  0.29906933
  0.29993053  0.30123856  0.30130598  0.30191111  0.30236495  0.30251624
  0.3028188   0.30312136  0.30327265  0.30342393  0.30364507  0.30372649
  0.30426185  0.30433162  0.30583983  0.30794849  0.30874209  0.30891666
  0.31192831  0.31224252  0.3151704   0.31537522  0.31609209  0.31688804
  0.31724647  0.31775853  0.31840086  0.3188734   0.31892458  0.31900598
  0.32169187  0.32271592  0.32283695  0.32285555  0.32326517  0.32441955
  0.32534123  0.32599989  0.33001004  0.33064541  0.33097362  0.33125054
  0.33215127  0.33465321  0.33496742  0.33543761  0.33709701  0.34118633
  0.34148654  0.34179145  0.34209861  0.34386048  0.34437255  0.34453548
  0.34523366  0.34663708  0.34718868  0.34834542  0.35261859  0.35383814
  0.35384743  0.3552532   0.35793441  0.36055973  0.36444649  0.36520525
  0.36548217  0.36576379  0.3685404   0.37017192  0.37050706  0.37119369
  0.37132169  0.37144734  0.37380268  0.37387021  0.37395397  0.37528292
  0.37592299  0.37629071  0.37639082  0.37649318  0.3768935   0.37719841
  0.37845984  0.37936753  0.38001924  0.38041721  0.38088035  0.38282608
  0.38286101  0.38353131  0.38420161  0.38442735  0.38463217  0.38473923
  0.3850325   0.38560269  0.38602865  0.3868153   0.38762523  0.38819083
  0.39056241  0.39121413  0.39210782  0.39268036  0.39283164  0.39328549
  0.3946168   0.39525215  0.39721882  0.39826154  0.40006294  0.40272086
  0.4046247   0.40603046  0.4064075   0.40761541  0.40838346  0.41084819
  0.41234703  0.41545184  0.41611049  0.42010664  0.42090729  0.42183826
  0.42214082  0.42249693  0.422995    0.42317652  0.42464511  0.42780344
  0.42861103  0.4308919   0.43093379  0.43194158  0.43296797  0.43359403
  0.43537449  0.43607737  0.43614256  0.4399479   0.44642974  0.44848252
  0.44990921  0.45062375  0.45122653  0.45198294  0.45229715  0.45300004
  0.45425217  0.45613273  0.456284    0.4563934   0.45697525  0.4571917
  0.46007771  0.46397845  0.4656309   0.46623605  0.46711114  0.467309
  0.467565    0.46922911  0.47119576  0.4723432   0.47394213  0.47404918
  0.47519426  0.47899961  0.48026339  0.48184138  0.48223935  0.48325646
  0.48460635  0.48465988  0.48481114  0.48581661  0.48755752  0.4875901
  0.48770881  0.49024802  0.49032247  0.49380196  0.49791918  0.49933423
  0.50042813  0.50984951  0.51213969  0.51612656  0.52599247  0.52681404
  0.53056352  0.53486458]

  warnings.warn(

2022-11-03 10:49:28,199:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.08810503 -0.08202341 -0.08130011 -0.07682153 -0.0759418  -0.0752185
 -0.07492918 -0.07014951 -0.06784671 -0.06102414 -0.05278439 -0.04757663
 -0.04366491 -0.03872293 -0.03859592 -0.03801728 -0.02742767 -0.02424514
 -0.02242927 -0.02134605 -0.01860928 -0.01425182 -0.01368494 -0.01062943
 -0.00788088 -0.00353519 -0.00275547  0.00427058  0.01447086  0.01619502
  0.02803604  0.03111856  0.03531371  0.04032628  0.04556105  0.04758282
  0.04991852  0.05439362  0.05598488  0.05716917  0.05828768  0.0624863
  0.06395644  0.06401526  0.06573942  0.06697077  0.06697665  0.06730127
  0.06877729  0.06958295  0.06970996  0.07293127  0.07351578  0.07688415
  0.07872008  0.08418534  0.08447468  0.08482282  0.08592128  0.08610363
  0.08840991  0.08940487  0.09096914  0.09143601  0.09266388  0.09597342
  0.0982551   0.09908535  0.09993219  0.09994396  0.10012391  0.10087663
  0.10171865  0.10183043  0.10360512  0.10461773  0.1056563   0.10567742
  0.10807782  0.10849414  0.11082047  0.11258583  0.11497686  0.11507098
  0.11572716  0.11660688  0.11682803  0.11789118  0.11812997  0.11980706
  0.12154298  0.12303318  0.12606863  0.12694835  0.12853615  0.12868081
  0.13220319  0.13238074  0.1348635   0.13517876  0.13519641  0.13601731
  0.13617133  0.13915624  0.14123444  0.1427305   0.14514614  0.14636814
  0.14827572  0.14832279  0.15020337  0.15069858  0.1509208   0.15192164
  0.15275431  0.15327654  0.15494427  0.1565743   0.15694945  0.15709412
  0.15880544  0.16000496  0.1600285   0.16016381  0.16191496  0.16248772
  0.16300059  0.16311584  0.16473411  0.16503172  0.16641708  0.16664169
  0.16722622  0.16740618  0.16967956  0.1705887   0.1717048   0.17241527
  0.1731514   0.17373005  0.1748991   0.17580476  0.17633394  0.17662326
  0.17705028  0.17770888  0.17820756  0.17898729  0.17907553  0.18014217
  0.18036918  0.18113713  0.18171578  0.18313885  0.18404211  0.18691178
  0.18703879  0.18778215  0.18822549  0.18849021  0.18925577  0.18997907
  0.19008147  0.19022131  0.19037079  0.1906553   0.19108233  0.19272173
  0.19306987  0.19330277  0.19432475  0.19529513  0.19558552  0.19654171
  0.19716394  0.20053337  0.20184121  0.20303378  0.20545771  0.20788752
  0.21054436  0.21093608  0.211323    0.21262254  0.21485475  0.21531466
  0.21817498  0.22018845  0.2221054   0.22353329  0.2238178   0.22405069
  0.22519274  0.22589251  0.22628772  0.22734847  0.22808943  0.22848222
  0.22936783  0.2293796   0.23084384  0.23249394  0.232641    0.23516601
  0.23541656  0.23558234  0.23561763  0.2361551   0.23863785  0.23960823
  0.24057727  0.24117943  0.24180862  0.2429413   0.24333758  0.24432907
  0.24582622  0.24811135  0.24848063  0.2493427   0.24960155  0.25183136
  0.25224421  0.25352851  0.25495986  0.25519384  0.25677333  0.25681103
  0.25732255  0.25741909  0.25887157  0.25918548  0.26198109  0.26241748
  0.2645192   0.26480852  0.2659987   0.26784639  0.26920476  0.2695882
  0.27057489  0.2732047   0.27436787  0.27498528  0.27552515  0.27581447
  0.27601903  0.27730333  0.27940505  0.28185491  0.28258997  0.28280282
  0.28363548  0.28513263  0.28667576  0.28696508  0.28768838  0.28845045
  0.2888256   0.28899032  0.28913498  0.28927964  0.29089551  0.29262795
  0.29304669  0.29465666  0.29579524  0.2961268   0.29631852  0.29680786
  0.29710306  0.2976817   0.29815445  0.298405    0.29854966  0.29886011
  0.29945987  0.30202739  0.30273304  0.3027507   0.30332933  0.30361866
  0.30376331  0.30390798  0.30395851  0.30405263  0.30448661  0.30535458
  0.3056439   0.30700226  0.3075162   0.30791487  0.30835474  0.30844887
  0.30867     0.30903927  0.31081986  0.31127976  0.31223115  0.31270872
  0.31340261  0.31523961  0.31667445  0.31772236  0.31895264  0.31900318
  0.31953128  0.31987703  0.32083323  0.32097789  0.32262556  0.32267612
  0.32408153  0.32499308  0.32671135  0.32733706  0.32892832  0.32937167
  0.33108993  0.33199319  0.33214722  0.33257182  0.33284003  0.33292935
  0.33551558  0.3364953   0.33666696  0.33781837  0.33854167  0.34018587
  0.34040703  0.34240285  0.34252638  0.34275688  0.34591     0.3461052
  0.3461699   0.34641805  0.34665441  0.34942889  0.34984523  0.35408155
  0.35463078  0.35498828  0.35685122  0.3606418   0.36074769  0.36667636
  0.36868048  0.369165    0.3708798   0.37191943  0.37201943  0.37219109
  0.37257213  0.37412463  0.37421287  0.3747915   0.37511959  0.375149
  0.37540303  0.37736946  0.37737774  0.37739539  0.37754005  0.37969231
  0.38055198  0.3825537   0.38357807  0.38594559  0.38645604  0.38668065
  0.38708762  0.38822726  0.38898345  0.38944097  0.38996318  0.39015839
  0.3913039   0.39133331  0.39244006  0.39272697  0.39330802  0.39360669
  0.39378665  0.39460996  0.39755371  0.39792539  0.39850991  0.40113492
  0.4017018   0.40288848  0.40512311  0.4057488   0.4068002   0.40766815
  0.40769756  0.40948644  0.41167399  0.4141038   0.41426022  0.41609964
  0.41768501  0.41807192  0.41872116  0.41939739  0.42035948  0.42090872
  0.42105925  0.42129564  0.42306097  0.42479689  0.42663631  0.42691384
  0.42704676  0.43247564  0.43421746  0.43687776  0.43698366  0.43890302
  0.44060604  0.44349925  0.4438744   0.44465653  0.44504933  0.44552448
  0.44668765  0.44738153  0.44767087  0.44784493  0.44841181  0.44853294
  0.44885755  0.44982312  0.45073814  0.45088281  0.45121089  0.45152962
  0.45160611  0.45182484  0.45210828  0.45243289  0.45377013  0.45563306
  0.45592238  0.45615287  0.45676094  0.45768183  0.459406    0.46007047
  0.46201576  0.46258852  0.46332358  0.46373993  0.46375756  0.4638787
  0.46433622  0.46445145  0.46457258  0.46790565  0.46968864  0.47255831
  0.47281821  0.47312519  0.47402257  0.47416722  0.47648767  0.48097215
  0.48193181  0.48196711  0.48226232  0.4830891   0.48443223  0.48703022
  0.48800754  0.49138768  0.49471487  0.49772919  0.50060476  0.50237597
  0.50366027  0.50860224  0.51132727  0.5135148   0.51495553  0.51568471
  0.51727598  0.5187167 ]

  warnings.warn(

2022-11-03 10:49:28,199:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.17171363e-02 -8.06973340e-02 -7.36368784e-02 -7.30541350e-02
 -6.68570992e-02 -6.38544625e-02 -5.88229246e-02 -5.71635809e-02
 -5.67833089e-02 -5.62112635e-02 -4.86141533e-02 -4.69548096e-02
 -4.34369199e-02 -3.98836560e-02 -2.78484494e-02 -2.53717747e-02
 -2.09904707e-02 -1.65309727e-02 -1.62256255e-02 -8.14863669e-03
 -2.96813971e-03 -5.02163009e-04  7.73112793e-05  5.98297256e-03
  8.10405700e-03  1.13980627e-02  1.23289619e-02  1.40171472e-02
  1.78831982e-02  2.32982772e-02  2.39913364e-02  2.62655342e-02
  3.28922000e-02  3.78669454e-02  4.26606377e-02  4.49520884e-02
  4.51405760e-02  4.52829969e-02  4.55175845e-02  4.99054267e-02
  5.12775557e-02  5.14404931e-02  5.62481580e-02  5.67955105e-02
  5.89207547e-02  6.32345683e-02  6.62264959e-02  6.72463038e-02
  6.75376783e-02  6.84858304e-02  6.87031706e-02  7.03550799e-02
  7.06752960e-02  7.43207266e-02  7.47717693e-02  7.72698455e-02
  8.03860450e-02  8.32150519e-02  8.35664880e-02  8.46002685e-02
  8.78663233e-02  9.05380349e-02  9.17174999e-02  9.26344431e-02
  9.47843580e-02  9.49407460e-02  9.67490378e-02  9.92405869e-02
  9.95845941e-02  1.02306543e-01  1.02352632e-01  1.04054779e-01
  1.04652402e-01  1.07540569e-01  1.08400709e-01  1.08617164e-01
  1.10092163e-01  1.12266760e-01  1.12696395e-01  1.14153262e-01
  1.14590305e-01  1.14832332e-01  1.14942642e-01  1.16527050e-01
  1.17941103e-01  1.18940400e-01  1.18971609e-01  1.19960208e-01
  1.20304209e-01  1.20595578e-01  1.22329847e-01  1.23243510e-01
  1.23409705e-01  1.23555401e-01  1.24554686e-01  1.26089753e-01
  1.27801713e-01  1.28586938e-01  1.28712112e-01  1.28960672e-01
  1.29461067e-01  1.29752436e-01  1.34180714e-01  1.34503309e-01
  1.35725577e-01  1.35918252e-01  1.36901778e-01  1.36955296e-01
  1.38458252e-01  1.38735654e-01  1.39243478e-01  1.39901140e-01
  1.39997478e-01  1.42349870e-01  1.42707839e-01  1.43223993e-01
  1.43237960e-01  1.43884040e-01  1.45038834e-01  1.45963195e-01
  1.46944354e-01  1.48151759e-01  1.51303366e-01  1.51605439e-01
  1.51950325e-01  1.54295277e-01  1.54586668e-01  1.55383477e-01
  1.56015579e-01  1.56506161e-01  1.56687220e-01  1.57038656e-01
  1.57667482e-01  1.58150636e-01  1.60212544e-01  1.60481609e-01
  1.61378036e-01  1.63541038e-01  1.63672756e-01  1.65872034e-01
  1.67478740e-01  1.67681217e-01  1.69222810e-01  1.69226987e-01
  1.69802296e-01  1.70431122e-01  1.70441826e-01  1.71116741e-01
  1.71280570e-01  1.71418814e-01  1.72651808e-01  1.75721941e-01
  1.75924406e-01  1.77201100e-01  1.78753402e-01  1.78859547e-01
  1.79037353e-01  1.79095025e-01  1.80081838e-01  1.82743742e-01
  1.83501011e-01  1.84381667e-01  1.87287972e-01  1.87701245e-01
  1.87963778e-01  1.87992614e-01  1.88261685e-01  1.89147409e-01
  1.89502108e-01  1.89910337e-01  1.90081583e-01  1.90604276e-01
  1.92150040e-01  1.93730288e-01  1.93787080e-01  1.94661209e-01
  1.97021019e-01  1.97327268e-01  1.97486040e-01  1.97955215e-01
  1.98695237e-01  2.00332277e-01  2.00496105e-01  2.03144032e-01
  2.04948158e-01  2.07165569e-01  2.10718855e-01  2.10943619e-01
  2.14955354e-01  2.15299350e-01  2.18530031e-01  2.18725073e-01
  2.18735777e-01  2.19467485e-01  2.19861735e-01  2.20949918e-01
  2.21095603e-01  2.21812437e-01  2.22630664e-01  2.23270200e-01
  2.23841360e-01  2.24233221e-01  2.25544397e-01  2.27693422e-01
  2.28507483e-01  2.28653179e-01  2.30195669e-01  2.30284566e-01
  2.30867315e-01  2.30933909e-01  2.33936546e-01  2.34281433e-01
  2.35024735e-01  2.40113066e-01  2.41278563e-01  2.41469435e-01
  2.46690373e-01  2.47127438e-01  2.47766968e-01  2.48181735e-01
  2.49167048e-01  2.50119366e-01  2.55864473e-01  2.55879342e-01
  2.55945948e-01  2.56550093e-01  2.57459585e-01  2.59495937e-01
  2.60104253e-01  2.61639314e-01  2.63010552e-01  2.65135796e-01
  2.65433714e-01  2.66076519e-01  2.66108630e-01  2.66623872e-01
  2.68273414e-01  2.69168037e-01  2.69260215e-01  2.69370525e-01
  2.70443845e-01  2.70885069e-01  2.71293292e-01  2.72341936e-01
  2.75024351e-01  2.76278741e-01  2.76510060e-01  2.77384183e-01
  2.78161969e-01  2.78292785e-01  2.80304455e-01  2.81104544e-01
  2.81150628e-01  2.81195815e-01  2.81882336e-01  2.81935853e-01
  2.82791828e-01  2.84550767e-01  2.84824905e-01  2.86359070e-01
  2.88321388e-01  2.88889257e-01  2.90118976e-01  2.91724813e-01
  2.91750375e-01  2.92094382e-01  2.92737187e-01  2.93601492e-01
  2.94038545e-01  2.96176866e-01  2.96535731e-01  2.98093095e-01
  3.00099698e-01  3.00210904e-01  3.00323593e-01  3.00939338e-01
  3.01230712e-01  3.01376397e-01  3.01667771e-01  3.02833264e-01
  3.03270328e-01  3.03273592e-01  3.03280141e-01  3.04144446e-01
  3.04890132e-01  3.05153550e-01  3.05309938e-01  3.06104972e-01
  3.08156182e-01  3.10916791e-01  3.11360405e-01  3.13002495e-01
  3.13016462e-01  3.15508006e-01  3.15511275e-01  3.15844568e-01
  3.16382130e-01  3.16673498e-01  3.16804325e-01  3.17120376e-01
  3.19398733e-01  3.20016857e-01  3.20159277e-01  3.20187234e-01
  3.20596336e-01  3.20834193e-01  3.20929623e-01  3.20993845e-01
  3.23073011e-01  3.23232674e-01  3.23470536e-01  3.23801445e-01
  3.24671397e-01  3.24784982e-01  3.25747992e-01  3.26298641e-01
  3.26724980e-01  3.29233782e-01  3.29987777e-01  3.30033866e-01
  3.30232177e-01  3.32713012e-01  3.32918763e-01  3.33153340e-01
  3.33437280e-01  3.33490797e-01  3.33576426e-01  3.35893444e-01
  3.35956763e-01  3.40370178e-01  3.40767703e-01  3.41265714e-01
  3.42725855e-01  3.44019785e-01  3.44381913e-01  3.44630468e-01
  3.44687260e-01  3.46094775e-01  3.46478310e-01  3.47085752e-01
  3.47996135e-01  3.49591257e-01  3.49644775e-01  3.50888462e-01
  3.51364175e-01  3.53663060e-01  3.54822009e-01  3.54878790e-01
  3.55468088e-01  3.55702665e-01  3.56225358e-01  3.56268172e-01
  3.59511934e-01  3.60251071e-01  3.60336689e-01  3.60339952e-01
  3.61896426e-01  3.62670948e-01  3.62908799e-01  3.64948426e-01
  3.65751784e-01  3.67375732e-01  3.67457206e-01  3.68459766e-01
  3.69188200e-01  3.69916633e-01  3.70022778e-01  3.70111681e-01
  3.72787553e-01  3.73054251e-01  3.73977721e-01  3.75931697e-01
  3.76340828e-01  3.76685714e-01  3.77530985e-01  3.77865168e-01
  3.78007589e-01  3.78828200e-01  3.80423312e-01  3.80984665e-01
  3.81780572e-01  3.83067084e-01  3.84779934e-01  3.88788406e-01
  3.89296230e-01  3.89576895e-01  3.90006531e-01  3.90884814e-01
  3.92903012e-01  3.93805075e-01  3.94516278e-01  3.98460517e-01
  3.99156840e-01  3.99448209e-01  4.00073772e-01  4.00222731e-01
  4.00357723e-01  4.00816172e-01  4.00997253e-01  4.01132223e-01
  4.01800612e-01  4.03082947e-01  4.04668256e-01  4.05421372e-01
  4.06668322e-01  4.09219916e-01  4.13444843e-01  4.13960085e-01
  4.14691793e-01  4.14951051e-01  4.15697639e-01  4.16244991e-01
  4.17925742e-01  4.20103603e-01  4.20942346e-01  4.21120152e-01
  4.22558887e-01  4.22903773e-01  4.26581326e-01  4.27544336e-01
  4.28013511e-01  4.28429168e-01  4.29562533e-01  4.30241625e-01
  4.31310791e-01  4.32362704e-01  4.32416222e-01  4.33943837e-01
  4.34124918e-01  4.35411408e-01  4.35543125e-01  4.35613896e-01
  4.37205732e-01  4.37586004e-01  4.41071794e-01  4.41740161e-01
  4.43591278e-01  4.43690885e-01  4.43779788e-01  4.44465407e-01
  4.44788886e-01  4.45751896e-01  4.46188961e-01  4.46607893e-01
  4.47354459e-01  4.47937197e-01  4.48374261e-01  4.48431053e-01
  4.49294457e-01  4.50694553e-01  4.51344787e-01  4.51579364e-01
  4.54382787e-01  4.54482394e-01  4.55928557e-01  4.55974646e-01
  4.56589494e-01  4.57182958e-01  4.58550922e-01  4.60526305e-01
  4.60903325e-01  4.61397159e-01  4.61777431e-01  4.64399796e-01
  4.64513381e-01  4.64545492e-01  4.64826157e-01  4.68031259e-01
  4.71563116e-01  4.72291550e-01  4.72380453e-01  4.75180602e-01
  4.75617666e-01  4.77511597e-01  4.79114160e-01  4.83474057e-01
  4.86131784e-01  4.88651268e-01  4.89649685e-01  4.90040661e-01
  4.92058858e-01  4.92876195e-01  4.96138090e-01  5.03859489e-01
  5.04026569e-01  5.05405236e-01  5.11445895e-01  5.13364486e-01
  5.42040098e-01]

  warnings.warn(

2022-11-03 10:49:28,215:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.09020317 -0.08723064 -0.08473821 -0.08131981 -0.07734112 -0.07415154
 -0.0732138  -0.07311115 -0.07225361 -0.07013863 -0.0676462  -0.06734895
 -0.06596533 -0.06574828 -0.06302703 -0.06028332 -0.0475699  -0.04385424
 -0.03510775 -0.03481049 -0.01614134 -0.01042403 -0.00808023 -0.00570221
 -0.00552045  0.00359281  0.00935502  0.01009815  0.01273921  0.01561869
  0.0229602   0.0271089   0.02986436  0.03047171  0.03136346  0.03663381
  0.03682842  0.03846331  0.03967369  0.04199612  0.04370905  0.04430354
  0.04775833  0.05099172  0.05262878  0.05597658  0.05685874  0.05840059
  0.06124695  0.0655571   0.06712355  0.06766248  0.07473771  0.07567761
  0.07614485  0.07637368  0.0832447   0.08334842  0.08601301  0.086708
  0.08834505  0.08909995  0.09037767  0.09818644  0.09841527  0.09933055
  0.09983417  0.10182191  0.1046458   0.10464687  0.10663676  0.10765469
  0.10922007  0.10989369  0.11037486  0.11148261  0.11210065  0.1126727
  0.1152111   0.1174651   0.120289    0.12187575  0.12258682  0.12270123
  0.12499691  0.1279459   0.12953804  0.13007263  0.13093126  0.13197057
  0.13476133  0.13594073  0.13719817  0.13733287  0.13782473  0.13791669
  0.13909394  0.14164304  0.14229745  0.14405636  0.14475351  0.1448786
  0.14508712  0.14627614  0.14854935  0.1504815   0.15116581  0.15170365
  0.15812987  0.15999573  0.16143385  0.16162739  0.16182198  0.16200699
  0.16222512  0.16239296  0.16253198  0.16335315  0.16339913  0.16409628
  0.16455608  0.16492176  0.16676086  0.1672442   0.16757244  0.17048721
  0.17094484  0.17179063  0.17226327  0.17322022  0.17432051  0.17650176
  0.17655843  0.17672951  0.1772802   0.17747264  0.17793244  0.17801263
  0.17880067  0.179809    0.18044517  0.180998    0.18364652  0.18472326
  0.18592296  0.18639021  0.18818873  0.18856618  0.1889746   0.19077096
  0.19210861  0.19304418  0.19308271  0.19386858  0.19653209  0.19679836
  0.19723993  0.19757356  0.19796384  0.19937844  0.1998949   0.20007559
  0.20033118  0.20058245  0.20235096  0.20251352  0.20374634  0.20445849
  0.20578221  0.20603348  0.20655848  0.20713377  0.20778171  0.20879002
  0.2092562   0.21143109  0.2126297   0.21299754  0.21349048  0.21430204
  0.21536487  0.21539801  0.21653997  0.21996905  0.22099876  0.22159649
  0.22173012  0.22201884  0.2227705   0.22489724  0.22502233  0.22517313
  0.22572381  0.22582537  0.22618891  0.22646694  0.22669144  0.22748055
  0.22884496  0.22955387  0.23036112  0.23173405  0.2331209   0.23509471
  0.23524335  0.23535991  0.23905096  0.23985399  0.24028593  0.24301043
  0.24379523  0.24485914  0.24538845  0.24539699  0.24539806  0.24541943
  0.24563649  0.24936714  0.2494548   0.25115065  0.25224447  0.25263584
  0.25310417  0.25321643  0.25340143  0.25453376  0.25468347  0.25590345
  0.25614298  0.25631836  0.25881079  0.25955392  0.26040715  0.26051088
  0.26086695  0.26104018  0.26118881  0.26240988  0.26376888  0.26378388
  0.26471839  0.26537171  0.26570318  0.26629662  0.26665376  0.26784277
  0.26905423  0.26925845  0.26969472  0.27101734  0.27136059  0.27137558
  0.27182146  0.27249401  0.27265224  0.27290675  0.27451811  0.27580015
  0.27591134  0.27651653  0.27848396  0.27882612  0.27892875  0.28046209
  0.28062247  0.28096462  0.28318225  0.28422263  0.28438626  0.28539987
  0.28640604  0.28653546  0.28673006  0.28762182  0.28962239  0.29083386
  0.29090013  0.29480931  0.29528196  0.29586361  0.29632234  0.29649018
  0.29655968  0.29736272  0.29751135  0.29765997  0.29772518  0.29914623
  0.29929486  0.29944349  0.30060682  0.30078113  0.30197014  0.30241386
  0.30296238  0.30325963  0.30327356  0.30372688  0.30380707  0.30489345
  0.30634765  0.30822097  0.30845086  0.30946664  0.31137525  0.31177946
  0.31213122  0.313517    0.31518502  0.31554002  0.31633981  0.31815646
  0.32052272  0.32099105  0.32163262  0.32250085  0.32271898  0.32394113
  0.32556533  0.32589895  0.32655866  0.32789628  0.32891638  0.32916658
  0.33017166  0.33072019  0.33151998  0.33471173  0.33726081  0.33735386
  0.33825631  0.33892885  0.33999491  0.34044079  0.34048463  0.34137637
  0.34479694  0.34541174  0.34607575  0.34624683  0.34640721  0.34660183
  0.34844201  0.35031748  0.35334666  0.35434323  0.35691477  0.35749643
  0.35751036  0.35767927  0.35796585  0.35805888  0.35846946  0.35886936
  0.35892604  0.35979534  0.36005945  0.36273365  0.36292933  0.36428834
  0.3653747   0.36758057  0.37002917  0.37033819  0.37086211  0.37104603
  0.37122995  0.37237298  0.37240503  0.37241896  0.37277072  0.37316209
  0.37331071  0.37386885  0.37390522  0.37509423  0.37551442  0.37593893
  0.37598599  0.37679647  0.37703599  0.37751716  0.37872969  0.38133437
  0.38295966  0.38298211  0.38356592  0.38595464  0.38705276  0.38757884
  0.38998891  0.39105175  0.39349927  0.39522504  0.39664288  0.39682787
  0.397571    0.39759346  0.39840502  0.40288625  0.40321665  0.406727
  0.40704778  0.40734503  0.41170119  0.41192892  0.4120102   0.41226147
  0.41319922  0.41347293  0.41364508  0.41417008  0.41624016  0.41681114
  0.41742918  0.41794348  0.41852731  0.41942974  0.42098443  0.42111062
  0.42264286  0.42314433  0.42388853  0.42462097  0.42467764  0.42594577
  0.42801586  0.42888409  0.42897605  0.43155934  0.43212072  0.43585882
  0.43624803  0.43758568  0.43862498  0.43879713  0.43910615  0.44154083
  0.44394237  0.44446737  0.44598894  0.44676629  0.44704     0.44763451
  0.44891549  0.44911008  0.44940733  0.45081233  0.45123575  0.45200241
  0.45362447  0.45434621  0.45583247  0.4570557   0.45773041  0.46169733
  0.46189194  0.46352682  0.4640978   0.46508153  0.46563004  0.46575621
  0.47031765  0.47074     0.47124363  0.47322176  0.47381625  0.47441075
  0.47482242  0.47515281  0.47942984  0.48772938  0.49072542  0.49282864
  0.49703616  0.49982584  0.50051122  0.50078603  0.50199748  0.50311808
  0.50316406  0.50494757  0.50950899  0.50952075  0.51026389  0.53860552
  0.54378498  0.54411646]

  warnings.warn(

2022-11-03 10:49:28,283:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.08518249 -0.08399202 -0.08265273 -0.0742899  -0.07399229 -0.06641776
 -0.05400758 -0.04772808 -0.04205858 -0.04027287 -0.03583809 -0.03539166
 -0.03356169 -0.03165668 -0.02793644 -0.00405319 -0.00400893 -0.00107571
 -0.00058632  0.00520249  0.01017221  0.0119002   0.01286682  0.01719833
  0.01732981  0.02253942  0.02285051  0.03490665  0.03642166  0.03939913
  0.04354975  0.04552853  0.04666259  0.04985875  0.05164447  0.05390869
  0.05753783  0.0587591   0.06164419  0.06268586  0.06417395  0.0643709
  0.06621432  0.06713797  0.06899744  0.07548346  0.07642057  0.0785039
  0.07968221  0.08061804  0.08088614  0.08188355  0.08357817  0.08503676
  0.08927848  0.09056133  0.09207893  0.0929718   0.09313147  0.09392365
  0.09423603  0.09729943  0.09770418  0.09951811  0.09984265  0.09987474
  0.10201967  0.10212163  0.10368606  0.10402406  0.10615552  0.10775905
  0.1086519   0.11009574  0.11098989  0.11258254  0.11414568  0.11427715
  0.11481468  0.11595831  0.11630535  0.11770364  0.11932451  0.1199332
  0.12076963  0.12185426  0.1232981   0.12448728  0.12453283  0.12538273
  0.12640447  0.13032165  0.1308835   0.13088867  0.1346089   0.13584105
  0.1358571   0.13600719  0.13786537  0.14015522  0.14110967  0.14162727
  0.14441428  0.14527762  0.14538088  0.14563555  0.14627503  0.14651365
  0.14703254  0.14822432  0.14829936  0.15045646  0.1505597   0.15164305
  0.15366997  0.15518496  0.15538192  0.15655246  0.1571477   0.15880453
  0.15884619  0.16160239  0.16222713  0.16226881  0.16282108  0.16473827
  0.16574914  0.16835653  0.16871574  0.16928148  0.1713648   0.1764352
  0.17805995  0.17972764  0.17997453  0.18195978  0.182106    0.18296548
  0.18380321  0.18423617  0.18471081  0.18490646  0.18545098  0.18648178
  0.1867191   0.1871386   0.18986143  0.19071002  0.19213524  0.19503894
  0.19622942  0.19889453  0.20042043  0.20172192  0.20175971  0.20200349
  0.20246597  0.20337228  0.20474106  0.20509899  0.206335    0.20679359
  0.20683397  0.20698279  0.20772683  0.20781922  0.21042015  0.21093075
  0.2111642   0.21172165  0.21299933  0.21320586  0.21413079  0.21549442
  0.21585493  0.21615254  0.21718204  0.21721155  0.21765668  0.21772656
  0.21784587  0.21817299  0.21970792  0.21975347  0.21981118  0.22039555
  0.22121465  0.22220559  0.22245895  0.22289579  0.22304461  0.2317506
  0.23190771  0.23212069  0.23246255  0.23253631  0.23277364  0.23310464
  0.23550992  0.23649517  0.23706219  0.24035075  0.24319159  0.24375474
  0.24428838  0.24522808  0.24548274  0.24592528  0.24659686  0.24827931
  0.24842942  0.2484698   0.24866932  0.24921384  0.25090016  0.25150757
  0.25297004  0.25375706  0.25379743  0.2544703   0.25532978  0.2557762
  0.25686858  0.25698789  0.25789163  0.25796409  0.25872934  0.25967862
  0.26077671  0.26156502  0.26241879  0.26359451  0.263844    0.26418976
  0.26420192  0.26592474  0.26780284  0.26802283  0.2688176   0.26923064
  0.26965973  0.27023063  0.27025497  0.27174306  0.27328188  0.273786
  0.27466282  0.27626893  0.27716567  0.27744852  0.27906681  0.27939263
  0.28080567  0.28233802  0.28332456  0.28464522  0.28488512  0.28726348
  0.28768557  0.28774589  0.28790945  0.28917108  0.28921922  0.29004661
  0.29114728  0.29125184  0.29278418  0.29323061  0.29352823  0.29443583
  0.29454039  0.29463148  0.29494256  0.29511129  0.29512734  0.29552434
  0.29562631  0.29564623  0.29592393  0.29637036  0.29696559  0.29766538
  0.29830488  0.29845369  0.29884627  0.29904893  0.29919773  0.29933695
  0.29949535  0.29979297  0.30068583  0.30083464  0.3015999   0.3021483
  0.3041232   0.30418737  0.30851501  0.30911025  0.31036102  0.31125129
  0.31131159  0.31344305  0.31613637  0.31765527  0.31821583  0.3187386
  0.32159678  0.32173211  0.32251655  0.32351654  0.32988326  0.33019692
  0.3322039   0.33281778  0.33295313  0.33360737  0.33473884  0.33521476
  0.33542129  0.33637445  0.3371159   0.34006517  0.34088168  0.34175721
  0.34196889  0.34263918  0.34436459  0.34532863  0.34595853  0.34835036
  0.35186407  0.35204368  0.35546371  0.35610578  0.3562828   0.35648933
  0.35736872  0.35790367  0.35826416  0.35903513  0.35954316  0.36120826
  0.36178746  0.36510552  0.3659875   0.36784568  0.36894506  0.36949604
  0.37138889  0.37156592  0.37211432  0.37270954  0.3728141   0.37360241
  0.37400845  0.37449526  0.37458766  0.37479288  0.37523931  0.37620721
  0.37625147  0.37848361  0.37903459  0.37911223  0.37989923  0.38037517
  0.38094219  0.38223463  0.38234048  0.38333531  0.38357263  0.38442382
  0.38470539  0.3872794   0.38732625  0.39077708  0.39129855  0.39171417
  0.39218882  0.39251981  0.39274239  0.39410859  0.39567172  0.39732338
  0.39774161  0.39924316  0.4006857   0.40101413  0.40147658  0.40172867
  0.40186143  0.40186272  0.40192301  0.4023534   0.40244191  0.40425713
  0.40802548  0.4087227   0.40876825  0.40882855  0.40903508  0.41044941
  0.41058347  0.41071752  0.41073226  0.41155137  0.41254748  0.41568853
  0.41589506  0.41689247  0.41750116  0.41760571  0.41803868  0.41860442
  0.42190901  0.42339452  0.42547787  0.4255247   0.42842454  0.43037511
  0.43084975  0.43116469  0.43129616  0.43251745  0.43474829  0.43784507
  0.43875267  0.43927287  0.44137095  0.44221954  0.44226382  0.44337924
  0.44443564  0.44651898  0.44662353  0.44666778  0.44726303  0.44815587
  0.44863312  0.44946566  0.44952596  0.45110258  0.45111862  0.45244185
  0.45408004  0.45553735  0.45602803  0.45753087  0.45767968  0.45844105
  0.45974954  0.45991182  0.46139992  0.46166933  0.46215871  0.46224851
  0.46226455  0.46361859  0.46446591  0.4661041   0.46689112  0.46894493
  0.46948117  0.4756856   0.47747133  0.47806656  0.4786323   0.47973426
  0.48123709  0.48163797  0.48171303  0.4825161   0.4827547   0.48326014
  0.48784371  0.48802204  0.49101297  0.49443689  0.49515013  0.50276892
  0.50604272  0.50887009  0.51020939  0.51107272  0.51616176  0.51631186
  0.53038845]

  warnings.warn(

2022-11-03 10:49:30,574:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.09117087 -0.08840855 -0.08571687 -0.08448917 -0.07726428 -0.07674541
 -0.0735227  -0.07306231 -0.072295   -0.07088951 -0.06614434 -0.0658959
 -0.06099727 -0.05890728 -0.05278096 -0.043773   -0.04254529 -0.03939323
 -0.02618331 -0.02389354 -0.00736593 -0.00559503 -0.00513464 -0.00075488
 -0.00060142  0.0021609   0.00561989  0.00691589  0.0102579   0.01107388
  0.01491044  0.01790904  0.01966542  0.02127304  0.02535569  0.02955766
  0.03129207  0.03215436  0.03499949  0.03763268  0.04370288  0.04956859
  0.05031392  0.05063301  0.05875435  0.05883716  0.05923907  0.06604989
  0.06815952  0.07290938  0.07413708  0.074444    0.07583263  0.07606144
  0.07633935  0.07759606  0.08066296  0.08273844  0.08600748  0.08688192
  0.08914736  0.09273035  0.09299843  0.09500561  0.09668153  0.09878135
  0.09987243  0.09990424  0.10139489  0.1026642   0.10307827  0.10380909
  0.1040332   0.10446414  0.10510745  0.1052609   0.10633513  0.10710245
  0.10724609  0.10735089  0.1074749   0.10768214  0.10795257  0.10825951
  0.10965752  0.11036166  0.11131893  0.1141884   0.1146951   0.11790564
  0.12184699  0.12198831  0.12268731  0.12290907  0.12305741  0.12306957
  0.12370072  0.12974424  0.12982705  0.13021679  0.13137383  0.13204617
  0.13477669  0.1350518   0.13520526  0.13626733  0.13651577  0.13767795
  0.13950734  0.14304401  0.1452023   0.14633033  0.14711684  0.14850313
  0.14999378  0.1506198   0.1516014   0.15349161  0.15543327  0.15615191
  0.15644667  0.15737961  0.15774971  0.15879214  0.15926983  0.15955476
  0.15974473  0.15981303  0.16097989  0.16105054  0.1619442   0.16311854
  0.16538867  0.16641425  0.17123475  0.17154168  0.17278432  0.17278902
  0.17508626  0.17575112  0.17848911  0.17884704  0.17910062  0.18253527
  0.18337324  0.18350705  0.18443252  0.18524614  0.18567517  0.18577014
  0.18613554  0.18683691  0.18851048  0.18981865  0.19107772  0.19310217
  0.1932042   0.19360889  0.19365521  0.19383769  0.19391581  0.1953213
  0.19630056  0.19650972  0.19683161  0.19729902  0.19983019  0.19984705
  0.19987652  0.20104807  0.20158892  0.20436576  0.20782944  0.20814619
  0.20818781  0.20864072  0.20879888  0.20987823  0.21143719  0.211469
  0.21198039  0.21317863  0.21374895  0.21418736  0.21727859  0.21817737
  0.22007227  0.22030152  0.22033801  0.22109081  0.22169485  0.22366553
  0.22376285  0.22436687  0.22936048  0.22984754  0.23010111  0.23015447
  0.23149209  0.23196464  0.23286109  0.23407384  0.23416177  0.23470263
  0.23477798  0.23479762  0.2361203   0.23628592  0.23676082  0.23834177
  0.23866832  0.23898463  0.23968878  0.24055854  0.24106246  0.24224663
  0.24229016  0.24332086  0.24332555  0.24363994  0.2439249   0.24424398
  0.24565416  0.24605606  0.2466774   0.24759583  0.24907665  0.24956651
  0.25003906  0.25021449  0.25088405  0.25204858  0.25238731  0.25265774
  0.2526891   0.25335161  0.25389012  0.25596047  0.25624072  0.25668659
  0.25742956  0.25758069  0.25769252  0.25892257  0.25895907  0.25967537
  0.25979981  0.26040149  0.2630319   0.26343895  0.26445938  0.26490526
  0.26686611  0.26877128  0.26909975  0.26952129  0.27018893  0.27022777
  0.27117054  0.27163796  0.27409335  0.27536269  0.27551379  0.27591805
  0.28034411  0.28048777  0.28062159  0.28232931  0.28377365  0.28546174
  0.2856152   0.28607558  0.2861902   0.28658741  0.28994395  0.29024853
  0.29069674  0.29122265  0.29134475  0.29311798  0.29338094  0.29406779
  0.29422125  0.294465    0.2948351   0.29518366  0.29535397  0.29590933
  0.2960628   0.29636973  0.29652319  0.29667665  0.29698357  0.29713704
  0.29719831  0.2975684   0.29761706  0.29897858  0.30012112  0.30107136
  0.30224058  0.30359975  0.30380656  0.30411349  0.30722436  0.30791868
  0.30868834  0.31060522  0.31074184  0.3124201   0.31249309  0.31252255
  0.3149265   0.31754518  0.31982044  0.32337439  0.32397843  0.32416839
  0.32492353  0.32508916  0.32764936  0.32955921  0.33124495  0.33155891
  0.33169316  0.33381218  0.33599481  0.33666948  0.33685242  0.33847218
  0.33904207  0.34141232  0.34255718  0.34409651  0.34497845  0.34549732
  0.34593571  0.35037629  0.35213034  0.35394008  0.35410102  0.35470739
  0.35603007  0.35662706  0.35689236  0.35824685  0.35875354  0.35909931
  0.35948906  0.36265328  0.36486772  0.36676774  0.36709152  0.36726931
  0.36915484  0.36957125  0.37033856  0.37079895  0.37092573  0.37110587
  0.37340781  0.37371474  0.37408014  0.37423361  0.37448205  0.37540282
  0.37562693  0.37636522  0.37711054  0.37769259  0.37878646  0.379354
  0.38027243  0.38060604  0.38161944  0.38193852  0.38353632  0.38378008
  0.38606984  0.38639874  0.3864521   0.38721942  0.38743137  0.38752635
  0.38790158  0.38829366  0.3886469   0.38952135  0.39028866  0.39223267
  0.39236415  0.3923983   0.39605707  0.3963738   0.39857563  0.39911649
  0.40276308  0.40317715  0.40421489  0.40456066  0.40717934  0.40838505
  0.40874813  0.41004646  0.41021444  0.41063832  0.41116702  0.41134715
  0.41153711  0.4127868   0.41337631  0.41360044  0.41639926  0.41680115
  0.41923222  0.42129787  0.42154631  0.4274242   0.42762397  0.43036431
  0.43246413  0.43272473  0.43923099  0.44009092  0.44246351  0.44299689
  0.44312368  0.44315036  0.44320884  0.44407112  0.44422459  0.44452916
  0.44482628  0.44606614  0.4463609   0.44645587  0.44651435  0.44781036
  0.44799049  0.44814397  0.44860436  0.44875781  0.44908671  0.44939365
  0.4501268   0.45056286  0.45089411  0.45228744  0.45269916  0.45288913
  0.4532522   0.45365644  0.45419729  0.45428246  0.45457722  0.45726889
  0.45870852  0.46093982  0.46226249  0.46241597  0.46287633  0.46400907
  0.46810387  0.46834016  0.46844731  0.46869575  0.46914396  0.47480991
  0.47507051  0.47828107  0.47883643  0.48051235  0.4860833   0.48696994
  0.48986138  0.49241175  0.49287215  0.49506693  0.49865505  0.50745107
  0.50845466  0.50860811  0.51473445  0.51910203  0.52356461  0.52554745
  0.52855821]

  warnings.warn(

2022-11-03 10:49:30,574:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.09006549 -0.08809969 -0.07766735 -0.07721225 -0.07494403 -0.07456818
 -0.07282703 -0.07207095 -0.06942542 -0.06836692 -0.06126275 -0.06096033
 -0.05408079 -0.04916705 -0.04417551 -0.04221263 -0.03850569 -0.03586017
 -0.03427459 -0.03298709 -0.0307952  -0.02988792 -0.02966328 -0.02399346
 -0.02376446 -0.02323593 -0.0230113  -0.01159387 -0.00077402  0.01289998
  0.01548228  0.01645133  0.0198675   0.02136508  0.02712143  0.02794364
  0.0377079   0.0404152   0.04231922  0.0459338   0.04980799  0.05063894
  0.05145241  0.0523597   0.05373955  0.05493034  0.05751847  0.05872817
  0.05978669  0.06248961  0.064627    0.06734013  0.07118518  0.07360461
  0.07520765  0.08040635  0.08259824  0.08274945  0.0829225   0.08411475
  0.08562253  0.08640044  0.08790094  0.08897255  0.090862    0.09129235
  0.09146687  0.09176783  0.09206589  0.09244902  0.09439152  0.09484516
  0.09545002  0.10152336  0.10514959  0.10633601  0.10696707  0.10704195
  0.10921054  0.10958639  0.11126432  0.11157839  0.11245947  0.11664335
  0.11732454  0.12283296  0.1228446   0.12450797  0.12463006  0.12508371
  0.12601721  0.12828542  0.12926614  0.12955254  0.13128495  0.13146238
  0.13168848  0.13297599  0.13516787  0.13657455  0.14037968  0.14241014
  0.14445663  0.14492193  0.14574998  0.14628141  0.14725047  0.14755436
  0.150125    0.15056845  0.15088836  0.15156955  0.15163861  0.15267672
  0.15413582  0.1546527   0.15572577  0.1559606   0.1563379   0.15653196
  0.15799834  0.16064096  0.16167326  0.16245846  0.16673821  0.16813118
  0.16877889  0.16881384  0.16938374  0.17024382  0.17044224  0.17077963
  0.17084139  0.17190135  0.17484348  0.1749373   0.17521933  0.17529276
  0.17682675  0.17732178  0.17763876  0.17770346  0.17939886  0.1802945
  0.18088916  0.18186987  0.18333042  0.18431113  0.18488396  0.18605229
  0.18861858  0.18869781  0.18964358  0.18966833  0.18974758  0.19103363
  0.19115573  0.19154759  0.19193655  0.19346035  0.19562602  0.19648464
  0.19663731  0.19753148  0.19843731  0.19875139  0.19886621  0.1989542
  0.19962228  0.19964122  0.19992471  0.20010068  0.20049111  0.20064377
  0.20264658  0.2028342   0.20518021  0.20683921  0.2069199   0.20722233
  0.20812671  0.2090369   0.20944978  0.20955961  0.2102408   0.21136836
  0.21197175  0.21439265  0.2147038   0.21617809  0.21731876  0.21733478
  0.21769024  0.21892616  0.21893637  0.21965749  0.22020348  0.22180944
  0.22245277  0.22475738  0.22731056  0.22797137  0.22934978  0.22936288
  0.22944651  0.22974456  0.23056823  0.23171617  0.23212905  0.23439726
  0.23624679  0.23940336  0.23946222  0.23955457  0.24016235  0.24136478
  0.24266393  0.24482961  0.24528325  0.24577185  0.24818254  0.24944093
  0.24955075  0.24974336  0.25013959  0.25098366  0.25204216  0.25318428
  0.25446304  0.25480832  0.25514133  0.25567858  0.25601803  0.25775627
  0.25813504  0.25860906  0.25873989  0.25907289  0.25950762  0.26293398
  0.26382962  0.26413786  0.26463581  0.26478702  0.26512294  0.26539188
  0.26722684  0.26811374  0.26849542  0.2694129   0.2704714   0.27056668
  0.27093233  0.27123912  0.27272506  0.2731787   0.27320929  0.27431645
  0.27433683  0.27521937  0.27538515  0.27569631  0.27585481  0.27637897
  0.27688273  0.27748759  0.27854609  0.27863553  0.27917278  0.27993323
  0.2805381   0.28137196  0.28300619  0.2831574   0.28407634  0.28560451
  0.28565171  0.28566773  0.2877215   0.28776872  0.28882722  0.28943354
  0.28946849  0.28990612  0.29026303  0.29058584  0.29086789  0.29132153
  0.29207761  0.29253125  0.29283368  0.29429424  0.29449704  0.2960252
  0.29676671  0.29747557  0.29755482  0.29805422  0.30063652  0.30122682
  0.30164054  0.30372986  0.30390148  0.30524346  0.30570002  0.30669238
  0.3068538   0.30795513  0.30834118  0.30901364  0.30916485  0.30931607
  0.30932189  0.31037457  0.31040806  0.31098962  0.31138439  0.31199217
  0.31212737  0.3125883   0.31267337  0.31319606  0.3133531   0.31402119
  0.3144894   0.3168325   0.31788081  0.31802329  0.31970703  0.32075097
  0.32075534  0.32128678  0.32172002  0.3221897   0.32423472  0.32444772
  0.32460912  0.32489117  0.32746181  0.32824701  0.32860393  0.33194377
  0.33336063  0.3342178   0.3351251   0.3372319   0.33865169  0.34017841
  0.34078616  0.34123835  0.34214418  0.34259929  0.34275924  0.3437312
  0.3453226   0.34546362  0.3454782   0.34630187  0.34690962  0.34873003
  0.35189971  0.35378916  0.35445726  0.35725836  0.35750192  0.35854733
  0.35878799  0.36081408  0.36151421  0.36203255  0.363376    0.36361957
  0.36413207  0.36526545  0.36821486  0.3689724   0.36919703  0.36934825
  0.36965067  0.37040674  0.37110251  0.37268226  0.37344852  0.37413553
  0.37433979  0.37450701  0.37464221  0.37728773  0.37851783  0.37910231
  0.37950291  0.37964539  0.38431849  0.38598185  0.38615637  0.38629447
  0.38870517  0.39045942  0.39128163  0.39325761  0.3959046   0.39703944
  0.39733896  0.39755778  0.39763556  0.39763994  0.39830656  0.4012618
  0.40383244  0.40489531  0.40527554  0.40860079  0.40959025  0.41018491
  0.41033467  0.41041101  0.41410775  0.42145259  0.42348742  0.42454739
  0.42545322  0.42569242  0.42681414  0.42772144  0.42946403  0.42969014
  0.43021429  0.43316224  0.43490193  0.4354363   0.43686628  0.43883205
  0.43958812  0.44004177  0.44005052  0.440193    0.44103851  0.44110027
  0.44126168  0.44140416  0.44283852  0.4432231   0.44344337  0.44450189
  0.44465309  0.44525796  0.44692131  0.4478286   0.44919099  0.45070167
  0.45206406  0.45395351  0.45456857  0.45472123  0.45493859  0.45516469
  0.45524101  0.45668556  0.45765755  0.45902866  0.46037942  0.46159786
  0.46431682  0.46816769  0.46990591  0.47006151  0.47058711  0.47066636
  0.47096443  0.47271141  0.47293895  0.47534965  0.47557866  0.47565354
  0.47626276  0.47936193  0.48147892  0.48185479  0.48404666  0.48760094
  0.48775361  0.48971503  0.49002329  0.49138275  0.49455389  0.49727722
  0.50294994  0.50763905  0.50997777  0.51277742  0.52411852  0.54838772]

  warnings.warn(

2022-11-03 10:49:30,574:INFO:Calculating mean and std
2022-11-03 10:49:30,574:INFO:Creating metrics dataframe
2022-11-03 10:49:30,589:INFO:Uploading results into container
2022-11-03 10:49:30,589:INFO:Uploading model into container now
2022-11-03 10:49:30,589:INFO:master_model_container: 4
2022-11-03 10:49:30,589:INFO:display_container: 2
2022-11-03 10:49:30,589:INFO:Lasso(random_state=4411)
2022-11-03 10:49:30,589:INFO:create_model() successfully completed......................................
2022-11-03 10:49:30,858:ERROR:create_model() for Lasso(random_state=4411) raised an exception or returned all 0.0:
2022-11-03 10:49:30,859:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:49:30,859:INFO:Initializing Ridge Regression
2022-11-03 10:49:30,859:INFO:Total runtime is 0.9158230225245158 minutes
2022-11-03 10:49:30,859:INFO:SubProcess create_model() called ==================================
2022-11-03 10:49:30,859:INFO:Initializing create_model()
2022-11-03 10:49:30,859:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:49:30,859:INFO:Checking exceptions
2022-11-03 10:49:30,867:INFO:Importing libraries
2022-11-03 10:49:30,867:INFO:Copying training dataset
2022-11-03 10:49:30,875:INFO:Defining folds
2022-11-03 10:49:30,875:INFO:Declaring metric variables
2022-11-03 10:49:30,875:INFO:Importing untrained model
2022-11-03 10:49:30,875:INFO:Ridge Regression Imported successfully
2022-11-03 10:49:30,883:INFO:Starting cross validation
2022-11-03 10:49:30,883:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:49:34,909:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.90241207e-01 -1.67230618e-01 -1.51018243e-01 -1.44197588e-01
 -1.37905010e-01 -1.33717090e-01 -1.31510382e-01 -1.27834552e-01
 -1.26057052e-01 -1.19986715e-01 -1.18351589e-01 -1.16380423e-01
 -1.16032107e-01 -1.13152013e-01 -1.09306238e-01 -1.08579317e-01
 -1.04699947e-01 -1.03299263e-01 -9.70875445e-02 -9.52027616e-02
 -9.28888843e-02 -9.15360456e-02 -8.88227568e-02 -8.84216077e-02
 -8.47065094e-02 -8.37797412e-02 -8.35923204e-02 -7.34933281e-02
 -6.97711872e-02 -6.92573227e-02 -6.46257503e-02 -6.19536004e-02
 -5.82896758e-02 -5.67911514e-02 -5.62423160e-02 -4.97295712e-02
 -4.86041567e-02 -4.85331316e-02 -4.55378938e-02 -4.42212701e-02
 -4.38621479e-02 -4.32153342e-02 -4.29683943e-02 -4.13158746e-02
 -3.91236334e-02 -3.85257074e-02 -3.75141411e-02 -3.62265866e-02
 -3.20445202e-02 -3.10023262e-02 -3.03179226e-02 -2.85701665e-02
 -2.82696516e-02 -2.70298911e-02 -2.36323195e-02 -2.27850709e-02
 -2.19217429e-02 -2.12422632e-02 -2.09314113e-02 -2.04153333e-02
 -1.76402782e-02 -1.53076241e-02 -1.09652765e-02 -9.41510278e-03
 -7.27299380e-03 -6.03713873e-03 -4.99587979e-03 -4.10015122e-03
 -3.76902334e-03 -3.37243951e-03 -2.36887101e-03 -1.12172053e-03
  2.43844600e-04  1.54896981e-03  1.63308642e-03  2.09149336e-03
  3.89356355e-03  4.89209753e-03  9.99984180e-03  1.00094335e-02
  1.02065946e-02  1.19734665e-02  1.32704385e-02  1.46137977e-02
  1.63770681e-02  1.88291804e-02  2.22200720e-02  2.27067574e-02
  2.49067639e-02  2.50439917e-02  2.63259482e-02  3.06155426e-02
  3.11429352e-02  3.13607631e-02  3.32194199e-02  3.44555982e-02
  3.53290920e-02  3.62123443e-02  3.72786668e-02  3.95732206e-02
  4.04505380e-02  4.05406998e-02  4.26828837e-02  4.44394796e-02
  4.60198952e-02  4.81155111e-02  4.93315555e-02  5.12501420e-02
  5.51132870e-02  5.53211541e-02  5.55884548e-02  5.95404688e-02
  6.11611686e-02  6.21353504e-02  6.39146317e-02  6.65056724e-02
  6.87090588e-02  8.26346854e-02  8.29731123e-02  8.42204116e-02
  8.48863877e-02  8.60038384e-02  8.69719302e-02  8.80788730e-02
  8.91768897e-02  9.10102314e-02  9.13685330e-02  9.20215124e-02
  9.22855886e-02  9.34904416e-02  9.40264954e-02  9.41909932e-02
  9.43978597e-02  9.60735166e-02  9.76212236e-02  9.76604425e-02
  9.85292026e-02  9.98193561e-02  1.01480451e-01  1.01965864e-01
  1.01968000e-01  1.02007892e-01  1.04336691e-01  1.07005128e-01
  1.07050364e-01  1.07564302e-01  1.13775573e-01  1.14682474e-01
  1.14895933e-01  1.18334196e-01  1.19978578e-01  1.20581377e-01
  1.21299530e-01  1.22481872e-01  1.22872474e-01  1.24414888e-01
  1.25046495e-01  1.25815616e-01  1.26182551e-01  1.27304837e-01
  1.28287105e-01  1.28716432e-01  1.29763872e-01  1.30840145e-01
  1.33774396e-01  1.33907973e-01  1.34741360e-01  1.35138400e-01
  1.37382321e-01  1.37755765e-01  1.38091815e-01  1.38470400e-01
  1.38587192e-01  1.43914690e-01  1.45234677e-01  1.47015937e-01
  1.47215692e-01  1.48336318e-01  1.51071323e-01  1.55700533e-01
  1.57000840e-01  1.57398807e-01  1.59963559e-01  1.61814790e-01
  1.61973413e-01  1.64092387e-01  1.65828563e-01  1.67443220e-01
  1.69295576e-01  1.70846790e-01  1.72470702e-01  1.73405558e-01
  1.77657550e-01  1.78500548e-01  1.78848262e-01  1.84290760e-01
  1.85773879e-01  1.85933799e-01  1.87810666e-01  1.89741949e-01
  1.92058320e-01  1.98669168e-01  1.99152620e-01  2.03035366e-01
  2.03188589e-01  2.06476262e-01  2.06591803e-01  2.07024881e-01
  2.08886205e-01  2.09160624e-01  2.09289789e-01  2.12215627e-01
  2.12818237e-01  2.12900009e-01  2.12939228e-01  2.13174370e-01
  2.14128879e-01  2.16171358e-01  2.17573486e-01  2.20028759e-01
  2.21971631e-01  2.23122973e-01  2.24103172e-01  2.27169346e-01
  2.31197547e-01  2.31456186e-01  2.32189172e-01  2.33815918e-01
  2.34105062e-01  2.35094694e-01  2.35305331e-01  2.35436880e-01
  2.36323139e-01  2.37164348e-01  2.37236252e-01  2.39842967e-01
  2.40841276e-01  2.42709404e-01  2.43700524e-01  2.44948712e-01
  2.45062618e-01  2.46164435e-01  2.46643904e-01  2.47285306e-01
  2.47474731e-01  2.48378133e-01  2.48391753e-01  2.50109555e-01
  2.50815291e-01  2.51272454e-01  2.52812496e-01  2.52945212e-01
  2.56594255e-01  2.57591284e-01  2.58187826e-01  2.59206084e-01
  2.60861072e-01  2.61654795e-01  2.61677069e-01  2.61806933e-01
  2.63319337e-01  2.63648406e-01  2.63909406e-01  2.64271477e-01
  2.64596399e-01  2.65999536e-01  2.68424918e-01  2.70623896e-01
  2.70664082e-01  2.70987740e-01  2.71767860e-01  2.73114556e-01
  2.73360918e-01  2.74445781e-01  2.74910302e-01  2.79468173e-01
  2.81505201e-01  2.82286437e-01  2.84308876e-01  2.85506804e-01
  2.85963899e-01  2.88355690e-01  2.96021164e-01  2.98794882e-01
  3.03564958e-01  3.10964851e-01  3.12105391e-01  3.12342926e-01
  3.12695479e-01  3.14718926e-01  3.14856580e-01  3.14983631e-01
  3.18860560e-01  3.19063059e-01  3.20431852e-01  3.21237823e-01
  3.23195470e-01  3.25803322e-01  3.25820934e-01  3.28469402e-01
  3.30391357e-01  3.30537381e-01  3.31288082e-01  3.32209520e-01
  3.32396844e-01  3.32402057e-01  3.32882600e-01  3.33974548e-01
  3.37387552e-01  3.37913533e-01  3.38452995e-01  3.38661302e-01
  3.42317896e-01  3.43890618e-01  3.52482606e-01  3.56163918e-01
  3.58857255e-01  3.61199411e-01  3.63942484e-01  3.64057359e-01
  3.65052448e-01  3.66361645e-01  3.67706416e-01  3.68116487e-01
  3.69285591e-01  3.69677984e-01  3.72490264e-01  3.74054552e-01
  3.74627501e-01  3.75218148e-01  3.75969790e-01  3.78047213e-01
  3.83951139e-01  3.84471915e-01  3.86844682e-01  3.87092231e-01
  3.88569243e-01  3.91488350e-01  3.93917485e-01  3.94957816e-01
  3.96048639e-01  3.97341572e-01  3.97568355e-01  4.02943097e-01
  4.06199964e-01  4.09601834e-01  4.10067951e-01  4.12506221e-01
  4.20909074e-01  4.21232016e-01  4.24133942e-01  4.24576525e-01
  4.25025212e-01  4.26019172e-01  4.31925694e-01  4.32060967e-01
  4.33407795e-01  4.34385816e-01  4.34801572e-01  4.38810197e-01
  4.39134227e-01  4.40060817e-01  4.47426028e-01  4.47895413e-01
  4.47941917e-01  4.48513439e-01  4.50585398e-01  4.52168132e-01
  4.55717672e-01  4.57689171e-01  4.58341706e-01  4.58985392e-01
  4.60774816e-01  4.61277006e-01  4.61279069e-01  4.61359874e-01
  4.62576533e-01  4.65618682e-01  4.66053050e-01  4.66969036e-01
  4.67689965e-01  4.68015586e-01  4.68050804e-01  4.70291481e-01
  4.70761485e-01  4.71446949e-01  4.72452949e-01  4.72722520e-01
  4.74846243e-01  4.75182139e-01  4.78268292e-01  4.78399892e-01
  4.78764504e-01  4.79341835e-01  4.81877889e-01  4.82867558e-01
  4.86547726e-01  4.86718689e-01  4.86736069e-01  4.87314548e-01
  4.92190514e-01  4.93200749e-01  4.94489696e-01  4.95348540e-01
  4.96307098e-01  5.00167004e-01  5.00787186e-01  5.02084264e-01
  5.02180983e-01  5.03261382e-01  5.07568531e-01  5.08768227e-01
  5.09650887e-01  5.11009658e-01  5.11340013e-01  5.12754284e-01
  5.19373130e-01  5.20496402e-01  5.21366594e-01  5.21444469e-01
  5.30378203e-01  5.31359784e-01  5.35947754e-01  5.36139198e-01
  5.36495817e-01  5.37848918e-01  5.40153789e-01  5.40497530e-01
  5.47798839e-01  5.48491881e-01  5.52471276e-01  5.54808006e-01
  5.62682972e-01  5.64855127e-01  5.67783284e-01  5.68342166e-01
  5.69271289e-01  5.70818440e-01  5.71284484e-01  5.73313401e-01
  5.76227843e-01  5.77874365e-01  5.80606161e-01  5.82437495e-01
  5.84895962e-01  5.89010691e-01  5.89231443e-01  5.89413392e-01
  5.91998418e-01  5.92229184e-01  5.93839180e-01  6.04453391e-01
  6.07009569e-01  6.09857166e-01  6.11254326e-01  6.14019381e-01
  6.14956162e-01  6.16405609e-01  6.17459528e-01  6.18835845e-01
  6.25064108e-01  6.26462863e-01  6.29701214e-01  6.30907720e-01
  6.31561330e-01  6.32459034e-01  6.36434032e-01  6.37723311e-01
  6.40514038e-01  6.42768711e-01  6.45556733e-01  6.47996290e-01
  6.49807025e-01  6.54379169e-01  6.57215228e-01  6.57760021e-01
  6.58989495e-01  6.59133913e-01  6.62194958e-01  6.64958529e-01
  6.74936359e-01  6.82115149e-01  6.86847675e-01  6.88202530e-01
  6.90448206e-01  6.91679778e-01  6.99167365e-01  7.01686358e-01
  7.11517959e-01  7.11649781e-01  7.18505995e-01  7.20910792e-01
  7.34399372e-01]

  warnings.warn(

2022-11-03 10:49:34,924:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.17826644 -0.17234718 -0.16894386 -0.16199299 -0.15876808 -0.15422958
 -0.14504911 -0.14437925 -0.14426339 -0.14092844 -0.14083386 -0.13948559
 -0.13944144 -0.13262528 -0.13215723 -0.13179996 -0.12992746 -0.12642014
 -0.12460899 -0.12319567 -0.11949849 -0.10674312 -0.09909473 -0.09683971
 -0.09634533 -0.09003178 -0.08578061 -0.08339223 -0.08286323 -0.0821077
 -0.08056422 -0.0784934  -0.07578167 -0.07384362 -0.07250723 -0.07213825
 -0.0718232  -0.07181273 -0.06808793 -0.06563253 -0.06545975 -0.0649026
 -0.06157109 -0.05370136 -0.05137067 -0.0505203  -0.04960706 -0.04925968
 -0.04457332 -0.04067486 -0.0380351  -0.0359727  -0.03443918 -0.03434552
 -0.02894469 -0.02735621 -0.02565374 -0.0248138  -0.0232316  -0.02260414
 -0.0202009  -0.0187633  -0.01860953 -0.01760112 -0.01712341 -0.01678397
 -0.01355266 -0.0107577  -0.00771943 -0.00754704 -0.00399974 -0.00237809
  0.00255853  0.00468443  0.00622677  0.00669986  0.00719583  0.00781647
  0.01067274  0.01155143  0.01482078  0.01690126  0.01808499  0.01862527
  0.02441693  0.02790534  0.02954016  0.03029665  0.03110806  0.03113291
  0.03450225  0.03546368  0.03974441  0.03975393  0.0399218   0.04037724
  0.04112859  0.05109527  0.05243057  0.05697585  0.06106517  0.06117116
  0.06255225  0.06590221  0.0661376   0.06632342  0.07044151  0.07236363
  0.07387349  0.07563775  0.07565681  0.0768757   0.07733464  0.08248783
  0.08472379  0.0849668   0.08617596  0.08634008  0.08683541  0.08700894
  0.09044267  0.09102355  0.09175373  0.09188713  0.09246118  0.09486224
  0.0949494   0.09573604  0.09698557  0.09702457  0.10407824  0.10456431
  0.10485986  0.1052789   0.10573755  0.10682456  0.10788134  0.11178992
  0.11202616  0.11210995  0.11389723  0.11494063  0.11580487  0.11749211
  0.11945788  0.12052068  0.12088428  0.12167318  0.12230402  0.12314575
  0.12594594  0.12720967  0.12897756  0.13270009  0.13302416  0.13506535
  0.1373614   0.13762209  0.13788451  0.13962162  0.1403331   0.1413303
  0.14352464  0.14449617  0.14549264  0.14557525  0.14693796  0.14711942
  0.15341286  0.15447336  0.1554847   0.1558831   0.15690361  0.15706746
  0.15761971  0.15818634  0.16281973  0.16308437  0.16424639  0.16466323
  0.16572323  0.16607743  0.16951941  0.17115072  0.17196986  0.17275228
  0.1733236   0.17414617  0.17455048  0.17472685  0.17489927  0.17496249
  0.17550989  0.17637877  0.17649287  0.17818572  0.1786891   0.17872571
  0.18008846  0.18126412  0.18252169  0.18278264  0.18361816  0.18457839
  0.18578814  0.19002733  0.1965609   0.19830943  0.19836831  0.19854498
  0.19867091  0.19949239  0.20109645  0.20136386  0.20299874  0.20393281
  0.20447713  0.2048398   0.20726473  0.20814806  0.20860401  0.20874165
  0.20951559  0.21005091  0.21170603  0.21394407  0.21412032  0.21552346
  0.2157415   0.21699643  0.21811392  0.22043051  0.22090457  0.22169958
  0.2224403   0.22514246  0.22568771  0.22570666  0.22896166  0.23153209
  0.23185286  0.23425009  0.23452793  0.2351852   0.23818189  0.23883168
  0.24329186  0.24395823  0.24918541  0.25108932  0.25136033  0.25175866
  0.252569    0.25438105  0.25882581  0.25909556  0.26398477  0.26417117
  0.26454591  0.26498291  0.26674233  0.26854093  0.27509214  0.27616879
  0.27951374  0.28020352  0.28056923  0.28068424  0.28612301  0.28863453
  0.28869287  0.28885014  0.29311999  0.29404721  0.29847467  0.29885587
  0.30031364  0.30441503  0.30517236  0.30566117  0.30590894  0.30747418
  0.30839486  0.31051665  0.31144881  0.31212743  0.31246215  0.31318324
  0.31421267  0.31479119  0.31895597  0.31904923  0.31921791  0.31982753
  0.31992476  0.32374863  0.32485817  0.32895816  0.33069151  0.33228587
  0.33382882  0.34011608  0.34265211  0.34416817  0.34516813  0.34633031
  0.34643996  0.34736753  0.34788792  0.3516353   0.3523436   0.35242394
  0.3526516   0.35377291  0.35424597  0.35771132  0.35810452  0.35836765
  0.35837498  0.36155156  0.36316071  0.3688915   0.37547566  0.37568956
  0.3768094   0.37732753  0.37751281  0.38157238  0.38210181  0.38210655
  0.38365359  0.38474749  0.38618114  0.39147125  0.39346626  0.39451255
  0.39473646  0.39507068  0.39513116  0.39543491  0.3994769   0.40376045
  0.40922547  0.40959027  0.41671169  0.4174282   0.41879047  0.41951901
  0.42062495  0.42338439  0.42393895  0.42507864  0.43112266  0.43302881
  0.4354124   0.43546805  0.4387485   0.44271232  0.44362801  0.4457611
  0.44696215  0.44824593  0.44836934  0.45254819  0.45264927  0.45283973
  0.45331472  0.45614772  0.45656706  0.45775535  0.45953532  0.45970766
  0.4607226   0.46409681  0.46606859  0.4662427   0.47197001  0.4731346
  0.47530303  0.47620911  0.47767327  0.4805722   0.48248726  0.48348821
  0.48654352  0.48860248  0.49080554  0.49203207  0.49774784  0.49777303
  0.4981981   0.50105819  0.50111852  0.50113346  0.50137202  0.5019526
  0.50509438  0.50629687  0.50641866  0.51045872  0.51218247  0.51474628
  0.51692337  0.5184923   0.51954329  0.52005822  0.52195901  0.52913715
  0.53179634  0.5333213   0.53384476  0.53423776  0.53955516  0.53959492
  0.53978778  0.54133779  0.54155146  0.54325092  0.5493651   0.54965661
  0.54987128  0.55095797  0.55151812  0.55187671  0.55207101  0.55207966
  0.55219464  0.55256471  0.55316053  0.55436076  0.55765723  0.55772417
  0.56226105  0.56243932  0.56380886  0.56923339  0.5700894   0.57135025
  0.58691327  0.588631    0.58893616  0.59637671  0.59640591  0.60357276
  0.60575762  0.60773543  0.60791626  0.60897677  0.60972208  0.61031502
  0.61049072  0.61168034  0.61638737  0.61742856  0.62589194  0.62950173
  0.62957199  0.63267483  0.63352627  0.6361167   0.63839179  0.64261007
  0.64442739  0.64584221  0.64634359  0.64922925  0.65299463  0.65656364
  0.65717537  0.65772542  0.66287274  0.66649307  0.66760435  0.66871074
  0.67300703  0.67430704  0.67713527  0.68060527  0.68194641  0.6820186
  0.69182536  0.69400506  0.69429951  0.70254558  0.71813265  0.71944208
  0.72429479  0.73333311  0.75771847  0.76317853  0.76439665  0.76872448
  0.77696361]

  warnings.warn(

2022-11-03 10:49:34,967:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.83230643e-01 -1.81971487e-01 -1.58196862e-01 -1.50383759e-01
 -1.46418086e-01 -1.45482305e-01 -1.43876589e-01 -1.43429447e-01
 -1.38338329e-01 -1.38280088e-01 -1.38034727e-01 -1.36696177e-01
 -1.35698651e-01 -1.35125741e-01 -1.33602139e-01 -1.33264358e-01
 -1.33209307e-01 -1.30886130e-01 -1.29841948e-01 -1.24666263e-01
 -1.23896229e-01 -1.19665363e-01 -1.16875026e-01 -1.16685460e-01
 -1.13505353e-01 -1.10351811e-01 -1.04604346e-01 -1.02984092e-01
 -1.02972800e-01 -1.01137690e-01 -1.00598722e-01 -9.93540267e-02
 -9.71693821e-02 -9.70143891e-02 -9.37204423e-02 -9.27287339e-02
 -8.88196801e-02 -8.85996147e-02 -8.48875757e-02 -8.21332840e-02
 -8.06483185e-02 -7.72323441e-02 -7.50215421e-02 -7.48787450e-02
 -7.41867761e-02 -7.39542364e-02 -6.75029386e-02 -6.63084478e-02
 -6.60754936e-02 -6.57080813e-02 -6.53110098e-02 -6.21285034e-02
 -6.10969201e-02 -5.59789080e-02 -5.44846026e-02 -5.28888630e-02
 -5.22421779e-02 -4.88479465e-02 -4.87135981e-02 -4.86776402e-02
 -4.80577512e-02 -4.60442872e-02 -4.58066652e-02 -4.52627778e-02
 -4.41135140e-02 -4.28543520e-02 -4.22190744e-02 -4.20548686e-02
 -3.86032921e-02 -3.76769863e-02 -3.73378733e-02 -3.69196361e-02
 -3.65066392e-02 -3.51006326e-02 -3.39107760e-02 -3.07015639e-02
 -3.05930830e-02 -3.02656499e-02 -2.74519202e-02 -2.60835327e-02
 -2.56998461e-02 -2.50876692e-02 -2.37398323e-02 -2.36601044e-02
 -2.01036390e-02 -1.86675741e-02 -1.83870203e-02 -1.55601182e-02
 -4.69685754e-03 -3.86345175e-03 -3.70539022e-03 -1.93840339e-03
 -1.68687538e-03 -5.31066368e-04  3.36123133e-04  8.08900561e-04
  1.19224995e-03  2.96331059e-03  9.23174885e-03  1.96779387e-02
  2.09488533e-02  2.11235591e-02  2.15039832e-02  2.17666666e-02
  2.68786554e-02  2.88199815e-02  3.10169847e-02  3.19520021e-02
  3.20001026e-02  3.20629736e-02  3.22010825e-02  3.42149864e-02
  3.53672961e-02  3.63386254e-02  3.79655287e-02  4.01013244e-02
  4.06962279e-02  4.10495578e-02  4.20054045e-02  4.20938138e-02
  4.60150155e-02  4.72652036e-02  5.01333186e-02  5.20478000e-02
  5.59121960e-02  5.60296607e-02  5.69962344e-02  5.83840444e-02
  5.90657861e-02  5.99792898e-02  6.02267099e-02  6.48609985e-02
  6.58287216e-02  6.66819884e-02  6.69780288e-02  6.72020775e-02
  6.86490168e-02  6.86727521e-02  7.04862114e-02  7.19513013e-02
  7.52010323e-02  7.73316573e-02  7.85404221e-02  8.12643588e-02
  8.12908141e-02  8.29663475e-02  9.13075989e-02  9.17312726e-02
  9.21557449e-02  9.54061185e-02  9.71318500e-02  1.01013838e-01
  1.01116507e-01  1.02415660e-01  1.03327851e-01  1.04295513e-01
  1.05685857e-01  1.06376814e-01  1.08812066e-01  1.12754352e-01
  1.14214623e-01  1.15763600e-01  1.17074338e-01  1.18513118e-01
  1.19136863e-01  1.19691004e-01  1.21139555e-01  1.23585138e-01
  1.24799089e-01  1.25764254e-01  1.26491984e-01  1.28131386e-01
  1.29076402e-01  1.30222568e-01  1.30249408e-01  1.32621052e-01
  1.33908868e-01  1.35367927e-01  1.37444905e-01  1.37583721e-01
  1.40569472e-01  1.40802869e-01  1.42076930e-01  1.43089284e-01
  1.43153625e-01  1.44388207e-01  1.44432952e-01  1.49921410e-01
  1.50399428e-01  1.51476782e-01  1.51856664e-01  1.55998318e-01
  1.56336960e-01  1.59784042e-01  1.60882069e-01  1.62176966e-01
  1.62395539e-01  1.62850880e-01  1.63246844e-01  1.64098950e-01
  1.64194340e-01  1.64326266e-01  1.64738809e-01  1.64745869e-01
  1.64957770e-01  1.66110553e-01  1.66502140e-01  1.69467424e-01
  1.70713544e-01  1.74241812e-01  1.75974017e-01  1.78048210e-01
  1.78280221e-01  1.79878315e-01  1.80122855e-01  1.84112275e-01
  1.85663676e-01  1.87615308e-01  1.90171398e-01  1.90903674e-01
  1.91174571e-01  1.91217431e-01  1.93261686e-01  1.93789229e-01
  1.95752035e-01  1.95841272e-01  1.98034261e-01  1.99478173e-01
  2.00481357e-01  2.02195907e-01  2.06197529e-01  2.07266705e-01
  2.09311891e-01  2.11095533e-01  2.12531392e-01  2.12925322e-01
  2.14383806e-01  2.16706821e-01  2.17198668e-01  2.17705572e-01
  2.18700353e-01  2.18926021e-01  2.19221126e-01  2.21209271e-01
  2.23812236e-01  2.24880614e-01  2.26731849e-01  2.27102239e-01
  2.29631402e-01  2.30121507e-01  2.30278668e-01  2.36283245e-01
  2.41507016e-01  2.41642583e-01  2.41940305e-01  2.43612555e-01
  2.44655172e-01  2.47174989e-01  2.47757812e-01  2.50976047e-01
  2.52323615e-01  2.55709268e-01  2.56007675e-01  2.57350998e-01
  2.59434488e-01  2.59619736e-01  2.59937073e-01  2.60536243e-01
  2.61252294e-01  2.64158014e-01  2.65447353e-01  2.65572177e-01
  2.69501808e-01  2.76237192e-01  2.76738742e-01  2.78385946e-01
  2.79201668e-01  2.79287371e-01  2.82465969e-01  2.82717495e-01
  2.84978823e-01  2.89660710e-01  2.98900528e-01  2.99617755e-01
  2.99793358e-01  3.01212412e-01  3.01763006e-01  3.10420714e-01
  3.11942521e-01  3.12117640e-01  3.14732697e-01  3.16377155e-01
  3.18995454e-01  3.22369940e-01  3.22517532e-01  3.23786792e-01
  3.24447845e-01  3.24463653e-01  3.30334958e-01  3.33297274e-01
  3.33423471e-01  3.34884909e-01  3.39116900e-01  3.40582192e-01
  3.42234472e-01  3.43487971e-01  3.43711392e-01  3.45771687e-01
  3.47369073e-01  3.50309769e-01  3.50484462e-01  3.53031914e-01
  3.54114122e-01  3.54980558e-01  3.55269245e-01  3.58002423e-01
  3.61992243e-01  3.62206056e-01  3.65854232e-01  3.67330287e-01
  3.70674692e-01  3.70700006e-01  3.74194301e-01  3.77675215e-01
  3.79163437e-01  3.79505394e-01  3.82019669e-01  3.82477137e-01
  3.84911556e-01  3.87454453e-01  3.90582627e-01  3.92243340e-01
  3.93147295e-01  3.93732671e-01  3.96163183e-01  3.96477712e-01
  3.98028785e-01  3.98841856e-01  4.00449150e-01  4.03555804e-01
  4.05429940e-01  4.06740654e-01  4.06968453e-01  4.09362549e-01
  4.15416267e-01  4.17684889e-01  4.19612923e-01  4.23197170e-01
  4.25232637e-01  4.28616822e-01  4.29391163e-01  4.29972739e-01
  4.30607321e-01  4.30814014e-01  4.31945043e-01  4.31989259e-01
  4.36522244e-01  4.36906534e-01  4.37498726e-01  4.39536425e-01
  4.40690348e-01  4.41161575e-01  4.43196202e-01  4.43242185e-01
  4.43500924e-01  4.43652711e-01  4.46110830e-01  4.48980624e-01
  4.50229192e-01  4.51917187e-01  4.53419311e-01  4.56120763e-01
  4.56630464e-01  4.57136379e-01  4.57917547e-01  4.62749962e-01
  4.63740254e-01  4.67061521e-01  4.67263087e-01  4.69520676e-01
  4.70068053e-01  4.71667330e-01  4.71901889e-01  4.72859510e-01
  4.73798765e-01  4.73975601e-01  4.74481395e-01  4.75562366e-01
  4.75811028e-01  4.76819090e-01  4.79935789e-01  4.80629353e-01
  4.85571118e-01  4.87050413e-01  4.89615148e-01  4.90332031e-01
  4.91402706e-01  4.95897640e-01  4.96041770e-01  5.00910063e-01
  5.00987426e-01  5.05706683e-01  5.06108614e-01  5.06764210e-01
  5.06937862e-01  5.07736775e-01  5.11234103e-01  5.17062763e-01
  5.21370730e-01  5.24910860e-01  5.25813216e-01  5.26647821e-01
  5.27090778e-01  5.30966299e-01  5.31072649e-01  5.33013580e-01
  5.34072808e-01  5.35622005e-01  5.39489788e-01  5.41532657e-01
  5.43297200e-01  5.50051606e-01  5.52905453e-01  5.53531194e-01
  5.53941368e-01  5.54075991e-01  5.55745193e-01  5.56108480e-01
  5.56157584e-01  5.56797972e-01  5.59740395e-01  5.59921892e-01
  5.59951772e-01  5.61977481e-01  5.65810740e-01  5.66447038e-01
  5.69908849e-01  5.70197266e-01  5.70204612e-01  5.70464682e-01
  5.73274640e-01  5.75340577e-01  5.75467864e-01  5.75552962e-01
  5.76227484e-01  5.76372693e-01  5.77727262e-01  5.80067846e-01
  5.80618733e-01  5.85305589e-01  5.89577152e-01  5.90195771e-01
  5.92672781e-01  6.07218122e-01  6.08187330e-01  6.09090965e-01
  6.09618687e-01  6.12705698e-01  6.15531448e-01  6.16468652e-01
  6.17201527e-01  6.22524943e-01  6.25601851e-01  6.26750773e-01
  6.29451291e-01  6.31304913e-01  6.35804281e-01  6.36449897e-01
  6.39446559e-01  6.41026609e-01  6.48246391e-01  6.48469394e-01
  6.50280462e-01  6.52777048e-01  6.56008981e-01  6.57420082e-01
  6.58511571e-01  6.64184276e-01  6.69684485e-01  6.70442013e-01
  6.78415295e-01  6.83323143e-01  6.88322030e-01  6.91748415e-01
  6.93797997e-01  6.95197525e-01  6.96520301e-01  6.97593285e-01
  7.08619264e-01  7.17094208e-01  7.18677189e-01  7.22639509e-01
  7.31996244e-01]

  warnings.warn(

2022-11-03 10:49:34,993:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.18745831 -0.16311797 -0.15900914 -0.15883344 -0.15010697 -0.14763518
 -0.14639865 -0.14317316 -0.1362097  -0.13523003 -0.13169017 -0.13078211
 -0.12835658 -0.12649965 -0.12505871 -0.12354537 -0.1227975  -0.12026609
 -0.1193303  -0.11652706 -0.11649579 -0.10596804 -0.10549934 -0.10529539
 -0.10230498 -0.10180733 -0.10002881 -0.09961749 -0.09356317 -0.09308952
 -0.08862921 -0.08681793 -0.08331538 -0.0826999  -0.08167022 -0.07808066
 -0.07466755 -0.07417455 -0.0710878  -0.06451197 -0.06293416 -0.06109506
 -0.05996846 -0.05983454 -0.0564422  -0.05432776 -0.05070241 -0.04722992
 -0.04506284 -0.04497181 -0.04345612 -0.04298906 -0.0416783  -0.04097525
 -0.0354235  -0.03147211 -0.02940238 -0.02392765 -0.02381952 -0.02317386
 -0.0228202  -0.0218468  -0.02025563 -0.01903897 -0.01744274 -0.01557021
 -0.01537939 -0.01493063 -0.01424922 -0.01261151 -0.01171266 -0.00981027
 -0.00732871 -0.00602887 -0.00600043 -0.0058158  -0.0034049  -0.00080369
  0.00162149  0.00190835  0.00412464  0.00634621  0.00754361  0.00757419
  0.00968119  0.01091312  0.01332005  0.0135065   0.01562241  0.01574563
  0.01726303  0.01875107  0.02100895  0.0214936   0.02204916  0.02252294
  0.02472409  0.0256057   0.02682743  0.02912907  0.03403014  0.03452509
  0.03501513  0.0364093   0.03754289  0.03773687  0.03799269  0.04188939
  0.04271044  0.04272699  0.04369798  0.04672124  0.0469811   0.04799234
  0.05134618  0.05211709  0.05251899  0.05617845  0.05669346  0.05697249
  0.06173972  0.06256785  0.06362487  0.06783588  0.07078164  0.07325804
  0.07330939  0.0741549   0.07537854  0.07901662  0.07905199  0.08266979
  0.08339164  0.08342042  0.08349512  0.08369234  0.08369512  0.08454595
  0.0853157   0.08678785  0.09222005  0.0922688   0.09790937  0.10030612
  0.10326713  0.10496996  0.10720449  0.10969995  0.1102727   0.11041119
  0.11077957  0.11278459  0.11304179  0.11540907  0.11579326  0.11595116
  0.11801586  0.11817891  0.12031743  0.12534215  0.12696522  0.12941154
  0.13020633  0.13252466  0.13369436  0.13483716  0.14079358  0.14125162
  0.14126057  0.14195471  0.142874    0.1438802   0.14612067  0.14621607
  0.14655469  0.1466045   0.14739681  0.15084555  0.15115571  0.15175196
  0.15280168  0.16046866  0.16094963  0.1610689   0.16213746  0.1635105
  0.17084003  0.17277384  0.1729739   0.17379104  0.1749712   0.1769521
  0.17973955  0.18238321  0.18389389  0.18905715  0.18953569  0.19083565
  0.19137174  0.19357296  0.20064897  0.20339929  0.2083674   0.20872392
  0.21033469  0.21076496  0.21125166  0.21132207  0.21192862  0.21446662
  0.21683457  0.21710497  0.21806328  0.21871792  0.21975906  0.22335116
  0.22389206  0.22498965  0.22648336  0.22704515  0.22706915  0.22786223
  0.22947716  0.23142761  0.23556118  0.23610253  0.23685267  0.2386326
  0.24212496  0.24218816  0.24391713  0.24510095  0.24518183  0.24647854
  0.24747751  0.25545022  0.25809567  0.25946322  0.25982359  0.2630215
  0.2634853   0.26444884  0.26530997  0.26560251  0.26723262  0.26889931
  0.27366442  0.27444884  0.27650731  0.27732531  0.27824222  0.27952017
  0.28153879  0.28171261  0.28196618  0.28220774  0.28382446  0.28403214
  0.28433186  0.28507168  0.29140226  0.29362747  0.29683812  0.29700432
  0.29709298  0.29792439  0.30043394  0.30254891  0.30310351  0.30321664
  0.30589785  0.30600491  0.30763495  0.30884397  0.31077968  0.31091929
  0.31506369  0.31660539  0.31697758  0.31745496  0.31841013  0.32205446
  0.32742576  0.32754271  0.32866774  0.32898276  0.32941454  0.33571933
  0.33599391  0.3361049   0.3377091   0.33932824  0.34206559  0.34458572
  0.34585602  0.34673571  0.34828063  0.34942072  0.3542484   0.35463291
  0.35791652  0.36148105  0.36261748  0.36342088  0.36454327  0.36671784
  0.36767445  0.36793453  0.36867857  0.37059718  0.37209644  0.37226442
  0.37414494  0.37518847  0.37636275  0.37679286  0.37697699  0.3790967
  0.3798273   0.38006152  0.38006223  0.38097918  0.38159234  0.38761286
  0.38879898  0.39222495  0.39286069  0.39641643  0.39750174  0.39794275
  0.39902755  0.39952793  0.40030578  0.40205827  0.40286644  0.4034535
  0.40466228  0.40541956  0.40556266  0.40628029  0.40700653  0.40976834
  0.4102466   0.41617082  0.41642714  0.41742686  0.4179699   0.41797249
  0.41799833  0.42154953  0.42765913  0.42838449  0.42978203  0.43340727
  0.43579739  0.43585496  0.4375358   0.43801075  0.44012065  0.44049314
  0.44310442  0.44589826  0.4481817   0.44965723  0.4510229   0.45252851
  0.45350971  0.45509121  0.45576372  0.45594533  0.45961427  0.4658934
  0.46647819  0.46683862  0.46931235  0.46979461  0.47056151  0.47121822
  0.47246927  0.47299022  0.4767122   0.4776309   0.47877084  0.47971697
  0.47984736  0.48131475  0.48431022  0.48832669  0.49015795  0.49092667
  0.49273387  0.4953687   0.49711237  0.49784283  0.49902405  0.5019787
  0.50304936  0.50434192  0.50759075  0.5131539   0.51401558  0.51557545
  0.51562714  0.51694785  0.51747391  0.5184264   0.5192208   0.52446494
  0.52618304  0.52654427  0.52715098  0.5311996   0.5329353   0.53318802
  0.53353027  0.53573544  0.53623117  0.53663054  0.5370767   0.53960489
  0.5423801   0.54274699  0.54278692  0.54476928  0.54524993  0.54907611
  0.55444586  0.55612758  0.55809002  0.56034747  0.5612653   0.56202614
  0.56648153  0.57046249  0.5725706   0.57669734  0.57732556  0.57812077
  0.57882551  0.59080873  0.59151478  0.59607806  0.59774286  0.59938931
  0.60096004  0.60099082  0.60204404  0.6028418   0.60746249  0.60829754
  0.61076195  0.6109618   0.61178853  0.61294405  0.61346695  0.61369982
  0.61548946  0.61559443  0.61789095  0.62000089  0.62120751  0.62145502
  0.62310962  0.62608093  0.62622932  0.62637637  0.6267288   0.62779219
  0.6295627   0.62984214  0.63270658  0.63597221  0.63923713  0.64275646
  0.65150969  0.66003487  0.66109777  0.66258356  0.66403011  0.66627917
  0.66842485  0.66860195  0.67405354  0.67739619  0.68416893  0.68585126
  0.68859108  0.70728528  0.70950519  0.71819853  0.74263155  0.74304965
  0.75808064]

  warnings.warn(

2022-11-03 10:49:35,004:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.22657301 -0.18056787 -0.16702363 -0.15798243 -0.15485692 -0.14971286
 -0.13486099 -0.13183853 -0.13167579 -0.13122138 -0.13015354 -0.12732392
 -0.12674191 -0.11781733 -0.11731469 -0.11728559 -0.11240004 -0.11201742
 -0.11155993 -0.10812495 -0.10798894 -0.10728279 -0.10634199 -0.10454029
 -0.09620606 -0.0888599  -0.0876128  -0.08241855 -0.08229686 -0.0811841
 -0.06999725 -0.06894179 -0.06884659 -0.06747883 -0.06453352 -0.06221831
 -0.05969373 -0.05950208 -0.05721118 -0.05711585 -0.05556589 -0.05483172
 -0.05436248 -0.05115247 -0.04822117 -0.046139   -0.04444356 -0.04411948
 -0.04126646 -0.04126425 -0.03211252 -0.03173396 -0.03116283 -0.03064262
 -0.0305486  -0.02962239 -0.02644914 -0.0250672  -0.0239212  -0.02341289
 -0.02136788 -0.0201588  -0.01999216 -0.01897884 -0.01434592 -0.00524102
 -0.00293332  0.00434455  0.00809702  0.00819065  0.00890449  0.00943604
  0.01019142  0.01114917  0.01265099  0.01775392  0.01821809  0.01833767
  0.02151963  0.02169868  0.02367268  0.02490299  0.02550301  0.02685723
  0.02690429  0.0284003   0.03286617  0.03393791  0.03641687  0.03768583
  0.03818589  0.04312428  0.04340464  0.04381202  0.04636536  0.0470937
  0.04717933  0.04802373  0.04854424  0.04883676  0.04902181  0.05030669
  0.05157633  0.05242098  0.05458068  0.05487543  0.05687592  0.0591597
  0.06010227  0.0657264   0.06657872  0.06678809  0.06697832  0.06729794
  0.06895846  0.07428823  0.07631594  0.07683336  0.07975882  0.0805036
  0.08052325  0.08308088  0.08335027  0.08382028  0.08538153  0.08639809
  0.08666538  0.08939363  0.09066874  0.09097883  0.0932916   0.09979535
  0.10009006  0.10062306  0.10274177  0.10307494  0.10411565  0.10412159
  0.1041267   0.10558184  0.10621674  0.10651552  0.10711163  0.10853626
  0.10871681  0.11082383  0.11134134  0.11674478  0.11753464  0.11994859
  0.12032552  0.12034826  0.1235484   0.12363803  0.12443638  0.12599835
  0.1265631   0.12717976  0.12802831  0.12833423  0.12909067  0.12942736
  0.13684612  0.13853212  0.14012628  0.14038946  0.14252091  0.14574568
  0.14598321  0.14834874  0.14905481  0.14941769  0.1514593   0.15418854
  0.15504592  0.15531687  0.15558176  0.15704129  0.16077394  0.16478764
  0.16832924  0.16879086  0.17154497  0.17250966  0.17259067  0.17441491
  0.17890322  0.17911446  0.17974779  0.17998434  0.18060306  0.18183506
  0.18427576  0.1846277   0.18544463  0.18570193  0.18613793  0.18707602
  0.18795156  0.18817251  0.18895145  0.19098411  0.1948904   0.19644258
  0.19904079  0.20256446  0.20311306  0.20387546  0.20414672  0.20430635
  0.20514187  0.20581666  0.20626762  0.20642696  0.20658697  0.20745285
  0.20816725  0.20828631  0.20862714  0.20890858  0.20951361  0.20971491
  0.21474568  0.21597413  0.21665752  0.21677115  0.21757435  0.21874343
  0.22179713  0.22334681  0.22457177  0.22503132  0.22856615  0.23105219
  0.23187497  0.23245706  0.23260935  0.23381215  0.23460276  0.23493053
  0.23637032  0.23652794  0.23675313  0.2387536   0.23997234  0.24297427
  0.2432612   0.24476383  0.24567548  0.24586916  0.24774891  0.24970563
  0.25009564  0.25071145  0.25086452  0.25415414  0.25520384  0.25569936
  0.25837725  0.26161687  0.26281359  0.26289756  0.26368549  0.26369872
  0.26550952  0.26607123  0.26906028  0.27233265  0.27377493  0.28535035
  0.28560916  0.28630545  0.28903541  0.28965617  0.29000754  0.29014543
  0.29034928  0.29199274  0.29338572  0.29349677  0.29451051  0.29469621
  0.29578773  0.29635508  0.29826958  0.29900815  0.30111999  0.30145863
  0.30158126  0.30164968  0.30396204  0.30399501  0.30417485  0.30436597
  0.30524498  0.30585657  0.30588384  0.30721137  0.3079349   0.30929158
  0.30983566  0.31093214  0.3110928   0.31261921  0.31419987  0.31611708
  0.31975398  0.32058247  0.32184573  0.3239108   0.32413679  0.32638508
  0.3277026   0.32978292  0.33008782  0.33057569  0.33074496  0.33092232
  0.33253826  0.33395164  0.33451367  0.33531703  0.33645913  0.34511469
  0.34533122  0.34556567  0.35060292  0.35098241  0.35200623  0.35225406
  0.35359796  0.35403307  0.35548305  0.35700805  0.35967571  0.36112005
  0.36249949  0.36405138  0.36559257  0.36626328  0.36831982  0.36849138
  0.36853142  0.37033181  0.37346665  0.37553347  0.37615893  0.37734819
  0.37795514  0.38173338  0.3835209   0.38352974  0.38461792  0.38538833
  0.38597452  0.38918742  0.40423065  0.404712    0.40533428  0.40596589
  0.4088778   0.40940763  0.41136874  0.41504347  0.41738581  0.41835345
  0.42201539  0.42295153  0.42438336  0.42555481  0.42955533  0.43113035
  0.43123128  0.43183058  0.4333905   0.43511477  0.43523473  0.43644021
  0.43654383  0.43674011  0.43945833  0.44328675  0.44928143  0.45053008
  0.45157682  0.45235362  0.45304356  0.4535548   0.46370866  0.46414208
  0.46580851  0.46892918  0.46984887  0.47247797  0.47492156  0.47791617
  0.47928028  0.48104399  0.48139329  0.48157962  0.48179672  0.4845107
  0.4850305   0.48848273  0.49100834  0.49601389  0.49990553  0.50404184
  0.50498858  0.50682698  0.51138719  0.51156347  0.51437427  0.51534692
  0.51558221  0.5176185   0.51775609  0.5235495   0.52632035  0.52657938
  0.52914876  0.53023381  0.53034798  0.5312201   0.5356681   0.53695964
  0.53974196  0.54053922  0.54148226  0.54500694  0.54727919  0.54816153
  0.54947701  0.54957359  0.55111739  0.55514707  0.55524523  0.55936644
  0.56593295  0.56600668  0.57212553  0.57428065  0.57779505  0.57936754
  0.58411013  0.58527028  0.58743094  0.58924804  0.59131346  0.59485606
  0.59789825  0.60088916  0.60095995  0.60185332  0.60473481  0.60549435
  0.60576261  0.60713975  0.61006638  0.6128267   0.61530048  0.61730524
  0.62201733  0.62367948  0.6237128   0.62780835  0.62801428  0.63314846
  0.64384935  0.64700646  0.64711106  0.64781727  0.659514    0.66388884
  0.66477161  0.66515939  0.66589887  0.66811503  0.67358263  0.6772808
  0.68419047  0.68443388  0.68640172  0.68684346  0.69512287  0.69516918
  0.70116978  0.71072608  0.71738876  0.71759364  0.72353895  0.72416261
  0.7306626 ]

  warnings.warn(

2022-11-03 10:49:35,052:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.19316158e-01 -2.01762112e-01 -1.93175648e-01 -1.75638928e-01
 -1.74917472e-01 -1.72997374e-01 -1.65048712e-01 -1.64929873e-01
 -1.64819574e-01 -1.64381407e-01 -1.56765217e-01 -1.55060916e-01
 -1.53754117e-01 -1.51797646e-01 -1.42810621e-01 -1.39259005e-01
 -1.37843993e-01 -1.36352713e-01 -1.36310687e-01 -1.30450308e-01
 -1.29018702e-01 -1.18626559e-01 -1.17995432e-01 -1.15449353e-01
 -1.12787093e-01 -1.12581921e-01 -1.11112261e-01 -1.09941931e-01
 -1.09122262e-01 -1.08701814e-01 -1.04919574e-01 -9.62600224e-02
 -9.40550475e-02 -9.33714901e-02 -9.23989447e-02 -8.98458728e-02
 -8.98203374e-02 -8.46304601e-02 -8.44014466e-02 -8.31378184e-02
 -8.28930166e-02 -8.25677134e-02 -8.12874785e-02 -7.89587971e-02
 -7.72851774e-02 -7.55389571e-02 -6.71672407e-02 -6.64551748e-02
 -6.63336270e-02 -6.28223029e-02 -6.24163107e-02 -6.23464225e-02
 -6.21807349e-02 -5.61528133e-02 -5.49877941e-02 -5.48532877e-02
 -5.36198030e-02 -5.27295788e-02 -5.21718664e-02 -4.63318512e-02
 -4.63174844e-02 -4.62817912e-02 -4.57472783e-02 -4.50414135e-02
 -4.39408500e-02 -4.12139562e-02 -3.83479885e-02 -3.68695930e-02
 -3.63482935e-02 -3.28846125e-02 -3.28568720e-02 -3.24175175e-02
 -2.39237447e-02 -1.96008851e-02 -1.72921664e-02 -1.65665775e-02
 -1.55055745e-02 -1.45866905e-02 -1.39999483e-02 -9.95296052e-03
 -9.66856333e-03 -7.34029994e-03 -6.79986354e-03 -4.03439926e-03
 -1.46505110e-03  5.97319966e-04  1.37019075e-03  6.38256219e-03
  1.01765060e-02  1.06038356e-02  1.22372299e-02  1.48628934e-02
  1.64915994e-02  1.86692539e-02  1.95591002e-02  2.12136602e-02
  2.15112063e-02  2.31472466e-02  2.48616621e-02  2.96069107e-02
  2.97072540e-02  3.14411503e-02  3.18943754e-02  3.75885180e-02
  3.89441357e-02  4.06412044e-02  4.11376670e-02  4.49984159e-02
  5.03218567e-02  5.17971900e-02  5.37715987e-02  5.45260471e-02
  5.48877969e-02  5.59734515e-02  5.80816733e-02  5.85682940e-02
  5.87594269e-02  5.99670374e-02  6.04677592e-02  6.57424608e-02
  6.86854933e-02  6.99426432e-02  7.11402948e-02  7.62445820e-02
  7.70431431e-02  7.81409114e-02  8.01504791e-02  8.10683997e-02
  8.54509538e-02  8.58794582e-02  8.70953629e-02  8.73959570e-02
  8.93448152e-02  9.26674290e-02  9.33219072e-02  9.37606640e-02
  9.47874102e-02  9.82535577e-02  1.00252596e-01  1.01048589e-01
  1.01099721e-01  1.05403204e-01  1.05558089e-01  1.08570306e-01
  1.08797926e-01  1.09469003e-01  1.10876224e-01  1.13169176e-01
  1.14020860e-01  1.19419226e-01  1.22078785e-01  1.22462181e-01
  1.22909225e-01  1.24856302e-01  1.28015422e-01  1.30870963e-01
  1.31888206e-01  1.32111907e-01  1.32659986e-01  1.33356444e-01
  1.36129491e-01  1.37810215e-01  1.38176656e-01  1.39687037e-01
  1.42923405e-01  1.43268543e-01  1.43680264e-01  1.46877981e-01
  1.47986145e-01  1.48722397e-01  1.55040576e-01  1.56033438e-01
  1.57844800e-01  1.57984538e-01  1.58220594e-01  1.58452089e-01
  1.59705320e-01  1.61312920e-01  1.64536737e-01  1.67554885e-01
  1.69678386e-01  1.72297264e-01  1.73733653e-01  1.76266955e-01
  1.81714505e-01  1.82073746e-01  1.82384654e-01  1.83943042e-01
  1.85323761e-01  1.87295999e-01  1.92229931e-01  1.92291237e-01
  1.92414660e-01  1.94179296e-01  1.95362815e-01  1.97215166e-01
  1.97528387e-01  1.97634353e-01  1.97676924e-01  1.98077011e-01
  1.98294520e-01  1.99789061e-01  2.01409444e-01  2.02343049e-01
  2.04497499e-01  2.05289809e-01  2.06490007e-01  2.07082930e-01
  2.07147048e-01  2.07519988e-01  2.10900649e-01  2.11748303e-01
  2.14403546e-01  2.18532860e-01  2.19304461e-01  2.19543374e-01
  2.20925433e-01  2.21116700e-01  2.22797754e-01  2.23470700e-01
  2.24240841e-01  2.24819372e-01  2.25001307e-01  2.25508654e-01
  2.26163871e-01  2.32122493e-01  2.33053203e-01  2.36650235e-01
  2.39450272e-01  2.41275807e-01  2.42627535e-01  2.43026564e-01
  2.44452461e-01  2.45791891e-01  2.46102760e-01  2.46930632e-01
  2.49066205e-01  2.51944304e-01  2.54316083e-01  2.54450044e-01
  2.54467655e-01  2.54711150e-01  2.55248629e-01  2.56211437e-01
  2.57642773e-01  2.58053238e-01  2.62619192e-01  2.63601493e-01
  2.64023609e-01  2.65521621e-01  2.65571877e-01  2.66202111e-01
  2.69850313e-01  2.72084802e-01  2.74097884e-01  2.75170625e-01
  2.75419371e-01  2.76723239e-01  2.76837272e-01  2.77065573e-01
  2.77105629e-01  2.78470470e-01  2.80209436e-01  2.80286084e-01
  2.82945086e-01  2.83762377e-01  2.85245829e-01  2.85763946e-01
  2.86521480e-01  2.86618051e-01  2.88394307e-01  2.89106939e-01
  2.90028177e-01  2.90735676e-01  2.93391772e-01  2.97279777e-01
  2.98425364e-01  2.99216310e-01  3.09891431e-01  3.12139799e-01
  3.12301046e-01  3.13097810e-01  3.13489183e-01  3.13923740e-01
  3.15973341e-01  3.16595133e-01  3.17533962e-01  3.18018009e-01
  3.18704679e-01  3.18878104e-01  3.20725036e-01  3.22005195e-01
  3.23668995e-01  3.24123559e-01  3.24308447e-01  3.24385508e-01
  3.25729487e-01  3.26816832e-01  3.28294132e-01  3.28743600e-01
  3.28811935e-01  3.32142002e-01  3.32584189e-01  3.35126088e-01
  3.41735795e-01  3.42962001e-01  3.46534749e-01  3.47202854e-01
  3.50650066e-01  3.50886188e-01  3.51497713e-01  3.51676451e-01
  3.52312495e-01  3.53076386e-01  3.53641879e-01  3.55965549e-01
  3.56811972e-01  3.57557115e-01  3.58175639e-01  3.60334297e-01
  3.62552478e-01  3.63652319e-01  3.68732698e-01  3.69167280e-01
  3.71838741e-01  3.72026646e-01  3.73274940e-01  3.77212437e-01
  3.79146891e-01  3.81012874e-01  3.85885736e-01  3.86026482e-01
  3.87524582e-01  3.87839945e-01  3.88846818e-01  3.90161062e-01
  3.90605493e-01  3.91193116e-01  3.91819132e-01  3.91908560e-01
  3.93249963e-01  3.95219064e-01  3.97460884e-01  3.99048676e-01
  4.02393013e-01  4.06378628e-01  4.06397074e-01  4.13136374e-01
  4.13190160e-01  4.16200437e-01  4.16497342e-01  4.19879388e-01
  4.21062181e-01  4.21373459e-01  4.22840840e-01  4.24087910e-01
  4.24661415e-01  4.29349574e-01  4.30237145e-01  4.31018313e-01
  4.31589475e-01  4.32388086e-01  4.34032126e-01  4.40770250e-01
  4.40937117e-01  4.41081160e-01  4.41149248e-01  4.41459793e-01
  4.41945756e-01  4.43508596e-01  4.44191011e-01  4.45226480e-01
  4.46928935e-01  4.47238311e-01  4.47314041e-01  4.48040799e-01
  4.53464642e-01  4.57442266e-01  4.63631869e-01  4.63788625e-01
  4.66356819e-01  4.67367463e-01  4.68660635e-01  4.71062136e-01
  4.71238153e-01  4.74372220e-01  4.79613770e-01  4.80612425e-01
  4.81027457e-01  4.84206098e-01  4.85446750e-01  4.86847432e-01
  4.87541261e-01  4.89973268e-01  4.90390969e-01  4.91334271e-01
  4.91929323e-01  4.92533166e-01  4.93552791e-01  4.95085567e-01
  4.98618674e-01  4.99264090e-01  5.00689093e-01  5.02294248e-01
  5.03227724e-01  5.04358482e-01  5.04571118e-01  5.04735461e-01
  5.05545391e-01  5.07053204e-01  5.10178569e-01  5.15192811e-01
  5.15219138e-01  5.17010871e-01  5.21267392e-01  5.21293016e-01
  5.23494074e-01  5.31819747e-01  5.33138016e-01  5.34715718e-01
  5.35005456e-01  5.36845707e-01  5.37002542e-01  5.37189135e-01
  5.37691849e-01  5.42918706e-01  5.44884310e-01  5.45339883e-01
  5.47725579e-01  5.55025524e-01  5.57866054e-01  5.60377196e-01
  5.62065843e-01  5.63611702e-01  5.63905639e-01  5.66210609e-01
  5.66548462e-01  5.70533358e-01  5.72729967e-01  5.75416113e-01
  5.75746924e-01  5.75812984e-01  5.77443828e-01  5.77664808e-01
  5.79518297e-01  5.79957113e-01  5.87642149e-01  5.92062479e-01
  5.94888750e-01  5.94977888e-01  5.97355727e-01  5.98086195e-01
  6.04311270e-01  6.10028405e-01  6.10062966e-01  6.11374718e-01
  6.12916882e-01  6.13724993e-01  6.13828167e-01  6.15872654e-01
  6.16220738e-01  6.24411661e-01  6.26924471e-01  6.27654834e-01
  6.29301938e-01  6.36694343e-01  6.42380908e-01  6.44876444e-01
  6.45906733e-01  6.46845326e-01  6.47520614e-01  6.48189879e-01
  6.48510663e-01  6.50896885e-01  6.51140666e-01  6.54380960e-01
  6.62403222e-01  6.62494656e-01  6.63341199e-01  6.63818340e-01
  6.64795754e-01  6.65054612e-01  6.66502566e-01  6.72483199e-01
  6.75747763e-01  6.94584864e-01  7.03616132e-01  7.14529745e-01
  7.38300200e-01  7.45219960e-01  7.59067915e-01  7.67385068e-01
  7.68830725e-01]

  warnings.warn(

2022-11-03 10:49:35,060:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.76242728e-01 -1.72134510e-01 -1.59744682e-01 -1.54367836e-01
 -1.53607894e-01 -1.36149587e-01 -1.32536805e-01 -1.32114512e-01
 -1.26328721e-01 -1.17578218e-01 -1.17492079e-01 -1.15428769e-01
 -1.12551795e-01 -1.12544210e-01 -1.10804137e-01 -1.08946709e-01
 -1.07729536e-01 -1.06371604e-01 -1.03190567e-01 -1.01256243e-01
 -1.00874183e-01 -9.84555999e-02 -9.80823640e-02 -9.55394551e-02
 -9.49029929e-02 -9.41392996e-02 -9.10008619e-02 -8.25146668e-02
 -7.88312686e-02 -7.82106992e-02 -7.77212127e-02 -7.64154819e-02
 -7.56471275e-02 -6.85942816e-02 -6.72959399e-02 -6.19457346e-02
 -6.03656108e-02 -5.99080884e-02 -5.82910863e-02 -5.64013525e-02
 -5.62291121e-02 -5.42594352e-02 -5.22531110e-02 -5.10257691e-02
 -5.02341668e-02 -4.86175821e-02 -4.76507966e-02 -4.73359435e-02
 -4.53437871e-02 -4.42995337e-02 -4.36651242e-02 -4.17386276e-02
 -4.11936246e-02 -3.77797935e-02 -3.70020117e-02 -3.66019161e-02
 -3.64060816e-02 -3.46959488e-02 -3.37030168e-02 -3.36323506e-02
 -2.92562481e-02 -2.66496121e-02 -2.60838586e-02 -2.59075184e-02
 -2.54699222e-02 -2.49852535e-02 -2.42466547e-02 -2.15500922e-02
 -1.97654739e-02 -1.84662998e-02 -1.59845298e-02 -1.50019150e-02
 -1.44617152e-02 -1.05023210e-02 -7.67207816e-03 -2.82960191e-03
 -2.28521403e-03 -1.44659556e-03 -2.08170196e-04  4.19926283e-04
  4.98482836e-03  8.64282660e-03  9.58110454e-03  1.21358178e-02
  1.40901838e-02  1.44768404e-02  1.87282160e-02  1.99472574e-02
  2.33036968e-02  2.35700763e-02  2.43048451e-02  2.49655879e-02
  2.58268001e-02  2.67504259e-02  2.78760542e-02  2.88622585e-02
  2.96731832e-02  3.07107778e-02  3.15325036e-02  3.25437253e-02
  3.41997393e-02  3.70682786e-02  3.75529605e-02  3.77561477e-02
  3.81039301e-02  3.92763183e-02  4.15299809e-02  4.17727317e-02
  4.24510604e-02  4.36866353e-02  4.42047566e-02  4.43307988e-02
  4.52128097e-02  5.13691168e-02  5.13778740e-02  5.34868100e-02
  5.50269302e-02  5.52677859e-02  5.63201766e-02  5.63542968e-02
  5.65924340e-02  5.77606165e-02  5.87351295e-02  6.19478494e-02
  6.49542497e-02  6.58648717e-02  6.81894491e-02  6.93364533e-02
  6.94129245e-02  6.95237457e-02  6.95785754e-02  6.96574671e-02
  7.03087818e-02  7.56979684e-02  7.64741275e-02  7.64929921e-02
  7.82686305e-02  7.95709894e-02  8.01024176e-02  8.02956013e-02
  8.25170732e-02  8.41421783e-02  8.62036354e-02  8.71677159e-02
  8.89211150e-02  8.91337956e-02  9.07742172e-02  9.14128649e-02
  9.14728677e-02  9.16749807e-02  9.44229990e-02  9.61374127e-02
  9.69539217e-02  9.89536610e-02  9.98626543e-02  1.02463243e-01
  1.04565979e-01  1.05456408e-01  1.05820119e-01  1.06175454e-01
  1.06382696e-01  1.08371019e-01  1.09041818e-01  1.11310902e-01
  1.13890265e-01  1.15020316e-01  1.17387978e-01  1.17759518e-01
  1.22096603e-01  1.26993483e-01  1.28167363e-01  1.28184225e-01
  1.29967891e-01  1.33706266e-01  1.33963552e-01  1.39542343e-01
  1.40161241e-01  1.49959021e-01  1.51395031e-01  1.51864861e-01
  1.55419203e-01  1.55642930e-01  1.57371573e-01  1.58036013e-01
  1.58301460e-01  1.58696270e-01  1.59971690e-01  1.61714355e-01
  1.61806016e-01  1.62830617e-01  1.63703732e-01  1.64365720e-01
  1.65716661e-01  1.68670270e-01  1.69192653e-01  1.70030477e-01
  1.71977621e-01  1.73046650e-01  1.74607746e-01  1.76487674e-01
  1.77347245e-01  1.80367869e-01  1.80468931e-01  1.82714671e-01
  1.83756866e-01  1.87115629e-01  1.87540205e-01  1.89412724e-01
  1.90109742e-01  1.91815904e-01  1.94698109e-01  1.98902807e-01
  1.99588766e-01  2.00382046e-01  2.01147800e-01  2.03006640e-01
  2.04949916e-01  2.06163816e-01  2.07013705e-01  2.07427147e-01
  2.07849118e-01  2.07978108e-01  2.09198724e-01  2.11811477e-01
  2.11876085e-01  2.13293039e-01  2.18391729e-01  2.19139598e-01
  2.20965908e-01  2.22951956e-01  2.24613900e-01  2.27079464e-01
  2.27992762e-01  2.28406246e-01  2.28471647e-01  2.31423656e-01
  2.31847116e-01  2.35135441e-01  2.36896891e-01  2.38554216e-01
  2.38900860e-01  2.40597698e-01  2.41208727e-01  2.41442464e-01
  2.44243425e-01  2.46495399e-01  2.46886220e-01  2.49495871e-01
  2.49624243e-01  2.53328808e-01  2.57267099e-01  2.57471384e-01
  2.59908729e-01  2.60800391e-01  2.61174571e-01  2.63958914e-01
  2.63978308e-01  2.66829555e-01  2.66952058e-01  2.68213439e-01
  2.69429466e-01  2.70258388e-01  2.70743023e-01  2.71436348e-01
  2.76848956e-01  2.77117640e-01  2.79122284e-01  2.79497981e-01
  2.79541697e-01  2.81316590e-01  2.84473369e-01  2.86288027e-01
  2.86795761e-01  2.88976340e-01  2.92282198e-01  2.92785577e-01
  2.93003865e-01  2.95051373e-01  2.96773202e-01  2.97073839e-01
  2.98043238e-01  2.98959516e-01  2.99059043e-01  2.99246456e-01
  3.00351756e-01  3.03752858e-01  3.05365066e-01  3.05904002e-01
  3.06438199e-01  3.07214814e-01  3.07464968e-01  3.07493110e-01
  3.08376700e-01  3.10606077e-01  3.13360764e-01  3.13434706e-01
  3.14635494e-01  3.15929325e-01  3.19151255e-01  3.19490207e-01
  3.19547610e-01  3.19969875e-01  3.22147966e-01  3.22487017e-01
  3.22665829e-01  3.28842880e-01  3.28898479e-01  3.29150801e-01
  3.30736729e-01  3.33811457e-01  3.34568366e-01  3.35230513e-01
  3.38513769e-01  3.42136044e-01  3.45753293e-01  3.47437346e-01
  3.47910079e-01  3.48296899e-01  3.52879257e-01  3.53841549e-01
  3.55488459e-01  3.56437656e-01  3.57085613e-01  3.58619308e-01
  3.58897346e-01  3.59571291e-01  3.59881921e-01  3.62972786e-01
  3.65001404e-01  3.65594287e-01  3.65694901e-01  3.67175067e-01
  3.67979175e-01  3.68363752e-01  3.70366117e-01  3.71422078e-01
  3.74047661e-01  3.74931950e-01  3.76401820e-01  3.82128525e-01
  3.83299893e-01  3.84905439e-01  3.85146588e-01  3.88766688e-01
  3.89947117e-01  3.92148256e-01  3.95408816e-01  4.01609855e-01
  4.04565562e-01  4.07787479e-01  4.12526606e-01  4.12887664e-01
  4.13743961e-01  4.13856313e-01  4.14247659e-01  4.14804959e-01
  4.14979133e-01  4.18096308e-01  4.23679515e-01  4.24207208e-01
  4.25028412e-01  4.25071858e-01  4.25603080e-01  4.26238115e-01
  4.33321273e-01  4.36073400e-01  4.36595146e-01  4.39497024e-01
  4.42380014e-01  4.43120110e-01  4.46400625e-01  4.49195392e-01
  4.49291859e-01  4.49309072e-01  4.50346627e-01  4.51306374e-01
  4.56309049e-01  4.56658288e-01  4.57153978e-01  4.59356333e-01
  4.60451323e-01  4.60484707e-01  4.60611069e-01  4.63313262e-01
  4.67514915e-01  4.68659178e-01  4.70447576e-01  4.73671877e-01
  4.75815315e-01  4.77700658e-01  4.78327231e-01  4.79394334e-01
  4.82891803e-01  4.86743819e-01  4.91152074e-01  4.91471203e-01
  4.93250725e-01  4.94033612e-01  4.95377220e-01  4.97567992e-01
  5.01533459e-01  5.02262735e-01  5.03182292e-01  5.06033007e-01
  5.09977484e-01  5.11474571e-01  5.13577121e-01  5.14069332e-01
  5.19549797e-01  5.21964670e-01  5.22269284e-01  5.25812139e-01
  5.27969131e-01  5.28601875e-01  5.29105039e-01  5.29133200e-01
  5.31167377e-01  5.36526465e-01  5.39547381e-01  5.45553805e-01
  5.46290453e-01  5.48379930e-01  5.48754437e-01  5.51098033e-01
  5.52148558e-01  5.53309548e-01  5.58010232e-01  5.61823557e-01
  5.67772450e-01  5.76166598e-01  5.78189518e-01  5.82021565e-01
  5.84854315e-01  5.85487769e-01  5.88400501e-01  5.92475660e-01
  5.93597838e-01  5.94394041e-01  5.96774240e-01  5.97488997e-01
  5.97519716e-01  5.97778436e-01  6.00528280e-01  6.02435333e-01
  6.03536634e-01  6.04235039e-01  6.04708962e-01  6.05292260e-01
  6.06622677e-01  6.06888963e-01  6.12021733e-01  6.12443246e-01
  6.12455169e-01  6.12992139e-01  6.13191307e-01  6.16415773e-01
  6.18668090e-01  6.19562692e-01  6.24096156e-01  6.26688733e-01
  6.27859090e-01  6.28368247e-01  6.33363217e-01  6.34688014e-01
  6.37413923e-01  6.42634031e-01  6.43720004e-01  6.47996010e-01
  6.50012209e-01  6.54799945e-01  6.54941260e-01  6.57855081e-01
  6.59626568e-01  6.60294608e-01  6.60653806e-01  6.61112349e-01
  6.66021677e-01  6.66181499e-01  6.67447717e-01  6.73967727e-01
  6.83057009e-01  6.84216794e-01  7.00676507e-01  7.08366611e-01
  7.08430971e-01  7.11246359e-01  7.20602747e-01  7.20697195e-01
  7.22342919e-01  7.24346863e-01  7.26463715e-01  7.63663916e-01]

  warnings.warn(

2022-11-03 10:49:35,060:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.25690761 -0.18940117 -0.18475104 -0.17906132 -0.17373001 -0.17250519
 -0.16978581 -0.16713089 -0.16354426 -0.15934475 -0.15814905 -0.15435991
 -0.14849787 -0.14743818 -0.14707374 -0.14111913 -0.13552147 -0.13382836
 -0.12914079 -0.12905275 -0.12322654 -0.11992988 -0.1191206  -0.11751635
 -0.11593106 -0.11090326 -0.10500064 -0.10264593 -0.10242281 -0.10167166
 -0.09734455 -0.09330283 -0.09082425 -0.09060501 -0.08603801 -0.08106999
 -0.08006477 -0.07909002 -0.0784691  -0.0780664  -0.06979688 -0.06677989
 -0.06431733 -0.06246962 -0.06173978 -0.06163014 -0.06134673 -0.05872202
 -0.05776078 -0.05742044 -0.05716504 -0.05707215 -0.05438514 -0.054382
 -0.05405994 -0.05356601 -0.05085176 -0.04982835 -0.04936947 -0.04928023
 -0.04882484 -0.04318798 -0.04165547 -0.04155354 -0.03679582 -0.03160613
 -0.03017105 -0.02824428 -0.0275062  -0.02687555 -0.0258564  -0.02473897
 -0.02191313 -0.0216041  -0.01857714 -0.01674825 -0.01477343 -0.01199099
 -0.01097533 -0.00728416 -0.00674136 -0.00316013 -0.00151448 -0.00095975
  0.00395452  0.01018921  0.01185683  0.01488171  0.01588494  0.01788882
  0.01813183  0.02445911  0.02486369  0.02751157  0.02778761  0.02846424
  0.02939637  0.03185524  0.03342597  0.0335482   0.0359935   0.03804446
  0.03814176  0.04049847  0.04196426  0.04198724  0.04302871  0.04590399
  0.04674617  0.04749957  0.04815723  0.04863074  0.04887772  0.04897733
  0.05063298  0.05118402  0.05347599  0.05748061  0.05922178  0.06027766
  0.06046162  0.06084609  0.06104168  0.06133322  0.06163739  0.06361143
  0.06439508  0.06464031  0.06549587  0.06584739  0.06695692  0.06748382
  0.06838781  0.06957048  0.06972397  0.07061813  0.07249107  0.07295467
  0.07326125  0.07896725  0.07927848  0.08045351  0.08118579  0.08141055
  0.0815476   0.08432907  0.08487682  0.08554011  0.08623837  0.08882783
  0.08931913  0.09805537  0.10071336  0.10107748  0.10143382  0.10565382
  0.10927297  0.11256565  0.11670338  0.12027792  0.12316996  0.12348342
  0.12399573  0.12659018  0.12711235  0.1279968   0.13035715  0.13114911
  0.13369831  0.13557469  0.13657575  0.13811808  0.13827491  0.13863758
  0.14051518  0.14196264  0.14267072  0.14296546  0.1449944   0.14540343
  0.146557    0.14835695  0.14881887  0.15172081  0.15192802  0.15200417
  0.15211227  0.15547506  0.1559946   0.15667074  0.15695364  0.16122191
  0.16224865  0.1631659   0.16711259  0.16861929  0.17204549  0.17314664
  0.17514007  0.1775052   0.18078207  0.18167874  0.18354266  0.18378454
  0.18581082  0.18650047  0.18731246  0.18835653  0.18911539  0.18921648
  0.19279723  0.19771904  0.19790602  0.19838271  0.19842248  0.19964102
  0.20007457  0.20068123  0.20100124  0.20158078  0.20290636  0.20456122
  0.20496294  0.2076835   0.20837394  0.20911231  0.20922356  0.20968064
  0.21141031  0.21262512  0.21553425  0.21572677  0.21585672  0.219078
  0.22013762  0.22183631  0.22190492  0.22417446  0.22439137  0.22461158
  0.22779807  0.22829136  0.22968477  0.23062735  0.23515693  0.23578258
  0.23769958  0.23827049  0.23860639  0.23895492  0.23919039  0.24069332
  0.24173265  0.24222476  0.24406398  0.24490774  0.24715208  0.2504729
  0.25429533  0.25569654  0.25714436  0.25734428  0.25782556  0.25793788
  0.2600279   0.26548323  0.26597955  0.26701463  0.26729723  0.26827979
  0.26933866  0.26972913  0.27226188  0.27312463  0.27319367  0.27343993
  0.27496192  0.27578311  0.27625413  0.27629375  0.27692413  0.27736654
  0.2796044   0.28086541  0.28323782  0.28370489  0.28701824  0.28719885
  0.28746971  0.28894915  0.29088767  0.29122975  0.29246953  0.29255183
  0.29541891  0.30112494  0.30318483  0.30444356  0.30570563  0.30714116
  0.30792558  0.30827228  0.3092557   0.30939021  0.31013372  0.31115906
  0.31423196  0.31462555  0.31528433  0.31568093  0.32047115  0.32227358
  0.32342369  0.32525835  0.32967064  0.33504585  0.33619138  0.33620868
  0.33903985  0.34418965  0.34510405  0.34521076  0.34771945  0.34809944
  0.34942613  0.35042177  0.35282801  0.35373395  0.35441708  0.35680653
  0.35914555  0.360025    0.36064667  0.36096456  0.36115307  0.36367212
  0.36368522  0.36403408  0.36424193  0.36823888  0.37356551  0.37413951
  0.37570129  0.37701015  0.37915337  0.38084448  0.38719883  0.38741643
  0.38930823  0.39314488  0.39864794  0.40177333  0.40370722  0.40554164
  0.41010645  0.41052135  0.41539458  0.41571754  0.41762743  0.42206165
  0.42216902  0.42671753  0.42675201  0.42802286  0.42961348  0.42981172
  0.43202714  0.43289453  0.44045007  0.44193301  0.4423659   0.44728675
  0.44777332  0.44797545  0.44804125  0.44962147  0.4507991   0.4585057
  0.4639489   0.46514034  0.4728929   0.47582991  0.47837452  0.47979574
  0.48125199  0.48161086  0.48241546  0.48377573  0.484043    0.48585799
  0.48728502  0.48939809  0.49323225  0.49517841  0.49538391  0.49597402
  0.50065954  0.5011468   0.50372153  0.50401407  0.50521515  0.51019437
  0.51126538  0.51149826  0.51763547  0.52211128  0.52259656  0.52315947
  0.52547172  0.52642224  0.5283543   0.53056361  0.53106553  0.532367
  0.53313287  0.53525199  0.53543991  0.53669018  0.5389059   0.54094792
  0.54140258  0.54241963  0.54324717  0.54341688  0.54584257  0.54811106
  0.5482965   0.55060123  0.55102202  0.55130007  0.55414172  0.55594116
  0.55629568  0.55702309  0.55816615  0.55833106  0.56017015  0.56349622
  0.57210876  0.57603606  0.57741002  0.57933007  0.58019827  0.5805286
  0.58161902  0.58797944  0.58817059  0.58871734  0.58935096  0.59132952
  0.59158681  0.59809872  0.59969416  0.60185641  0.60198417  0.60615162
  0.61313907  0.61467918  0.61621821  0.62516263  0.62536663  0.62592072
  0.6270853   0.62769153  0.62813827  0.62915252  0.6299306   0.63422958
  0.63901513  0.64096999  0.64715534  0.65096477  0.65382149  0.65603967
  0.65624578  0.66767393  0.67169426  0.67469622  0.67547634  0.67645298
  0.67825978  0.67849539  0.67859076  0.67886554  0.68034321  0.68333941
  0.68805402  0.68906745  0.69239505  0.71360871  0.71662912  0.72197061
  0.72941499]

  warnings.warn(

2022-11-03 10:49:37,507:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.21432575 -0.20915961 -0.20791527 -0.18921325 -0.18680293 -0.17121524
 -0.16830562 -0.15160767 -0.15157323 -0.15128439 -0.15037714 -0.14978996
 -0.13911794 -0.1326331  -0.13229535 -0.13160777 -0.12827284 -0.1271102
 -0.12657809 -0.12558135 -0.12521806 -0.12350088 -0.1229853  -0.12180216
 -0.11995844 -0.11845603 -0.10650828 -0.10548376 -0.0997422  -0.09863517
 -0.09692832 -0.0950306  -0.0940512  -0.09289899 -0.09259429 -0.09175097
 -0.0891734  -0.08878996 -0.08426669 -0.07720144 -0.07559931 -0.07356759
 -0.0722139  -0.07092258 -0.07081206 -0.06416651 -0.05988964 -0.05940984
 -0.05869446 -0.05825753 -0.05376722 -0.0528725  -0.05026195 -0.04977644
 -0.046849   -0.04536542 -0.04408323 -0.04325224 -0.04293387 -0.04130654
 -0.04041285 -0.03970555 -0.03753727 -0.03634789 -0.03394797 -0.03297701
 -0.03023057 -0.02713822 -0.02674404 -0.02299184 -0.02096777 -0.01930232
 -0.01852931 -0.01806938 -0.01501503 -0.0149507  -0.01392916 -0.01257512
 -0.00976471 -0.00894614 -0.00881419 -0.00551878 -0.00470655 -0.00403525
 -0.00277455 -0.00145009 -0.00096758  0.00104525  0.00290014  0.00345951
  0.00571034  0.00617081  0.00707581  0.00726417  0.0087533   0.01546348
  0.02066352  0.02178529  0.02399607  0.02422463  0.02618142  0.03033284
  0.03072948  0.0322268   0.03307079  0.03336734  0.03626738  0.03647388
  0.039351    0.04434012  0.04807028  0.0485243   0.04922196  0.05019411
  0.05281392  0.05647377  0.05774747  0.0599485   0.06060414  0.06086514
  0.06438305  0.06444106  0.06502394  0.06552688  0.06644404  0.0679345
  0.06834904  0.0707403   0.07176733  0.07199705  0.07329301  0.0737398
  0.07526476  0.0755645   0.07976338  0.08383478  0.08472818  0.08484468
  0.08649352  0.08717087  0.08725554  0.08779266  0.08788906  0.09126126
  0.09220816  0.09295036  0.09545396  0.09643859  0.0971498   0.10072905
  0.10131565  0.1024909   0.10386723  0.10543723  0.10963344  0.11119583
  0.11201533  0.11230207  0.11246315  0.11314778  0.11481981  0.11627443
  0.11702268  0.12128137  0.12227058  0.12250885  0.12272898  0.12319785
  0.12376312  0.12542172  0.12557133  0.12699232  0.12812387  0.1308207
  0.13134887  0.13371837  0.1338412   0.1397901   0.13994993  0.14053035
  0.14122813  0.14142549  0.14247067  0.14247331  0.14287564  0.14498169
  0.14885007  0.15336173  0.15371908  0.15424427  0.1545088   0.15697737
  0.1574109   0.15796585  0.15968576  0.16083229  0.16288138  0.16409322
  0.16501335  0.16569555  0.16584969  0.16698434  0.17028677  0.17373801
  0.17564718  0.176041    0.17766445  0.17885069  0.18129899  0.18188957
  0.18355034  0.18361126  0.18595422  0.1884306   0.18890081  0.19211844
  0.19356487  0.19459893  0.19602973  0.19720509  0.20215478  0.20231263
  0.20246503  0.20249907  0.20266475  0.20280033  0.20357712  0.20457983
  0.20486556  0.21023325  0.21231251  0.21603912  0.22206245  0.22375065
  0.22555222  0.22558101  0.2257447   0.23097056  0.23167674  0.23289065
  0.23311821  0.23325418  0.23371461  0.23489627  0.23555786  0.2382231
  0.2395991   0.24006083  0.24333648  0.24426848  0.246679    0.24679893
  0.24981765  0.25096062  0.25429097  0.25884075  0.25937206  0.26384629
  0.26420356  0.26580457  0.26639224  0.2672591   0.27029108  0.27287517
  0.28096224  0.28104909  0.28142058  0.28208281  0.28555334  0.28649478
  0.28901905  0.29269605  0.29301303  0.29368406  0.29446475  0.29632853
  0.29633989  0.29656633  0.29704973  0.29714406  0.29831999  0.29854034
  0.29942754  0.30126585  0.30141456  0.30277017  0.30374595  0.3068095
  0.30845257  0.31001705  0.31156121  0.31425199  0.32080874  0.32214317
  0.32655846  0.3265866   0.32669599  0.32694119  0.32699425  0.32789028
  0.32821161  0.33057694  0.33059094  0.33162624  0.3353508   0.33963408
  0.34051903  0.34178676  0.34183559  0.3456976   0.34609432  0.35241965
  0.35254213  0.35344767  0.35535938  0.35704798  0.35982676  0.36055158
  0.36079508  0.36203796  0.36361011  0.36425843  0.36458001  0.36695079
  0.36735022  0.36807765  0.37126644  0.37160416  0.37294934  0.37344423
  0.37804981  0.38063587  0.38376174  0.38660413  0.39233114  0.39256532
  0.39611462  0.39661283  0.39761099  0.39879679  0.39911487  0.40164456
  0.40230874  0.40309514  0.40395172  0.40534851  0.40595674  0.40636809
  0.40797786  0.40898859  0.41116711  0.41582184  0.41707974  0.41722271
  0.41835377  0.42090925  0.42168023  0.42361748  0.42780905  0.4281501
  0.42918541  0.43277408  0.43420888  0.4348614   0.4377755   0.43779796
  0.43876035  0.43878193  0.43880369  0.43998905  0.4475005   0.44800052
  0.44859009  0.45221879  0.45240003  0.45342304  0.45524553  0.45705282
  0.45862708  0.45948676  0.46238267  0.46465339  0.46477532  0.46728532
  0.47509869  0.47819666  0.48048022  0.48210343  0.48243785  0.48435085
  0.48471818  0.48821901  0.48856266  0.48942344  0.49083299  0.49419969
  0.49577869  0.49595087  0.4982549   0.49934494  0.50096918  0.50227867
  0.50363109  0.50719999  0.50757899  0.51006893  0.51036637  0.5141592
  0.51467064  0.51507753  0.51679368  0.51923177  0.52115335  0.52333783
  0.52348293  0.52355623  0.52723573  0.52843067  0.52963841  0.53012561
  0.53029386  0.53098755  0.53145454  0.53210976  0.53237229  0.53336794
  0.53676171  0.54081558  0.54219438  0.5476443   0.54771855  0.55171065
  0.55185358  0.55318694  0.55450589  0.55960641  0.56467133  0.56550716
  0.57318026  0.57395568  0.5761066   0.57784092  0.58566929  0.5870199
  0.5881131   0.58851793  0.59220981  0.59254658  0.59304826  0.59566656
  0.59753891  0.60201453  0.60211828  0.60221185  0.6025733   0.61504696
  0.61803726  0.61934914  0.61957626  0.62159344  0.62306742  0.62524723
  0.62624202  0.62671543  0.62689366  0.62740403  0.6290509   0.63160918
  0.63204987  0.63734243  0.64130743  0.64323364  0.64875286  0.65131014
  0.6518844   0.65786265  0.66029963  0.66420335  0.66631098  0.68123443
  0.68762988  0.68790606  0.69507184  0.70214414  0.7078524   0.71122402
  0.71204875  0.71448966  0.71612508  0.720116    0.72130836  0.72595891
  0.75949915]

  warnings.warn(

2022-11-03 10:49:37,507:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16481319 -0.15781951 -0.15593834 -0.15470605 -0.14842616 -0.14312364
 -0.13555675 -0.13392082 -0.13083604 -0.12776983 -0.12640217 -0.12527365
 -0.1222633  -0.11934519 -0.11902853 -0.11844589 -0.11747659 -0.11582113
 -0.11328324 -0.10889979 -0.10746377 -0.1026733  -0.10250101 -0.10209196
 -0.10015891 -0.09670632 -0.09354852 -0.09268045 -0.09174063 -0.08887465
 -0.0864854  -0.08542962 -0.08199076 -0.07994004 -0.07890181 -0.0785153
 -0.07769418 -0.07163926 -0.07129202 -0.06978141 -0.06963817 -0.06692314
 -0.06527235 -0.06271823 -0.05577298 -0.05367968 -0.05348476 -0.0524461
 -0.05159123 -0.05044261 -0.05039237 -0.04808571 -0.04298163 -0.03821043
 -0.03539955 -0.03344661 -0.03244747 -0.03058758 -0.0266695  -0.02356809
 -0.02321036 -0.02081267 -0.01828832 -0.01692369 -0.01596892 -0.01348024
 -0.01077111 -0.01076616 -0.00995118 -0.00971348 -0.00818555 -0.00648091
 -0.00644855 -0.00523683 -0.00491942 -0.00150737  0.00474102  0.0062609
  0.00669988  0.00695042  0.00775181  0.00849396  0.01053688  0.01171565
  0.01263079  0.01421695  0.01564708  0.016884    0.01833705  0.02073335
  0.02386797  0.03000242  0.03044346  0.03154443  0.0333016   0.03676784
  0.03710088  0.0391029   0.03925505  0.04014374  0.04679995  0.04771426
  0.04969532  0.05206026  0.05306574  0.05647656  0.05651537  0.05719914
  0.06098699  0.06289301  0.06404664  0.06720242  0.06750611  0.06913461
  0.07064745  0.07392976  0.07403383  0.07418237  0.0759018   0.07610602
  0.07780442  0.07995455  0.08222529  0.08301234  0.08306903  0.0845005
  0.0846448   0.08592113  0.08608397  0.08612756  0.08645225  0.08663784
  0.08926362  0.09048849  0.09096368  0.09325279  0.09342667  0.0947941
  0.09642185  0.09827215  0.10460246  0.10498673  0.10508636  0.1078677
  0.10950877  0.11000435  0.11016848  0.11407632  0.1159785   0.12067357
  0.12099215  0.12253009  0.12333249  0.12355059  0.12684025  0.12821589
  0.1302988   0.13093688  0.13616904  0.13624812  0.13798287  0.13932644
  0.14091606  0.14393805  0.14412504  0.14664022  0.14686884  0.14836507
  0.14884301  0.15021184  0.156229    0.15669909  0.15695591  0.15706082
  0.15716003  0.15902106  0.16379972  0.16419352  0.16557552  0.16628482
  0.16725429  0.17403071  0.17733717  0.1781946   0.17957299  0.18098207
  0.18112734  0.18216153  0.18235789  0.18356008  0.18539107  0.18651676
  0.18890085  0.18895366  0.18946165  0.19023078  0.19097541  0.19130441
  0.19159453  0.19280831  0.194921    0.19492938  0.19888152  0.20222653
  0.2029442   0.20414801  0.20567749  0.20616701  0.2068731   0.20731665
  0.20823314  0.20917186  0.20979604  0.20989292  0.20989365  0.21091868
  0.21203848  0.21250755  0.2134225   0.21369123  0.21548343  0.2159646
  0.21629523  0.21648682  0.21866222  0.21922892  0.21989609  0.22061153
  0.22127285  0.22262066  0.22289817  0.22705908  0.22790181  0.23334726
  0.2346986   0.23770664  0.23773484  0.23807072  0.23820468  0.2388273
  0.24106049  0.24117102  0.24397026  0.24419419  0.24481664  0.24560513
  0.24960159  0.25076966  0.25092603  0.25369986  0.25673047  0.25766317
  0.259569    0.25973993  0.26044089  0.26238249  0.26287652  0.26605318
  0.26734797  0.2677879   0.2687489   0.27024893  0.27177728  0.27488714
  0.27592769  0.27681369  0.27718775  0.2790337   0.27943295  0.28526092
  0.28692698  0.28731904  0.28816438  0.28960828  0.29252189  0.29335151
  0.29834526  0.29948662  0.30093962  0.30710696  0.3077026   0.30815776
  0.30880463  0.30974749  0.31214116  0.31313679  0.31468749  0.31488879
  0.31526573  0.31552559  0.31630633  0.31792592  0.32052146  0.32260264
  0.32293184  0.32299208  0.32622634  0.32723279  0.33026845  0.33068715
  0.3368909   0.33739509  0.33770122  0.33988125  0.34061799  0.34192399
  0.34345021  0.34420282  0.34444403  0.34943523  0.34948137  0.35038639
  0.35154591  0.35378271  0.35512876  0.35517671  0.36015103  0.36127173
  0.36360277  0.36485101  0.3661138   0.36900737  0.37104962  0.37151158
  0.37282649  0.37577962  0.37606883  0.37858086  0.38037422  0.38317945
  0.38332691  0.38364194  0.38869955  0.39001611  0.39050282  0.39818445
  0.40192879  0.40403022  0.40431836  0.40559258  0.40640388  0.40857775
  0.4089485   0.41098767  0.41316794  0.41323277  0.4137794   0.41400009
  0.41412379  0.41481257  0.41519481  0.41586411  0.41612571  0.41720672
  0.42281724  0.42357311  0.42573863  0.42636918  0.42673454  0.43351453
  0.43768445  0.43900057  0.44387063  0.44625151  0.44933132  0.45020361
  0.45040992  0.45224558  0.45437747  0.45532666  0.45557306  0.45813834
  0.45886467  0.46254656  0.47166689  0.47167614  0.47185423  0.47396011
  0.47566046  0.47612416  0.47643519  0.47740487  0.47993092  0.48015959
  0.48061565  0.48257216  0.48305271  0.48931699  0.49159659  0.49215809
  0.49507248  0.49555113  0.49818632  0.49824511  0.49950169  0.49962209
  0.49976959  0.50202732  0.50240183  0.50337525  0.50565018  0.50585908
  0.50643085  0.51423438  0.51500117  0.51705473  0.52294882  0.52704163
  0.52887359  0.52930457  0.5299609   0.53094604  0.53406105  0.53782503
  0.53802572  0.53934167  0.54090445  0.54309757  0.5487862   0.55070703
  0.55101218  0.55232852  0.55362244  0.55983906  0.5625184   0.56276657
  0.56361523  0.56509001  0.56523392  0.56579448  0.56722087  0.56819881
  0.57396302  0.57417237  0.57469515  0.57522937  0.57526098  0.57723185
  0.58238585  0.58316966  0.58423602  0.58432962  0.58649338  0.58781413
  0.58854638  0.59089692  0.59107177  0.59122102  0.59152913  0.59355491
  0.59787683  0.59804171  0.60254554  0.60455694  0.60610709  0.60626002
  0.60896311  0.60928717  0.60941532  0.61104652  0.61331151  0.62149481
  0.62183094  0.62303706  0.62334101  0.62498232  0.62783002  0.62949557
  0.63006714  0.64217353  0.64634433  0.64753122  0.64965175  0.65249947
  0.65507128  0.65617535  0.65649124  0.66190019  0.66220111  0.66734926
  0.66871334  0.67149808  0.67629239  0.67710095  0.68412087  0.68489105
  0.69783021  0.69852166  0.69918726  0.70671733  0.72617778  0.73080806
  0.73275922]

  warnings.warn(

2022-11-03 10:49:37,507:INFO:Calculating mean and std
2022-11-03 10:49:37,507:INFO:Creating metrics dataframe
2022-11-03 10:49:37,515:INFO:Uploading results into container
2022-11-03 10:49:37,515:INFO:Uploading model into container now
2022-11-03 10:49:37,515:INFO:master_model_container: 5
2022-11-03 10:49:37,515:INFO:display_container: 2
2022-11-03 10:49:37,515:INFO:Ridge(random_state=4411)
2022-11-03 10:49:37,515:INFO:create_model() successfully completed......................................
2022-11-03 10:49:37,780:WARNING:create_model() for Ridge(random_state=4411) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:49:37,780:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:49:37,780:INFO:Initializing create_model()
2022-11-03 10:49:37,780:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:49:37,780:INFO:Checking exceptions
2022-11-03 10:49:37,795:INFO:Importing libraries
2022-11-03 10:49:37,795:INFO:Copying training dataset
2022-11-03 10:49:37,795:INFO:Defining folds
2022-11-03 10:49:37,795:INFO:Declaring metric variables
2022-11-03 10:49:37,795:INFO:Importing untrained model
2022-11-03 10:49:37,795:INFO:Ridge Regression Imported successfully
2022-11-03 10:49:37,795:INFO:Starting cross validation
2022-11-03 10:49:37,811:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:49:41,775:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.17826644 -0.17234718 -0.16894386 -0.16199299 -0.15876808 -0.15422958
 -0.14504911 -0.14437925 -0.14426339 -0.14092844 -0.14083386 -0.13948559
 -0.13944144 -0.13262528 -0.13215723 -0.13179996 -0.12992746 -0.12642014
 -0.12460899 -0.12319567 -0.11949849 -0.10674312 -0.09909473 -0.09683971
 -0.09634533 -0.09003178 -0.08578061 -0.08339223 -0.08286323 -0.0821077
 -0.08056422 -0.0784934  -0.07578167 -0.07384362 -0.07250723 -0.07213825
 -0.0718232  -0.07181273 -0.06808793 -0.06563253 -0.06545975 -0.0649026
 -0.06157109 -0.05370136 -0.05137067 -0.0505203  -0.04960706 -0.04925968
 -0.04457332 -0.04067486 -0.0380351  -0.0359727  -0.03443918 -0.03434552
 -0.02894469 -0.02735621 -0.02565374 -0.0248138  -0.0232316  -0.02260414
 -0.0202009  -0.0187633  -0.01860953 -0.01760112 -0.01712341 -0.01678397
 -0.01355266 -0.0107577  -0.00771943 -0.00754704 -0.00399974 -0.00237809
  0.00255853  0.00468443  0.00622677  0.00669986  0.00719583  0.00781647
  0.01067274  0.01155143  0.01482078  0.01690126  0.01808499  0.01862527
  0.02441693  0.02790534  0.02954016  0.03029665  0.03110806  0.03113291
  0.03450225  0.03546368  0.03974441  0.03975393  0.0399218   0.04037724
  0.04112859  0.05109527  0.05243057  0.05697585  0.06106517  0.06117116
  0.06255225  0.06590221  0.0661376   0.06632342  0.07044151  0.07236363
  0.07387349  0.07563775  0.07565681  0.0768757   0.07733464  0.08248783
  0.08472379  0.0849668   0.08617596  0.08634008  0.08683541  0.08700894
  0.09044267  0.09102355  0.09175373  0.09188713  0.09246118  0.09486224
  0.0949494   0.09573604  0.09698557  0.09702457  0.10407824  0.10456431
  0.10485986  0.1052789   0.10573755  0.10682456  0.10788134  0.11178992
  0.11202616  0.11210995  0.11389723  0.11494063  0.11580487  0.11749211
  0.11945788  0.12052068  0.12088428  0.12167318  0.12230402  0.12314575
  0.12594594  0.12720967  0.12897756  0.13270009  0.13302416  0.13506535
  0.1373614   0.13762209  0.13788451  0.13962162  0.1403331   0.1413303
  0.14352464  0.14449617  0.14549264  0.14557525  0.14693796  0.14711942
  0.15341286  0.15447336  0.1554847   0.1558831   0.15690361  0.15706746
  0.15761971  0.15818634  0.16281973  0.16308437  0.16424639  0.16466323
  0.16572323  0.16607743  0.16951941  0.17115072  0.17196986  0.17275228
  0.1733236   0.17414617  0.17455048  0.17472685  0.17489927  0.17496249
  0.17550989  0.17637877  0.17649287  0.17818572  0.1786891   0.17872571
  0.18008846  0.18126412  0.18252169  0.18278264  0.18361816  0.18457839
  0.18578814  0.19002733  0.1965609   0.19830943  0.19836831  0.19854498
  0.19867091  0.19949239  0.20109645  0.20136386  0.20299874  0.20393281
  0.20447713  0.2048398   0.20726473  0.20814806  0.20860401  0.20874165
  0.20951559  0.21005091  0.21170603  0.21394407  0.21412032  0.21552346
  0.2157415   0.21699643  0.21811392  0.22043051  0.22090457  0.22169958
  0.2224403   0.22514246  0.22568771  0.22570666  0.22896166  0.23153209
  0.23185286  0.23425009  0.23452793  0.2351852   0.23818189  0.23883168
  0.24329186  0.24395823  0.24918541  0.25108932  0.25136033  0.25175866
  0.252569    0.25438105  0.25882581  0.25909556  0.26398477  0.26417117
  0.26454591  0.26498291  0.26674233  0.26854093  0.27509214  0.27616879
  0.27951374  0.28020352  0.28056923  0.28068424  0.28612301  0.28863453
  0.28869287  0.28885014  0.29311999  0.29404721  0.29847467  0.29885587
  0.30031364  0.30441503  0.30517236  0.30566117  0.30590894  0.30747418
  0.30839486  0.31051665  0.31144881  0.31212743  0.31246215  0.31318324
  0.31421267  0.31479119  0.31895597  0.31904923  0.31921791  0.31982753
  0.31992476  0.32374863  0.32485817  0.32895816  0.33069151  0.33228587
  0.33382882  0.34011608  0.34265211  0.34416817  0.34516813  0.34633031
  0.34643996  0.34736753  0.34788792  0.3516353   0.3523436   0.35242394
  0.3526516   0.35377291  0.35424597  0.35771132  0.35810452  0.35836765
  0.35837498  0.36155156  0.36316071  0.3688915   0.37547566  0.37568956
  0.3768094   0.37732753  0.37751281  0.38157238  0.38210181  0.38210655
  0.38365359  0.38474749  0.38618114  0.39147125  0.39346626  0.39451255
  0.39473646  0.39507068  0.39513116  0.39543491  0.3994769   0.40376045
  0.40922547  0.40959027  0.41671169  0.4174282   0.41879047  0.41951901
  0.42062495  0.42338439  0.42393895  0.42507864  0.43112266  0.43302881
  0.4354124   0.43546805  0.4387485   0.44271232  0.44362801  0.4457611
  0.44696215  0.44824593  0.44836934  0.45254819  0.45264927  0.45283973
  0.45331472  0.45614772  0.45656706  0.45775535  0.45953532  0.45970766
  0.4607226   0.46409681  0.46606859  0.4662427   0.47197001  0.4731346
  0.47530303  0.47620911  0.47767327  0.4805722   0.48248726  0.48348821
  0.48654352  0.48860248  0.49080554  0.49203207  0.49774784  0.49777303
  0.4981981   0.50105819  0.50111852  0.50113346  0.50137202  0.5019526
  0.50509438  0.50629687  0.50641866  0.51045872  0.51218247  0.51474628
  0.51692337  0.5184923   0.51954329  0.52005822  0.52195901  0.52913715
  0.53179634  0.5333213   0.53384476  0.53423776  0.53955516  0.53959492
  0.53978778  0.54133779  0.54155146  0.54325092  0.5493651   0.54965661
  0.54987128  0.55095797  0.55151812  0.55187671  0.55207101  0.55207966
  0.55219464  0.55256471  0.55316053  0.55436076  0.55765723  0.55772417
  0.56226105  0.56243932  0.56380886  0.56923339  0.5700894   0.57135025
  0.58691327  0.588631    0.58893616  0.59637671  0.59640591  0.60357276
  0.60575762  0.60773543  0.60791626  0.60897677  0.60972208  0.61031502
  0.61049072  0.61168034  0.61638737  0.61742856  0.62589194  0.62950173
  0.62957199  0.63267483  0.63352627  0.6361167   0.63839179  0.64261007
  0.64442739  0.64584221  0.64634359  0.64922925  0.65299463  0.65656364
  0.65717537  0.65772542  0.66287274  0.66649307  0.66760435  0.66871074
  0.67300703  0.67430704  0.67713527  0.68060527  0.68194641  0.6820186
  0.69182536  0.69400506  0.69429951  0.70254558  0.71813265  0.71944208
  0.72429479  0.73333311  0.75771847  0.76317853  0.76439665  0.76872448
  0.77696361]

  warnings.warn(

2022-11-03 10:49:41,808:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.22657301 -0.18056787 -0.16702363 -0.15798243 -0.15485692 -0.14971286
 -0.13486099 -0.13183853 -0.13167579 -0.13122138 -0.13015354 -0.12732392
 -0.12674191 -0.11781733 -0.11731469 -0.11728559 -0.11240004 -0.11201742
 -0.11155993 -0.10812495 -0.10798894 -0.10728279 -0.10634199 -0.10454029
 -0.09620606 -0.0888599  -0.0876128  -0.08241855 -0.08229686 -0.0811841
 -0.06999725 -0.06894179 -0.06884659 -0.06747883 -0.06453352 -0.06221831
 -0.05969373 -0.05950208 -0.05721118 -0.05711585 -0.05556589 -0.05483172
 -0.05436248 -0.05115247 -0.04822117 -0.046139   -0.04444356 -0.04411948
 -0.04126646 -0.04126425 -0.03211252 -0.03173396 -0.03116283 -0.03064262
 -0.0305486  -0.02962239 -0.02644914 -0.0250672  -0.0239212  -0.02341289
 -0.02136788 -0.0201588  -0.01999216 -0.01897884 -0.01434592 -0.00524102
 -0.00293332  0.00434455  0.00809702  0.00819065  0.00890449  0.00943604
  0.01019142  0.01114917  0.01265099  0.01775392  0.01821809  0.01833767
  0.02151963  0.02169868  0.02367268  0.02490299  0.02550301  0.02685723
  0.02690429  0.0284003   0.03286617  0.03393791  0.03641687  0.03768583
  0.03818589  0.04312428  0.04340464  0.04381202  0.04636536  0.0470937
  0.04717933  0.04802373  0.04854424  0.04883676  0.04902181  0.05030669
  0.05157633  0.05242098  0.05458068  0.05487543  0.05687592  0.0591597
  0.06010227  0.0657264   0.06657872  0.06678809  0.06697832  0.06729794
  0.06895846  0.07428823  0.07631594  0.07683336  0.07975882  0.0805036
  0.08052325  0.08308088  0.08335027  0.08382028  0.08538153  0.08639809
  0.08666538  0.08939363  0.09066874  0.09097883  0.0932916   0.09979535
  0.10009006  0.10062306  0.10274177  0.10307494  0.10411565  0.10412159
  0.1041267   0.10558184  0.10621674  0.10651552  0.10711163  0.10853626
  0.10871681  0.11082383  0.11134134  0.11674478  0.11753464  0.11994859
  0.12032552  0.12034826  0.1235484   0.12363803  0.12443638  0.12599835
  0.1265631   0.12717976  0.12802831  0.12833423  0.12909067  0.12942736
  0.13684612  0.13853212  0.14012628  0.14038946  0.14252091  0.14574568
  0.14598321  0.14834874  0.14905481  0.14941769  0.1514593   0.15418854
  0.15504592  0.15531687  0.15558176  0.15704129  0.16077394  0.16478764
  0.16832924  0.16879086  0.17154497  0.17250966  0.17259067  0.17441491
  0.17890322  0.17911446  0.17974779  0.17998434  0.18060306  0.18183506
  0.18427576  0.1846277   0.18544463  0.18570193  0.18613793  0.18707602
  0.18795156  0.18817251  0.18895145  0.19098411  0.1948904   0.19644258
  0.19904079  0.20256446  0.20311306  0.20387546  0.20414672  0.20430635
  0.20514187  0.20581666  0.20626762  0.20642696  0.20658697  0.20745285
  0.20816725  0.20828631  0.20862714  0.20890858  0.20951361  0.20971491
  0.21474568  0.21597413  0.21665752  0.21677115  0.21757435  0.21874343
  0.22179713  0.22334681  0.22457177  0.22503132  0.22856615  0.23105219
  0.23187497  0.23245706  0.23260935  0.23381215  0.23460276  0.23493053
  0.23637032  0.23652794  0.23675313  0.2387536   0.23997234  0.24297427
  0.2432612   0.24476383  0.24567548  0.24586916  0.24774891  0.24970563
  0.25009564  0.25071145  0.25086452  0.25415414  0.25520384  0.25569936
  0.25837725  0.26161687  0.26281359  0.26289756  0.26368549  0.26369872
  0.26550952  0.26607123  0.26906028  0.27233265  0.27377493  0.28535035
  0.28560916  0.28630545  0.28903541  0.28965617  0.29000754  0.29014543
  0.29034928  0.29199274  0.29338572  0.29349677  0.29451051  0.29469621
  0.29578773  0.29635508  0.29826958  0.29900815  0.30111999  0.30145863
  0.30158126  0.30164968  0.30396204  0.30399501  0.30417485  0.30436597
  0.30524498  0.30585657  0.30588384  0.30721137  0.3079349   0.30929158
  0.30983566  0.31093214  0.3110928   0.31261921  0.31419987  0.31611708
  0.31975398  0.32058247  0.32184573  0.3239108   0.32413679  0.32638508
  0.3277026   0.32978292  0.33008782  0.33057569  0.33074496  0.33092232
  0.33253826  0.33395164  0.33451367  0.33531703  0.33645913  0.34511469
  0.34533122  0.34556567  0.35060292  0.35098241  0.35200623  0.35225406
  0.35359796  0.35403307  0.35548305  0.35700805  0.35967571  0.36112005
  0.36249949  0.36405138  0.36559257  0.36626328  0.36831982  0.36849138
  0.36853142  0.37033181  0.37346665  0.37553347  0.37615893  0.37734819
  0.37795514  0.38173338  0.3835209   0.38352974  0.38461792  0.38538833
  0.38597452  0.38918742  0.40423065  0.404712    0.40533428  0.40596589
  0.4088778   0.40940763  0.41136874  0.41504347  0.41738581  0.41835345
  0.42201539  0.42295153  0.42438336  0.42555481  0.42955533  0.43113035
  0.43123128  0.43183058  0.4333905   0.43511477  0.43523473  0.43644021
  0.43654383  0.43674011  0.43945833  0.44328675  0.44928143  0.45053008
  0.45157682  0.45235362  0.45304356  0.4535548   0.46370866  0.46414208
  0.46580851  0.46892918  0.46984887  0.47247797  0.47492156  0.47791617
  0.47928028  0.48104399  0.48139329  0.48157962  0.48179672  0.4845107
  0.4850305   0.48848273  0.49100834  0.49601389  0.49990553  0.50404184
  0.50498858  0.50682698  0.51138719  0.51156347  0.51437427  0.51534692
  0.51558221  0.5176185   0.51775609  0.5235495   0.52632035  0.52657938
  0.52914876  0.53023381  0.53034798  0.5312201   0.5356681   0.53695964
  0.53974196  0.54053922  0.54148226  0.54500694  0.54727919  0.54816153
  0.54947701  0.54957359  0.55111739  0.55514707  0.55524523  0.55936644
  0.56593295  0.56600668  0.57212553  0.57428065  0.57779505  0.57936754
  0.58411013  0.58527028  0.58743094  0.58924804  0.59131346  0.59485606
  0.59789825  0.60088916  0.60095995  0.60185332  0.60473481  0.60549435
  0.60576261  0.60713975  0.61006638  0.6128267   0.61530048  0.61730524
  0.62201733  0.62367948  0.6237128   0.62780835  0.62801428  0.63314846
  0.64384935  0.64700646  0.64711106  0.64781727  0.659514    0.66388884
  0.66477161  0.66515939  0.66589887  0.66811503  0.67358263  0.6772808
  0.68419047  0.68443388  0.68640172  0.68684346  0.69512287  0.69516918
  0.70116978  0.71072608  0.71738876  0.71759364  0.72353895  0.72416261
  0.7306626 ]

  warnings.warn(

2022-11-03 10:49:41,877:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.18745831 -0.16311797 -0.15900914 -0.15883344 -0.15010697 -0.14763518
 -0.14639865 -0.14317316 -0.1362097  -0.13523003 -0.13169017 -0.13078211
 -0.12835658 -0.12649965 -0.12505871 -0.12354537 -0.1227975  -0.12026609
 -0.1193303  -0.11652706 -0.11649579 -0.10596804 -0.10549934 -0.10529539
 -0.10230498 -0.10180733 -0.10002881 -0.09961749 -0.09356317 -0.09308952
 -0.08862921 -0.08681793 -0.08331538 -0.0826999  -0.08167022 -0.07808066
 -0.07466755 -0.07417455 -0.0710878  -0.06451197 -0.06293416 -0.06109506
 -0.05996846 -0.05983454 -0.0564422  -0.05432776 -0.05070241 -0.04722992
 -0.04506284 -0.04497181 -0.04345612 -0.04298906 -0.0416783  -0.04097525
 -0.0354235  -0.03147211 -0.02940238 -0.02392765 -0.02381952 -0.02317386
 -0.0228202  -0.0218468  -0.02025563 -0.01903897 -0.01744274 -0.01557021
 -0.01537939 -0.01493063 -0.01424922 -0.01261151 -0.01171266 -0.00981027
 -0.00732871 -0.00602887 -0.00600043 -0.0058158  -0.0034049  -0.00080369
  0.00162149  0.00190835  0.00412464  0.00634621  0.00754361  0.00757419
  0.00968119  0.01091312  0.01332005  0.0135065   0.01562241  0.01574563
  0.01726303  0.01875107  0.02100895  0.0214936   0.02204916  0.02252294
  0.02472409  0.0256057   0.02682743  0.02912907  0.03403014  0.03452509
  0.03501513  0.0364093   0.03754289  0.03773687  0.03799269  0.04188939
  0.04271044  0.04272699  0.04369798  0.04672124  0.0469811   0.04799234
  0.05134618  0.05211709  0.05251899  0.05617845  0.05669346  0.05697249
  0.06173972  0.06256785  0.06362487  0.06783588  0.07078164  0.07325804
  0.07330939  0.0741549   0.07537854  0.07901662  0.07905199  0.08266979
  0.08339164  0.08342042  0.08349512  0.08369234  0.08369512  0.08454595
  0.0853157   0.08678785  0.09222005  0.0922688   0.09790937  0.10030612
  0.10326713  0.10496996  0.10720449  0.10969995  0.1102727   0.11041119
  0.11077957  0.11278459  0.11304179  0.11540907  0.11579326  0.11595116
  0.11801586  0.11817891  0.12031743  0.12534215  0.12696522  0.12941154
  0.13020633  0.13252466  0.13369436  0.13483716  0.14079358  0.14125162
  0.14126057  0.14195471  0.142874    0.1438802   0.14612067  0.14621607
  0.14655469  0.1466045   0.14739681  0.15084555  0.15115571  0.15175196
  0.15280168  0.16046866  0.16094963  0.1610689   0.16213746  0.1635105
  0.17084003  0.17277384  0.1729739   0.17379104  0.1749712   0.1769521
  0.17973955  0.18238321  0.18389389  0.18905715  0.18953569  0.19083565
  0.19137174  0.19357296  0.20064897  0.20339929  0.2083674   0.20872392
  0.21033469  0.21076496  0.21125166  0.21132207  0.21192862  0.21446662
  0.21683457  0.21710497  0.21806328  0.21871792  0.21975906  0.22335116
  0.22389206  0.22498965  0.22648336  0.22704515  0.22706915  0.22786223
  0.22947716  0.23142761  0.23556118  0.23610253  0.23685267  0.2386326
  0.24212496  0.24218816  0.24391713  0.24510095  0.24518183  0.24647854
  0.24747751  0.25545022  0.25809567  0.25946322  0.25982359  0.2630215
  0.2634853   0.26444884  0.26530997  0.26560251  0.26723262  0.26889931
  0.27366442  0.27444884  0.27650731  0.27732531  0.27824222  0.27952017
  0.28153879  0.28171261  0.28196618  0.28220774  0.28382446  0.28403214
  0.28433186  0.28507168  0.29140226  0.29362747  0.29683812  0.29700432
  0.29709298  0.29792439  0.30043394  0.30254891  0.30310351  0.30321664
  0.30589785  0.30600491  0.30763495  0.30884397  0.31077968  0.31091929
  0.31506369  0.31660539  0.31697758  0.31745496  0.31841013  0.32205446
  0.32742576  0.32754271  0.32866774  0.32898276  0.32941454  0.33571933
  0.33599391  0.3361049   0.3377091   0.33932824  0.34206559  0.34458572
  0.34585602  0.34673571  0.34828063  0.34942072  0.3542484   0.35463291
  0.35791652  0.36148105  0.36261748  0.36342088  0.36454327  0.36671784
  0.36767445  0.36793453  0.36867857  0.37059718  0.37209644  0.37226442
  0.37414494  0.37518847  0.37636275  0.37679286  0.37697699  0.3790967
  0.3798273   0.38006152  0.38006223  0.38097918  0.38159234  0.38761286
  0.38879898  0.39222495  0.39286069  0.39641643  0.39750174  0.39794275
  0.39902755  0.39952793  0.40030578  0.40205827  0.40286644  0.4034535
  0.40466228  0.40541956  0.40556266  0.40628029  0.40700653  0.40976834
  0.4102466   0.41617082  0.41642714  0.41742686  0.4179699   0.41797249
  0.41799833  0.42154953  0.42765913  0.42838449  0.42978203  0.43340727
  0.43579739  0.43585496  0.4375358   0.43801075  0.44012065  0.44049314
  0.44310442  0.44589826  0.4481817   0.44965723  0.4510229   0.45252851
  0.45350971  0.45509121  0.45576372  0.45594533  0.45961427  0.4658934
  0.46647819  0.46683862  0.46931235  0.46979461  0.47056151  0.47121822
  0.47246927  0.47299022  0.4767122   0.4776309   0.47877084  0.47971697
  0.47984736  0.48131475  0.48431022  0.48832669  0.49015795  0.49092667
  0.49273387  0.4953687   0.49711237  0.49784283  0.49902405  0.5019787
  0.50304936  0.50434192  0.50759075  0.5131539   0.51401558  0.51557545
  0.51562714  0.51694785  0.51747391  0.5184264   0.5192208   0.52446494
  0.52618304  0.52654427  0.52715098  0.5311996   0.5329353   0.53318802
  0.53353027  0.53573544  0.53623117  0.53663054  0.5370767   0.53960489
  0.5423801   0.54274699  0.54278692  0.54476928  0.54524993  0.54907611
  0.55444586  0.55612758  0.55809002  0.56034747  0.5612653   0.56202614
  0.56648153  0.57046249  0.5725706   0.57669734  0.57732556  0.57812077
  0.57882551  0.59080873  0.59151478  0.59607806  0.59774286  0.59938931
  0.60096004  0.60099082  0.60204404  0.6028418   0.60746249  0.60829754
  0.61076195  0.6109618   0.61178853  0.61294405  0.61346695  0.61369982
  0.61548946  0.61559443  0.61789095  0.62000089  0.62120751  0.62145502
  0.62310962  0.62608093  0.62622932  0.62637637  0.6267288   0.62779219
  0.6295627   0.62984214  0.63270658  0.63597221  0.63923713  0.64275646
  0.65150969  0.66003487  0.66109777  0.66258356  0.66403011  0.66627917
  0.66842485  0.66860195  0.67405354  0.67739619  0.68416893  0.68585126
  0.68859108  0.70728528  0.70950519  0.71819853  0.74263155  0.74304965
  0.75808064]

  warnings.warn(

2022-11-03 10:49:41,897:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.90241207e-01 -1.67230618e-01 -1.51018243e-01 -1.44197588e-01
 -1.37905010e-01 -1.33717090e-01 -1.31510382e-01 -1.27834552e-01
 -1.26057052e-01 -1.19986715e-01 -1.18351589e-01 -1.16380423e-01
 -1.16032107e-01 -1.13152013e-01 -1.09306238e-01 -1.08579317e-01
 -1.04699947e-01 -1.03299263e-01 -9.70875445e-02 -9.52027616e-02
 -9.28888843e-02 -9.15360456e-02 -8.88227568e-02 -8.84216077e-02
 -8.47065094e-02 -8.37797412e-02 -8.35923204e-02 -7.34933281e-02
 -6.97711872e-02 -6.92573227e-02 -6.46257503e-02 -6.19536004e-02
 -5.82896758e-02 -5.67911514e-02 -5.62423160e-02 -4.97295712e-02
 -4.86041567e-02 -4.85331316e-02 -4.55378938e-02 -4.42212701e-02
 -4.38621479e-02 -4.32153342e-02 -4.29683943e-02 -4.13158746e-02
 -3.91236334e-02 -3.85257074e-02 -3.75141411e-02 -3.62265866e-02
 -3.20445202e-02 -3.10023262e-02 -3.03179226e-02 -2.85701665e-02
 -2.82696516e-02 -2.70298911e-02 -2.36323195e-02 -2.27850709e-02
 -2.19217429e-02 -2.12422632e-02 -2.09314113e-02 -2.04153333e-02
 -1.76402782e-02 -1.53076241e-02 -1.09652765e-02 -9.41510278e-03
 -7.27299380e-03 -6.03713873e-03 -4.99587979e-03 -4.10015122e-03
 -3.76902334e-03 -3.37243951e-03 -2.36887101e-03 -1.12172053e-03
  2.43844600e-04  1.54896981e-03  1.63308642e-03  2.09149336e-03
  3.89356355e-03  4.89209753e-03  9.99984180e-03  1.00094335e-02
  1.02065946e-02  1.19734665e-02  1.32704385e-02  1.46137977e-02
  1.63770681e-02  1.88291804e-02  2.22200720e-02  2.27067574e-02
  2.49067639e-02  2.50439917e-02  2.63259482e-02  3.06155426e-02
  3.11429352e-02  3.13607631e-02  3.32194199e-02  3.44555982e-02
  3.53290920e-02  3.62123443e-02  3.72786668e-02  3.95732206e-02
  4.04505380e-02  4.05406998e-02  4.26828837e-02  4.44394796e-02
  4.60198952e-02  4.81155111e-02  4.93315555e-02  5.12501420e-02
  5.51132870e-02  5.53211541e-02  5.55884548e-02  5.95404688e-02
  6.11611686e-02  6.21353504e-02  6.39146317e-02  6.65056724e-02
  6.87090588e-02  8.26346854e-02  8.29731123e-02  8.42204116e-02
  8.48863877e-02  8.60038384e-02  8.69719302e-02  8.80788730e-02
  8.91768897e-02  9.10102314e-02  9.13685330e-02  9.20215124e-02
  9.22855886e-02  9.34904416e-02  9.40264954e-02  9.41909932e-02
  9.43978597e-02  9.60735166e-02  9.76212236e-02  9.76604425e-02
  9.85292026e-02  9.98193561e-02  1.01480451e-01  1.01965864e-01
  1.01968000e-01  1.02007892e-01  1.04336691e-01  1.07005128e-01
  1.07050364e-01  1.07564302e-01  1.13775573e-01  1.14682474e-01
  1.14895933e-01  1.18334196e-01  1.19978578e-01  1.20581377e-01
  1.21299530e-01  1.22481872e-01  1.22872474e-01  1.24414888e-01
  1.25046495e-01  1.25815616e-01  1.26182551e-01  1.27304837e-01
  1.28287105e-01  1.28716432e-01  1.29763872e-01  1.30840145e-01
  1.33774396e-01  1.33907973e-01  1.34741360e-01  1.35138400e-01
  1.37382321e-01  1.37755765e-01  1.38091815e-01  1.38470400e-01
  1.38587192e-01  1.43914690e-01  1.45234677e-01  1.47015937e-01
  1.47215692e-01  1.48336318e-01  1.51071323e-01  1.55700533e-01
  1.57000840e-01  1.57398807e-01  1.59963559e-01  1.61814790e-01
  1.61973413e-01  1.64092387e-01  1.65828563e-01  1.67443220e-01
  1.69295576e-01  1.70846790e-01  1.72470702e-01  1.73405558e-01
  1.77657550e-01  1.78500548e-01  1.78848262e-01  1.84290760e-01
  1.85773879e-01  1.85933799e-01  1.87810666e-01  1.89741949e-01
  1.92058320e-01  1.98669168e-01  1.99152620e-01  2.03035366e-01
  2.03188589e-01  2.06476262e-01  2.06591803e-01  2.07024881e-01
  2.08886205e-01  2.09160624e-01  2.09289789e-01  2.12215627e-01
  2.12818237e-01  2.12900009e-01  2.12939228e-01  2.13174370e-01
  2.14128879e-01  2.16171358e-01  2.17573486e-01  2.20028759e-01
  2.21971631e-01  2.23122973e-01  2.24103172e-01  2.27169346e-01
  2.31197547e-01  2.31456186e-01  2.32189172e-01  2.33815918e-01
  2.34105062e-01  2.35094694e-01  2.35305331e-01  2.35436880e-01
  2.36323139e-01  2.37164348e-01  2.37236252e-01  2.39842967e-01
  2.40841276e-01  2.42709404e-01  2.43700524e-01  2.44948712e-01
  2.45062618e-01  2.46164435e-01  2.46643904e-01  2.47285306e-01
  2.47474731e-01  2.48378133e-01  2.48391753e-01  2.50109555e-01
  2.50815291e-01  2.51272454e-01  2.52812496e-01  2.52945212e-01
  2.56594255e-01  2.57591284e-01  2.58187826e-01  2.59206084e-01
  2.60861072e-01  2.61654795e-01  2.61677069e-01  2.61806933e-01
  2.63319337e-01  2.63648406e-01  2.63909406e-01  2.64271477e-01
  2.64596399e-01  2.65999536e-01  2.68424918e-01  2.70623896e-01
  2.70664082e-01  2.70987740e-01  2.71767860e-01  2.73114556e-01
  2.73360918e-01  2.74445781e-01  2.74910302e-01  2.79468173e-01
  2.81505201e-01  2.82286437e-01  2.84308876e-01  2.85506804e-01
  2.85963899e-01  2.88355690e-01  2.96021164e-01  2.98794882e-01
  3.03564958e-01  3.10964851e-01  3.12105391e-01  3.12342926e-01
  3.12695479e-01  3.14718926e-01  3.14856580e-01  3.14983631e-01
  3.18860560e-01  3.19063059e-01  3.20431852e-01  3.21237823e-01
  3.23195470e-01  3.25803322e-01  3.25820934e-01  3.28469402e-01
  3.30391357e-01  3.30537381e-01  3.31288082e-01  3.32209520e-01
  3.32396844e-01  3.32402057e-01  3.32882600e-01  3.33974548e-01
  3.37387552e-01  3.37913533e-01  3.38452995e-01  3.38661302e-01
  3.42317896e-01  3.43890618e-01  3.52482606e-01  3.56163918e-01
  3.58857255e-01  3.61199411e-01  3.63942484e-01  3.64057359e-01
  3.65052448e-01  3.66361645e-01  3.67706416e-01  3.68116487e-01
  3.69285591e-01  3.69677984e-01  3.72490264e-01  3.74054552e-01
  3.74627501e-01  3.75218148e-01  3.75969790e-01  3.78047213e-01
  3.83951139e-01  3.84471915e-01  3.86844682e-01  3.87092231e-01
  3.88569243e-01  3.91488350e-01  3.93917485e-01  3.94957816e-01
  3.96048639e-01  3.97341572e-01  3.97568355e-01  4.02943097e-01
  4.06199964e-01  4.09601834e-01  4.10067951e-01  4.12506221e-01
  4.20909074e-01  4.21232016e-01  4.24133942e-01  4.24576525e-01
  4.25025212e-01  4.26019172e-01  4.31925694e-01  4.32060967e-01
  4.33407795e-01  4.34385816e-01  4.34801572e-01  4.38810197e-01
  4.39134227e-01  4.40060817e-01  4.47426028e-01  4.47895413e-01
  4.47941917e-01  4.48513439e-01  4.50585398e-01  4.52168132e-01
  4.55717672e-01  4.57689171e-01  4.58341706e-01  4.58985392e-01
  4.60774816e-01  4.61277006e-01  4.61279069e-01  4.61359874e-01
  4.62576533e-01  4.65618682e-01  4.66053050e-01  4.66969036e-01
  4.67689965e-01  4.68015586e-01  4.68050804e-01  4.70291481e-01
  4.70761485e-01  4.71446949e-01  4.72452949e-01  4.72722520e-01
  4.74846243e-01  4.75182139e-01  4.78268292e-01  4.78399892e-01
  4.78764504e-01  4.79341835e-01  4.81877889e-01  4.82867558e-01
  4.86547726e-01  4.86718689e-01  4.86736069e-01  4.87314548e-01
  4.92190514e-01  4.93200749e-01  4.94489696e-01  4.95348540e-01
  4.96307098e-01  5.00167004e-01  5.00787186e-01  5.02084264e-01
  5.02180983e-01  5.03261382e-01  5.07568531e-01  5.08768227e-01
  5.09650887e-01  5.11009658e-01  5.11340013e-01  5.12754284e-01
  5.19373130e-01  5.20496402e-01  5.21366594e-01  5.21444469e-01
  5.30378203e-01  5.31359784e-01  5.35947754e-01  5.36139198e-01
  5.36495817e-01  5.37848918e-01  5.40153789e-01  5.40497530e-01
  5.47798839e-01  5.48491881e-01  5.52471276e-01  5.54808006e-01
  5.62682972e-01  5.64855127e-01  5.67783284e-01  5.68342166e-01
  5.69271289e-01  5.70818440e-01  5.71284484e-01  5.73313401e-01
  5.76227843e-01  5.77874365e-01  5.80606161e-01  5.82437495e-01
  5.84895962e-01  5.89010691e-01  5.89231443e-01  5.89413392e-01
  5.91998418e-01  5.92229184e-01  5.93839180e-01  6.04453391e-01
  6.07009569e-01  6.09857166e-01  6.11254326e-01  6.14019381e-01
  6.14956162e-01  6.16405609e-01  6.17459528e-01  6.18835845e-01
  6.25064108e-01  6.26462863e-01  6.29701214e-01  6.30907720e-01
  6.31561330e-01  6.32459034e-01  6.36434032e-01  6.37723311e-01
  6.40514038e-01  6.42768711e-01  6.45556733e-01  6.47996290e-01
  6.49807025e-01  6.54379169e-01  6.57215228e-01  6.57760021e-01
  6.58989495e-01  6.59133913e-01  6.62194958e-01  6.64958529e-01
  6.74936359e-01  6.82115149e-01  6.86847675e-01  6.88202530e-01
  6.90448206e-01  6.91679778e-01  6.99167365e-01  7.01686358e-01
  7.11517959e-01  7.11649781e-01  7.18505995e-01  7.20910792e-01
  7.34399372e-01]

  warnings.warn(

2022-11-03 10:49:41,918:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.83230643e-01 -1.81971487e-01 -1.58196862e-01 -1.50383759e-01
 -1.46418086e-01 -1.45482305e-01 -1.43876589e-01 -1.43429447e-01
 -1.38338329e-01 -1.38280088e-01 -1.38034727e-01 -1.36696177e-01
 -1.35698651e-01 -1.35125741e-01 -1.33602139e-01 -1.33264358e-01
 -1.33209307e-01 -1.30886130e-01 -1.29841948e-01 -1.24666263e-01
 -1.23896229e-01 -1.19665363e-01 -1.16875026e-01 -1.16685460e-01
 -1.13505353e-01 -1.10351811e-01 -1.04604346e-01 -1.02984092e-01
 -1.02972800e-01 -1.01137690e-01 -1.00598722e-01 -9.93540267e-02
 -9.71693821e-02 -9.70143891e-02 -9.37204423e-02 -9.27287339e-02
 -8.88196801e-02 -8.85996147e-02 -8.48875757e-02 -8.21332840e-02
 -8.06483185e-02 -7.72323441e-02 -7.50215421e-02 -7.48787450e-02
 -7.41867761e-02 -7.39542364e-02 -6.75029386e-02 -6.63084478e-02
 -6.60754936e-02 -6.57080813e-02 -6.53110098e-02 -6.21285034e-02
 -6.10969201e-02 -5.59789080e-02 -5.44846026e-02 -5.28888630e-02
 -5.22421779e-02 -4.88479465e-02 -4.87135981e-02 -4.86776402e-02
 -4.80577512e-02 -4.60442872e-02 -4.58066652e-02 -4.52627778e-02
 -4.41135140e-02 -4.28543520e-02 -4.22190744e-02 -4.20548686e-02
 -3.86032921e-02 -3.76769863e-02 -3.73378733e-02 -3.69196361e-02
 -3.65066392e-02 -3.51006326e-02 -3.39107760e-02 -3.07015639e-02
 -3.05930830e-02 -3.02656499e-02 -2.74519202e-02 -2.60835327e-02
 -2.56998461e-02 -2.50876692e-02 -2.37398323e-02 -2.36601044e-02
 -2.01036390e-02 -1.86675741e-02 -1.83870203e-02 -1.55601182e-02
 -4.69685754e-03 -3.86345175e-03 -3.70539022e-03 -1.93840339e-03
 -1.68687538e-03 -5.31066368e-04  3.36123133e-04  8.08900561e-04
  1.19224995e-03  2.96331059e-03  9.23174885e-03  1.96779387e-02
  2.09488533e-02  2.11235591e-02  2.15039832e-02  2.17666666e-02
  2.68786554e-02  2.88199815e-02  3.10169847e-02  3.19520021e-02
  3.20001026e-02  3.20629736e-02  3.22010825e-02  3.42149864e-02
  3.53672961e-02  3.63386254e-02  3.79655287e-02  4.01013244e-02
  4.06962279e-02  4.10495578e-02  4.20054045e-02  4.20938138e-02
  4.60150155e-02  4.72652036e-02  5.01333186e-02  5.20478000e-02
  5.59121960e-02  5.60296607e-02  5.69962344e-02  5.83840444e-02
  5.90657861e-02  5.99792898e-02  6.02267099e-02  6.48609985e-02
  6.58287216e-02  6.66819884e-02  6.69780288e-02  6.72020775e-02
  6.86490168e-02  6.86727521e-02  7.04862114e-02  7.19513013e-02
  7.52010323e-02  7.73316573e-02  7.85404221e-02  8.12643588e-02
  8.12908141e-02  8.29663475e-02  9.13075989e-02  9.17312726e-02
  9.21557449e-02  9.54061185e-02  9.71318500e-02  1.01013838e-01
  1.01116507e-01  1.02415660e-01  1.03327851e-01  1.04295513e-01
  1.05685857e-01  1.06376814e-01  1.08812066e-01  1.12754352e-01
  1.14214623e-01  1.15763600e-01  1.17074338e-01  1.18513118e-01
  1.19136863e-01  1.19691004e-01  1.21139555e-01  1.23585138e-01
  1.24799089e-01  1.25764254e-01  1.26491984e-01  1.28131386e-01
  1.29076402e-01  1.30222568e-01  1.30249408e-01  1.32621052e-01
  1.33908868e-01  1.35367927e-01  1.37444905e-01  1.37583721e-01
  1.40569472e-01  1.40802869e-01  1.42076930e-01  1.43089284e-01
  1.43153625e-01  1.44388207e-01  1.44432952e-01  1.49921410e-01
  1.50399428e-01  1.51476782e-01  1.51856664e-01  1.55998318e-01
  1.56336960e-01  1.59784042e-01  1.60882069e-01  1.62176966e-01
  1.62395539e-01  1.62850880e-01  1.63246844e-01  1.64098950e-01
  1.64194340e-01  1.64326266e-01  1.64738809e-01  1.64745869e-01
  1.64957770e-01  1.66110553e-01  1.66502140e-01  1.69467424e-01
  1.70713544e-01  1.74241812e-01  1.75974017e-01  1.78048210e-01
  1.78280221e-01  1.79878315e-01  1.80122855e-01  1.84112275e-01
  1.85663676e-01  1.87615308e-01  1.90171398e-01  1.90903674e-01
  1.91174571e-01  1.91217431e-01  1.93261686e-01  1.93789229e-01
  1.95752035e-01  1.95841272e-01  1.98034261e-01  1.99478173e-01
  2.00481357e-01  2.02195907e-01  2.06197529e-01  2.07266705e-01
  2.09311891e-01  2.11095533e-01  2.12531392e-01  2.12925322e-01
  2.14383806e-01  2.16706821e-01  2.17198668e-01  2.17705572e-01
  2.18700353e-01  2.18926021e-01  2.19221126e-01  2.21209271e-01
  2.23812236e-01  2.24880614e-01  2.26731849e-01  2.27102239e-01
  2.29631402e-01  2.30121507e-01  2.30278668e-01  2.36283245e-01
  2.41507016e-01  2.41642583e-01  2.41940305e-01  2.43612555e-01
  2.44655172e-01  2.47174989e-01  2.47757812e-01  2.50976047e-01
  2.52323615e-01  2.55709268e-01  2.56007675e-01  2.57350998e-01
  2.59434488e-01  2.59619736e-01  2.59937073e-01  2.60536243e-01
  2.61252294e-01  2.64158014e-01  2.65447353e-01  2.65572177e-01
  2.69501808e-01  2.76237192e-01  2.76738742e-01  2.78385946e-01
  2.79201668e-01  2.79287371e-01  2.82465969e-01  2.82717495e-01
  2.84978823e-01  2.89660710e-01  2.98900528e-01  2.99617755e-01
  2.99793358e-01  3.01212412e-01  3.01763006e-01  3.10420714e-01
  3.11942521e-01  3.12117640e-01  3.14732697e-01  3.16377155e-01
  3.18995454e-01  3.22369940e-01  3.22517532e-01  3.23786792e-01
  3.24447845e-01  3.24463653e-01  3.30334958e-01  3.33297274e-01
  3.33423471e-01  3.34884909e-01  3.39116900e-01  3.40582192e-01
  3.42234472e-01  3.43487971e-01  3.43711392e-01  3.45771687e-01
  3.47369073e-01  3.50309769e-01  3.50484462e-01  3.53031914e-01
  3.54114122e-01  3.54980558e-01  3.55269245e-01  3.58002423e-01
  3.61992243e-01  3.62206056e-01  3.65854232e-01  3.67330287e-01
  3.70674692e-01  3.70700006e-01  3.74194301e-01  3.77675215e-01
  3.79163437e-01  3.79505394e-01  3.82019669e-01  3.82477137e-01
  3.84911556e-01  3.87454453e-01  3.90582627e-01  3.92243340e-01
  3.93147295e-01  3.93732671e-01  3.96163183e-01  3.96477712e-01
  3.98028785e-01  3.98841856e-01  4.00449150e-01  4.03555804e-01
  4.05429940e-01  4.06740654e-01  4.06968453e-01  4.09362549e-01
  4.15416267e-01  4.17684889e-01  4.19612923e-01  4.23197170e-01
  4.25232637e-01  4.28616822e-01  4.29391163e-01  4.29972739e-01
  4.30607321e-01  4.30814014e-01  4.31945043e-01  4.31989259e-01
  4.36522244e-01  4.36906534e-01  4.37498726e-01  4.39536425e-01
  4.40690348e-01  4.41161575e-01  4.43196202e-01  4.43242185e-01
  4.43500924e-01  4.43652711e-01  4.46110830e-01  4.48980624e-01
  4.50229192e-01  4.51917187e-01  4.53419311e-01  4.56120763e-01
  4.56630464e-01  4.57136379e-01  4.57917547e-01  4.62749962e-01
  4.63740254e-01  4.67061521e-01  4.67263087e-01  4.69520676e-01
  4.70068053e-01  4.71667330e-01  4.71901889e-01  4.72859510e-01
  4.73798765e-01  4.73975601e-01  4.74481395e-01  4.75562366e-01
  4.75811028e-01  4.76819090e-01  4.79935789e-01  4.80629353e-01
  4.85571118e-01  4.87050413e-01  4.89615148e-01  4.90332031e-01
  4.91402706e-01  4.95897640e-01  4.96041770e-01  5.00910063e-01
  5.00987426e-01  5.05706683e-01  5.06108614e-01  5.06764210e-01
  5.06937862e-01  5.07736775e-01  5.11234103e-01  5.17062763e-01
  5.21370730e-01  5.24910860e-01  5.25813216e-01  5.26647821e-01
  5.27090778e-01  5.30966299e-01  5.31072649e-01  5.33013580e-01
  5.34072808e-01  5.35622005e-01  5.39489788e-01  5.41532657e-01
  5.43297200e-01  5.50051606e-01  5.52905453e-01  5.53531194e-01
  5.53941368e-01  5.54075991e-01  5.55745193e-01  5.56108480e-01
  5.56157584e-01  5.56797972e-01  5.59740395e-01  5.59921892e-01
  5.59951772e-01  5.61977481e-01  5.65810740e-01  5.66447038e-01
  5.69908849e-01  5.70197266e-01  5.70204612e-01  5.70464682e-01
  5.73274640e-01  5.75340577e-01  5.75467864e-01  5.75552962e-01
  5.76227484e-01  5.76372693e-01  5.77727262e-01  5.80067846e-01
  5.80618733e-01  5.85305589e-01  5.89577152e-01  5.90195771e-01
  5.92672781e-01  6.07218122e-01  6.08187330e-01  6.09090965e-01
  6.09618687e-01  6.12705698e-01  6.15531448e-01  6.16468652e-01
  6.17201527e-01  6.22524943e-01  6.25601851e-01  6.26750773e-01
  6.29451291e-01  6.31304913e-01  6.35804281e-01  6.36449897e-01
  6.39446559e-01  6.41026609e-01  6.48246391e-01  6.48469394e-01
  6.50280462e-01  6.52777048e-01  6.56008981e-01  6.57420082e-01
  6.58511571e-01  6.64184276e-01  6.69684485e-01  6.70442013e-01
  6.78415295e-01  6.83323143e-01  6.88322030e-01  6.91748415e-01
  6.93797997e-01  6.95197525e-01  6.96520301e-01  6.97593285e-01
  7.08619264e-01  7.17094208e-01  7.18677189e-01  7.22639509e-01
  7.31996244e-01]

  warnings.warn(

2022-11-03 10:49:41,954:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.25690761 -0.18940117 -0.18475104 -0.17906132 -0.17373001 -0.17250519
 -0.16978581 -0.16713089 -0.16354426 -0.15934475 -0.15814905 -0.15435991
 -0.14849787 -0.14743818 -0.14707374 -0.14111913 -0.13552147 -0.13382836
 -0.12914079 -0.12905275 -0.12322654 -0.11992988 -0.1191206  -0.11751635
 -0.11593106 -0.11090326 -0.10500064 -0.10264593 -0.10242281 -0.10167166
 -0.09734455 -0.09330283 -0.09082425 -0.09060501 -0.08603801 -0.08106999
 -0.08006477 -0.07909002 -0.0784691  -0.0780664  -0.06979688 -0.06677989
 -0.06431733 -0.06246962 -0.06173978 -0.06163014 -0.06134673 -0.05872202
 -0.05776078 -0.05742044 -0.05716504 -0.05707215 -0.05438514 -0.054382
 -0.05405994 -0.05356601 -0.05085176 -0.04982835 -0.04936947 -0.04928023
 -0.04882484 -0.04318798 -0.04165547 -0.04155354 -0.03679582 -0.03160613
 -0.03017105 -0.02824428 -0.0275062  -0.02687555 -0.0258564  -0.02473897
 -0.02191313 -0.0216041  -0.01857714 -0.01674825 -0.01477343 -0.01199099
 -0.01097533 -0.00728416 -0.00674136 -0.00316013 -0.00151448 -0.00095975
  0.00395452  0.01018921  0.01185683  0.01488171  0.01588494  0.01788882
  0.01813183  0.02445911  0.02486369  0.02751157  0.02778761  0.02846424
  0.02939637  0.03185524  0.03342597  0.0335482   0.0359935   0.03804446
  0.03814176  0.04049847  0.04196426  0.04198724  0.04302871  0.04590399
  0.04674617  0.04749957  0.04815723  0.04863074  0.04887772  0.04897733
  0.05063298  0.05118402  0.05347599  0.05748061  0.05922178  0.06027766
  0.06046162  0.06084609  0.06104168  0.06133322  0.06163739  0.06361143
  0.06439508  0.06464031  0.06549587  0.06584739  0.06695692  0.06748382
  0.06838781  0.06957048  0.06972397  0.07061813  0.07249107  0.07295467
  0.07326125  0.07896725  0.07927848  0.08045351  0.08118579  0.08141055
  0.0815476   0.08432907  0.08487682  0.08554011  0.08623837  0.08882783
  0.08931913  0.09805537  0.10071336  0.10107748  0.10143382  0.10565382
  0.10927297  0.11256565  0.11670338  0.12027792  0.12316996  0.12348342
  0.12399573  0.12659018  0.12711235  0.1279968   0.13035715  0.13114911
  0.13369831  0.13557469  0.13657575  0.13811808  0.13827491  0.13863758
  0.14051518  0.14196264  0.14267072  0.14296546  0.1449944   0.14540343
  0.146557    0.14835695  0.14881887  0.15172081  0.15192802  0.15200417
  0.15211227  0.15547506  0.1559946   0.15667074  0.15695364  0.16122191
  0.16224865  0.1631659   0.16711259  0.16861929  0.17204549  0.17314664
  0.17514007  0.1775052   0.18078207  0.18167874  0.18354266  0.18378454
  0.18581082  0.18650047  0.18731246  0.18835653  0.18911539  0.18921648
  0.19279723  0.19771904  0.19790602  0.19838271  0.19842248  0.19964102
  0.20007457  0.20068123  0.20100124  0.20158078  0.20290636  0.20456122
  0.20496294  0.2076835   0.20837394  0.20911231  0.20922356  0.20968064
  0.21141031  0.21262512  0.21553425  0.21572677  0.21585672  0.219078
  0.22013762  0.22183631  0.22190492  0.22417446  0.22439137  0.22461158
  0.22779807  0.22829136  0.22968477  0.23062735  0.23515693  0.23578258
  0.23769958  0.23827049  0.23860639  0.23895492  0.23919039  0.24069332
  0.24173265  0.24222476  0.24406398  0.24490774  0.24715208  0.2504729
  0.25429533  0.25569654  0.25714436  0.25734428  0.25782556  0.25793788
  0.2600279   0.26548323  0.26597955  0.26701463  0.26729723  0.26827979
  0.26933866  0.26972913  0.27226188  0.27312463  0.27319367  0.27343993
  0.27496192  0.27578311  0.27625413  0.27629375  0.27692413  0.27736654
  0.2796044   0.28086541  0.28323782  0.28370489  0.28701824  0.28719885
  0.28746971  0.28894915  0.29088767  0.29122975  0.29246953  0.29255183
  0.29541891  0.30112494  0.30318483  0.30444356  0.30570563  0.30714116
  0.30792558  0.30827228  0.3092557   0.30939021  0.31013372  0.31115906
  0.31423196  0.31462555  0.31528433  0.31568093  0.32047115  0.32227358
  0.32342369  0.32525835  0.32967064  0.33504585  0.33619138  0.33620868
  0.33903985  0.34418965  0.34510405  0.34521076  0.34771945  0.34809944
  0.34942613  0.35042177  0.35282801  0.35373395  0.35441708  0.35680653
  0.35914555  0.360025    0.36064667  0.36096456  0.36115307  0.36367212
  0.36368522  0.36403408  0.36424193  0.36823888  0.37356551  0.37413951
  0.37570129  0.37701015  0.37915337  0.38084448  0.38719883  0.38741643
  0.38930823  0.39314488  0.39864794  0.40177333  0.40370722  0.40554164
  0.41010645  0.41052135  0.41539458  0.41571754  0.41762743  0.42206165
  0.42216902  0.42671753  0.42675201  0.42802286  0.42961348  0.42981172
  0.43202714  0.43289453  0.44045007  0.44193301  0.4423659   0.44728675
  0.44777332  0.44797545  0.44804125  0.44962147  0.4507991   0.4585057
  0.4639489   0.46514034  0.4728929   0.47582991  0.47837452  0.47979574
  0.48125199  0.48161086  0.48241546  0.48377573  0.484043    0.48585799
  0.48728502  0.48939809  0.49323225  0.49517841  0.49538391  0.49597402
  0.50065954  0.5011468   0.50372153  0.50401407  0.50521515  0.51019437
  0.51126538  0.51149826  0.51763547  0.52211128  0.52259656  0.52315947
  0.52547172  0.52642224  0.5283543   0.53056361  0.53106553  0.532367
  0.53313287  0.53525199  0.53543991  0.53669018  0.5389059   0.54094792
  0.54140258  0.54241963  0.54324717  0.54341688  0.54584257  0.54811106
  0.5482965   0.55060123  0.55102202  0.55130007  0.55414172  0.55594116
  0.55629568  0.55702309  0.55816615  0.55833106  0.56017015  0.56349622
  0.57210876  0.57603606  0.57741002  0.57933007  0.58019827  0.5805286
  0.58161902  0.58797944  0.58817059  0.58871734  0.58935096  0.59132952
  0.59158681  0.59809872  0.59969416  0.60185641  0.60198417  0.60615162
  0.61313907  0.61467918  0.61621821  0.62516263  0.62536663  0.62592072
  0.6270853   0.62769153  0.62813827  0.62915252  0.6299306   0.63422958
  0.63901513  0.64096999  0.64715534  0.65096477  0.65382149  0.65603967
  0.65624578  0.66767393  0.67169426  0.67469622  0.67547634  0.67645298
  0.67825978  0.67849539  0.67859076  0.67886554  0.68034321  0.68333941
  0.68805402  0.68906745  0.69239505  0.71360871  0.71662912  0.72197061
  0.72941499]

  warnings.warn(

2022-11-03 10:49:41,985:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.76242728e-01 -1.72134510e-01 -1.59744682e-01 -1.54367836e-01
 -1.53607894e-01 -1.36149587e-01 -1.32536805e-01 -1.32114512e-01
 -1.26328721e-01 -1.17578218e-01 -1.17492079e-01 -1.15428769e-01
 -1.12551795e-01 -1.12544210e-01 -1.10804137e-01 -1.08946709e-01
 -1.07729536e-01 -1.06371604e-01 -1.03190567e-01 -1.01256243e-01
 -1.00874183e-01 -9.84555999e-02 -9.80823640e-02 -9.55394551e-02
 -9.49029929e-02 -9.41392996e-02 -9.10008619e-02 -8.25146668e-02
 -7.88312686e-02 -7.82106992e-02 -7.77212127e-02 -7.64154819e-02
 -7.56471275e-02 -6.85942816e-02 -6.72959399e-02 -6.19457346e-02
 -6.03656108e-02 -5.99080884e-02 -5.82910863e-02 -5.64013525e-02
 -5.62291121e-02 -5.42594352e-02 -5.22531110e-02 -5.10257691e-02
 -5.02341668e-02 -4.86175821e-02 -4.76507966e-02 -4.73359435e-02
 -4.53437871e-02 -4.42995337e-02 -4.36651242e-02 -4.17386276e-02
 -4.11936246e-02 -3.77797935e-02 -3.70020117e-02 -3.66019161e-02
 -3.64060816e-02 -3.46959488e-02 -3.37030168e-02 -3.36323506e-02
 -2.92562481e-02 -2.66496121e-02 -2.60838586e-02 -2.59075184e-02
 -2.54699222e-02 -2.49852535e-02 -2.42466547e-02 -2.15500922e-02
 -1.97654739e-02 -1.84662998e-02 -1.59845298e-02 -1.50019150e-02
 -1.44617152e-02 -1.05023210e-02 -7.67207816e-03 -2.82960191e-03
 -2.28521403e-03 -1.44659556e-03 -2.08170196e-04  4.19926283e-04
  4.98482836e-03  8.64282660e-03  9.58110454e-03  1.21358178e-02
  1.40901838e-02  1.44768404e-02  1.87282160e-02  1.99472574e-02
  2.33036968e-02  2.35700763e-02  2.43048451e-02  2.49655879e-02
  2.58268001e-02  2.67504259e-02  2.78760542e-02  2.88622585e-02
  2.96731832e-02  3.07107778e-02  3.15325036e-02  3.25437253e-02
  3.41997393e-02  3.70682786e-02  3.75529605e-02  3.77561477e-02
  3.81039301e-02  3.92763183e-02  4.15299809e-02  4.17727317e-02
  4.24510604e-02  4.36866353e-02  4.42047566e-02  4.43307988e-02
  4.52128097e-02  5.13691168e-02  5.13778740e-02  5.34868100e-02
  5.50269302e-02  5.52677859e-02  5.63201766e-02  5.63542968e-02
  5.65924340e-02  5.77606165e-02  5.87351295e-02  6.19478494e-02
  6.49542497e-02  6.58648717e-02  6.81894491e-02  6.93364533e-02
  6.94129245e-02  6.95237457e-02  6.95785754e-02  6.96574671e-02
  7.03087818e-02  7.56979684e-02  7.64741275e-02  7.64929921e-02
  7.82686305e-02  7.95709894e-02  8.01024176e-02  8.02956013e-02
  8.25170732e-02  8.41421783e-02  8.62036354e-02  8.71677159e-02
  8.89211150e-02  8.91337956e-02  9.07742172e-02  9.14128649e-02
  9.14728677e-02  9.16749807e-02  9.44229990e-02  9.61374127e-02
  9.69539217e-02  9.89536610e-02  9.98626543e-02  1.02463243e-01
  1.04565979e-01  1.05456408e-01  1.05820119e-01  1.06175454e-01
  1.06382696e-01  1.08371019e-01  1.09041818e-01  1.11310902e-01
  1.13890265e-01  1.15020316e-01  1.17387978e-01  1.17759518e-01
  1.22096603e-01  1.26993483e-01  1.28167363e-01  1.28184225e-01
  1.29967891e-01  1.33706266e-01  1.33963552e-01  1.39542343e-01
  1.40161241e-01  1.49959021e-01  1.51395031e-01  1.51864861e-01
  1.55419203e-01  1.55642930e-01  1.57371573e-01  1.58036013e-01
  1.58301460e-01  1.58696270e-01  1.59971690e-01  1.61714355e-01
  1.61806016e-01  1.62830617e-01  1.63703732e-01  1.64365720e-01
  1.65716661e-01  1.68670270e-01  1.69192653e-01  1.70030477e-01
  1.71977621e-01  1.73046650e-01  1.74607746e-01  1.76487674e-01
  1.77347245e-01  1.80367869e-01  1.80468931e-01  1.82714671e-01
  1.83756866e-01  1.87115629e-01  1.87540205e-01  1.89412724e-01
  1.90109742e-01  1.91815904e-01  1.94698109e-01  1.98902807e-01
  1.99588766e-01  2.00382046e-01  2.01147800e-01  2.03006640e-01
  2.04949916e-01  2.06163816e-01  2.07013705e-01  2.07427147e-01
  2.07849118e-01  2.07978108e-01  2.09198724e-01  2.11811477e-01
  2.11876085e-01  2.13293039e-01  2.18391729e-01  2.19139598e-01
  2.20965908e-01  2.22951956e-01  2.24613900e-01  2.27079464e-01
  2.27992762e-01  2.28406246e-01  2.28471647e-01  2.31423656e-01
  2.31847116e-01  2.35135441e-01  2.36896891e-01  2.38554216e-01
  2.38900860e-01  2.40597698e-01  2.41208727e-01  2.41442464e-01
  2.44243425e-01  2.46495399e-01  2.46886220e-01  2.49495871e-01
  2.49624243e-01  2.53328808e-01  2.57267099e-01  2.57471384e-01
  2.59908729e-01  2.60800391e-01  2.61174571e-01  2.63958914e-01
  2.63978308e-01  2.66829555e-01  2.66952058e-01  2.68213439e-01
  2.69429466e-01  2.70258388e-01  2.70743023e-01  2.71436348e-01
  2.76848956e-01  2.77117640e-01  2.79122284e-01  2.79497981e-01
  2.79541697e-01  2.81316590e-01  2.84473369e-01  2.86288027e-01
  2.86795761e-01  2.88976340e-01  2.92282198e-01  2.92785577e-01
  2.93003865e-01  2.95051373e-01  2.96773202e-01  2.97073839e-01
  2.98043238e-01  2.98959516e-01  2.99059043e-01  2.99246456e-01
  3.00351756e-01  3.03752858e-01  3.05365066e-01  3.05904002e-01
  3.06438199e-01  3.07214814e-01  3.07464968e-01  3.07493110e-01
  3.08376700e-01  3.10606077e-01  3.13360764e-01  3.13434706e-01
  3.14635494e-01  3.15929325e-01  3.19151255e-01  3.19490207e-01
  3.19547610e-01  3.19969875e-01  3.22147966e-01  3.22487017e-01
  3.22665829e-01  3.28842880e-01  3.28898479e-01  3.29150801e-01
  3.30736729e-01  3.33811457e-01  3.34568366e-01  3.35230513e-01
  3.38513769e-01  3.42136044e-01  3.45753293e-01  3.47437346e-01
  3.47910079e-01  3.48296899e-01  3.52879257e-01  3.53841549e-01
  3.55488459e-01  3.56437656e-01  3.57085613e-01  3.58619308e-01
  3.58897346e-01  3.59571291e-01  3.59881921e-01  3.62972786e-01
  3.65001404e-01  3.65594287e-01  3.65694901e-01  3.67175067e-01
  3.67979175e-01  3.68363752e-01  3.70366117e-01  3.71422078e-01
  3.74047661e-01  3.74931950e-01  3.76401820e-01  3.82128525e-01
  3.83299893e-01  3.84905439e-01  3.85146588e-01  3.88766688e-01
  3.89947117e-01  3.92148256e-01  3.95408816e-01  4.01609855e-01
  4.04565562e-01  4.07787479e-01  4.12526606e-01  4.12887664e-01
  4.13743961e-01  4.13856313e-01  4.14247659e-01  4.14804959e-01
  4.14979133e-01  4.18096308e-01  4.23679515e-01  4.24207208e-01
  4.25028412e-01  4.25071858e-01  4.25603080e-01  4.26238115e-01
  4.33321273e-01  4.36073400e-01  4.36595146e-01  4.39497024e-01
  4.42380014e-01  4.43120110e-01  4.46400625e-01  4.49195392e-01
  4.49291859e-01  4.49309072e-01  4.50346627e-01  4.51306374e-01
  4.56309049e-01  4.56658288e-01  4.57153978e-01  4.59356333e-01
  4.60451323e-01  4.60484707e-01  4.60611069e-01  4.63313262e-01
  4.67514915e-01  4.68659178e-01  4.70447576e-01  4.73671877e-01
  4.75815315e-01  4.77700658e-01  4.78327231e-01  4.79394334e-01
  4.82891803e-01  4.86743819e-01  4.91152074e-01  4.91471203e-01
  4.93250725e-01  4.94033612e-01  4.95377220e-01  4.97567992e-01
  5.01533459e-01  5.02262735e-01  5.03182292e-01  5.06033007e-01
  5.09977484e-01  5.11474571e-01  5.13577121e-01  5.14069332e-01
  5.19549797e-01  5.21964670e-01  5.22269284e-01  5.25812139e-01
  5.27969131e-01  5.28601875e-01  5.29105039e-01  5.29133200e-01
  5.31167377e-01  5.36526465e-01  5.39547381e-01  5.45553805e-01
  5.46290453e-01  5.48379930e-01  5.48754437e-01  5.51098033e-01
  5.52148558e-01  5.53309548e-01  5.58010232e-01  5.61823557e-01
  5.67772450e-01  5.76166598e-01  5.78189518e-01  5.82021565e-01
  5.84854315e-01  5.85487769e-01  5.88400501e-01  5.92475660e-01
  5.93597838e-01  5.94394041e-01  5.96774240e-01  5.97488997e-01
  5.97519716e-01  5.97778436e-01  6.00528280e-01  6.02435333e-01
  6.03536634e-01  6.04235039e-01  6.04708962e-01  6.05292260e-01
  6.06622677e-01  6.06888963e-01  6.12021733e-01  6.12443246e-01
  6.12455169e-01  6.12992139e-01  6.13191307e-01  6.16415773e-01
  6.18668090e-01  6.19562692e-01  6.24096156e-01  6.26688733e-01
  6.27859090e-01  6.28368247e-01  6.33363217e-01  6.34688014e-01
  6.37413923e-01  6.42634031e-01  6.43720004e-01  6.47996010e-01
  6.50012209e-01  6.54799945e-01  6.54941260e-01  6.57855081e-01
  6.59626568e-01  6.60294608e-01  6.60653806e-01  6.61112349e-01
  6.66021677e-01  6.66181499e-01  6.67447717e-01  6.73967727e-01
  6.83057009e-01  6.84216794e-01  7.00676507e-01  7.08366611e-01
  7.08430971e-01  7.11246359e-01  7.20602747e-01  7.20697195e-01
  7.22342919e-01  7.24346863e-01  7.26463715e-01  7.63663916e-01]

  warnings.warn(

2022-11-03 10:49:42,001:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.19316158e-01 -2.01762112e-01 -1.93175648e-01 -1.75638928e-01
 -1.74917472e-01 -1.72997374e-01 -1.65048712e-01 -1.64929873e-01
 -1.64819574e-01 -1.64381407e-01 -1.56765217e-01 -1.55060916e-01
 -1.53754117e-01 -1.51797646e-01 -1.42810621e-01 -1.39259005e-01
 -1.37843993e-01 -1.36352713e-01 -1.36310687e-01 -1.30450308e-01
 -1.29018702e-01 -1.18626559e-01 -1.17995432e-01 -1.15449353e-01
 -1.12787093e-01 -1.12581921e-01 -1.11112261e-01 -1.09941931e-01
 -1.09122262e-01 -1.08701814e-01 -1.04919574e-01 -9.62600224e-02
 -9.40550475e-02 -9.33714901e-02 -9.23989447e-02 -8.98458728e-02
 -8.98203374e-02 -8.46304601e-02 -8.44014466e-02 -8.31378184e-02
 -8.28930166e-02 -8.25677134e-02 -8.12874785e-02 -7.89587971e-02
 -7.72851774e-02 -7.55389571e-02 -6.71672407e-02 -6.64551748e-02
 -6.63336270e-02 -6.28223029e-02 -6.24163107e-02 -6.23464225e-02
 -6.21807349e-02 -5.61528133e-02 -5.49877941e-02 -5.48532877e-02
 -5.36198030e-02 -5.27295788e-02 -5.21718664e-02 -4.63318512e-02
 -4.63174844e-02 -4.62817912e-02 -4.57472783e-02 -4.50414135e-02
 -4.39408500e-02 -4.12139562e-02 -3.83479885e-02 -3.68695930e-02
 -3.63482935e-02 -3.28846125e-02 -3.28568720e-02 -3.24175175e-02
 -2.39237447e-02 -1.96008851e-02 -1.72921664e-02 -1.65665775e-02
 -1.55055745e-02 -1.45866905e-02 -1.39999483e-02 -9.95296052e-03
 -9.66856333e-03 -7.34029994e-03 -6.79986354e-03 -4.03439926e-03
 -1.46505110e-03  5.97319966e-04  1.37019075e-03  6.38256219e-03
  1.01765060e-02  1.06038356e-02  1.22372299e-02  1.48628934e-02
  1.64915994e-02  1.86692539e-02  1.95591002e-02  2.12136602e-02
  2.15112063e-02  2.31472466e-02  2.48616621e-02  2.96069107e-02
  2.97072540e-02  3.14411503e-02  3.18943754e-02  3.75885180e-02
  3.89441357e-02  4.06412044e-02  4.11376670e-02  4.49984159e-02
  5.03218567e-02  5.17971900e-02  5.37715987e-02  5.45260471e-02
  5.48877969e-02  5.59734515e-02  5.80816733e-02  5.85682940e-02
  5.87594269e-02  5.99670374e-02  6.04677592e-02  6.57424608e-02
  6.86854933e-02  6.99426432e-02  7.11402948e-02  7.62445820e-02
  7.70431431e-02  7.81409114e-02  8.01504791e-02  8.10683997e-02
  8.54509538e-02  8.58794582e-02  8.70953629e-02  8.73959570e-02
  8.93448152e-02  9.26674290e-02  9.33219072e-02  9.37606640e-02
  9.47874102e-02  9.82535577e-02  1.00252596e-01  1.01048589e-01
  1.01099721e-01  1.05403204e-01  1.05558089e-01  1.08570306e-01
  1.08797926e-01  1.09469003e-01  1.10876224e-01  1.13169176e-01
  1.14020860e-01  1.19419226e-01  1.22078785e-01  1.22462181e-01
  1.22909225e-01  1.24856302e-01  1.28015422e-01  1.30870963e-01
  1.31888206e-01  1.32111907e-01  1.32659986e-01  1.33356444e-01
  1.36129491e-01  1.37810215e-01  1.38176656e-01  1.39687037e-01
  1.42923405e-01  1.43268543e-01  1.43680264e-01  1.46877981e-01
  1.47986145e-01  1.48722397e-01  1.55040576e-01  1.56033438e-01
  1.57844800e-01  1.57984538e-01  1.58220594e-01  1.58452089e-01
  1.59705320e-01  1.61312920e-01  1.64536737e-01  1.67554885e-01
  1.69678386e-01  1.72297264e-01  1.73733653e-01  1.76266955e-01
  1.81714505e-01  1.82073746e-01  1.82384654e-01  1.83943042e-01
  1.85323761e-01  1.87295999e-01  1.92229931e-01  1.92291237e-01
  1.92414660e-01  1.94179296e-01  1.95362815e-01  1.97215166e-01
  1.97528387e-01  1.97634353e-01  1.97676924e-01  1.98077011e-01
  1.98294520e-01  1.99789061e-01  2.01409444e-01  2.02343049e-01
  2.04497499e-01  2.05289809e-01  2.06490007e-01  2.07082930e-01
  2.07147048e-01  2.07519988e-01  2.10900649e-01  2.11748303e-01
  2.14403546e-01  2.18532860e-01  2.19304461e-01  2.19543374e-01
  2.20925433e-01  2.21116700e-01  2.22797754e-01  2.23470700e-01
  2.24240841e-01  2.24819372e-01  2.25001307e-01  2.25508654e-01
  2.26163871e-01  2.32122493e-01  2.33053203e-01  2.36650235e-01
  2.39450272e-01  2.41275807e-01  2.42627535e-01  2.43026564e-01
  2.44452461e-01  2.45791891e-01  2.46102760e-01  2.46930632e-01
  2.49066205e-01  2.51944304e-01  2.54316083e-01  2.54450044e-01
  2.54467655e-01  2.54711150e-01  2.55248629e-01  2.56211437e-01
  2.57642773e-01  2.58053238e-01  2.62619192e-01  2.63601493e-01
  2.64023609e-01  2.65521621e-01  2.65571877e-01  2.66202111e-01
  2.69850313e-01  2.72084802e-01  2.74097884e-01  2.75170625e-01
  2.75419371e-01  2.76723239e-01  2.76837272e-01  2.77065573e-01
  2.77105629e-01  2.78470470e-01  2.80209436e-01  2.80286084e-01
  2.82945086e-01  2.83762377e-01  2.85245829e-01  2.85763946e-01
  2.86521480e-01  2.86618051e-01  2.88394307e-01  2.89106939e-01
  2.90028177e-01  2.90735676e-01  2.93391772e-01  2.97279777e-01
  2.98425364e-01  2.99216310e-01  3.09891431e-01  3.12139799e-01
  3.12301046e-01  3.13097810e-01  3.13489183e-01  3.13923740e-01
  3.15973341e-01  3.16595133e-01  3.17533962e-01  3.18018009e-01
  3.18704679e-01  3.18878104e-01  3.20725036e-01  3.22005195e-01
  3.23668995e-01  3.24123559e-01  3.24308447e-01  3.24385508e-01
  3.25729487e-01  3.26816832e-01  3.28294132e-01  3.28743600e-01
  3.28811935e-01  3.32142002e-01  3.32584189e-01  3.35126088e-01
  3.41735795e-01  3.42962001e-01  3.46534749e-01  3.47202854e-01
  3.50650066e-01  3.50886188e-01  3.51497713e-01  3.51676451e-01
  3.52312495e-01  3.53076386e-01  3.53641879e-01  3.55965549e-01
  3.56811972e-01  3.57557115e-01  3.58175639e-01  3.60334297e-01
  3.62552478e-01  3.63652319e-01  3.68732698e-01  3.69167280e-01
  3.71838741e-01  3.72026646e-01  3.73274940e-01  3.77212437e-01
  3.79146891e-01  3.81012874e-01  3.85885736e-01  3.86026482e-01
  3.87524582e-01  3.87839945e-01  3.88846818e-01  3.90161062e-01
  3.90605493e-01  3.91193116e-01  3.91819132e-01  3.91908560e-01
  3.93249963e-01  3.95219064e-01  3.97460884e-01  3.99048676e-01
  4.02393013e-01  4.06378628e-01  4.06397074e-01  4.13136374e-01
  4.13190160e-01  4.16200437e-01  4.16497342e-01  4.19879388e-01
  4.21062181e-01  4.21373459e-01  4.22840840e-01  4.24087910e-01
  4.24661415e-01  4.29349574e-01  4.30237145e-01  4.31018313e-01
  4.31589475e-01  4.32388086e-01  4.34032126e-01  4.40770250e-01
  4.40937117e-01  4.41081160e-01  4.41149248e-01  4.41459793e-01
  4.41945756e-01  4.43508596e-01  4.44191011e-01  4.45226480e-01
  4.46928935e-01  4.47238311e-01  4.47314041e-01  4.48040799e-01
  4.53464642e-01  4.57442266e-01  4.63631869e-01  4.63788625e-01
  4.66356819e-01  4.67367463e-01  4.68660635e-01  4.71062136e-01
  4.71238153e-01  4.74372220e-01  4.79613770e-01  4.80612425e-01
  4.81027457e-01  4.84206098e-01  4.85446750e-01  4.86847432e-01
  4.87541261e-01  4.89973268e-01  4.90390969e-01  4.91334271e-01
  4.91929323e-01  4.92533166e-01  4.93552791e-01  4.95085567e-01
  4.98618674e-01  4.99264090e-01  5.00689093e-01  5.02294248e-01
  5.03227724e-01  5.04358482e-01  5.04571118e-01  5.04735461e-01
  5.05545391e-01  5.07053204e-01  5.10178569e-01  5.15192811e-01
  5.15219138e-01  5.17010871e-01  5.21267392e-01  5.21293016e-01
  5.23494074e-01  5.31819747e-01  5.33138016e-01  5.34715718e-01
  5.35005456e-01  5.36845707e-01  5.37002542e-01  5.37189135e-01
  5.37691849e-01  5.42918706e-01  5.44884310e-01  5.45339883e-01
  5.47725579e-01  5.55025524e-01  5.57866054e-01  5.60377196e-01
  5.62065843e-01  5.63611702e-01  5.63905639e-01  5.66210609e-01
  5.66548462e-01  5.70533358e-01  5.72729967e-01  5.75416113e-01
  5.75746924e-01  5.75812984e-01  5.77443828e-01  5.77664808e-01
  5.79518297e-01  5.79957113e-01  5.87642149e-01  5.92062479e-01
  5.94888750e-01  5.94977888e-01  5.97355727e-01  5.98086195e-01
  6.04311270e-01  6.10028405e-01  6.10062966e-01  6.11374718e-01
  6.12916882e-01  6.13724993e-01  6.13828167e-01  6.15872654e-01
  6.16220738e-01  6.24411661e-01  6.26924471e-01  6.27654834e-01
  6.29301938e-01  6.36694343e-01  6.42380908e-01  6.44876444e-01
  6.45906733e-01  6.46845326e-01  6.47520614e-01  6.48189879e-01
  6.48510663e-01  6.50896885e-01  6.51140666e-01  6.54380960e-01
  6.62403222e-01  6.62494656e-01  6.63341199e-01  6.63818340e-01
  6.64795754e-01  6.65054612e-01  6.66502566e-01  6.72483199e-01
  6.75747763e-01  6.94584864e-01  7.03616132e-01  7.14529745e-01
  7.38300200e-01  7.45219960e-01  7.59067915e-01  7.67385068e-01
  7.68830725e-01]

  warnings.warn(

2022-11-03 10:49:44,226:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.21432575 -0.20915961 -0.20791527 -0.18921325 -0.18680293 -0.17121524
 -0.16830562 -0.15160767 -0.15157323 -0.15128439 -0.15037714 -0.14978996
 -0.13911794 -0.1326331  -0.13229535 -0.13160777 -0.12827284 -0.1271102
 -0.12657809 -0.12558135 -0.12521806 -0.12350088 -0.1229853  -0.12180216
 -0.11995844 -0.11845603 -0.10650828 -0.10548376 -0.0997422  -0.09863517
 -0.09692832 -0.0950306  -0.0940512  -0.09289899 -0.09259429 -0.09175097
 -0.0891734  -0.08878996 -0.08426669 -0.07720144 -0.07559931 -0.07356759
 -0.0722139  -0.07092258 -0.07081206 -0.06416651 -0.05988964 -0.05940984
 -0.05869446 -0.05825753 -0.05376722 -0.0528725  -0.05026195 -0.04977644
 -0.046849   -0.04536542 -0.04408323 -0.04325224 -0.04293387 -0.04130654
 -0.04041285 -0.03970555 -0.03753727 -0.03634789 -0.03394797 -0.03297701
 -0.03023057 -0.02713822 -0.02674404 -0.02299184 -0.02096777 -0.01930232
 -0.01852931 -0.01806938 -0.01501503 -0.0149507  -0.01392916 -0.01257512
 -0.00976471 -0.00894614 -0.00881419 -0.00551878 -0.00470655 -0.00403525
 -0.00277455 -0.00145009 -0.00096758  0.00104525  0.00290014  0.00345951
  0.00571034  0.00617081  0.00707581  0.00726417  0.0087533   0.01546348
  0.02066352  0.02178529  0.02399607  0.02422463  0.02618142  0.03033284
  0.03072948  0.0322268   0.03307079  0.03336734  0.03626738  0.03647388
  0.039351    0.04434012  0.04807028  0.0485243   0.04922196  0.05019411
  0.05281392  0.05647377  0.05774747  0.0599485   0.06060414  0.06086514
  0.06438305  0.06444106  0.06502394  0.06552688  0.06644404  0.0679345
  0.06834904  0.0707403   0.07176733  0.07199705  0.07329301  0.0737398
  0.07526476  0.0755645   0.07976338  0.08383478  0.08472818  0.08484468
  0.08649352  0.08717087  0.08725554  0.08779266  0.08788906  0.09126126
  0.09220816  0.09295036  0.09545396  0.09643859  0.0971498   0.10072905
  0.10131565  0.1024909   0.10386723  0.10543723  0.10963344  0.11119583
  0.11201533  0.11230207  0.11246315  0.11314778  0.11481981  0.11627443
  0.11702268  0.12128137  0.12227058  0.12250885  0.12272898  0.12319785
  0.12376312  0.12542172  0.12557133  0.12699232  0.12812387  0.1308207
  0.13134887  0.13371837  0.1338412   0.1397901   0.13994993  0.14053035
  0.14122813  0.14142549  0.14247067  0.14247331  0.14287564  0.14498169
  0.14885007  0.15336173  0.15371908  0.15424427  0.1545088   0.15697737
  0.1574109   0.15796585  0.15968576  0.16083229  0.16288138  0.16409322
  0.16501335  0.16569555  0.16584969  0.16698434  0.17028677  0.17373801
  0.17564718  0.176041    0.17766445  0.17885069  0.18129899  0.18188957
  0.18355034  0.18361126  0.18595422  0.1884306   0.18890081  0.19211844
  0.19356487  0.19459893  0.19602973  0.19720509  0.20215478  0.20231263
  0.20246503  0.20249907  0.20266475  0.20280033  0.20357712  0.20457983
  0.20486556  0.21023325  0.21231251  0.21603912  0.22206245  0.22375065
  0.22555222  0.22558101  0.2257447   0.23097056  0.23167674  0.23289065
  0.23311821  0.23325418  0.23371461  0.23489627  0.23555786  0.2382231
  0.2395991   0.24006083  0.24333648  0.24426848  0.246679    0.24679893
  0.24981765  0.25096062  0.25429097  0.25884075  0.25937206  0.26384629
  0.26420356  0.26580457  0.26639224  0.2672591   0.27029108  0.27287517
  0.28096224  0.28104909  0.28142058  0.28208281  0.28555334  0.28649478
  0.28901905  0.29269605  0.29301303  0.29368406  0.29446475  0.29632853
  0.29633989  0.29656633  0.29704973  0.29714406  0.29831999  0.29854034
  0.29942754  0.30126585  0.30141456  0.30277017  0.30374595  0.3068095
  0.30845257  0.31001705  0.31156121  0.31425199  0.32080874  0.32214317
  0.32655846  0.3265866   0.32669599  0.32694119  0.32699425  0.32789028
  0.32821161  0.33057694  0.33059094  0.33162624  0.3353508   0.33963408
  0.34051903  0.34178676  0.34183559  0.3456976   0.34609432  0.35241965
  0.35254213  0.35344767  0.35535938  0.35704798  0.35982676  0.36055158
  0.36079508  0.36203796  0.36361011  0.36425843  0.36458001  0.36695079
  0.36735022  0.36807765  0.37126644  0.37160416  0.37294934  0.37344423
  0.37804981  0.38063587  0.38376174  0.38660413  0.39233114  0.39256532
  0.39611462  0.39661283  0.39761099  0.39879679  0.39911487  0.40164456
  0.40230874  0.40309514  0.40395172  0.40534851  0.40595674  0.40636809
  0.40797786  0.40898859  0.41116711  0.41582184  0.41707974  0.41722271
  0.41835377  0.42090925  0.42168023  0.42361748  0.42780905  0.4281501
  0.42918541  0.43277408  0.43420888  0.4348614   0.4377755   0.43779796
  0.43876035  0.43878193  0.43880369  0.43998905  0.4475005   0.44800052
  0.44859009  0.45221879  0.45240003  0.45342304  0.45524553  0.45705282
  0.45862708  0.45948676  0.46238267  0.46465339  0.46477532  0.46728532
  0.47509869  0.47819666  0.48048022  0.48210343  0.48243785  0.48435085
  0.48471818  0.48821901  0.48856266  0.48942344  0.49083299  0.49419969
  0.49577869  0.49595087  0.4982549   0.49934494  0.50096918  0.50227867
  0.50363109  0.50719999  0.50757899  0.51006893  0.51036637  0.5141592
  0.51467064  0.51507753  0.51679368  0.51923177  0.52115335  0.52333783
  0.52348293  0.52355623  0.52723573  0.52843067  0.52963841  0.53012561
  0.53029386  0.53098755  0.53145454  0.53210976  0.53237229  0.53336794
  0.53676171  0.54081558  0.54219438  0.5476443   0.54771855  0.55171065
  0.55185358  0.55318694  0.55450589  0.55960641  0.56467133  0.56550716
  0.57318026  0.57395568  0.5761066   0.57784092  0.58566929  0.5870199
  0.5881131   0.58851793  0.59220981  0.59254658  0.59304826  0.59566656
  0.59753891  0.60201453  0.60211828  0.60221185  0.6025733   0.61504696
  0.61803726  0.61934914  0.61957626  0.62159344  0.62306742  0.62524723
  0.62624202  0.62671543  0.62689366  0.62740403  0.6290509   0.63160918
  0.63204987  0.63734243  0.64130743  0.64323364  0.64875286  0.65131014
  0.6518844   0.65786265  0.66029963  0.66420335  0.66631098  0.68123443
  0.68762988  0.68790606  0.69507184  0.70214414  0.7078524   0.71122402
  0.71204875  0.71448966  0.71612508  0.720116    0.72130836  0.72595891
  0.75949915]

  warnings.warn(

2022-11-03 10:49:44,292:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16481319 -0.15781951 -0.15593834 -0.15470605 -0.14842616 -0.14312364
 -0.13555675 -0.13392082 -0.13083604 -0.12776983 -0.12640217 -0.12527365
 -0.1222633  -0.11934519 -0.11902853 -0.11844589 -0.11747659 -0.11582113
 -0.11328324 -0.10889979 -0.10746377 -0.1026733  -0.10250101 -0.10209196
 -0.10015891 -0.09670632 -0.09354852 -0.09268045 -0.09174063 -0.08887465
 -0.0864854  -0.08542962 -0.08199076 -0.07994004 -0.07890181 -0.0785153
 -0.07769418 -0.07163926 -0.07129202 -0.06978141 -0.06963817 -0.06692314
 -0.06527235 -0.06271823 -0.05577298 -0.05367968 -0.05348476 -0.0524461
 -0.05159123 -0.05044261 -0.05039237 -0.04808571 -0.04298163 -0.03821043
 -0.03539955 -0.03344661 -0.03244747 -0.03058758 -0.0266695  -0.02356809
 -0.02321036 -0.02081267 -0.01828832 -0.01692369 -0.01596892 -0.01348024
 -0.01077111 -0.01076616 -0.00995118 -0.00971348 -0.00818555 -0.00648091
 -0.00644855 -0.00523683 -0.00491942 -0.00150737  0.00474102  0.0062609
  0.00669988  0.00695042  0.00775181  0.00849396  0.01053688  0.01171565
  0.01263079  0.01421695  0.01564708  0.016884    0.01833705  0.02073335
  0.02386797  0.03000242  0.03044346  0.03154443  0.0333016   0.03676784
  0.03710088  0.0391029   0.03925505  0.04014374  0.04679995  0.04771426
  0.04969532  0.05206026  0.05306574  0.05647656  0.05651537  0.05719914
  0.06098699  0.06289301  0.06404664  0.06720242  0.06750611  0.06913461
  0.07064745  0.07392976  0.07403383  0.07418237  0.0759018   0.07610602
  0.07780442  0.07995455  0.08222529  0.08301234  0.08306903  0.0845005
  0.0846448   0.08592113  0.08608397  0.08612756  0.08645225  0.08663784
  0.08926362  0.09048849  0.09096368  0.09325279  0.09342667  0.0947941
  0.09642185  0.09827215  0.10460246  0.10498673  0.10508636  0.1078677
  0.10950877  0.11000435  0.11016848  0.11407632  0.1159785   0.12067357
  0.12099215  0.12253009  0.12333249  0.12355059  0.12684025  0.12821589
  0.1302988   0.13093688  0.13616904  0.13624812  0.13798287  0.13932644
  0.14091606  0.14393805  0.14412504  0.14664022  0.14686884  0.14836507
  0.14884301  0.15021184  0.156229    0.15669909  0.15695591  0.15706082
  0.15716003  0.15902106  0.16379972  0.16419352  0.16557552  0.16628482
  0.16725429  0.17403071  0.17733717  0.1781946   0.17957299  0.18098207
  0.18112734  0.18216153  0.18235789  0.18356008  0.18539107  0.18651676
  0.18890085  0.18895366  0.18946165  0.19023078  0.19097541  0.19130441
  0.19159453  0.19280831  0.194921    0.19492938  0.19888152  0.20222653
  0.2029442   0.20414801  0.20567749  0.20616701  0.2068731   0.20731665
  0.20823314  0.20917186  0.20979604  0.20989292  0.20989365  0.21091868
  0.21203848  0.21250755  0.2134225   0.21369123  0.21548343  0.2159646
  0.21629523  0.21648682  0.21866222  0.21922892  0.21989609  0.22061153
  0.22127285  0.22262066  0.22289817  0.22705908  0.22790181  0.23334726
  0.2346986   0.23770664  0.23773484  0.23807072  0.23820468  0.2388273
  0.24106049  0.24117102  0.24397026  0.24419419  0.24481664  0.24560513
  0.24960159  0.25076966  0.25092603  0.25369986  0.25673047  0.25766317
  0.259569    0.25973993  0.26044089  0.26238249  0.26287652  0.26605318
  0.26734797  0.2677879   0.2687489   0.27024893  0.27177728  0.27488714
  0.27592769  0.27681369  0.27718775  0.2790337   0.27943295  0.28526092
  0.28692698  0.28731904  0.28816438  0.28960828  0.29252189  0.29335151
  0.29834526  0.29948662  0.30093962  0.30710696  0.3077026   0.30815776
  0.30880463  0.30974749  0.31214116  0.31313679  0.31468749  0.31488879
  0.31526573  0.31552559  0.31630633  0.31792592  0.32052146  0.32260264
  0.32293184  0.32299208  0.32622634  0.32723279  0.33026845  0.33068715
  0.3368909   0.33739509  0.33770122  0.33988125  0.34061799  0.34192399
  0.34345021  0.34420282  0.34444403  0.34943523  0.34948137  0.35038639
  0.35154591  0.35378271  0.35512876  0.35517671  0.36015103  0.36127173
  0.36360277  0.36485101  0.3661138   0.36900737  0.37104962  0.37151158
  0.37282649  0.37577962  0.37606883  0.37858086  0.38037422  0.38317945
  0.38332691  0.38364194  0.38869955  0.39001611  0.39050282  0.39818445
  0.40192879  0.40403022  0.40431836  0.40559258  0.40640388  0.40857775
  0.4089485   0.41098767  0.41316794  0.41323277  0.4137794   0.41400009
  0.41412379  0.41481257  0.41519481  0.41586411  0.41612571  0.41720672
  0.42281724  0.42357311  0.42573863  0.42636918  0.42673454  0.43351453
  0.43768445  0.43900057  0.44387063  0.44625151  0.44933132  0.45020361
  0.45040992  0.45224558  0.45437747  0.45532666  0.45557306  0.45813834
  0.45886467  0.46254656  0.47166689  0.47167614  0.47185423  0.47396011
  0.47566046  0.47612416  0.47643519  0.47740487  0.47993092  0.48015959
  0.48061565  0.48257216  0.48305271  0.48931699  0.49159659  0.49215809
  0.49507248  0.49555113  0.49818632  0.49824511  0.49950169  0.49962209
  0.49976959  0.50202732  0.50240183  0.50337525  0.50565018  0.50585908
  0.50643085  0.51423438  0.51500117  0.51705473  0.52294882  0.52704163
  0.52887359  0.52930457  0.5299609   0.53094604  0.53406105  0.53782503
  0.53802572  0.53934167  0.54090445  0.54309757  0.5487862   0.55070703
  0.55101218  0.55232852  0.55362244  0.55983906  0.5625184   0.56276657
  0.56361523  0.56509001  0.56523392  0.56579448  0.56722087  0.56819881
  0.57396302  0.57417237  0.57469515  0.57522937  0.57526098  0.57723185
  0.58238585  0.58316966  0.58423602  0.58432962  0.58649338  0.58781413
  0.58854638  0.59089692  0.59107177  0.59122102  0.59152913  0.59355491
  0.59787683  0.59804171  0.60254554  0.60455694  0.60610709  0.60626002
  0.60896311  0.60928717  0.60941532  0.61104652  0.61331151  0.62149481
  0.62183094  0.62303706  0.62334101  0.62498232  0.62783002  0.62949557
  0.63006714  0.64217353  0.64634433  0.64753122  0.64965175  0.65249947
  0.65507128  0.65617535  0.65649124  0.66190019  0.66220111  0.66734926
  0.66871334  0.67149808  0.67629239  0.67710095  0.68412087  0.68489105
  0.69783021  0.69852166  0.69918726  0.70671733  0.72617778  0.73080806
  0.73275922]

  warnings.warn(

2022-11-03 10:49:44,292:INFO:Calculating mean and std
2022-11-03 10:49:44,300:INFO:Creating metrics dataframe
2022-11-03 10:49:44,311:INFO:Uploading results into container
2022-11-03 10:49:44,311:INFO:Uploading model into container now
2022-11-03 10:49:44,311:INFO:master_model_container: 6
2022-11-03 10:49:44,311:INFO:display_container: 2
2022-11-03 10:49:44,311:INFO:Ridge(random_state=4411)
2022-11-03 10:49:44,311:INFO:create_model() successfully completed......................................
2022-11-03 10:49:44,573:ERROR:create_model() for Ridge(random_state=4411) raised an exception or returned all 0.0:
2022-11-03 10:49:44,573:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:49:44,573:INFO:Initializing Elastic Net
2022-11-03 10:49:44,573:INFO:Total runtime is 1.144397250811259 minutes
2022-11-03 10:49:44,573:INFO:SubProcess create_model() called ==================================
2022-11-03 10:49:44,573:INFO:Initializing create_model()
2022-11-03 10:49:44,573:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:49:44,573:INFO:Checking exceptions
2022-11-03 10:49:44,573:INFO:Importing libraries
2022-11-03 10:49:44,573:INFO:Copying training dataset
2022-11-03 10:49:44,589:INFO:Defining folds
2022-11-03 10:49:44,589:INFO:Declaring metric variables
2022-11-03 10:49:44,589:INFO:Importing untrained model
2022-11-03 10:49:44,589:INFO:Elastic Net Imported successfully
2022-11-03 10:49:44,589:INFO:Starting cross validation
2022-11-03 10:49:44,604:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:49:48,566:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.15704637 -0.15612418 -0.13841816 -0.13804928 -0.13658168 -0.13602837
 -0.13602047 -0.13332592 -0.12250821 -0.12158603 -0.11814582 -0.11666241
 -0.11518691 -0.11230793 -0.11156227 -0.10702335 -0.10646213 -0.09239656
 -0.08950967 -0.08471429 -0.07307098 -0.07015162 -0.06590955 -0.050978
 -0.04829221 -0.04746578 -0.03652774 -0.03561346 -0.02996001 -0.01881381
 -0.01459546 -0.01450762 -0.01037005 -0.00931089 -0.00401839  0.00070496
  0.00249229  0.003295    0.00515519  0.00691088  0.00986273  0.01318177
  0.0220427   0.02597209  0.02705499  0.02739138  0.02756085  0.0317388
  0.0377761   0.03817746  0.04237123  0.0447531   0.04612495  0.04786398
  0.05750303  0.06099238  0.06670911  0.07465575  0.07613916  0.07756721
  0.07860097  0.07884161  0.07896194  0.07961268  0.07982791  0.08019678
  0.08035043  0.08273147  0.08492099  0.08549803  0.0865572   0.08659591
  0.08678033  0.08892949  0.09114357  0.0924338   0.09313285  0.09452674
  0.09990083  0.10079762  0.10301086  0.10392599  0.10467082  0.10522411
  0.10561758  0.10591526  0.10771842  0.1080723   0.11107077  0.11435109
  0.11470331  0.11654853  0.11706854  0.12254545  0.12378032  0.12386108
  0.12435112  0.12861693  0.1304692   0.13075024  0.13167243  0.13182356
  0.13362754  0.13588908  0.13969817  0.14173489  0.14352474  0.14373997
  0.14412551  0.14567303  0.14590493  0.14787838  0.14827097  0.1491782
  0.15041136  0.15082855  0.15269042  0.15317793  0.15434245  0.15538496
  0.15670683  0.15705113  0.15815153  0.15889551  0.15999591  0.16110876
  0.16250517  0.16339489  0.16398942  0.16726017  0.16821398  0.16830185
  0.16852666  0.16915199  0.16971321  0.17130282  0.17407562  0.1804106
  0.18139691  0.1815822   0.18268797  0.18385249  0.18660157  0.18746754
  0.18751583  0.18847841  0.18881649  0.18951219  0.19024993  0.19025952
  0.19040273  0.19043438  0.19053096  0.1922954   0.19281001  0.19339411
  0.1937322   0.194492    0.1946448   0.1952077   0.19553534  0.1956003
  0.19578306  0.19613611  0.19656912  0.19691427  0.19748338  0.19846969
  0.19883234  0.19934358  0.19940938  0.19984069  0.20066797  0.20197402
  0.20231209  0.20307273  0.20420309  0.2059613   0.20600792  0.20633017
  0.20688348  0.21001018  0.21040277  0.2107146   0.21092362  0.21130124
  0.21161307  0.21267931  0.21695386  0.21697758  0.22006642  0.22009014
  0.22046609  0.22132584  0.22169471  0.22348119  0.22495048  0.22739479
  0.22748263  0.22751509  0.22808591  0.22845479  0.22846893  0.23060393
  0.23406703  0.23501378  0.23718748  0.23749847  0.23883951  0.24255907
  0.24289547  0.24430853  0.24668081  0.24726575  0.24745895  0.25185297
  0.25417073  0.25490933  0.25838201  0.25929545  0.26073056  0.26078761
  0.26134883  0.26170189  0.26315198  0.26350502  0.2642199   0.26459584
  0.26477321  0.26530987  0.26661675  0.26858145  0.27048203  0.27116442
  0.27191008  0.27267862  0.27271901  0.27283227  0.27299213  0.27377733
  0.27409086  0.27559092  0.27608719  0.27627247  0.27701646  0.2779245
  0.27866225  0.28007363  0.28118026  0.28324863  0.28357795  0.28431571
  0.28468458  0.28494726  0.28504387  0.28521333  0.28576663  0.2860152
  0.2866651   0.28686536  0.28763559  0.28794035  0.288477    0.28850073
  0.29033804  0.29034594  0.29089135  0.29107578  0.29126023  0.29158788
  0.29218241  0.29272614  0.29341558  0.29348139  0.29378445  0.29396268
  0.2945239   0.29470042  0.29525374  0.29672924  0.2974749   0.29802031
  0.29818725  0.29838918  0.2989425   0.29912693  0.29925347  0.30004912
  0.30115407  0.30121818  0.30218866  0.30288772  0.30386611  0.30494816
  0.30608644  0.30736082  0.30964611  0.31139557  0.31230112  0.31242227
  0.31422625  0.31480414  0.31639829  0.31792209  0.32102588  0.32250138
  0.32265418  0.3230951   0.32323915  0.32371252  0.32428249  0.32541201
  0.32670307  0.32726513  0.32762525  0.32890926  0.33051215  0.33145017
  0.33327955  0.33347107  0.33398567  0.33709571  0.33870819  0.34019949
  0.34242857  0.34372755  0.34606196  0.34675143  0.34710363  0.34995013
  0.35019162  0.35024699  0.35103305  0.35242861  0.35332707  0.35412809
  0.35655825  0.35917995  0.36096021  0.36099352  0.36635718  0.36757623
  0.37162595  0.37186744  0.3723154   0.37345451  0.37359064  0.37420017
  0.37507407  0.3760129   0.37946102  0.38163472  0.38403242  0.38513904
  0.38565988  0.38588471  0.38802593  0.38855468  0.39039905  0.39100774
  0.39135372  0.39207481  0.39208356  0.39403952  0.39571612  0.39775284
  0.39861797  0.39866629  0.39970088  0.40316564  0.40377516  0.40407202
  0.40557916  0.40722245  0.40739189  0.40831408  0.40905974  0.40923627
  0.41065557  0.41198619  0.41233049  0.41235506  0.4127485   0.41322103
  0.41397544  0.41494511  0.41541056  0.41608421  0.41632485  0.4203579
  0.42104029  0.42217856  0.4224113   0.42362157  0.4284178   0.42913888
  0.43175349  0.43444805  0.43471241  0.43564252  0.43665338  0.43884207
  0.44026136  0.44042206  0.44247461  0.44280312  0.44318863  0.4444314
  0.44535358  0.44698105  0.45144004  0.45285143  0.45322822  0.45504885
  0.45707768  0.45763889  0.45822386  0.45888166  0.46006031  0.460149
  0.4620092   0.46560137  0.46671591  0.46710935  0.46946665  0.46970729
  0.47182395  0.47488738  0.47513591  0.47848037  0.47858488  0.47950706
  0.48064532  0.48136727  0.48227363  0.48245805  0.4826425   0.4829789
  0.48323537  0.48516843  0.48706025  0.4884321   0.48862444  0.48960285
  0.4948154   0.49617848  0.49647533  0.49717271  0.49846377  0.49905745
  0.50127073  0.50385284  0.50732468  0.50751703  0.50926564  0.51199974
  0.5129544   0.51313885  0.51473466  0.51528798  0.51694793  0.51863951
  0.52095727  0.52189525  0.52390826  0.53134988  0.53227207  0.53288161
  0.53527138  0.53557613  0.53779731  0.53847886  0.53871949  0.54074039
  0.54240825  0.54940896  0.54985777  0.55009051  0.55235998  0.55383549
  0.5564255   0.56024249  0.56330591  0.57025044  0.57430806  0.57559912
  0.59533398]

  warnings.warn(

2022-11-03 10:49:48,596:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16361618 -0.15633406 -0.15542246 -0.14939513 -0.14905195 -0.14814035
 -0.1477757  -0.14213448 -0.13846658 -0.131399   -0.12063131 -0.11406775
 -0.10952045 -0.10482304 -0.10351458 -0.1027853  -0.09135274 -0.08734168
 -0.08407063 -0.08084235 -0.07985577 -0.07551226 -0.07403222 -0.07132961
 -0.06786551 -0.06277125 -0.05604665 -0.05255052 -0.03510119 -0.03216257
 -0.02527749 -0.014885   -0.00959768 -0.00940481 -0.00179026  0.00484863
  0.00602809  0.00803362  0.00919214  0.01170152  0.01718207  0.02274814
  0.02517182  0.02568675  0.0261802   0.02648027  0.02650191  0.02951549
  0.02956917  0.03145679  0.03183217  0.03515654  0.03743047  0.03778436
  0.03935011  0.04407954  0.05050391  0.05352839  0.05365687  0.05389305
  0.05571627  0.05693883  0.05847236  0.0593412   0.05939453  0.06151833
  0.06155018  0.06251546  0.06379189  0.06458541  0.06533616  0.06658021
  0.0671914   0.07230748  0.07371238  0.07525665  0.07570718  0.07698342
  0.07829152  0.08210989  0.08378298  0.08429757  0.08594936  0.08626035
  0.08794382  0.08854463  0.09018552  0.09289904  0.09324221  0.09449664
  0.09467896  0.09600924  0.09612715  0.098315    0.10133931  0.10168248
  0.10250804  0.10363418  0.10524304  0.10660499  0.11123799  0.11273986
  0.11433781  0.11560333  0.1205149   0.12198457  0.12281045  0.12386127
  0.12653186  0.13034951  0.1309183   0.13199059  0.13328848  0.13392131
  0.13457547  0.13486497  0.13563683  0.13660246  0.13769604  0.1378034
  0.13781395  0.13994867  0.14093522  0.14523597  0.1454183   0.14643707
  0.14673695  0.14739145  0.14793858  0.14843187  0.14953653  0.15219585
  0.15295769  0.15365442  0.15474835  0.15573547  0.15636794  0.15723662
  0.15745149  0.1578054   0.15974653  0.16037902  0.16128024  0.16383273
  0.16565594  0.16584847  0.16633138  0.16638524  0.16709305  0.16785401
  0.16821865  0.16966703  0.16988101  0.17003167  0.17140436  0.1733775
  0.17380672  0.1740422   0.17453602  0.17670241  0.17716312  0.17786074
  0.17880419  0.18185034  0.18274048  0.1828906   0.18315879  0.1838022
  0.18425254  0.18716985  0.18764168  0.18771682  0.18862824  0.18889646
  0.19260675  0.19316499  0.19615675  0.19748701  0.19795884  0.19875254
  0.20134744  0.20331057  0.20465069  0.20501551  0.2073644   0.20771795
  0.20819031  0.20828691  0.21080667  0.21478572  0.21553683  0.21719883
  0.21957956  0.22176777  0.22352674  0.22682947  0.22721613  0.22762353
  0.22783805  0.22896401  0.23001518  0.23064748  0.23071224  0.23073354
  0.23076592  0.23079813  0.23622447  0.23672885  0.23730803  0.23865888
  0.23898112  0.2395279   0.24008577  0.24030013  0.24168372  0.24301348
  0.24423605  0.24502938  0.24644555  0.24812884  0.24887978  0.24903006
  0.24927645  0.24957668  0.25164681  0.25370619  0.25517548  0.25631183
  0.25639808  0.25667647  0.2583392   0.25863924  0.26050522  0.26109548
  0.26231769  0.26370165  0.26544954  0.26687572  0.26719795  0.26765903
  0.26795892  0.2679591   0.26941749  0.26978213  0.27466222  0.27562732
  0.27969189  0.27985274  0.28270547  0.28307011  0.28398172  0.28496883
  0.2850973   0.28540845  0.28562261  0.28580493  0.28598725  0.28773586
  0.28844348  0.29035222  0.29183244  0.29381633  0.29508184  0.29509258
  0.29581113  0.29613318  0.29645507  0.29672273  0.29690505  0.29824588
  0.30090539  0.30181699  0.30254627  0.30291092  0.30294312  0.30309324
  0.30327556  0.30329721  0.30345788  0.30400484  0.30509877  0.30524943
  0.30533495  0.30546342  0.30749076  0.30946392  0.30998976  0.31004291
  0.31031147  0.31291729  0.31294932  0.31412948  0.31471903  0.31474051
  0.31589866  0.31777574  0.31868716  0.32057496  0.32074654  0.32223696
  0.32296625  0.32347064  0.32460715  0.32478946  0.3275461   0.32842533
  0.32914389  0.33214672  0.33260798  0.33318698  0.33325174  0.33484969
  0.33519251  0.33546071  0.33582572  0.33879636  0.33973981  0.3432472
  0.34455549  0.34671113  0.3474404   0.34786946  0.35123695  0.3523307
  0.3528673   0.35570947  0.35578426  0.35644913  0.35727521  0.35910914
  0.36021364  0.36048183  0.3609431   0.3618547   0.36233709  0.36370976
  0.36457862  0.36538287  0.36757091  0.36804253  0.36870778  0.37064871
  0.37131394  0.37567855  0.37607558  0.37957188  0.38243535  0.38720774
  0.38806567  0.39065037  0.39188368  0.39261295  0.39272049  0.39394306
  0.39589474  0.39607706  0.39702088  0.3970318   0.39751435  0.39803982
  0.39852256  0.39991671  0.39993801  0.40065673  0.40244792  0.40696302
  0.40811026  0.41069497  0.41075938  0.41097408  0.41288282  0.41485615
  0.4150281   0.41509233  0.41525317  0.41555358  0.41595008  0.41759097
  0.41796653  0.41892106  0.41925352  0.41958594  0.42146283  0.42253512
  0.42288902  0.42306078  0.42478755  0.42513072  0.42742576  0.4316297
  0.43390345  0.43423606  0.43529776  0.4357161   0.43787157  0.4390621
  0.43948025  0.43984507  0.44093899  0.44388816  0.44445659  0.44888599
  0.45004414  0.45104163  0.45126689  0.45245725  0.45265033  0.45481687
  0.45495625  0.45620031  0.45651127  0.45869912  0.46369696  0.46481233
  0.46574543  0.46761138  0.46942388  0.47176185  0.47594449  0.47851863
  0.47959092  0.48104949  0.48107115  0.4821434   0.4832266   0.48389166
  0.48422409  0.48468517  0.48596141  0.48601509  0.48616521  0.48637976
  0.48784905  0.4883316   0.48851393  0.48942553  0.4902407   0.49253574
  0.4931471   0.49429471  0.49430543  0.49503474  0.49581769  0.49603204
  0.49639668  0.49708302  0.4993675   0.50002164  0.50051509  0.50292804
  0.5040327   0.50419354  0.50468702  0.50474051  0.50546981  0.50586666
  0.50642434  0.507529    0.50921284  0.51221569  0.51303085  0.51736362
  0.51806071  0.51824301  0.51884366  0.51960514  0.52078478  0.52643675
  0.5288284   0.52883911  0.53109139  0.53156321  0.53254996  0.53522036
  0.53874887  0.53956387  0.54375726  0.5490875   0.55379565  0.55386007
  0.55656268  0.56126009  0.56622571  0.56783438  0.57003298  0.5705692
  0.57257474  0.57477334]

  warnings.warn(

2022-11-03 10:49:48,610:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.15737131 -0.15379265 -0.1424948  -0.13837058 -0.12220843 -0.11532388
 -0.10082011 -0.09311721 -0.09044599 -0.08191934 -0.07953175 -0.07713874
 -0.07319274 -0.06444986 -0.06077665 -0.05979637 -0.05887264 -0.05548306
 -0.05245537 -0.0523554  -0.04731287 -0.04547626 -0.0379516  -0.02839582
 -0.02812847 -0.02669945 -0.01572866 -0.01104803 -0.00585667  0.0008388
  0.00207014  0.00333177  0.00500871  0.00571937  0.00792329  0.0080744
  0.0112966   0.01383528  0.01692268  0.01928855  0.0276881   0.02856614
  0.0330631   0.04757772  0.05287132  0.05457539  0.05475905  0.05558054
  0.05579904  0.05677932  0.06080672  0.06338884  0.06412347  0.06493637
  0.06702948  0.06733481  0.06760987  0.0678935   0.06834767  0.07301202
  0.07495403  0.07654184  0.07776005  0.07972605  0.08697024  0.0869865
  0.08890681  0.09038152  0.09042722  0.09074883  0.09157802  0.09369285
  0.09441347  0.09854312  0.10121435  0.10359967  0.10488529  0.10516893
  0.10561134  0.10589498  0.10691642  0.10958991  0.11791119  0.11855443
  0.12092574  0.12178207  0.12268636  0.12351327  0.12390231  0.12591399
  0.12687029  0.12791798  0.13029786  0.13323644  0.13541322  0.13664774
  0.13811386  0.13822785  0.14235977  0.14290848  0.14331064  0.14364539
  0.14371594  0.14445601  0.14563309  0.14583846  0.14644371  0.14673506
  0.14691872  0.14775334  0.15549194  0.15683096  0.15749049  0.15751222
  0.15916516  0.1599532   0.16050418  0.1624299   0.16274384  0.16316229
  0.16445017  0.16476953  0.16568784  0.16647044  0.16789176  0.16797544
  0.16875578  0.16917197  0.17180203  0.17354409  0.17373635  0.174409
  0.17555893  0.17598596  0.17622932  0.17715306  0.17734214  0.18011018
  0.18019702  0.18023956  0.1830085   0.1831356   0.18329755  0.18362143
  0.18373455  0.18411588  0.1847614   0.1852961   0.18547977  0.18594164
  0.18638948  0.18730009  0.18743804  0.18787277  0.18834777  0.18853143
  0.18956373  0.18986047  0.19049515  0.19063312  0.19101673  0.19103302
  0.19125377  0.19185677  0.1937662   0.19487359  0.19505182  0.19562453
  0.19813834  0.20001609  0.20057793  0.20092898  0.20152566  0.20183641
  0.20248195  0.2024928   0.2025462   0.20263531  0.20871555  0.20893721
  0.21078693  0.21190746  0.21212912  0.2122051   0.2122888   0.21272353
  0.21273979  0.21289633  0.21772579  0.22600679  0.22741865  0.22902363
  0.22955289  0.22984738  0.23035722  0.23176226  0.23181879  0.2327077
  0.23368254  0.23568653  0.2356888   0.23709384  0.24122893  0.24449367
  0.24512062  0.24690069  0.24692468  0.24891009  0.2491743   0.24927427
  0.25032826  0.25048479  0.25119456  0.25422226  0.25477324  0.2558752
  0.25599146  0.25642618  0.2567935   0.25716082  0.25734448  0.26056443
  0.26173379  0.26190116  0.26195771  0.26197943  0.26459637  0.26496369
  0.26506139  0.26525275  0.26528531  0.26618734  0.26652209  0.26818362
  0.27055492  0.27170801  0.27194595  0.27332296  0.27343153  0.27368486
  0.27551919  0.27675053  0.2768962   0.27702647  0.27705046  0.27730152
  0.27873281  0.27894135  0.27910013  0.27955657  0.28059656  0.28143117
  0.28348944  0.28451541  0.28579876  0.28665509  0.28804611  0.2888536
  0.2893503   0.28950456  0.29176729  0.29250194  0.29350936  0.29413632
  0.29469275  0.29589694  0.29764759  0.29907346  0.29998091  0.3018632
  0.30197946  0.30227169  0.30241418  0.30278151  0.3029706   0.30374639
  0.30388347  0.30425079  0.30461811  0.30480177  0.30490718  0.30498544
  0.3066361   0.31020935  0.31308596  0.31407935  0.315792    0.31707221
  0.31709935  0.31747436  0.31750693  0.31839267  0.31877942  0.32404046
  0.32426981  0.3243915   0.32578795  0.32745718  0.32775708  0.32847545
  0.32951001  0.32955028  0.32990674  0.33391471  0.33492528  0.33584902
  0.33908435  0.33998952  0.34153391  0.34182612  0.34189354  0.34329769
  0.34432682  0.34649818  0.34769241  0.3487998   0.34954215  0.35001488
  0.35111684  0.35147873  0.35259698  0.35461182  0.35718307  0.35850127
  0.35880117  0.36018134  0.36181802  0.36202565  0.36311992  0.36402194
  0.36432184  0.36993391  0.37105215  0.37286163  0.37312127  0.3738125
  0.3747525   0.37491447  0.37644573  0.37763681  0.37779105  0.37811495
  0.37854741  0.37934402  0.38351938  0.38430829  0.38470503  0.38484843
  0.38559391  0.38659048  0.38689811  0.38721973  0.38810004  0.38847821
  0.38883468  0.39058217  0.39090924  0.39122228  0.39202203  0.39206231
  0.39240793  0.39350989  0.39369355  0.39395864  0.39461185  0.39699945
  0.39736676  0.39875779  0.40049443  0.40086717  0.40345471  0.40370577
  0.40496199  0.40756264  0.40882428  0.40929157  0.40930471  0.40948068
  0.41114131  0.41426128  0.41440152  0.41481453  0.41485252  0.41536551
  0.41573283  0.41575453  0.4159382   0.41709987  0.41814212  0.41972767
  0.41973852  0.42060255  0.42124808  0.42200986  0.42202071  0.4231064
  0.42327378  0.42895639  0.43130371  0.43428256  0.43493351  0.4354845
  0.43899575  0.44063784  0.44393289  0.4449566   0.44677149  0.44680405
  0.44837331  0.44911338  0.44965893  0.45009366  0.45050984  0.45130101
  0.45133359  0.45140101  0.45168462  0.45211935  0.45320504  0.45542525
  0.45724014  0.45912016  0.46040037  0.46052748  0.4608568   0.4615132
  0.4644192   0.4647191   0.46499733  0.46518097  0.46799786  0.46801415
  0.46838146  0.46938347  0.47175478  0.47179277  0.47554425  0.48121057
  0.48148336  0.48281782  0.48315802  0.48368728  0.48478926  0.48607488
  0.48625852  0.48644219  0.48656389  0.4891971   0.49242474  0.49429933
  0.49830731  0.49939841  0.5012296   0.50150236  0.50223702  0.50370629
  0.50574827  0.50591024  0.50601562  0.50609388  0.51013441  0.51365112
  0.51393476  0.51500416  0.51573879  0.5161061   0.51849914  0.523358
  0.52456536  0.52493267  0.52721486  0.52758762  0.53144991  0.53566326
  0.53567955  0.53881263  0.539431    0.5437769   0.54485171  0.54844125
  0.55366743  0.5562441   0.55725696  0.56560537  0.57230628  0.57680325
  0.58131109  0.60196475  0.60197019  0.61280616]

  warnings.warn(

2022-11-03 10:49:48,724:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.15609304 -0.15480984 -0.14555967 -0.14482642 -0.13739489 -0.13471505
 -0.12875005 -0.12618364 -0.1255638  -0.12473162 -0.11663673 -0.11345048
 -0.11048841 -0.10491907 -0.09087369 -0.08775733 -0.08297662 -0.07846363
 -0.07405223 -0.06828238 -0.05756569 -0.05737054 -0.05353543 -0.0505008
 -0.04343858 -0.04185596 -0.04039211 -0.03812245 -0.03735748 -0.02893949
 -0.0272488  -0.02717623 -0.01371248 -0.00891727 -0.00691264 -0.00441611
 -0.00361296  0.00096724  0.00246546  0.00365893  0.00989164  0.01145708
  0.01244888  0.01563245  0.01677856  0.01750649  0.02017715  0.02146036
  0.02182699  0.02329351  0.02540889  0.03196203  0.03290495  0.03475529
  0.03935002  0.04045525  0.04102887  0.04604826  0.04675519  0.04704009
  0.05208317  0.05699181  0.06033502  0.06061993  0.06072801  0.06087046
  0.0625031   0.0659651   0.0684681   0.07240214  0.07399658  0.07588512
  0.07619636  0.0783698   0.07939981  0.08131469  0.08257422  0.08286828
  0.08321124  0.08376768  0.08449445  0.08499818  0.08825967  0.08890051
  0.0899095   0.09073366  0.0907917   0.09128359  0.09147494  0.09477461
  0.09479829  0.09549984  0.09606429  0.09627665  0.09664328  0.09801624
  0.10128956  0.10201479  0.10219812  0.10420809  0.10607675  0.10816311
  0.10926301  0.10934092  0.10962964  0.11083646  0.11193636  0.11429958
  0.11434313  0.11450926  0.11580964  0.11598112  0.11625152  0.11714555
  0.11789333  0.11802127  0.12010763  0.12233911  0.12666343  0.12741121
  0.12760255  0.12959649  0.12985235  0.13194407  0.13221445  0.13368097
  0.13389981  0.13473465  0.13498786  0.13712692  0.13747905  0.14087761
  0.14103459  0.14124426  0.14365637  0.14609486  0.14758123  0.14893433
  0.14901606  0.14902788  0.14978102  0.15249256  0.15287751  0.15487565
  0.15581057  0.15599655  0.1563922   0.15789958  0.15861833  0.1600268
  0.16205778  0.1624099   0.16286628  0.16298239  0.16397534  0.16497251
  0.16533913  0.16710622  0.16752441  0.16845931  0.1687152   0.16898559
  0.16935756  0.17020425  0.17068812  0.1722589   0.17332443  0.17360932
  0.17455759  0.17635256  0.17858786  0.17917218  0.18215257  0.1827579
  0.18440392  0.18605758  0.18695046  0.18866752  0.1898373   0.1909372
  0.19278754  0.19385191  0.19398367  0.19558997  0.1985337   0.19880523
  0.20266555  0.20278278  0.20337627  0.20461213  0.20479545  0.20636088
  0.20826392  0.20837735  0.20878217  0.20894563  0.20950092  0.21002718
  0.2103159   0.21446227  0.21562555  0.21635882  0.21661202  0.21788722
  0.22034554  0.22052886  0.22122393  0.22144008  0.22472526  0.22517895
  0.22835871  0.22982523  0.23127189  0.23181916  0.234499    0.2351704
  0.23572035  0.23762338  0.23828676  0.23911893  0.24600051  0.24690638
  0.25131665  0.25288208  0.25462014  0.25696505  0.25728165  0.26046522
  0.26075662  0.26086356  0.26146086  0.26226668  0.26446377  0.26488198
  0.26671513  0.26717533  0.27086149  0.27168565  0.27196139  0.27196786
  0.27235169  0.27384189  0.27457782  0.27553525  0.2759481   0.27627655
  0.27642816  0.27688836  0.27816508  0.27931232  0.27953001  0.28025144
  0.28059171  0.28384518  0.28499128  0.28515473  0.28755233  0.28893981
  0.28903336  0.29067786  0.2938431   0.29419791  0.29474784  0.29506291
  0.29655465  0.29668907  0.29716645  0.29885334  0.29976992  0.30013655
  0.30031986  0.30062311  0.3006865   0.30072202  0.30215302  0.30270297
  0.30380286  0.30526938  0.30580481  0.30738858  0.30746648  0.30848465
  0.31061835  0.31143602  0.31352774  0.31620222  0.31628661  0.31699353
  0.3174499   0.31810144  0.31920134  0.31956796  0.3216463   0.32190485
  0.32286496  0.32468628  0.32523622  0.32592595  0.32594962  0.32752691
  0.32835258  0.3291966   0.32926648  0.32926916  0.33071583  0.33258067
  0.33277584  0.33342467  0.33440848  0.33536591  0.33654105  0.33741943
  0.33773716  0.33930796  0.34061484  0.34295441  0.34306782  0.34377742
  0.34448433  0.34559605  0.34595735  0.34722602  0.34818615  0.34909088
  0.34989404  0.35090685  0.35134604  0.35493978  0.35518114  0.35690622
  0.35746801  0.3578056   0.35898074  0.36207991  0.3624874   0.36288573
  0.36406469  0.36408454  0.36572902  0.36599408  0.36622359  0.36647948
  0.3668988   0.36903406  0.37020383  0.37028289  0.37158596  0.37200528
  0.37446095  0.37709458  0.37743752  0.37966099  0.38046415  0.38220104
  0.38232898  0.3829897   0.38318752  0.38509591  0.38513409  0.38601248
  0.38692906  0.38754623  0.38812518  0.38909446  0.39051096  0.39166089
  0.392621    0.39347036  0.39376443  0.39475089  0.39581259  0.39674367
  0.3982629   0.39903971  0.40132387  0.40143731  0.40383224  0.40657929
  0.40893604  0.410309    0.41039072  0.41079822  0.41110681  0.4124744
  0.4140452   0.4160735   0.41715887  0.4180491   0.41934683  0.4231911
  0.42357759  0.4248753   0.42665041  0.42701703  0.42723853  0.42851259
  0.42982214  0.42983664  0.43029036  0.43072422  0.43262725  0.4335293
  0.43453297  0.43708219  0.4385804   0.44049528  0.44389656  0.44417877
  0.44522329  0.44697204  0.44709732  0.44985887  0.45172374  0.45238174
  0.45369665  0.45520403  0.4565308   0.45975791  0.46210282  0.46222808
  0.46372096  0.46383437  0.46421553  0.46557129  0.46783827  0.46916503
  0.46987193  0.47003808  0.47261898  0.47373338  0.4750865   0.47540959
  0.47720186  0.47730346  0.4778217   0.47986989  0.48085634  0.48187183
  0.48198525  0.48232019  0.4825352   0.48256155  0.48366145  0.48400173
  0.48473498  0.48528493  0.48645472  0.48827334  0.48893673  0.48928618
  0.48931786  0.49012102  0.49370291  0.49540812  0.49625481  0.49672035
  0.49673488  0.49814335  0.49955449  0.50105005  0.50214993  0.50276976
  0.5054496   0.50563293  0.50643338  0.5067183   0.50778918  0.51075123
  0.51768819  0.51775807  0.51867465  0.51901492  0.51956487  0.52194798
  0.52396445  0.53018266  0.53608959  0.53633095  0.5377131   0.53905168
  0.54164711  0.54171699  0.54684983  0.55531134  0.55656554  0.5574122
  0.56537918  0.57108828  0.60387537]

  warnings.warn(

2022-11-03 10:49:48,730:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.15924555 -0.1588771  -0.152023   -0.1459058  -0.14321775 -0.14174392
 -0.14063856 -0.1402701  -0.1397886  -0.13942015 -0.13898051 -0.1382436
 -0.13695401 -0.13415291 -0.12619344 -0.11230101 -0.10507844 -0.09755861
 -0.08997595 -0.08587691 -0.08580573 -0.08440726 -0.07681207 -0.07437943
 -0.07117638 -0.06510522 -0.06232927 -0.05455403 -0.04994833 -0.04196372
 -0.03797354 -0.03082216 -0.02327718 -0.01999042 -0.01718932 -0.01619282
 -0.01612163 -0.01490322 -0.01265481 -0.00666319 -0.00589278 -0.00370301
  0.0025984   0.00271145  0.00692781  0.00901287  0.0112236   0.01273511
  0.01411268  0.02096678  0.02145663  0.02828559  0.03251865  0.03570499
  0.03736722  0.03843492  0.04546906  0.04608449  0.05059393  0.0509456
  0.05503628  0.05674464  0.05979269  0.06053804  0.06092321  0.06096507
  0.0631088   0.06366991  0.06646258  0.06701526  0.06830485  0.06850588
  0.07089249  0.07277244  0.07288965  0.074309    0.07840812  0.07881428
  0.07914505  0.07950088  0.07967258  0.08316454  0.08459225  0.08710453
  0.08717144  0.08725106  0.08921891  0.09016519  0.09425588  0.09655034
  0.09790275  0.10327886  0.10387342  0.10589574  0.10663264  0.10862148
  0.10894798  0.10942948  0.11045948  0.11075675  0.1114016   0.11259903
  0.11304712  0.11543789  0.11714197  0.11823899  0.11892561  0.12019005
  0.12074273  0.12262696  0.12409659  0.12551177  0.1261942   0.12843008
  0.13198487  0.13205597  0.13206442  0.13279296  0.1336303   0.13406577
  0.13953395  0.14032115  0.14279984  0.14426111  0.14484305  0.1452115
  0.14686955  0.14854858  0.15044115  0.1514125   0.15191498  0.15294916
  0.1529952   0.1548626   0.15781435  0.15829594  0.15959389  0.16194282
  0.16200973  0.16306905  0.16412003  0.16571526  0.16720588  0.16890569
  0.16927414  0.16957569  0.16960082  0.16980594  0.17006135  0.17033774
  0.17181157  0.17557982  0.17572221  0.17638375  0.17658051  0.17690284
  0.17800821  0.17870335  0.1787996   0.17911357  0.18175985  0.18231254
  0.18317084  0.18429291  0.1843809   0.18486231  0.18523076  0.18564117
  0.18987005  0.19207661  0.19450916  0.1963557   0.19691247  0.19838629
  0.19872132  0.20228866  0.20253145  0.20271568  0.20346931  0.20433605
  0.20503112  0.20505617  0.20947353  0.20976665  0.21062068  0.21087609
  0.21106032  0.21150841  0.21168429  0.2134511   0.21401641  0.21455655
  0.21629414  0.21659142  0.21736174  0.21909524  0.22174561  0.22193393
  0.22555573  0.22672811  0.22805112  0.2298934   0.23026185  0.23493873
  0.235236    0.23526115  0.23531144  0.23560453  0.23652986  0.23667223
  0.23766864  0.23819626  0.24052421  0.24056607  0.24377757  0.24533928
  0.24644047  0.24839579  0.24935035  0.25039298  0.25099168  0.25100839
  0.25465118  0.25713404  0.25749404  0.25769507  0.25789609  0.25877538
  0.26055055  0.26102796  0.26137544  0.26183605  0.26191133  0.26324279
  0.26388345  0.26644584  0.2688073   0.27230762  0.27257983  0.27805636
  0.27825322  0.27960564  0.28014986  0.28122591  0.28211781  0.28232718
  0.2825155   0.28258251  0.28306818  0.28510312  0.28630902  0.28817634
  0.28929016  0.28965017  0.29054626  0.29086858  0.29124957  0.29224614
  0.2934603   0.29364452  0.29394189  0.29456566  0.29530258  0.29586789
  0.29622371  0.29652942  0.29732908  0.29758876  0.29781485  0.29865635
  0.30197245  0.30215668  0.30252513  0.30270936  0.3036305   0.30384405
  0.30385249  0.30451406  0.30461865  0.30528855  0.30872614  0.30879731
  0.30886015  0.30930396  0.30987337  0.31002408  0.31152304  0.31292987
  0.31294239  0.31554673  0.31638414  0.31659762  0.32024032  0.32178115
  0.32383697  0.32449854  0.32482511  0.32613142  0.3264287   0.32673442
  0.32893252  0.33071623  0.3339528   0.33601689  0.33683759  0.33776292
  0.33779224  0.33869241  0.34050118  0.34283339  0.34612017  0.34634626
  0.34671471  0.3467859   0.34786193  0.34815502  0.35071325  0.3509101
  0.35461971  0.35685142  0.35740408  0.35770136  0.35987024  0.36059462
  0.36067414  0.36189256  0.36295181  0.3630733   0.36412418  0.36496995
  0.36697558  0.36713883  0.36729792  0.37022885  0.37056798  0.37058477
  0.37114163  0.37146823  0.37309278  0.37375849  0.37408928  0.37775703
  0.37792873  0.38137883  0.38173885  0.38175148  0.38313731  0.38394959
  0.38398727  0.38497959  0.38612681  0.38631529  0.38668794  0.38779331
  0.3880194   0.38838785  0.38909959  0.39003327  0.3906111   0.39110522
  0.39208911  0.39408629  0.39444221  0.39708006  0.39942471  0.40125863
  0.40509391  0.40620763  0.40694454  0.40716645  0.40943167  0.41210291
  0.41453556  0.41592565  0.41619361  0.41654117  0.4174832   0.41812804
  0.41994934  0.42192145  0.4226039   0.42458854  0.42772458  0.42815588
  0.42872108  0.42875042  0.43001068  0.43130027  0.43173992  0.43222141
  0.4327741   0.43387946  0.43415168  0.43504347  0.4358976   0.43680622
  0.4392849   0.43972452  0.44336306  0.44348863  0.44432188  0.44484941
  0.44507131  0.44647813  0.44664565  0.44680473  0.44728623  0.45071537
  0.45137691  0.45144811  0.45370068  0.45542575  0.45616684  0.45623801
  0.45959179  0.4618402   0.46227985  0.46235521  0.46437336  0.4671661
  0.46762244  0.4689539   0.47067477  0.47142003  0.47193924  0.47470265
  0.47576615  0.47893153  0.47897754  0.47934602  0.48022112  0.48104174
  0.48269978  0.48321062  0.48358324  0.48516594  0.48767395  0.48822661
  0.4887793   0.49054621  0.49062157  0.49109474  0.49286582  0.50103885
  0.5021484   0.50226144  0.50366407  0.50388596  0.50406603  0.50568222
  0.50764171  0.50775475  0.50844981  0.51128022  0.51190827  0.5132439
  0.5147889   0.51490194  0.51582308  0.51600732  0.51663117  0.51699965
  0.51783707  0.52027805  0.5209061   0.52104846  0.52175607  0.52194028
  0.52300378  0.5232969   0.52598911  0.53011331  0.53059897  0.53066181
  0.53118518  0.53866313  0.54216764  0.54319346  0.54529117  0.55041604
  0.55377399  0.55524782  0.5573455   0.5596651   0.5685122   0.5725652
  0.58089313  0.58984494  0.59135642  0.59846595]

  warnings.warn(

2022-11-03 10:49:48,761:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.1603132  -0.15882241 -0.15714526 -0.14729622 -0.14692353 -0.13712647
 -0.12283252 -0.11559237 -0.10880436 -0.10656818 -0.10039119 -0.09983214
 -0.09847572 -0.09546667 -0.09080794 -0.06333814 -0.06245837 -0.05905211
 -0.05561247 -0.05149124 -0.04651474 -0.04033481 -0.03998661 -0.03786325
 -0.03210502 -0.03126159 -0.02822211 -0.01489034 -0.00992947 -0.00804444
 -0.00565232 -0.00410955  0.00100131  0.00323751  0.00479211  0.01168407
  0.0118645   0.01513638  0.01588769  0.01644083  0.01830432  0.02491189
  0.02696766  0.0285624   0.02933229  0.03246685  0.03270517  0.03531406
  0.04045536  0.04135667  0.04141456  0.04333595  0.04359877  0.04380668
  0.05067709  0.0526965   0.05976509  0.06078896  0.0610332   0.06104207
  0.06210523  0.06216017  0.06218469  0.06571347  0.06644028  0.06724061
  0.06870985  0.06894226  0.07007894  0.07119703  0.0739403   0.07411399
  0.0755773   0.07679935  0.07817731  0.07886481  0.08003489  0.08110692
  0.08390216  0.08462305  0.089065    0.09173179  0.09200349  0.09249281
  0.0925076   0.09489972  0.09601485  0.09759396  0.09764299  0.09779002
  0.09825401  0.10337079  0.10581488  0.10929762  0.1102657   0.11062064
  0.11158281  0.11212328  0.11307953  0.11492443  0.11542855  0.11776572
  0.12404961  0.12439782  0.12534518  0.1258062   0.12610241  0.12828662
  0.12837578  0.12842101  0.12912118  0.12983236  0.13040407  0.13041593
  0.13115834  0.13206854  0.13321116  0.13536495  0.13623583  0.13906742
  0.14051595  0.14052187  0.14174391  0.14335346  0.14526594  0.14748059
  0.14763946  0.15157435  0.15645956  0.15701186  0.15763851  0.15834378
  0.15989329  0.16095266  0.163941    0.16621946  0.16754544  0.17080171
  0.17154118  0.17236602  0.17332228  0.17359694  0.17489547  0.1755124
  0.17791044  0.17885781  0.18070567  0.18081637  0.18249351  0.18318018
  0.18517804  0.18716998  0.18853612  0.18866078  0.18872247  0.18965422
  0.19271524  0.19364699  0.19542724  0.19651492  0.19725143  0.19760554
  0.19869616  0.19875786  0.19877265  0.19896789  0.19989964  0.20101477
  0.20155309  0.20211214  0.20232595  0.20337349  0.20457693  0.2047447
  0.2068503   0.20728089  0.20731429  0.20747908  0.20768698  0.20810277
  0.2093337   0.21014289  0.21086677  0.21157284  0.21237614  0.21493601
  0.21552252  0.21767631  0.21837649  0.2202607   0.22143075  0.22161711
  0.22538126  0.22631894  0.22789212  0.2293584   0.22986549  0.23325695
  0.23407798  0.23409658  0.23530678  0.23546187  0.23757636  0.23818147
  0.23854741  0.23911321  0.23973311  0.24082373  0.24235384  0.24248229
  0.24410451  0.24511272  0.24547655  0.24646027  0.24697326  0.24701931
  0.248418    0.25069432  0.25275895  0.25308264  0.25411241  0.25452821
  0.25458019  0.25527953  0.25571309  0.25878002  0.25910453  0.26063463
  0.26142017  0.26273349  0.26303857  0.26347001  0.26381144  0.26421541
  0.26505209  0.2660789   0.2669274   0.26885555  0.26893794  0.26905374
  0.27083778  0.27158319  0.27673418  0.27706675  0.27721675  0.2772776
  0.27752859  0.27773945  0.27872317  0.2789243   0.28086133  0.28265723
  0.28317104  0.28457943  0.28720905  0.28783568  0.28787498  0.28899984
  0.28913125  0.29019737  0.29055739  0.29118109  0.29174014  0.29211283
  0.29293767  0.29382632  0.29400379  0.29442847  0.29480116  0.29536022
  0.29544261  0.29610561  0.29778275  0.29791712  0.2979691   0.29820066
  0.29848209  0.2987145   0.29890084  0.29927354  0.29960018  0.29964624
  0.30003967  0.30014658  0.30076434  0.30095069  0.30230331  0.30475711
  0.30661088  0.30781139  0.31063705  0.31185612  0.31217684  0.31257111
  0.31399513  0.31493279  0.31567818  0.31594101  0.31605976  0.31718378
  0.31911487  0.32334004  0.32702689  0.32867065  0.32903743  0.33008791
  0.33448383  0.3347437   0.33745948  0.3383422   0.33982117  0.34085686
  0.34301953  0.3501253   0.35066577  0.35219669  0.35236741  0.35383667
  0.3542643   0.3554314   0.356473    0.35650048  0.35666907  0.35702317
  0.36056676  0.36067664  0.3626352   0.36296479  0.36325509  0.36930362
  0.37064526  0.37139953  0.37216647  0.37397503  0.37411828  0.37475085
  0.37517552  0.37663885  0.37826992  0.37852685  0.3806071   0.38174675
  0.38226272  0.38253145  0.38717835  0.38880351  0.39016586  0.39021191
  0.3909573   0.39202342  0.3920754   0.39251486  0.3931935   0.39356619
  0.39412524  0.39482459  0.3960162   0.39689597  0.39834367  0.39881144
  0.4004366   0.40079072  0.40287097  0.40456079  0.40523266  0.40526012
  0.40600256  0.40613398  0.40832708  0.40968349  0.4101876   0.41034354
  0.41263169  0.41279651  0.4156623   0.41798977  0.4183781   0.41857037
  0.41880867  0.42078204  0.4218942   0.42352531  0.42365079  0.42480523
  0.42599094  0.4263666   0.42777499  0.42791528  0.42881364  0.42889605
  0.43041431  0.43046332  0.43057318  0.43089391  0.43228074  0.43283979
  0.43917271  0.43925216  0.44141186  0.44258194  0.44306154  0.44324787
  0.44335775  0.44354114  0.44359902  0.44489754  0.44583226  0.44704839
  0.44753096  0.4481145   0.44971518  0.45163655  0.45208572  0.45341765
  0.45567537  0.45828428  0.46004972  0.46259777  0.46364233  0.46469959
  0.46525861  0.46784597  0.46895814  0.46928183  0.47101685  0.4764945
  0.47657689  0.47740173  0.47881013  0.47976639  0.47992824  0.48077756
  0.48080799  0.48338647  0.4835728   0.48431821  0.48445258  0.48543629
  0.48769996  0.48852777  0.48964584  0.48974981  0.49142694  0.49257547
  0.49560607  0.49592679  0.49659569  0.49717629  0.49736265  0.50015789
  0.50202138  0.50265985  0.5033837   0.50401923  0.50423896  0.50609951
  0.50684489  0.50710774  0.50822879  0.50920956  0.51227649  0.51240494
  0.51432337  0.51991087  0.52214709  0.52289247  0.52422439  0.52736484
  0.52747472  0.52877621  0.52901748  0.52904494  0.52970796  0.53032192
  0.53567112  0.53607127  0.53910484  0.54490913  0.5465098   0.55382643
  0.55792611  0.56146673  0.56314389  0.56484848  0.57059785  0.57390312
  0.5896604 ]

  warnings.warn(

2022-11-03 10:49:48,777:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.66415823e-01 -1.66226807e-01 -1.63202632e-01 -1.53374053e-01
 -1.49992117e-01 -1.48270752e-01 -1.47912990e-01 -1.46191633e-01
 -1.44490530e-01 -1.41304356e-01 -1.34108427e-01 -1.29032142e-01
 -1.16928681e-01 -1.15814872e-01 -1.10907340e-01 -1.10677808e-01
 -1.09206238e-01 -1.08072174e-01 -9.90063961e-02 -9.61577214e-02
 -8.63561585e-02 -7.93695001e-02 -7.55960320e-02 -7.48197297e-02
 -7.06885001e-02 -6.87983890e-02 -5.88820073e-02 -5.71876584e-02
 -4.61238101e-02 -4.55567782e-02 -4.23435879e-02 -3.04696598e-02
 -3.01996029e-02 -2.93355888e-02 -2.63114140e-02 -2.35571979e-02
 -1.90209284e-02 -1.81771837e-02 -1.63477538e-02 -1.38501846e-02
 -1.27497858e-02 -1.18047303e-02 -9.28015209e-03 -9.13166098e-03
 -8.37562087e-03 -2.06396114e-03  4.80977331e-04  3.67391266e-03
  4.29487888e-03  4.42995998e-03  5.03066403e-03  7.02883999e-03
  1.10250866e-02  1.15853716e-02  1.37050009e-02  1.55883579e-02
  1.59797988e-02  1.68911821e-02  1.74176969e-02  1.91728322e-02
  2.06106178e-02  2.55181497e-02  2.99531083e-02  3.33553141e-02
  3.51914980e-02  3.79051825e-02  3.86612443e-02  3.94914665e-02
  4.09225124e-02  4.92458490e-02  4.99882901e-02  5.74340136e-02
  5.86557820e-02  5.90474047e-02  5.91553787e-02  6.13154485e-02
  6.22808788e-02  6.24698726e-02  6.29962820e-02  6.48863931e-02
  6.48999012e-02  6.62433407e-02  7.03609641e-02  7.16638936e-02
  7.19473533e-02  7.60651585e-02  7.92512417e-02  7.92986326e-02
  8.13911547e-02  8.19311631e-02  8.39631238e-02  8.52794417e-02
  8.66092678e-02  8.66969652e-02  9.03827359e-02  9.06257910e-02
  9.36634595e-02  9.60057907e-02  9.95025532e-02  1.00089854e-01
  1.06975195e-01  1.14353272e-01  1.18876146e-01  1.19996688e-01
  1.22474100e-01  1.24748982e-01  1.25032561e-01  1.25160768e-01
  1.26389298e-01  1.27678705e-01  1.31161756e-01  1.31971934e-01
  1.32599759e-01  1.32937251e-01  1.34975861e-01  1.36548713e-01
  1.39842756e-01  1.40153450e-01  1.43589306e-01  1.43764820e-01
  1.45803521e-01  1.45891113e-01  1.46647160e-01  1.48260671e-01
  1.48422558e-01  1.49799753e-01  1.53633811e-01  1.54524940e-01
  1.58905971e-01  1.59128644e-01  1.61315829e-01  1.62787504e-01
  1.63091121e-01  1.63219553e-01  1.63975586e-01  1.66155940e-01
  1.71745224e-01  1.71940874e-01  1.73237148e-01  1.73986426e-01
  1.74843679e-01  1.76288128e-01  1.76396298e-01  1.77469590e-01
  1.78171485e-01  1.78272901e-01  1.78671180e-01  1.82282551e-01
  1.82518746e-01  1.84193044e-01  1.84388791e-01  1.84975889e-01
  1.85920945e-01  1.86508239e-01  1.87980020e-01  1.87993310e-01
  1.88925075e-01  1.89080299e-01  1.89492086e-01  1.91719718e-01
  1.92306990e-01  1.92711834e-01  1.92941479e-01  1.93204767e-01
  1.93420806e-01  1.94406386e-01  1.94595409e-01  1.94622207e-01
  1.96269376e-01  1.96424733e-01  1.96667759e-01  1.98206667e-01
  1.99860680e-01  2.01588603e-01  2.01622374e-01  2.01669743e-01
  2.01824995e-01  2.02155733e-01  2.02155853e-01  2.02931945e-01
  2.04423770e-01  2.04842430e-01  2.05139489e-01  2.05720036e-01
  2.07576377e-01  2.09331302e-01  2.09621741e-01  2.10465373e-01
  2.10647845e-01  2.11214659e-01  2.14394297e-01  2.15372905e-01
  2.16243764e-01  2.16270780e-01  2.18322897e-01  2.18518765e-01
  2.19625798e-01  2.20969161e-01  2.21286391e-01  2.21414507e-01
  2.21569942e-01  2.23871650e-01  2.24175590e-01  2.25572753e-01
  2.25620144e-01  2.26585462e-01  2.27422543e-01  2.27780207e-01
  2.28698337e-01  2.29157206e-01  2.29886448e-01  2.30669301e-01
  2.31000235e-01  2.31087947e-01  2.31600953e-01  2.31958708e-01
  2.32255675e-01  2.33686721e-01  2.35536413e-01  2.37102428e-01
  2.37170067e-01  2.37277935e-01  2.38479631e-01  2.38911708e-01
  2.39168046e-01  2.39222079e-01  2.39451702e-01  2.42833646e-01
  2.43454612e-01  2.43906938e-01  2.44568414e-01  2.45202888e-01
  2.49381396e-01  2.49847222e-01  2.51082611e-01  2.51453874e-01
  2.53080668e-01  2.53512626e-01  2.54383597e-01  2.55233991e-01
  2.55686407e-01  2.58271764e-01  2.59642144e-01  2.61329619e-01
  2.63374975e-01  2.63564081e-01  2.63753104e-01  2.64002884e-01
  2.66918903e-01  2.67648145e-01  2.67715566e-01  2.68052974e-01
  2.69868994e-01  2.70739755e-01  2.72521801e-01  2.72852659e-01
  2.74357880e-01  2.74438929e-01  2.75984688e-01  2.76389826e-01
  2.78968444e-01  2.80298375e-01  2.80460262e-01  2.80629113e-01
  2.85043712e-01  2.85091103e-01  2.86204890e-01  2.86407421e-01
  2.86697634e-01  2.87561641e-01  2.87737247e-01  2.87878878e-01
  2.88736335e-01  2.89924445e-01  2.90336021e-01  2.91146101e-01
  2.91483593e-01  2.93326622e-01  2.94602325e-01  2.94804856e-01
  2.95054545e-01  2.95830855e-01  2.96377624e-01  2.96755648e-01
  2.97511688e-01  2.98969939e-01  2.99962077e-01  3.00272778e-01
  3.00718124e-01  3.01285156e-01  3.01474172e-01  3.01852188e-01
  3.02230212e-01  3.02345129e-01  3.02419227e-01  3.02439595e-01
  3.02608235e-01  3.02986259e-01  3.03742299e-01  3.03762667e-01
  3.05160033e-01  3.09568081e-01  3.10472612e-01  3.11640461e-01
  3.12693370e-01  3.13206468e-01  3.13577632e-01  3.14070566e-01
  3.15690529e-01  3.15697388e-01  3.16352110e-01  3.16737001e-01
  3.17540116e-01  3.18316418e-01  3.18357041e-01  3.19430227e-01
  3.21320338e-01  3.22076378e-01  3.23662760e-01  3.24088160e-01
  3.26005197e-01  3.28496005e-01  3.30838434e-01  3.32296601e-01
  3.32391038e-01  3.32904136e-01  3.33936903e-01  3.35975498e-01
  3.36184769e-01  3.37818436e-01  3.38081754e-01  3.38560961e-01
  3.39715196e-01  3.40471244e-01  3.42314076e-01  3.43394213e-01
  3.44426966e-01  3.48382808e-01  3.48807997e-01  3.49510103e-01
  3.51960394e-01  3.55193854e-01  3.56368547e-01  3.57630727e-01
  3.58629815e-01  3.59109157e-01  3.59352092e-01  3.61829505e-01
  3.63550975e-01  3.64307008e-01  3.64435244e-01  3.66359125e-01
  3.72488404e-01  3.75573380e-01  3.76309150e-01  3.77146261e-01
  3.80487560e-01  3.82215679e-01  3.82411561e-01  3.82634235e-01
  3.84652567e-01  3.84659427e-01  3.86346916e-01  3.87109703e-01
  3.88399125e-01  3.88628552e-01  3.90883171e-01  3.91072194e-01
  3.92388512e-01  3.93900606e-01  3.95432949e-01  3.96357742e-01
  3.96472576e-01  3.96729110e-01  3.97491806e-01  3.98349179e-01
  3.99381917e-01  3.99496736e-01  4.00009834e-01  4.00630814e-01
  4.01899854e-01  4.03189155e-01  4.05187331e-01  4.05470911e-01
  4.05626043e-01  4.06611623e-01  4.08393760e-01  4.08542259e-01
  4.09318554e-01  4.09919363e-01  4.10074616e-01  4.11444966e-01
  4.11478631e-01  4.11937710e-01  4.11957973e-01  4.13753527e-01
  4.14124789e-01  4.14313797e-01  4.14880837e-01  4.15447981e-01
  4.17337972e-01  4.19174171e-01  4.19795123e-01  4.20213783e-01
  4.21017003e-01  4.27821386e-01  4.30454051e-01  4.32107952e-01
  4.35537083e-01  4.36961466e-01  4.38216997e-01  4.38696233e-01
  4.38844823e-01  4.40782212e-01  4.43360815e-01  4.49591434e-01
  4.49800719e-01  4.50455441e-01  4.53520154e-01  4.56125774e-01
  4.56355321e-01  4.57914694e-01  4.58123950e-01  4.58501966e-01
  4.60601362e-01  4.61424845e-01  4.63294694e-01  4.65994870e-01
  4.66264941e-01  4.67810687e-01  4.69721060e-01  4.70402918e-01
  4.70490630e-01  4.70747164e-01  4.71341219e-01  4.78685654e-01
  4.83343391e-01  4.85219994e-01  4.87272082e-01  4.87690623e-01
  4.88217138e-01  4.90330042e-01  4.91052305e-01  4.91335884e-01
  4.92240415e-01  4.94090002e-01  4.94278996e-01  4.94454510e-01
  4.95413074e-01  4.97168195e-01  5.00739161e-01  5.05268662e-01
  5.05612908e-01  5.06368970e-01  5.07118249e-01  5.07685288e-01
  5.09764422e-01  5.12221543e-01  5.14246735e-01  5.17162860e-01
  5.17783931e-01  5.17905384e-01  5.19093495e-01  5.26019360e-01
  5.26100408e-01  5.26647185e-01  5.29043549e-01  5.29232543e-01
  5.29306837e-01  5.30008867e-01  5.30042637e-01  5.31520961e-01
  5.36104495e-01  5.36293517e-01  5.36489265e-01  5.36806495e-01
  5.39121930e-01  5.41153756e-01  5.46641850e-01  5.47377621e-01
  5.53905298e-01  5.62923804e-01  5.64408883e-01  5.68357865e-01
  5.81028358e-01  5.82742954e-01  5.87083461e-01  5.93145319e-01]

  warnings.warn(

2022-11-03 10:49:48,808:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16532099 -0.16159331 -0.15881327 -0.15452645 -0.14988257 -0.14519156
 -0.14505231 -0.14388687 -0.14315705 -0.1401592  -0.13737915 -0.13700639
 -0.13630799 -0.13534465 -0.13296879 -0.12883693 -0.11358489 -0.1089253
 -0.09968467 -0.0993119  -0.073481   -0.07011252 -0.06751886 -0.06453672
 -0.05947081 -0.05357151 -0.04669102 -0.0457591  -0.04279268 -0.03261585
 -0.03032069 -0.02159207 -0.02047378 -0.01924335 -0.01647902 -0.01396606
 -0.01317339 -0.01191583 -0.00783111 -0.0041777  -0.00095063  0.00261066
  0.00335618  0.00481156  0.01080725  0.01174344  0.01730353  0.01930663
  0.02460393  0.02539231  0.03000904  0.0312823   0.03233349  0.03873552
  0.0391397   0.04117635  0.04437201  0.04460339  0.04769765  0.04812897
  0.04856458  0.05549646  0.0611351   0.06642813  0.06821556  0.06983231
  0.0702365   0.07416197  0.07453688  0.0746133   0.07548666  0.07902794
  0.0799263   0.08102888  0.08197224  0.08457232  0.08460161  0.08551353
  0.08561208  0.08640259  0.0876016   0.08888414  0.08908624  0.09745779
  0.09752491  0.10143898  0.10410476  0.10577078  0.10637278  0.10790169
  0.109532    0.11022828  0.11156652  0.11173078  0.11453081  0.11488358
  0.11637466  0.1164889   0.1169088   0.1173894   0.1211935   0.12231394
  0.12310446  0.1260259   0.12717348  0.12959647  0.1325429   0.13533865
  0.13596708  0.13736602  0.13886638  0.13987899  0.14128722  0.14332601
  0.1437909   0.1443679   0.14494062  0.14723579  0.14727362  0.1480663
  0.15075422  0.15167042  0.15258022  0.15264947  0.15358139  0.15381276
  0.15552165  0.15635288  0.15784394  0.15997485  0.16056543  0.1607875
  0.16159589  0.16216647  0.1626042   0.16288486  0.16441878  0.16533712
  0.16748161  0.16871703  0.16893484  0.169424    0.17011812  0.17035592
  0.17196197  0.17408359  0.17473272  0.17662156  0.17740708  0.17873889
  0.18041636  0.18070629  0.18084768  0.18153893  0.1820581   0.18236088
  0.185388    0.18553655  0.18692408  0.18776172  0.18889146  0.18897215
  0.18953773  0.19011043  0.19514494  0.19540773  0.19651747  0.19718873
  0.1973187   0.19756365  0.19822635  0.19846628  0.19891546  0.19913541
  0.20015517  0.20128704  0.20142843  0.2033751   0.20525465  0.20599162
  0.20616657  0.20848601  0.20926082  0.20928583  0.21082188  0.2117538
  0.21312776  0.21473594  0.21527938  0.21647837  0.21807942  0.21871927
  0.21975259  0.2205381   0.22101871  0.22232339  0.2239537   0.22661022
  0.22687513  0.22926671  0.22931384  0.23016293  0.23146333  0.23165186
  0.23252736  0.23444547  0.23495248  0.23908935  0.23916075  0.23934715
  0.24132595  0.24204864  0.24206007  0.24256064  0.24541068  0.24655113
  0.24746091  0.24768085  0.24795007  0.24830712  0.24927046  0.24938043
  0.24971536  0.249996    0.25024095  0.25036878  0.25117287  0.25303671
  0.25322309  0.2550955   0.25716856  0.2582483   0.25877604  0.2597308
  0.26007643  0.2607634   0.26122187  0.26348276  0.26408476  0.26423543
  0.26446895  0.26468889  0.2653066   0.26545014  0.26586575  0.2665413
  0.26986692  0.27050963  0.27068245  0.27068674  0.270781    0.27129087
  0.27200926  0.27500283  0.27546772  0.28014731  0.28075357  0.28093141
  0.2828038   0.28352506  0.28378072  0.28431773  0.28464337  0.28597019
  0.28623298  0.28741413  0.28764979  0.28784331  0.28985497  0.29011775
  0.29102038  0.29109039  0.29355836  0.29428103  0.29486305  0.29558571
  0.29616773  0.29635412  0.2965405   0.29840433  0.29844431  0.29859072
  0.2987771   0.29936051  0.30016248  0.30045456  0.30084946  0.30194562
  0.3032346   0.30656449  0.30920743  0.31027861  0.31079064  0.3114869
  0.31198535  0.31261877  0.31321147  0.31358423  0.31424694  0.31698413
  0.3174626   0.31979987  0.32012553  0.32039045  0.32187939  0.32241854
  0.32439235  0.32549494  0.32551493  0.3257463   0.33067297  0.33195339
  0.33261107  0.33455133  0.33487696  0.33696074  0.33840468  0.33919306
  0.33927589  0.34017426  0.34124757  0.34503595  0.34694905  0.34862649
  0.34942845  0.3504432   0.35095309  0.3521678   0.35239488  0.35295404
  0.3536703   0.35524418  0.3558776   0.35738011  0.36095711  0.3620754
  0.36280737  0.36586378  0.3658952   0.36665644  0.36783759  0.36886377
  0.36969714  0.37024487  0.37052337  0.37263214  0.37435673  0.37637767
  0.37639551  0.37755667  0.37901845  0.38011891  0.38055666  0.38080589
  0.38206344  0.38270115  0.38298178  0.38355665  0.38540264  0.38768208
  0.38804342  0.38836905  0.38867684  0.3888004   0.39029146  0.39122338
  0.39127051  0.39140976  0.39215529  0.39271659  0.39364637  0.39389988
  0.39476467  0.39663279  0.39889652  0.40014052  0.40023478  0.40064109
  0.40074893  0.40096459  0.40179083  0.40368395  0.40427023  0.40489223
  0.40592268  0.40664823  0.4093026   0.40933402  0.41037591  0.41045659
  0.42124685  0.42165532  0.42217877  0.42393479  0.42398405  0.42471602
  0.42564365  0.42856721  0.42965411  0.43299115  0.4382099   0.43824348
  0.43858266  0.44057218  0.44201399  0.44300875  0.4433658   0.44350507
  0.4440642   0.44661287  0.4470956   0.44800968  0.44845887  0.44852384
  0.44950076  0.45083686  0.45149028  0.45270069  0.45377188  0.45465666
  0.45534147  0.45610058  0.45824507  0.4582922   0.46185135  0.46312248
  0.46363663  0.46501774  0.46619673  0.46697583  0.47033072  0.47102698
  0.47261232  0.47270445  0.47495676  0.47595152  0.47850019  0.48008337
  0.48033045  0.48255137  0.48387176  0.48479009  0.4864654   0.48683817
  0.48697956  0.48725809  0.48772511  0.49078365  0.49258678  0.4933773
  0.49378364  0.49524114  0.49712069  0.49727566  0.49869247  0.50249441
  0.50328706  0.50454461  0.50718542  0.50734037  0.5080252   0.50890999
  0.51409732  0.5156041   0.51704805  0.51739152  0.51779357  0.51853909
  0.51870978  0.52396207  0.52448768  0.53766022  0.53865282  0.5430182
  0.5472579   0.5504107   0.55230596  0.55368919  0.55528811  0.55555303
  0.55626716  0.55752471  0.56291411  0.56384603  0.56428163  0.60042436
  0.60519177  0.60595303]

  warnings.warn(

2022-11-03 10:49:51,021:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16578022 -0.16332363 -0.15087291 -0.14971795 -0.1472931  -0.14688344
 -0.14423788 -0.14329304 -0.14028013 -0.13895736 -0.13125198 -0.13087405
 -0.12257004 -0.11672262 -0.10989862 -0.10861815 -0.10310635 -0.10009345
 -0.09899137 -0.09650305 -0.09405705 -0.09352187 -0.09292324 -0.08672961
 -0.08579535 -0.08556407 -0.08519671 -0.07123424 -0.05448874 -0.04179756
 -0.03823891 -0.03388066 -0.03239148 -0.02840059 -0.02387735 -0.0210943
 -0.00975764 -0.0072785  -0.00315435 -0.00180901  0.00344896  0.00458276
  0.00779521  0.00901505  0.00976035  0.0139282   0.01665555  0.01806436
  0.01957608  0.02089887  0.02740697  0.02753388  0.02781523  0.03083871
  0.03816615  0.03899747  0.04061216  0.04080113  0.04426601  0.04439152
  0.04981093  0.05118379  0.05134243  0.0541572   0.0547636   0.0553305
  0.05608637  0.05795771  0.0591853   0.06002578  0.06057153  0.06078167
  0.06171591  0.069104    0.07287278  0.07328245  0.07364205  0.07646317
  0.07700116  0.08024533  0.08044488  0.08083199  0.08591437  0.09079299
  0.0913599   0.09195077  0.09309515  0.09558206  0.09969563  0.10016876
  0.1002865   0.10236516  0.10307873  0.1051334   0.10591325  0.10625945
  0.11045906  0.11046962  0.11294737  0.11539336  0.11799519  0.11985457
  0.12260445  0.12437639  0.12463023  0.12525918  0.12559622  0.12744501
  0.12787582  0.12880867  0.12890104  0.12898848  0.13094937  0.13128642
  0.13269379  0.13383818  0.13528645  0.13532595  0.13815272  0.14136234
  0.14167048  0.14184886  0.14275139  0.14361938  0.14437525  0.14458818
  0.14569803  0.14803193  0.15400204  0.15469726  0.1559446   0.15600029
  0.15644945  0.1566081   0.15688169  0.15723705  0.15764672  0.15773275
  0.16018933  0.16026053  0.16402932  0.16403848  0.16437553  0.16509191
  0.16715998  0.16738985  0.16852083  0.17041051  0.17096049  0.17587364
  0.17621985  0.17646946  0.17851779  0.17927507  0.17945981  0.17980743
  0.18008242  0.18040465  0.18072337  0.18078259  0.18135231  0.18198266
  0.18228797  0.18418401  0.18455278  0.18794644  0.18804939  0.18834411
  0.18888986  0.19096852  0.1917258   0.19209951  0.19250282  0.19397225
  0.19643943  0.19721645  0.1990744   0.20021455  0.20210422  0.20288548
  0.20319997  0.20427806  0.20514887  0.20535264  0.20568828  0.20573057
  0.20592872  0.20601333  0.20609651  0.20715771  0.20776269  0.20799818
  0.20970808  0.21074669  0.2125729   0.21361433  0.21420098  0.21436598
  0.21831739  0.21926857  0.22145438  0.22190777  0.22210309  0.22312337
  0.22529861  0.22813948  0.22816346  0.22841588  0.23035844  0.23174469
  0.23455381  0.23480765  0.23499661  0.23512072  0.23516725  0.23550006
  0.23692861  0.23848407  0.2400233   0.24040124  0.24269281  0.24643763
  0.24794938  0.24822436  0.24929612  0.24980098  0.24982848  0.25059493
  0.2513508   0.25393711  0.25426218  0.25558495  0.25700009  0.25813249
  0.25832146  0.25907733  0.25916476  0.25919649  0.25939461  0.26023653
  0.26247875  0.26306045  0.26407791  0.26442131  0.26738417  0.26794824
  0.26851514  0.26957773  0.26967151  0.27011431  0.27077218  0.27095481
  0.27099428  0.27140254  0.27222469  0.27285083  0.2736067   0.27450147
  0.27492947  0.27664358  0.27684172  0.27902755  0.27963532  0.28021
  0.28039897  0.2806105   0.28074798  0.28086434  0.28248044  0.2832229
  0.28387725  0.28450618  0.28586846  0.28623723  0.28719123  0.2872152
  0.28797109  0.28853516  0.28869239  0.28944827  0.28969152  0.28989249
  0.29001517  0.29096001  0.29152691  0.29190485  0.2939835   0.29459553
  0.2967468   0.29724106  0.29730312  0.29740607  0.29989438  0.3023418
  0.30264853  0.3026866   0.30340297  0.30781129  0.30903887  0.30972352
  0.31080163  0.31128815  0.3115166   0.31212441  0.31231338  0.31250235
  0.31325962  0.31382512  0.31450836  0.31485457  0.31746065  0.31756641
  0.31831029  0.31869739  0.32158479  0.32261566  0.32509198  0.32541703
  0.32593245  0.32702395  0.32735042  0.32786444  0.32836648  0.32900883
  0.32956514  0.32976187  0.33170586  0.3318645   0.33196603  0.33412013
  0.33449948  0.33517849  0.33551553  0.3358843   0.33679742  0.33742777
  0.33867792  0.33982089  0.33983006  0.34081861  0.34313629  0.34788445
  0.34916211  0.34927987  0.35412957  0.3544666   0.35560042  0.35693237
  0.35884317  0.35886433  0.35938892  0.3605439   0.36060594  0.36107906
  0.3642612   0.36429153  0.36453337  0.3662235   0.36648652  0.3703794
  0.37084335  0.37245945  0.37452751  0.37534544  0.37842181  0.38035238
  0.38066826  0.38129722  0.38239153  0.38242045  0.38429181  0.38570836
  0.38639935  0.38733362  0.38739849  0.38752259  0.38790052  0.38793225
  0.38884535  0.38945738  0.39434658  0.39462017  0.39472451  0.39587032
  0.39773742  0.3996814   0.40000504  0.40100417  0.403178    0.40623041
  0.40625296  0.40748195  0.40830905  0.40959233  0.41229853  0.41280338
  0.41786319  0.41790693  0.42064625  0.42400395  0.42443617  0.42599941
  0.42697597  0.42803714  0.42874015  0.42895026  0.42974843  0.43033508
  0.43354753  0.43663447  0.437989    0.44155821  0.44167315  0.44206168
  0.44207224  0.44345707  0.44543277  0.45693724  0.45703244  0.45884808
  0.45939383  0.46109453  0.46222835  0.46446423  0.46467576  0.46586104
  0.46587161  0.46785507  0.47032222  0.47219077  0.47304183  0.47464733
  0.47559217  0.47615907  0.47634805  0.47748184  0.47844784  0.47936096
  0.47968743  0.48011682  0.48143962  0.48162857  0.48178721  0.48238445
  0.48247963  0.48297392  0.48446308  0.4855969   0.48788567  0.48918728
  0.49147605  0.49354411  0.49564393  0.49565449  0.49603242  0.49787981
  0.49841639  0.49919341  0.50106193  0.50128124  0.5036969   0.50632131
  0.50854663  0.51130711  0.51318619  0.51433057  0.51450899  0.51513935
  0.51589521  0.51991642  0.51998905  0.5204939   0.52095504  0.52115459
  0.52347508  0.52705489  0.52929078  0.52970043  0.53173677  0.53647156
  0.53724858  0.53794099  0.54067116  0.5417838   0.54398796  0.54797745
  0.55594582  0.56239187  0.56326267  0.56764066  0.58181325  0.61184856]

  warnings.warn(

2022-11-03 10:49:51,037:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16601159 -0.16257532 -0.15951041 -0.15798318 -0.14956251 -0.14806661
 -0.14405762 -0.14348491 -0.14253039 -0.1419159  -0.13598743 -0.13544605
 -0.12932668 -0.12757721 -0.11938922 -0.1096009  -0.10807366 -0.10443604
 -0.08857011 -0.08515473 -0.06601211 -0.06409262 -0.06351991 -0.05835505
 -0.05816415 -0.05472788 -0.05070845 -0.04512754 -0.04420727 -0.0429545
 -0.03943467 -0.03598795 -0.03123623 -0.02983434 -0.02332271 -0.01724511
 -0.01367015 -0.01344791 -0.01019211 -0.00805039 -0.00446789  0.00301915
  0.00318916  0.0048133   0.01315795  0.01350842  0.01368888  0.02248091
  0.02959659  0.03049134  0.03112382  0.03150563  0.03323421  0.03514325
  0.04003655  0.04208428  0.04264363  0.04862723  0.05311956  0.05378163
  0.05411124  0.05830067  0.05883915  0.06076907  0.06172069  0.06342085
  0.06418156  0.06742983  0.06799964  0.06830081  0.07089453  0.0736461
  0.0745484   0.07559402  0.07625609  0.07681836  0.07769686  0.07815471
  0.07834559  0.07937322  0.07968192  0.08009506  0.08063644  0.0814105
  0.08179233  0.08247992  0.08298996  0.08463208  0.0866798   0.08944356
  0.08999538  0.0912406   0.096055    0.0984114   0.09915415  0.10015044
  0.10029956  0.10378225  0.10397315  0.10531992  0.1058613   0.10833844
  0.1085189   0.10853978  0.10910958  0.10969564  0.11138246  0.11315743
  0.11697089  0.11794048  0.11896057  0.11981356  0.12550353  0.12558709
  0.12690543  0.12711723  0.12858471  0.13015371  0.13189736  0.13553498
  0.13627018  0.13764075  0.13875193  0.1402165   0.14080255  0.14173619
  0.14232979  0.14241335  0.14280561  0.14378854  0.14663412  0.14680877
  0.14709194  0.14719057  0.14733216  0.14746331  0.15497878  0.15911309
  0.16008096  0.16234338  0.16322652  0.16462088  0.16520694  0.16565605
  0.16906682  0.1694457   0.17217059  0.17291041  0.1730044   0.17418988
  0.17551578  0.17683292  0.17764704  0.17837934  0.17850466  0.17895203
  0.18002899  0.18036611  0.18353374  0.18478359  0.18637059  0.18683133
  0.18721313  0.18782761  0.18830634  0.18989623  0.19084032  0.19155343
  0.1926153   0.19268377  0.19428703  0.19576204  0.19596047  0.19612759
  0.19671828  0.19675833  0.19872713  0.20338484  0.20419024  0.20458249
  0.20496721  0.20586078  0.20817835  0.20989939  0.21018429  0.21314184
  0.21457509  0.2148884   0.21526268  0.21660364  0.21691815  0.21698545
  0.21725528  0.21808447  0.22045133  0.22081688  0.22157759  0.22166987
  0.22655734  0.22768651  0.22897353  0.22998781  0.23140596  0.23142685
  0.23151504  0.23189221  0.23200711  0.23367592  0.23508246  0.2352345
  0.23539116  0.23566852  0.23591627  0.23596677  0.23602944  0.23626675
  0.23818915  0.24194165  0.24203392  0.24211748  0.24220567  0.24301687
  0.2431869   0.24349269  0.24421744  0.24470831  0.24538373  0.24555376
  0.24615199  0.24795368  0.24832042  0.25017433  0.2514004   0.25190582
  0.25192671  0.25246517  0.25305587  0.25312144  0.25469625  0.25521094
  0.25618344  0.25626528  0.25792827  0.25824452  0.25830256  0.25937659
  0.25998644  0.26344824  0.26385965  0.26418344  0.26463834  0.26632224
  0.26632807  0.26690367  0.26821911  0.26935872  0.26937671  0.27307236
  0.27679645  0.27721422  0.27834749  0.27841014  0.279584    0.27974358
  0.28085184  0.28295179  0.28314269  0.28328428  0.28361676  0.2837154
  0.28557512  0.28622385  0.28924408  0.28983477  0.29309059  0.29328149
  0.29404511  0.29419715  0.29538143  0.29554101  0.29557234  0.29563791
  0.29595415  0.29614505  0.29625821  0.29633595  0.29671776  0.29690867
  0.29826126  0.29919951  0.29938579  0.30010473  0.30041341  0.30217624
  0.30326655  0.30374235  0.30458951  0.30548891  0.30587072  0.30594965
  0.31200926  0.31486991  0.31512639  0.3153637   0.31573044  0.31852841
  0.31912956  0.31960536  0.32006611  0.32244051  0.32279389  0.32347105
  0.32598122  0.32852101  0.33308644  0.33363073  0.33423186  0.334551
  0.33459278  0.33691786  0.33968452  0.34114619  0.34602904  0.34621704
  0.34892103  0.35091654  0.3518113   0.35215596  0.35293582  0.35532531
  0.35654965  0.3585034   0.3590732   0.36078087  0.36129847  0.36180852
  0.36330442  0.36370419  0.37012183  0.37107635  0.37187129  0.37209353
  0.37354013  0.37568938  0.37589073  0.37712553  0.37859763  0.37897652
  0.38055135  0.38064534  0.38415182  0.38506456  0.38701538  0.3879699
  0.38854262  0.38892442  0.38938517  0.39159999  0.39178798  0.3921698
  0.39259919  0.39312432  0.39347478  0.39351946  0.3936657   0.39426974
  0.394832    0.39768512  0.40263064  0.40425478  0.40438765  0.40457102
  0.40499462  0.4052824   0.40801483  0.40892002  0.40896935  0.40909006
  0.40935116  0.40939876  0.41008344  0.41030568  0.4112109   0.41183291
  0.4121625   0.41278743  0.41378664  0.41462626  0.41758672  0.41877104
  0.42079786  0.42115878  0.42309625  0.42648322  0.42717081  0.42999549
  0.43054151  0.43250567  0.43340797  0.43549747  0.43586593  0.43691735
  0.43870861  0.43950355  0.44085325  0.44093678  0.44096812  0.44275183
  0.44360945  0.4454767   0.44556317  0.44603899  0.4474693   0.44781979
  0.45162742  0.45393914  0.45448052  0.46266852  0.46433442  0.46519204
  0.46897878  0.470072    0.47423766  0.47814218  0.47863889  0.4788298
  0.47975298  0.47997521  0.48016612  0.48137712  0.48148156  0.48245697
  0.48265831  0.4833906   0.48358149  0.48361574  0.48423023  0.48456735
  0.48475827  0.48533098  0.48552187  0.48779183  0.48848233  0.48874635
  0.48886416  0.4891624   0.48946819  0.48991267  0.49122811  0.49218263
  0.49239443  0.49269267  0.49332806  0.49621542  0.49625719  0.49639297
  0.49903427  0.50152649  0.50232143  0.50251235  0.50308504  0.50534457
  0.51089123  0.51143261  0.51201576  0.51270626  0.51271671  0.51853782
  0.51963104  0.52293908  0.52309865  0.52559084  0.533057    0.53393838
  0.5377878   0.54010999  0.5406827   0.54483037  0.55014435  0.55881859
  0.55978356  0.55997445  0.56816245  0.57387911  0.57886351  0.58189708
  0.58479198]

  warnings.warn(

2022-11-03 10:49:51,037:INFO:Calculating mean and std
2022-11-03 10:49:51,037:INFO:Creating metrics dataframe
2022-11-03 10:49:51,037:INFO:Uploading results into container
2022-11-03 10:49:51,053:INFO:Uploading model into container now
2022-11-03 10:49:51,053:INFO:master_model_container: 7
2022-11-03 10:49:51,053:INFO:display_container: 2
2022-11-03 10:49:51,053:INFO:ElasticNet(random_state=4411)
2022-11-03 10:49:51,053:INFO:create_model() successfully completed......................................
2022-11-03 10:49:51,310:WARNING:create_model() for ElasticNet(random_state=4411) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:49:51,310:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:49:51,310:INFO:Initializing create_model()
2022-11-03 10:49:51,310:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:49:51,310:INFO:Checking exceptions
2022-11-03 10:49:51,310:INFO:Importing libraries
2022-11-03 10:49:51,310:INFO:Copying training dataset
2022-11-03 10:49:51,333:INFO:Defining folds
2022-11-03 10:49:51,333:INFO:Declaring metric variables
2022-11-03 10:49:51,334:INFO:Importing untrained model
2022-11-03 10:49:51,335:INFO:Elastic Net Imported successfully
2022-11-03 10:49:51,335:INFO:Starting cross validation
2022-11-03 10:49:51,338:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:49:55,295:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.15704637 -0.15612418 -0.13841816 -0.13804928 -0.13658168 -0.13602837
 -0.13602047 -0.13332592 -0.12250821 -0.12158603 -0.11814582 -0.11666241
 -0.11518691 -0.11230793 -0.11156227 -0.10702335 -0.10646213 -0.09239656
 -0.08950967 -0.08471429 -0.07307098 -0.07015162 -0.06590955 -0.050978
 -0.04829221 -0.04746578 -0.03652774 -0.03561346 -0.02996001 -0.01881381
 -0.01459546 -0.01450762 -0.01037005 -0.00931089 -0.00401839  0.00070496
  0.00249229  0.003295    0.00515519  0.00691088  0.00986273  0.01318177
  0.0220427   0.02597209  0.02705499  0.02739138  0.02756085  0.0317388
  0.0377761   0.03817746  0.04237123  0.0447531   0.04612495  0.04786398
  0.05750303  0.06099238  0.06670911  0.07465575  0.07613916  0.07756721
  0.07860097  0.07884161  0.07896194  0.07961268  0.07982791  0.08019678
  0.08035043  0.08273147  0.08492099  0.08549803  0.0865572   0.08659591
  0.08678033  0.08892949  0.09114357  0.0924338   0.09313285  0.09452674
  0.09990083  0.10079762  0.10301086  0.10392599  0.10467082  0.10522411
  0.10561758  0.10591526  0.10771842  0.1080723   0.11107077  0.11435109
  0.11470331  0.11654853  0.11706854  0.12254545  0.12378032  0.12386108
  0.12435112  0.12861693  0.1304692   0.13075024  0.13167243  0.13182356
  0.13362754  0.13588908  0.13969817  0.14173489  0.14352474  0.14373997
  0.14412551  0.14567303  0.14590493  0.14787838  0.14827097  0.1491782
  0.15041136  0.15082855  0.15269042  0.15317793  0.15434245  0.15538496
  0.15670683  0.15705113  0.15815153  0.15889551  0.15999591  0.16110876
  0.16250517  0.16339489  0.16398942  0.16726017  0.16821398  0.16830185
  0.16852666  0.16915199  0.16971321  0.17130282  0.17407562  0.1804106
  0.18139691  0.1815822   0.18268797  0.18385249  0.18660157  0.18746754
  0.18751583  0.18847841  0.18881649  0.18951219  0.19024993  0.19025952
  0.19040273  0.19043438  0.19053096  0.1922954   0.19281001  0.19339411
  0.1937322   0.194492    0.1946448   0.1952077   0.19553534  0.1956003
  0.19578306  0.19613611  0.19656912  0.19691427  0.19748338  0.19846969
  0.19883234  0.19934358  0.19940938  0.19984069  0.20066797  0.20197402
  0.20231209  0.20307273  0.20420309  0.2059613   0.20600792  0.20633017
  0.20688348  0.21001018  0.21040277  0.2107146   0.21092362  0.21130124
  0.21161307  0.21267931  0.21695386  0.21697758  0.22006642  0.22009014
  0.22046609  0.22132584  0.22169471  0.22348119  0.22495048  0.22739479
  0.22748263  0.22751509  0.22808591  0.22845479  0.22846893  0.23060393
  0.23406703  0.23501378  0.23718748  0.23749847  0.23883951  0.24255907
  0.24289547  0.24430853  0.24668081  0.24726575  0.24745895  0.25185297
  0.25417073  0.25490933  0.25838201  0.25929545  0.26073056  0.26078761
  0.26134883  0.26170189  0.26315198  0.26350502  0.2642199   0.26459584
  0.26477321  0.26530987  0.26661675  0.26858145  0.27048203  0.27116442
  0.27191008  0.27267862  0.27271901  0.27283227  0.27299213  0.27377733
  0.27409086  0.27559092  0.27608719  0.27627247  0.27701646  0.2779245
  0.27866225  0.28007363  0.28118026  0.28324863  0.28357795  0.28431571
  0.28468458  0.28494726  0.28504387  0.28521333  0.28576663  0.2860152
  0.2866651   0.28686536  0.28763559  0.28794035  0.288477    0.28850073
  0.29033804  0.29034594  0.29089135  0.29107578  0.29126023  0.29158788
  0.29218241  0.29272614  0.29341558  0.29348139  0.29378445  0.29396268
  0.2945239   0.29470042  0.29525374  0.29672924  0.2974749   0.29802031
  0.29818725  0.29838918  0.2989425   0.29912693  0.29925347  0.30004912
  0.30115407  0.30121818  0.30218866  0.30288772  0.30386611  0.30494816
  0.30608644  0.30736082  0.30964611  0.31139557  0.31230112  0.31242227
  0.31422625  0.31480414  0.31639829  0.31792209  0.32102588  0.32250138
  0.32265418  0.3230951   0.32323915  0.32371252  0.32428249  0.32541201
  0.32670307  0.32726513  0.32762525  0.32890926  0.33051215  0.33145017
  0.33327955  0.33347107  0.33398567  0.33709571  0.33870819  0.34019949
  0.34242857  0.34372755  0.34606196  0.34675143  0.34710363  0.34995013
  0.35019162  0.35024699  0.35103305  0.35242861  0.35332707  0.35412809
  0.35655825  0.35917995  0.36096021  0.36099352  0.36635718  0.36757623
  0.37162595  0.37186744  0.3723154   0.37345451  0.37359064  0.37420017
  0.37507407  0.3760129   0.37946102  0.38163472  0.38403242  0.38513904
  0.38565988  0.38588471  0.38802593  0.38855468  0.39039905  0.39100774
  0.39135372  0.39207481  0.39208356  0.39403952  0.39571612  0.39775284
  0.39861797  0.39866629  0.39970088  0.40316564  0.40377516  0.40407202
  0.40557916  0.40722245  0.40739189  0.40831408  0.40905974  0.40923627
  0.41065557  0.41198619  0.41233049  0.41235506  0.4127485   0.41322103
  0.41397544  0.41494511  0.41541056  0.41608421  0.41632485  0.4203579
  0.42104029  0.42217856  0.4224113   0.42362157  0.4284178   0.42913888
  0.43175349  0.43444805  0.43471241  0.43564252  0.43665338  0.43884207
  0.44026136  0.44042206  0.44247461  0.44280312  0.44318863  0.4444314
  0.44535358  0.44698105  0.45144004  0.45285143  0.45322822  0.45504885
  0.45707768  0.45763889  0.45822386  0.45888166  0.46006031  0.460149
  0.4620092   0.46560137  0.46671591  0.46710935  0.46946665  0.46970729
  0.47182395  0.47488738  0.47513591  0.47848037  0.47858488  0.47950706
  0.48064532  0.48136727  0.48227363  0.48245805  0.4826425   0.4829789
  0.48323537  0.48516843  0.48706025  0.4884321   0.48862444  0.48960285
  0.4948154   0.49617848  0.49647533  0.49717271  0.49846377  0.49905745
  0.50127073  0.50385284  0.50732468  0.50751703  0.50926564  0.51199974
  0.5129544   0.51313885  0.51473466  0.51528798  0.51694793  0.51863951
  0.52095727  0.52189525  0.52390826  0.53134988  0.53227207  0.53288161
  0.53527138  0.53557613  0.53779731  0.53847886  0.53871949  0.54074039
  0.54240825  0.54940896  0.54985777  0.55009051  0.55235998  0.55383549
  0.5564255   0.56024249  0.56330591  0.57025044  0.57430806  0.57559912
  0.59533398]

  warnings.warn(

2022-11-03 10:49:55,419:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.15924555 -0.1588771  -0.152023   -0.1459058  -0.14321775 -0.14174392
 -0.14063856 -0.1402701  -0.1397886  -0.13942015 -0.13898051 -0.1382436
 -0.13695401 -0.13415291 -0.12619344 -0.11230101 -0.10507844 -0.09755861
 -0.08997595 -0.08587691 -0.08580573 -0.08440726 -0.07681207 -0.07437943
 -0.07117638 -0.06510522 -0.06232927 -0.05455403 -0.04994833 -0.04196372
 -0.03797354 -0.03082216 -0.02327718 -0.01999042 -0.01718932 -0.01619282
 -0.01612163 -0.01490322 -0.01265481 -0.00666319 -0.00589278 -0.00370301
  0.0025984   0.00271145  0.00692781  0.00901287  0.0112236   0.01273511
  0.01411268  0.02096678  0.02145663  0.02828559  0.03251865  0.03570499
  0.03736722  0.03843492  0.04546906  0.04608449  0.05059393  0.0509456
  0.05503628  0.05674464  0.05979269  0.06053804  0.06092321  0.06096507
  0.0631088   0.06366991  0.06646258  0.06701526  0.06830485  0.06850588
  0.07089249  0.07277244  0.07288965  0.074309    0.07840812  0.07881428
  0.07914505  0.07950088  0.07967258  0.08316454  0.08459225  0.08710453
  0.08717144  0.08725106  0.08921891  0.09016519  0.09425588  0.09655034
  0.09790275  0.10327886  0.10387342  0.10589574  0.10663264  0.10862148
  0.10894798  0.10942948  0.11045948  0.11075675  0.1114016   0.11259903
  0.11304712  0.11543789  0.11714197  0.11823899  0.11892561  0.12019005
  0.12074273  0.12262696  0.12409659  0.12551177  0.1261942   0.12843008
  0.13198487  0.13205597  0.13206442  0.13279296  0.1336303   0.13406577
  0.13953395  0.14032115  0.14279984  0.14426111  0.14484305  0.1452115
  0.14686955  0.14854858  0.15044115  0.1514125   0.15191498  0.15294916
  0.1529952   0.1548626   0.15781435  0.15829594  0.15959389  0.16194282
  0.16200973  0.16306905  0.16412003  0.16571526  0.16720588  0.16890569
  0.16927414  0.16957569  0.16960082  0.16980594  0.17006135  0.17033774
  0.17181157  0.17557982  0.17572221  0.17638375  0.17658051  0.17690284
  0.17800821  0.17870335  0.1787996   0.17911357  0.18175985  0.18231254
  0.18317084  0.18429291  0.1843809   0.18486231  0.18523076  0.18564117
  0.18987005  0.19207661  0.19450916  0.1963557   0.19691247  0.19838629
  0.19872132  0.20228866  0.20253145  0.20271568  0.20346931  0.20433605
  0.20503112  0.20505617  0.20947353  0.20976665  0.21062068  0.21087609
  0.21106032  0.21150841  0.21168429  0.2134511   0.21401641  0.21455655
  0.21629414  0.21659142  0.21736174  0.21909524  0.22174561  0.22193393
  0.22555573  0.22672811  0.22805112  0.2298934   0.23026185  0.23493873
  0.235236    0.23526115  0.23531144  0.23560453  0.23652986  0.23667223
  0.23766864  0.23819626  0.24052421  0.24056607  0.24377757  0.24533928
  0.24644047  0.24839579  0.24935035  0.25039298  0.25099168  0.25100839
  0.25465118  0.25713404  0.25749404  0.25769507  0.25789609  0.25877538
  0.26055055  0.26102796  0.26137544  0.26183605  0.26191133  0.26324279
  0.26388345  0.26644584  0.2688073   0.27230762  0.27257983  0.27805636
  0.27825322  0.27960564  0.28014986  0.28122591  0.28211781  0.28232718
  0.2825155   0.28258251  0.28306818  0.28510312  0.28630902  0.28817634
  0.28929016  0.28965017  0.29054626  0.29086858  0.29124957  0.29224614
  0.2934603   0.29364452  0.29394189  0.29456566  0.29530258  0.29586789
  0.29622371  0.29652942  0.29732908  0.29758876  0.29781485  0.29865635
  0.30197245  0.30215668  0.30252513  0.30270936  0.3036305   0.30384405
  0.30385249  0.30451406  0.30461865  0.30528855  0.30872614  0.30879731
  0.30886015  0.30930396  0.30987337  0.31002408  0.31152304  0.31292987
  0.31294239  0.31554673  0.31638414  0.31659762  0.32024032  0.32178115
  0.32383697  0.32449854  0.32482511  0.32613142  0.3264287   0.32673442
  0.32893252  0.33071623  0.3339528   0.33601689  0.33683759  0.33776292
  0.33779224  0.33869241  0.34050118  0.34283339  0.34612017  0.34634626
  0.34671471  0.3467859   0.34786193  0.34815502  0.35071325  0.3509101
  0.35461971  0.35685142  0.35740408  0.35770136  0.35987024  0.36059462
  0.36067414  0.36189256  0.36295181  0.3630733   0.36412418  0.36496995
  0.36697558  0.36713883  0.36729792  0.37022885  0.37056798  0.37058477
  0.37114163  0.37146823  0.37309278  0.37375849  0.37408928  0.37775703
  0.37792873  0.38137883  0.38173885  0.38175148  0.38313731  0.38394959
  0.38398727  0.38497959  0.38612681  0.38631529  0.38668794  0.38779331
  0.3880194   0.38838785  0.38909959  0.39003327  0.3906111   0.39110522
  0.39208911  0.39408629  0.39444221  0.39708006  0.39942471  0.40125863
  0.40509391  0.40620763  0.40694454  0.40716645  0.40943167  0.41210291
  0.41453556  0.41592565  0.41619361  0.41654117  0.4174832   0.41812804
  0.41994934  0.42192145  0.4226039   0.42458854  0.42772458  0.42815588
  0.42872108  0.42875042  0.43001068  0.43130027  0.43173992  0.43222141
  0.4327741   0.43387946  0.43415168  0.43504347  0.4358976   0.43680622
  0.4392849   0.43972452  0.44336306  0.44348863  0.44432188  0.44484941
  0.44507131  0.44647813  0.44664565  0.44680473  0.44728623  0.45071537
  0.45137691  0.45144811  0.45370068  0.45542575  0.45616684  0.45623801
  0.45959179  0.4618402   0.46227985  0.46235521  0.46437336  0.4671661
  0.46762244  0.4689539   0.47067477  0.47142003  0.47193924  0.47470265
  0.47576615  0.47893153  0.47897754  0.47934602  0.48022112  0.48104174
  0.48269978  0.48321062  0.48358324  0.48516594  0.48767395  0.48822661
  0.4887793   0.49054621  0.49062157  0.49109474  0.49286582  0.50103885
  0.5021484   0.50226144  0.50366407  0.50388596  0.50406603  0.50568222
  0.50764171  0.50775475  0.50844981  0.51128022  0.51190827  0.5132439
  0.5147889   0.51490194  0.51582308  0.51600732  0.51663117  0.51699965
  0.51783707  0.52027805  0.5209061   0.52104846  0.52175607  0.52194028
  0.52300378  0.5232969   0.52598911  0.53011331  0.53059897  0.53066181
  0.53118518  0.53866313  0.54216764  0.54319346  0.54529117  0.55041604
  0.55377399  0.55524782  0.5573455   0.5596651   0.5685122   0.5725652
  0.58089313  0.58984494  0.59135642  0.59846595]

  warnings.warn(

2022-11-03 10:49:55,419:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.15737131 -0.15379265 -0.1424948  -0.13837058 -0.12220843 -0.11532388
 -0.10082011 -0.09311721 -0.09044599 -0.08191934 -0.07953175 -0.07713874
 -0.07319274 -0.06444986 -0.06077665 -0.05979637 -0.05887264 -0.05548306
 -0.05245537 -0.0523554  -0.04731287 -0.04547626 -0.0379516  -0.02839582
 -0.02812847 -0.02669945 -0.01572866 -0.01104803 -0.00585667  0.0008388
  0.00207014  0.00333177  0.00500871  0.00571937  0.00792329  0.0080744
  0.0112966   0.01383528  0.01692268  0.01928855  0.0276881   0.02856614
  0.0330631   0.04757772  0.05287132  0.05457539  0.05475905  0.05558054
  0.05579904  0.05677932  0.06080672  0.06338884  0.06412347  0.06493637
  0.06702948  0.06733481  0.06760987  0.0678935   0.06834767  0.07301202
  0.07495403  0.07654184  0.07776005  0.07972605  0.08697024  0.0869865
  0.08890681  0.09038152  0.09042722  0.09074883  0.09157802  0.09369285
  0.09441347  0.09854312  0.10121435  0.10359967  0.10488529  0.10516893
  0.10561134  0.10589498  0.10691642  0.10958991  0.11791119  0.11855443
  0.12092574  0.12178207  0.12268636  0.12351327  0.12390231  0.12591399
  0.12687029  0.12791798  0.13029786  0.13323644  0.13541322  0.13664774
  0.13811386  0.13822785  0.14235977  0.14290848  0.14331064  0.14364539
  0.14371594  0.14445601  0.14563309  0.14583846  0.14644371  0.14673506
  0.14691872  0.14775334  0.15549194  0.15683096  0.15749049  0.15751222
  0.15916516  0.1599532   0.16050418  0.1624299   0.16274384  0.16316229
  0.16445017  0.16476953  0.16568784  0.16647044  0.16789176  0.16797544
  0.16875578  0.16917197  0.17180203  0.17354409  0.17373635  0.174409
  0.17555893  0.17598596  0.17622932  0.17715306  0.17734214  0.18011018
  0.18019702  0.18023956  0.1830085   0.1831356   0.18329755  0.18362143
  0.18373455  0.18411588  0.1847614   0.1852961   0.18547977  0.18594164
  0.18638948  0.18730009  0.18743804  0.18787277  0.18834777  0.18853143
  0.18956373  0.18986047  0.19049515  0.19063312  0.19101673  0.19103302
  0.19125377  0.19185677  0.1937662   0.19487359  0.19505182  0.19562453
  0.19813834  0.20001609  0.20057793  0.20092898  0.20152566  0.20183641
  0.20248195  0.2024928   0.2025462   0.20263531  0.20871555  0.20893721
  0.21078693  0.21190746  0.21212912  0.2122051   0.2122888   0.21272353
  0.21273979  0.21289633  0.21772579  0.22600679  0.22741865  0.22902363
  0.22955289  0.22984738  0.23035722  0.23176226  0.23181879  0.2327077
  0.23368254  0.23568653  0.2356888   0.23709384  0.24122893  0.24449367
  0.24512062  0.24690069  0.24692468  0.24891009  0.2491743   0.24927427
  0.25032826  0.25048479  0.25119456  0.25422226  0.25477324  0.2558752
  0.25599146  0.25642618  0.2567935   0.25716082  0.25734448  0.26056443
  0.26173379  0.26190116  0.26195771  0.26197943  0.26459637  0.26496369
  0.26506139  0.26525275  0.26528531  0.26618734  0.26652209  0.26818362
  0.27055492  0.27170801  0.27194595  0.27332296  0.27343153  0.27368486
  0.27551919  0.27675053  0.2768962   0.27702647  0.27705046  0.27730152
  0.27873281  0.27894135  0.27910013  0.27955657  0.28059656  0.28143117
  0.28348944  0.28451541  0.28579876  0.28665509  0.28804611  0.2888536
  0.2893503   0.28950456  0.29176729  0.29250194  0.29350936  0.29413632
  0.29469275  0.29589694  0.29764759  0.29907346  0.29998091  0.3018632
  0.30197946  0.30227169  0.30241418  0.30278151  0.3029706   0.30374639
  0.30388347  0.30425079  0.30461811  0.30480177  0.30490718  0.30498544
  0.3066361   0.31020935  0.31308596  0.31407935  0.315792    0.31707221
  0.31709935  0.31747436  0.31750693  0.31839267  0.31877942  0.32404046
  0.32426981  0.3243915   0.32578795  0.32745718  0.32775708  0.32847545
  0.32951001  0.32955028  0.32990674  0.33391471  0.33492528  0.33584902
  0.33908435  0.33998952  0.34153391  0.34182612  0.34189354  0.34329769
  0.34432682  0.34649818  0.34769241  0.3487998   0.34954215  0.35001488
  0.35111684  0.35147873  0.35259698  0.35461182  0.35718307  0.35850127
  0.35880117  0.36018134  0.36181802  0.36202565  0.36311992  0.36402194
  0.36432184  0.36993391  0.37105215  0.37286163  0.37312127  0.3738125
  0.3747525   0.37491447  0.37644573  0.37763681  0.37779105  0.37811495
  0.37854741  0.37934402  0.38351938  0.38430829  0.38470503  0.38484843
  0.38559391  0.38659048  0.38689811  0.38721973  0.38810004  0.38847821
  0.38883468  0.39058217  0.39090924  0.39122228  0.39202203  0.39206231
  0.39240793  0.39350989  0.39369355  0.39395864  0.39461185  0.39699945
  0.39736676  0.39875779  0.40049443  0.40086717  0.40345471  0.40370577
  0.40496199  0.40756264  0.40882428  0.40929157  0.40930471  0.40948068
  0.41114131  0.41426128  0.41440152  0.41481453  0.41485252  0.41536551
  0.41573283  0.41575453  0.4159382   0.41709987  0.41814212  0.41972767
  0.41973852  0.42060255  0.42124808  0.42200986  0.42202071  0.4231064
  0.42327378  0.42895639  0.43130371  0.43428256  0.43493351  0.4354845
  0.43899575  0.44063784  0.44393289  0.4449566   0.44677149  0.44680405
  0.44837331  0.44911338  0.44965893  0.45009366  0.45050984  0.45130101
  0.45133359  0.45140101  0.45168462  0.45211935  0.45320504  0.45542525
  0.45724014  0.45912016  0.46040037  0.46052748  0.4608568   0.4615132
  0.4644192   0.4647191   0.46499733  0.46518097  0.46799786  0.46801415
  0.46838146  0.46938347  0.47175478  0.47179277  0.47554425  0.48121057
  0.48148336  0.48281782  0.48315802  0.48368728  0.48478926  0.48607488
  0.48625852  0.48644219  0.48656389  0.4891971   0.49242474  0.49429933
  0.49830731  0.49939841  0.5012296   0.50150236  0.50223702  0.50370629
  0.50574827  0.50591024  0.50601562  0.50609388  0.51013441  0.51365112
  0.51393476  0.51500416  0.51573879  0.5161061   0.51849914  0.523358
  0.52456536  0.52493267  0.52721486  0.52758762  0.53144991  0.53566326
  0.53567955  0.53881263  0.539431    0.5437769   0.54485171  0.54844125
  0.55366743  0.5562441   0.55725696  0.56560537  0.57230628  0.57680325
  0.58131109  0.60196475  0.60197019  0.61280616]

  warnings.warn(

2022-11-03 10:49:55,445:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.15609304 -0.15480984 -0.14555967 -0.14482642 -0.13739489 -0.13471505
 -0.12875005 -0.12618364 -0.1255638  -0.12473162 -0.11663673 -0.11345048
 -0.11048841 -0.10491907 -0.09087369 -0.08775733 -0.08297662 -0.07846363
 -0.07405223 -0.06828238 -0.05756569 -0.05737054 -0.05353543 -0.0505008
 -0.04343858 -0.04185596 -0.04039211 -0.03812245 -0.03735748 -0.02893949
 -0.0272488  -0.02717623 -0.01371248 -0.00891727 -0.00691264 -0.00441611
 -0.00361296  0.00096724  0.00246546  0.00365893  0.00989164  0.01145708
  0.01244888  0.01563245  0.01677856  0.01750649  0.02017715  0.02146036
  0.02182699  0.02329351  0.02540889  0.03196203  0.03290495  0.03475529
  0.03935002  0.04045525  0.04102887  0.04604826  0.04675519  0.04704009
  0.05208317  0.05699181  0.06033502  0.06061993  0.06072801  0.06087046
  0.0625031   0.0659651   0.0684681   0.07240214  0.07399658  0.07588512
  0.07619636  0.0783698   0.07939981  0.08131469  0.08257422  0.08286828
  0.08321124  0.08376768  0.08449445  0.08499818  0.08825967  0.08890051
  0.0899095   0.09073366  0.0907917   0.09128359  0.09147494  0.09477461
  0.09479829  0.09549984  0.09606429  0.09627665  0.09664328  0.09801624
  0.10128956  0.10201479  0.10219812  0.10420809  0.10607675  0.10816311
  0.10926301  0.10934092  0.10962964  0.11083646  0.11193636  0.11429958
  0.11434313  0.11450926  0.11580964  0.11598112  0.11625152  0.11714555
  0.11789333  0.11802127  0.12010763  0.12233911  0.12666343  0.12741121
  0.12760255  0.12959649  0.12985235  0.13194407  0.13221445  0.13368097
  0.13389981  0.13473465  0.13498786  0.13712692  0.13747905  0.14087761
  0.14103459  0.14124426  0.14365637  0.14609486  0.14758123  0.14893433
  0.14901606  0.14902788  0.14978102  0.15249256  0.15287751  0.15487565
  0.15581057  0.15599655  0.1563922   0.15789958  0.15861833  0.1600268
  0.16205778  0.1624099   0.16286628  0.16298239  0.16397534  0.16497251
  0.16533913  0.16710622  0.16752441  0.16845931  0.1687152   0.16898559
  0.16935756  0.17020425  0.17068812  0.1722589   0.17332443  0.17360932
  0.17455759  0.17635256  0.17858786  0.17917218  0.18215257  0.1827579
  0.18440392  0.18605758  0.18695046  0.18866752  0.1898373   0.1909372
  0.19278754  0.19385191  0.19398367  0.19558997  0.1985337   0.19880523
  0.20266555  0.20278278  0.20337627  0.20461213  0.20479545  0.20636088
  0.20826392  0.20837735  0.20878217  0.20894563  0.20950092  0.21002718
  0.2103159   0.21446227  0.21562555  0.21635882  0.21661202  0.21788722
  0.22034554  0.22052886  0.22122393  0.22144008  0.22472526  0.22517895
  0.22835871  0.22982523  0.23127189  0.23181916  0.234499    0.2351704
  0.23572035  0.23762338  0.23828676  0.23911893  0.24600051  0.24690638
  0.25131665  0.25288208  0.25462014  0.25696505  0.25728165  0.26046522
  0.26075662  0.26086356  0.26146086  0.26226668  0.26446377  0.26488198
  0.26671513  0.26717533  0.27086149  0.27168565  0.27196139  0.27196786
  0.27235169  0.27384189  0.27457782  0.27553525  0.2759481   0.27627655
  0.27642816  0.27688836  0.27816508  0.27931232  0.27953001  0.28025144
  0.28059171  0.28384518  0.28499128  0.28515473  0.28755233  0.28893981
  0.28903336  0.29067786  0.2938431   0.29419791  0.29474784  0.29506291
  0.29655465  0.29668907  0.29716645  0.29885334  0.29976992  0.30013655
  0.30031986  0.30062311  0.3006865   0.30072202  0.30215302  0.30270297
  0.30380286  0.30526938  0.30580481  0.30738858  0.30746648  0.30848465
  0.31061835  0.31143602  0.31352774  0.31620222  0.31628661  0.31699353
  0.3174499   0.31810144  0.31920134  0.31956796  0.3216463   0.32190485
  0.32286496  0.32468628  0.32523622  0.32592595  0.32594962  0.32752691
  0.32835258  0.3291966   0.32926648  0.32926916  0.33071583  0.33258067
  0.33277584  0.33342467  0.33440848  0.33536591  0.33654105  0.33741943
  0.33773716  0.33930796  0.34061484  0.34295441  0.34306782  0.34377742
  0.34448433  0.34559605  0.34595735  0.34722602  0.34818615  0.34909088
  0.34989404  0.35090685  0.35134604  0.35493978  0.35518114  0.35690622
  0.35746801  0.3578056   0.35898074  0.36207991  0.3624874   0.36288573
  0.36406469  0.36408454  0.36572902  0.36599408  0.36622359  0.36647948
  0.3668988   0.36903406  0.37020383  0.37028289  0.37158596  0.37200528
  0.37446095  0.37709458  0.37743752  0.37966099  0.38046415  0.38220104
  0.38232898  0.3829897   0.38318752  0.38509591  0.38513409  0.38601248
  0.38692906  0.38754623  0.38812518  0.38909446  0.39051096  0.39166089
  0.392621    0.39347036  0.39376443  0.39475089  0.39581259  0.39674367
  0.3982629   0.39903971  0.40132387  0.40143731  0.40383224  0.40657929
  0.40893604  0.410309    0.41039072  0.41079822  0.41110681  0.4124744
  0.4140452   0.4160735   0.41715887  0.4180491   0.41934683  0.4231911
  0.42357759  0.4248753   0.42665041  0.42701703  0.42723853  0.42851259
  0.42982214  0.42983664  0.43029036  0.43072422  0.43262725  0.4335293
  0.43453297  0.43708219  0.4385804   0.44049528  0.44389656  0.44417877
  0.44522329  0.44697204  0.44709732  0.44985887  0.45172374  0.45238174
  0.45369665  0.45520403  0.4565308   0.45975791  0.46210282  0.46222808
  0.46372096  0.46383437  0.46421553  0.46557129  0.46783827  0.46916503
  0.46987193  0.47003808  0.47261898  0.47373338  0.4750865   0.47540959
  0.47720186  0.47730346  0.4778217   0.47986989  0.48085634  0.48187183
  0.48198525  0.48232019  0.4825352   0.48256155  0.48366145  0.48400173
  0.48473498  0.48528493  0.48645472  0.48827334  0.48893673  0.48928618
  0.48931786  0.49012102  0.49370291  0.49540812  0.49625481  0.49672035
  0.49673488  0.49814335  0.49955449  0.50105005  0.50214993  0.50276976
  0.5054496   0.50563293  0.50643338  0.5067183   0.50778918  0.51075123
  0.51768819  0.51775807  0.51867465  0.51901492  0.51956487  0.52194798
  0.52396445  0.53018266  0.53608959  0.53633095  0.5377131   0.53905168
  0.54164711  0.54171699  0.54684983  0.55531134  0.55656554  0.5574122
  0.56537918  0.57108828  0.60387537]

  warnings.warn(

2022-11-03 10:49:55,477:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16361618 -0.15633406 -0.15542246 -0.14939513 -0.14905195 -0.14814035
 -0.1477757  -0.14213448 -0.13846658 -0.131399   -0.12063131 -0.11406775
 -0.10952045 -0.10482304 -0.10351458 -0.1027853  -0.09135274 -0.08734168
 -0.08407063 -0.08084235 -0.07985577 -0.07551226 -0.07403222 -0.07132961
 -0.06786551 -0.06277125 -0.05604665 -0.05255052 -0.03510119 -0.03216257
 -0.02527749 -0.014885   -0.00959768 -0.00940481 -0.00179026  0.00484863
  0.00602809  0.00803362  0.00919214  0.01170152  0.01718207  0.02274814
  0.02517182  0.02568675  0.0261802   0.02648027  0.02650191  0.02951549
  0.02956917  0.03145679  0.03183217  0.03515654  0.03743047  0.03778436
  0.03935011  0.04407954  0.05050391  0.05352839  0.05365687  0.05389305
  0.05571627  0.05693883  0.05847236  0.0593412   0.05939453  0.06151833
  0.06155018  0.06251546  0.06379189  0.06458541  0.06533616  0.06658021
  0.0671914   0.07230748  0.07371238  0.07525665  0.07570718  0.07698342
  0.07829152  0.08210989  0.08378298  0.08429757  0.08594936  0.08626035
  0.08794382  0.08854463  0.09018552  0.09289904  0.09324221  0.09449664
  0.09467896  0.09600924  0.09612715  0.098315    0.10133931  0.10168248
  0.10250804  0.10363418  0.10524304  0.10660499  0.11123799  0.11273986
  0.11433781  0.11560333  0.1205149   0.12198457  0.12281045  0.12386127
  0.12653186  0.13034951  0.1309183   0.13199059  0.13328848  0.13392131
  0.13457547  0.13486497  0.13563683  0.13660246  0.13769604  0.1378034
  0.13781395  0.13994867  0.14093522  0.14523597  0.1454183   0.14643707
  0.14673695  0.14739145  0.14793858  0.14843187  0.14953653  0.15219585
  0.15295769  0.15365442  0.15474835  0.15573547  0.15636794  0.15723662
  0.15745149  0.1578054   0.15974653  0.16037902  0.16128024  0.16383273
  0.16565594  0.16584847  0.16633138  0.16638524  0.16709305  0.16785401
  0.16821865  0.16966703  0.16988101  0.17003167  0.17140436  0.1733775
  0.17380672  0.1740422   0.17453602  0.17670241  0.17716312  0.17786074
  0.17880419  0.18185034  0.18274048  0.1828906   0.18315879  0.1838022
  0.18425254  0.18716985  0.18764168  0.18771682  0.18862824  0.18889646
  0.19260675  0.19316499  0.19615675  0.19748701  0.19795884  0.19875254
  0.20134744  0.20331057  0.20465069  0.20501551  0.2073644   0.20771795
  0.20819031  0.20828691  0.21080667  0.21478572  0.21553683  0.21719883
  0.21957956  0.22176777  0.22352674  0.22682947  0.22721613  0.22762353
  0.22783805  0.22896401  0.23001518  0.23064748  0.23071224  0.23073354
  0.23076592  0.23079813  0.23622447  0.23672885  0.23730803  0.23865888
  0.23898112  0.2395279   0.24008577  0.24030013  0.24168372  0.24301348
  0.24423605  0.24502938  0.24644555  0.24812884  0.24887978  0.24903006
  0.24927645  0.24957668  0.25164681  0.25370619  0.25517548  0.25631183
  0.25639808  0.25667647  0.2583392   0.25863924  0.26050522  0.26109548
  0.26231769  0.26370165  0.26544954  0.26687572  0.26719795  0.26765903
  0.26795892  0.2679591   0.26941749  0.26978213  0.27466222  0.27562732
  0.27969189  0.27985274  0.28270547  0.28307011  0.28398172  0.28496883
  0.2850973   0.28540845  0.28562261  0.28580493  0.28598725  0.28773586
  0.28844348  0.29035222  0.29183244  0.29381633  0.29508184  0.29509258
  0.29581113  0.29613318  0.29645507  0.29672273  0.29690505  0.29824588
  0.30090539  0.30181699  0.30254627  0.30291092  0.30294312  0.30309324
  0.30327556  0.30329721  0.30345788  0.30400484  0.30509877  0.30524943
  0.30533495  0.30546342  0.30749076  0.30946392  0.30998976  0.31004291
  0.31031147  0.31291729  0.31294932  0.31412948  0.31471903  0.31474051
  0.31589866  0.31777574  0.31868716  0.32057496  0.32074654  0.32223696
  0.32296625  0.32347064  0.32460715  0.32478946  0.3275461   0.32842533
  0.32914389  0.33214672  0.33260798  0.33318698  0.33325174  0.33484969
  0.33519251  0.33546071  0.33582572  0.33879636  0.33973981  0.3432472
  0.34455549  0.34671113  0.3474404   0.34786946  0.35123695  0.3523307
  0.3528673   0.35570947  0.35578426  0.35644913  0.35727521  0.35910914
  0.36021364  0.36048183  0.3609431   0.3618547   0.36233709  0.36370976
  0.36457862  0.36538287  0.36757091  0.36804253  0.36870778  0.37064871
  0.37131394  0.37567855  0.37607558  0.37957188  0.38243535  0.38720774
  0.38806567  0.39065037  0.39188368  0.39261295  0.39272049  0.39394306
  0.39589474  0.39607706  0.39702088  0.3970318   0.39751435  0.39803982
  0.39852256  0.39991671  0.39993801  0.40065673  0.40244792  0.40696302
  0.40811026  0.41069497  0.41075938  0.41097408  0.41288282  0.41485615
  0.4150281   0.41509233  0.41525317  0.41555358  0.41595008  0.41759097
  0.41796653  0.41892106  0.41925352  0.41958594  0.42146283  0.42253512
  0.42288902  0.42306078  0.42478755  0.42513072  0.42742576  0.4316297
  0.43390345  0.43423606  0.43529776  0.4357161   0.43787157  0.4390621
  0.43948025  0.43984507  0.44093899  0.44388816  0.44445659  0.44888599
  0.45004414  0.45104163  0.45126689  0.45245725  0.45265033  0.45481687
  0.45495625  0.45620031  0.45651127  0.45869912  0.46369696  0.46481233
  0.46574543  0.46761138  0.46942388  0.47176185  0.47594449  0.47851863
  0.47959092  0.48104949  0.48107115  0.4821434   0.4832266   0.48389166
  0.48422409  0.48468517  0.48596141  0.48601509  0.48616521  0.48637976
  0.48784905  0.4883316   0.48851393  0.48942553  0.4902407   0.49253574
  0.4931471   0.49429471  0.49430543  0.49503474  0.49581769  0.49603204
  0.49639668  0.49708302  0.4993675   0.50002164  0.50051509  0.50292804
  0.5040327   0.50419354  0.50468702  0.50474051  0.50546981  0.50586666
  0.50642434  0.507529    0.50921284  0.51221569  0.51303085  0.51736362
  0.51806071  0.51824301  0.51884366  0.51960514  0.52078478  0.52643675
  0.5288284   0.52883911  0.53109139  0.53156321  0.53254996  0.53522036
  0.53874887  0.53956387  0.54375726  0.5490875   0.55379565  0.55386007
  0.55656268  0.56126009  0.56622571  0.56783438  0.57003298  0.5705692
  0.57257474  0.57477334]

  warnings.warn(

2022-11-03 10:49:55,517:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.66415823e-01 -1.66226807e-01 -1.63202632e-01 -1.53374053e-01
 -1.49992117e-01 -1.48270752e-01 -1.47912990e-01 -1.46191633e-01
 -1.44490530e-01 -1.41304356e-01 -1.34108427e-01 -1.29032142e-01
 -1.16928681e-01 -1.15814872e-01 -1.10907340e-01 -1.10677808e-01
 -1.09206238e-01 -1.08072174e-01 -9.90063961e-02 -9.61577214e-02
 -8.63561585e-02 -7.93695001e-02 -7.55960320e-02 -7.48197297e-02
 -7.06885001e-02 -6.87983890e-02 -5.88820073e-02 -5.71876584e-02
 -4.61238101e-02 -4.55567782e-02 -4.23435879e-02 -3.04696598e-02
 -3.01996029e-02 -2.93355888e-02 -2.63114140e-02 -2.35571979e-02
 -1.90209284e-02 -1.81771837e-02 -1.63477538e-02 -1.38501846e-02
 -1.27497858e-02 -1.18047303e-02 -9.28015209e-03 -9.13166098e-03
 -8.37562087e-03 -2.06396114e-03  4.80977331e-04  3.67391266e-03
  4.29487888e-03  4.42995998e-03  5.03066403e-03  7.02883999e-03
  1.10250866e-02  1.15853716e-02  1.37050009e-02  1.55883579e-02
  1.59797988e-02  1.68911821e-02  1.74176969e-02  1.91728322e-02
  2.06106178e-02  2.55181497e-02  2.99531083e-02  3.33553141e-02
  3.51914980e-02  3.79051825e-02  3.86612443e-02  3.94914665e-02
  4.09225124e-02  4.92458490e-02  4.99882901e-02  5.74340136e-02
  5.86557820e-02  5.90474047e-02  5.91553787e-02  6.13154485e-02
  6.22808788e-02  6.24698726e-02  6.29962820e-02  6.48863931e-02
  6.48999012e-02  6.62433407e-02  7.03609641e-02  7.16638936e-02
  7.19473533e-02  7.60651585e-02  7.92512417e-02  7.92986326e-02
  8.13911547e-02  8.19311631e-02  8.39631238e-02  8.52794417e-02
  8.66092678e-02  8.66969652e-02  9.03827359e-02  9.06257910e-02
  9.36634595e-02  9.60057907e-02  9.95025532e-02  1.00089854e-01
  1.06975195e-01  1.14353272e-01  1.18876146e-01  1.19996688e-01
  1.22474100e-01  1.24748982e-01  1.25032561e-01  1.25160768e-01
  1.26389298e-01  1.27678705e-01  1.31161756e-01  1.31971934e-01
  1.32599759e-01  1.32937251e-01  1.34975861e-01  1.36548713e-01
  1.39842756e-01  1.40153450e-01  1.43589306e-01  1.43764820e-01
  1.45803521e-01  1.45891113e-01  1.46647160e-01  1.48260671e-01
  1.48422558e-01  1.49799753e-01  1.53633811e-01  1.54524940e-01
  1.58905971e-01  1.59128644e-01  1.61315829e-01  1.62787504e-01
  1.63091121e-01  1.63219553e-01  1.63975586e-01  1.66155940e-01
  1.71745224e-01  1.71940874e-01  1.73237148e-01  1.73986426e-01
  1.74843679e-01  1.76288128e-01  1.76396298e-01  1.77469590e-01
  1.78171485e-01  1.78272901e-01  1.78671180e-01  1.82282551e-01
  1.82518746e-01  1.84193044e-01  1.84388791e-01  1.84975889e-01
  1.85920945e-01  1.86508239e-01  1.87980020e-01  1.87993310e-01
  1.88925075e-01  1.89080299e-01  1.89492086e-01  1.91719718e-01
  1.92306990e-01  1.92711834e-01  1.92941479e-01  1.93204767e-01
  1.93420806e-01  1.94406386e-01  1.94595409e-01  1.94622207e-01
  1.96269376e-01  1.96424733e-01  1.96667759e-01  1.98206667e-01
  1.99860680e-01  2.01588603e-01  2.01622374e-01  2.01669743e-01
  2.01824995e-01  2.02155733e-01  2.02155853e-01  2.02931945e-01
  2.04423770e-01  2.04842430e-01  2.05139489e-01  2.05720036e-01
  2.07576377e-01  2.09331302e-01  2.09621741e-01  2.10465373e-01
  2.10647845e-01  2.11214659e-01  2.14394297e-01  2.15372905e-01
  2.16243764e-01  2.16270780e-01  2.18322897e-01  2.18518765e-01
  2.19625798e-01  2.20969161e-01  2.21286391e-01  2.21414507e-01
  2.21569942e-01  2.23871650e-01  2.24175590e-01  2.25572753e-01
  2.25620144e-01  2.26585462e-01  2.27422543e-01  2.27780207e-01
  2.28698337e-01  2.29157206e-01  2.29886448e-01  2.30669301e-01
  2.31000235e-01  2.31087947e-01  2.31600953e-01  2.31958708e-01
  2.32255675e-01  2.33686721e-01  2.35536413e-01  2.37102428e-01
  2.37170067e-01  2.37277935e-01  2.38479631e-01  2.38911708e-01
  2.39168046e-01  2.39222079e-01  2.39451702e-01  2.42833646e-01
  2.43454612e-01  2.43906938e-01  2.44568414e-01  2.45202888e-01
  2.49381396e-01  2.49847222e-01  2.51082611e-01  2.51453874e-01
  2.53080668e-01  2.53512626e-01  2.54383597e-01  2.55233991e-01
  2.55686407e-01  2.58271764e-01  2.59642144e-01  2.61329619e-01
  2.63374975e-01  2.63564081e-01  2.63753104e-01  2.64002884e-01
  2.66918903e-01  2.67648145e-01  2.67715566e-01  2.68052974e-01
  2.69868994e-01  2.70739755e-01  2.72521801e-01  2.72852659e-01
  2.74357880e-01  2.74438929e-01  2.75984688e-01  2.76389826e-01
  2.78968444e-01  2.80298375e-01  2.80460262e-01  2.80629113e-01
  2.85043712e-01  2.85091103e-01  2.86204890e-01  2.86407421e-01
  2.86697634e-01  2.87561641e-01  2.87737247e-01  2.87878878e-01
  2.88736335e-01  2.89924445e-01  2.90336021e-01  2.91146101e-01
  2.91483593e-01  2.93326622e-01  2.94602325e-01  2.94804856e-01
  2.95054545e-01  2.95830855e-01  2.96377624e-01  2.96755648e-01
  2.97511688e-01  2.98969939e-01  2.99962077e-01  3.00272778e-01
  3.00718124e-01  3.01285156e-01  3.01474172e-01  3.01852188e-01
  3.02230212e-01  3.02345129e-01  3.02419227e-01  3.02439595e-01
  3.02608235e-01  3.02986259e-01  3.03742299e-01  3.03762667e-01
  3.05160033e-01  3.09568081e-01  3.10472612e-01  3.11640461e-01
  3.12693370e-01  3.13206468e-01  3.13577632e-01  3.14070566e-01
  3.15690529e-01  3.15697388e-01  3.16352110e-01  3.16737001e-01
  3.17540116e-01  3.18316418e-01  3.18357041e-01  3.19430227e-01
  3.21320338e-01  3.22076378e-01  3.23662760e-01  3.24088160e-01
  3.26005197e-01  3.28496005e-01  3.30838434e-01  3.32296601e-01
  3.32391038e-01  3.32904136e-01  3.33936903e-01  3.35975498e-01
  3.36184769e-01  3.37818436e-01  3.38081754e-01  3.38560961e-01
  3.39715196e-01  3.40471244e-01  3.42314076e-01  3.43394213e-01
  3.44426966e-01  3.48382808e-01  3.48807997e-01  3.49510103e-01
  3.51960394e-01  3.55193854e-01  3.56368547e-01  3.57630727e-01
  3.58629815e-01  3.59109157e-01  3.59352092e-01  3.61829505e-01
  3.63550975e-01  3.64307008e-01  3.64435244e-01  3.66359125e-01
  3.72488404e-01  3.75573380e-01  3.76309150e-01  3.77146261e-01
  3.80487560e-01  3.82215679e-01  3.82411561e-01  3.82634235e-01
  3.84652567e-01  3.84659427e-01  3.86346916e-01  3.87109703e-01
  3.88399125e-01  3.88628552e-01  3.90883171e-01  3.91072194e-01
  3.92388512e-01  3.93900606e-01  3.95432949e-01  3.96357742e-01
  3.96472576e-01  3.96729110e-01  3.97491806e-01  3.98349179e-01
  3.99381917e-01  3.99496736e-01  4.00009834e-01  4.00630814e-01
  4.01899854e-01  4.03189155e-01  4.05187331e-01  4.05470911e-01
  4.05626043e-01  4.06611623e-01  4.08393760e-01  4.08542259e-01
  4.09318554e-01  4.09919363e-01  4.10074616e-01  4.11444966e-01
  4.11478631e-01  4.11937710e-01  4.11957973e-01  4.13753527e-01
  4.14124789e-01  4.14313797e-01  4.14880837e-01  4.15447981e-01
  4.17337972e-01  4.19174171e-01  4.19795123e-01  4.20213783e-01
  4.21017003e-01  4.27821386e-01  4.30454051e-01  4.32107952e-01
  4.35537083e-01  4.36961466e-01  4.38216997e-01  4.38696233e-01
  4.38844823e-01  4.40782212e-01  4.43360815e-01  4.49591434e-01
  4.49800719e-01  4.50455441e-01  4.53520154e-01  4.56125774e-01
  4.56355321e-01  4.57914694e-01  4.58123950e-01  4.58501966e-01
  4.60601362e-01  4.61424845e-01  4.63294694e-01  4.65994870e-01
  4.66264941e-01  4.67810687e-01  4.69721060e-01  4.70402918e-01
  4.70490630e-01  4.70747164e-01  4.71341219e-01  4.78685654e-01
  4.83343391e-01  4.85219994e-01  4.87272082e-01  4.87690623e-01
  4.88217138e-01  4.90330042e-01  4.91052305e-01  4.91335884e-01
  4.92240415e-01  4.94090002e-01  4.94278996e-01  4.94454510e-01
  4.95413074e-01  4.97168195e-01  5.00739161e-01  5.05268662e-01
  5.05612908e-01  5.06368970e-01  5.07118249e-01  5.07685288e-01
  5.09764422e-01  5.12221543e-01  5.14246735e-01  5.17162860e-01
  5.17783931e-01  5.17905384e-01  5.19093495e-01  5.26019360e-01
  5.26100408e-01  5.26647185e-01  5.29043549e-01  5.29232543e-01
  5.29306837e-01  5.30008867e-01  5.30042637e-01  5.31520961e-01
  5.36104495e-01  5.36293517e-01  5.36489265e-01  5.36806495e-01
  5.39121930e-01  5.41153756e-01  5.46641850e-01  5.47377621e-01
  5.53905298e-01  5.62923804e-01  5.64408883e-01  5.68357865e-01
  5.81028358e-01  5.82742954e-01  5.87083461e-01  5.93145319e-01]

  warnings.warn(

2022-11-03 10:49:55,549:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.1603132  -0.15882241 -0.15714526 -0.14729622 -0.14692353 -0.13712647
 -0.12283252 -0.11559237 -0.10880436 -0.10656818 -0.10039119 -0.09983214
 -0.09847572 -0.09546667 -0.09080794 -0.06333814 -0.06245837 -0.05905211
 -0.05561247 -0.05149124 -0.04651474 -0.04033481 -0.03998661 -0.03786325
 -0.03210502 -0.03126159 -0.02822211 -0.01489034 -0.00992947 -0.00804444
 -0.00565232 -0.00410955  0.00100131  0.00323751  0.00479211  0.01168407
  0.0118645   0.01513638  0.01588769  0.01644083  0.01830432  0.02491189
  0.02696766  0.0285624   0.02933229  0.03246685  0.03270517  0.03531406
  0.04045536  0.04135667  0.04141456  0.04333595  0.04359877  0.04380668
  0.05067709  0.0526965   0.05976509  0.06078896  0.0610332   0.06104207
  0.06210523  0.06216017  0.06218469  0.06571347  0.06644028  0.06724061
  0.06870985  0.06894226  0.07007894  0.07119703  0.0739403   0.07411399
  0.0755773   0.07679935  0.07817731  0.07886481  0.08003489  0.08110692
  0.08390216  0.08462305  0.089065    0.09173179  0.09200349  0.09249281
  0.0925076   0.09489972  0.09601485  0.09759396  0.09764299  0.09779002
  0.09825401  0.10337079  0.10581488  0.10929762  0.1102657   0.11062064
  0.11158281  0.11212328  0.11307953  0.11492443  0.11542855  0.11776572
  0.12404961  0.12439782  0.12534518  0.1258062   0.12610241  0.12828662
  0.12837578  0.12842101  0.12912118  0.12983236  0.13040407  0.13041593
  0.13115834  0.13206854  0.13321116  0.13536495  0.13623583  0.13906742
  0.14051595  0.14052187  0.14174391  0.14335346  0.14526594  0.14748059
  0.14763946  0.15157435  0.15645956  0.15701186  0.15763851  0.15834378
  0.15989329  0.16095266  0.163941    0.16621946  0.16754544  0.17080171
  0.17154118  0.17236602  0.17332228  0.17359694  0.17489547  0.1755124
  0.17791044  0.17885781  0.18070567  0.18081637  0.18249351  0.18318018
  0.18517804  0.18716998  0.18853612  0.18866078  0.18872247  0.18965422
  0.19271524  0.19364699  0.19542724  0.19651492  0.19725143  0.19760554
  0.19869616  0.19875786  0.19877265  0.19896789  0.19989964  0.20101477
  0.20155309  0.20211214  0.20232595  0.20337349  0.20457693  0.2047447
  0.2068503   0.20728089  0.20731429  0.20747908  0.20768698  0.20810277
  0.2093337   0.21014289  0.21086677  0.21157284  0.21237614  0.21493601
  0.21552252  0.21767631  0.21837649  0.2202607   0.22143075  0.22161711
  0.22538126  0.22631894  0.22789212  0.2293584   0.22986549  0.23325695
  0.23407798  0.23409658  0.23530678  0.23546187  0.23757636  0.23818147
  0.23854741  0.23911321  0.23973311  0.24082373  0.24235384  0.24248229
  0.24410451  0.24511272  0.24547655  0.24646027  0.24697326  0.24701931
  0.248418    0.25069432  0.25275895  0.25308264  0.25411241  0.25452821
  0.25458019  0.25527953  0.25571309  0.25878002  0.25910453  0.26063463
  0.26142017  0.26273349  0.26303857  0.26347001  0.26381144  0.26421541
  0.26505209  0.2660789   0.2669274   0.26885555  0.26893794  0.26905374
  0.27083778  0.27158319  0.27673418  0.27706675  0.27721675  0.2772776
  0.27752859  0.27773945  0.27872317  0.2789243   0.28086133  0.28265723
  0.28317104  0.28457943  0.28720905  0.28783568  0.28787498  0.28899984
  0.28913125  0.29019737  0.29055739  0.29118109  0.29174014  0.29211283
  0.29293767  0.29382632  0.29400379  0.29442847  0.29480116  0.29536022
  0.29544261  0.29610561  0.29778275  0.29791712  0.2979691   0.29820066
  0.29848209  0.2987145   0.29890084  0.29927354  0.29960018  0.29964624
  0.30003967  0.30014658  0.30076434  0.30095069  0.30230331  0.30475711
  0.30661088  0.30781139  0.31063705  0.31185612  0.31217684  0.31257111
  0.31399513  0.31493279  0.31567818  0.31594101  0.31605976  0.31718378
  0.31911487  0.32334004  0.32702689  0.32867065  0.32903743  0.33008791
  0.33448383  0.3347437   0.33745948  0.3383422   0.33982117  0.34085686
  0.34301953  0.3501253   0.35066577  0.35219669  0.35236741  0.35383667
  0.3542643   0.3554314   0.356473    0.35650048  0.35666907  0.35702317
  0.36056676  0.36067664  0.3626352   0.36296479  0.36325509  0.36930362
  0.37064526  0.37139953  0.37216647  0.37397503  0.37411828  0.37475085
  0.37517552  0.37663885  0.37826992  0.37852685  0.3806071   0.38174675
  0.38226272  0.38253145  0.38717835  0.38880351  0.39016586  0.39021191
  0.3909573   0.39202342  0.3920754   0.39251486  0.3931935   0.39356619
  0.39412524  0.39482459  0.3960162   0.39689597  0.39834367  0.39881144
  0.4004366   0.40079072  0.40287097  0.40456079  0.40523266  0.40526012
  0.40600256  0.40613398  0.40832708  0.40968349  0.4101876   0.41034354
  0.41263169  0.41279651  0.4156623   0.41798977  0.4183781   0.41857037
  0.41880867  0.42078204  0.4218942   0.42352531  0.42365079  0.42480523
  0.42599094  0.4263666   0.42777499  0.42791528  0.42881364  0.42889605
  0.43041431  0.43046332  0.43057318  0.43089391  0.43228074  0.43283979
  0.43917271  0.43925216  0.44141186  0.44258194  0.44306154  0.44324787
  0.44335775  0.44354114  0.44359902  0.44489754  0.44583226  0.44704839
  0.44753096  0.4481145   0.44971518  0.45163655  0.45208572  0.45341765
  0.45567537  0.45828428  0.46004972  0.46259777  0.46364233  0.46469959
  0.46525861  0.46784597  0.46895814  0.46928183  0.47101685  0.4764945
  0.47657689  0.47740173  0.47881013  0.47976639  0.47992824  0.48077756
  0.48080799  0.48338647  0.4835728   0.48431821  0.48445258  0.48543629
  0.48769996  0.48852777  0.48964584  0.48974981  0.49142694  0.49257547
  0.49560607  0.49592679  0.49659569  0.49717629  0.49736265  0.50015789
  0.50202138  0.50265985  0.5033837   0.50401923  0.50423896  0.50609951
  0.50684489  0.50710774  0.50822879  0.50920956  0.51227649  0.51240494
  0.51432337  0.51991087  0.52214709  0.52289247  0.52422439  0.52736484
  0.52747472  0.52877621  0.52901748  0.52904494  0.52970796  0.53032192
  0.53567112  0.53607127  0.53910484  0.54490913  0.5465098   0.55382643
  0.55792611  0.56146673  0.56314389  0.56484848  0.57059785  0.57390312
  0.5896604 ]

  warnings.warn(

2022-11-03 10:49:55,580:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16532099 -0.16159331 -0.15881327 -0.15452645 -0.14988257 -0.14519156
 -0.14505231 -0.14388687 -0.14315705 -0.1401592  -0.13737915 -0.13700639
 -0.13630799 -0.13534465 -0.13296879 -0.12883693 -0.11358489 -0.1089253
 -0.09968467 -0.0993119  -0.073481   -0.07011252 -0.06751886 -0.06453672
 -0.05947081 -0.05357151 -0.04669102 -0.0457591  -0.04279268 -0.03261585
 -0.03032069 -0.02159207 -0.02047378 -0.01924335 -0.01647902 -0.01396606
 -0.01317339 -0.01191583 -0.00783111 -0.0041777  -0.00095063  0.00261066
  0.00335618  0.00481156  0.01080725  0.01174344  0.01730353  0.01930663
  0.02460393  0.02539231  0.03000904  0.0312823   0.03233349  0.03873552
  0.0391397   0.04117635  0.04437201  0.04460339  0.04769765  0.04812897
  0.04856458  0.05549646  0.0611351   0.06642813  0.06821556  0.06983231
  0.0702365   0.07416197  0.07453688  0.0746133   0.07548666  0.07902794
  0.0799263   0.08102888  0.08197224  0.08457232  0.08460161  0.08551353
  0.08561208  0.08640259  0.0876016   0.08888414  0.08908624  0.09745779
  0.09752491  0.10143898  0.10410476  0.10577078  0.10637278  0.10790169
  0.109532    0.11022828  0.11156652  0.11173078  0.11453081  0.11488358
  0.11637466  0.1164889   0.1169088   0.1173894   0.1211935   0.12231394
  0.12310446  0.1260259   0.12717348  0.12959647  0.1325429   0.13533865
  0.13596708  0.13736602  0.13886638  0.13987899  0.14128722  0.14332601
  0.1437909   0.1443679   0.14494062  0.14723579  0.14727362  0.1480663
  0.15075422  0.15167042  0.15258022  0.15264947  0.15358139  0.15381276
  0.15552165  0.15635288  0.15784394  0.15997485  0.16056543  0.1607875
  0.16159589  0.16216647  0.1626042   0.16288486  0.16441878  0.16533712
  0.16748161  0.16871703  0.16893484  0.169424    0.17011812  0.17035592
  0.17196197  0.17408359  0.17473272  0.17662156  0.17740708  0.17873889
  0.18041636  0.18070629  0.18084768  0.18153893  0.1820581   0.18236088
  0.185388    0.18553655  0.18692408  0.18776172  0.18889146  0.18897215
  0.18953773  0.19011043  0.19514494  0.19540773  0.19651747  0.19718873
  0.1973187   0.19756365  0.19822635  0.19846628  0.19891546  0.19913541
  0.20015517  0.20128704  0.20142843  0.2033751   0.20525465  0.20599162
  0.20616657  0.20848601  0.20926082  0.20928583  0.21082188  0.2117538
  0.21312776  0.21473594  0.21527938  0.21647837  0.21807942  0.21871927
  0.21975259  0.2205381   0.22101871  0.22232339  0.2239537   0.22661022
  0.22687513  0.22926671  0.22931384  0.23016293  0.23146333  0.23165186
  0.23252736  0.23444547  0.23495248  0.23908935  0.23916075  0.23934715
  0.24132595  0.24204864  0.24206007  0.24256064  0.24541068  0.24655113
  0.24746091  0.24768085  0.24795007  0.24830712  0.24927046  0.24938043
  0.24971536  0.249996    0.25024095  0.25036878  0.25117287  0.25303671
  0.25322309  0.2550955   0.25716856  0.2582483   0.25877604  0.2597308
  0.26007643  0.2607634   0.26122187  0.26348276  0.26408476  0.26423543
  0.26446895  0.26468889  0.2653066   0.26545014  0.26586575  0.2665413
  0.26986692  0.27050963  0.27068245  0.27068674  0.270781    0.27129087
  0.27200926  0.27500283  0.27546772  0.28014731  0.28075357  0.28093141
  0.2828038   0.28352506  0.28378072  0.28431773  0.28464337  0.28597019
  0.28623298  0.28741413  0.28764979  0.28784331  0.28985497  0.29011775
  0.29102038  0.29109039  0.29355836  0.29428103  0.29486305  0.29558571
  0.29616773  0.29635412  0.2965405   0.29840433  0.29844431  0.29859072
  0.2987771   0.29936051  0.30016248  0.30045456  0.30084946  0.30194562
  0.3032346   0.30656449  0.30920743  0.31027861  0.31079064  0.3114869
  0.31198535  0.31261877  0.31321147  0.31358423  0.31424694  0.31698413
  0.3174626   0.31979987  0.32012553  0.32039045  0.32187939  0.32241854
  0.32439235  0.32549494  0.32551493  0.3257463   0.33067297  0.33195339
  0.33261107  0.33455133  0.33487696  0.33696074  0.33840468  0.33919306
  0.33927589  0.34017426  0.34124757  0.34503595  0.34694905  0.34862649
  0.34942845  0.3504432   0.35095309  0.3521678   0.35239488  0.35295404
  0.3536703   0.35524418  0.3558776   0.35738011  0.36095711  0.3620754
  0.36280737  0.36586378  0.3658952   0.36665644  0.36783759  0.36886377
  0.36969714  0.37024487  0.37052337  0.37263214  0.37435673  0.37637767
  0.37639551  0.37755667  0.37901845  0.38011891  0.38055666  0.38080589
  0.38206344  0.38270115  0.38298178  0.38355665  0.38540264  0.38768208
  0.38804342  0.38836905  0.38867684  0.3888004   0.39029146  0.39122338
  0.39127051  0.39140976  0.39215529  0.39271659  0.39364637  0.39389988
  0.39476467  0.39663279  0.39889652  0.40014052  0.40023478  0.40064109
  0.40074893  0.40096459  0.40179083  0.40368395  0.40427023  0.40489223
  0.40592268  0.40664823  0.4093026   0.40933402  0.41037591  0.41045659
  0.42124685  0.42165532  0.42217877  0.42393479  0.42398405  0.42471602
  0.42564365  0.42856721  0.42965411  0.43299115  0.4382099   0.43824348
  0.43858266  0.44057218  0.44201399  0.44300875  0.4433658   0.44350507
  0.4440642   0.44661287  0.4470956   0.44800968  0.44845887  0.44852384
  0.44950076  0.45083686  0.45149028  0.45270069  0.45377188  0.45465666
  0.45534147  0.45610058  0.45824507  0.4582922   0.46185135  0.46312248
  0.46363663  0.46501774  0.46619673  0.46697583  0.47033072  0.47102698
  0.47261232  0.47270445  0.47495676  0.47595152  0.47850019  0.48008337
  0.48033045  0.48255137  0.48387176  0.48479009  0.4864654   0.48683817
  0.48697956  0.48725809  0.48772511  0.49078365  0.49258678  0.4933773
  0.49378364  0.49524114  0.49712069  0.49727566  0.49869247  0.50249441
  0.50328706  0.50454461  0.50718542  0.50734037  0.5080252   0.50890999
  0.51409732  0.5156041   0.51704805  0.51739152  0.51779357  0.51853909
  0.51870978  0.52396207  0.52448768  0.53766022  0.53865282  0.5430182
  0.5472579   0.5504107   0.55230596  0.55368919  0.55528811  0.55555303
  0.55626716  0.55752471  0.56291411  0.56384603  0.56428163  0.60042436
  0.60519177  0.60595303]

  warnings.warn(

2022-11-03 10:49:57,764:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16601159 -0.16257532 -0.15951041 -0.15798318 -0.14956251 -0.14806661
 -0.14405762 -0.14348491 -0.14253039 -0.1419159  -0.13598743 -0.13544605
 -0.12932668 -0.12757721 -0.11938922 -0.1096009  -0.10807366 -0.10443604
 -0.08857011 -0.08515473 -0.06601211 -0.06409262 -0.06351991 -0.05835505
 -0.05816415 -0.05472788 -0.05070845 -0.04512754 -0.04420727 -0.0429545
 -0.03943467 -0.03598795 -0.03123623 -0.02983434 -0.02332271 -0.01724511
 -0.01367015 -0.01344791 -0.01019211 -0.00805039 -0.00446789  0.00301915
  0.00318916  0.0048133   0.01315795  0.01350842  0.01368888  0.02248091
  0.02959659  0.03049134  0.03112382  0.03150563  0.03323421  0.03514325
  0.04003655  0.04208428  0.04264363  0.04862723  0.05311956  0.05378163
  0.05411124  0.05830067  0.05883915  0.06076907  0.06172069  0.06342085
  0.06418156  0.06742983  0.06799964  0.06830081  0.07089453  0.0736461
  0.0745484   0.07559402  0.07625609  0.07681836  0.07769686  0.07815471
  0.07834559  0.07937322  0.07968192  0.08009506  0.08063644  0.0814105
  0.08179233  0.08247992  0.08298996  0.08463208  0.0866798   0.08944356
  0.08999538  0.0912406   0.096055    0.0984114   0.09915415  0.10015044
  0.10029956  0.10378225  0.10397315  0.10531992  0.1058613   0.10833844
  0.1085189   0.10853978  0.10910958  0.10969564  0.11138246  0.11315743
  0.11697089  0.11794048  0.11896057  0.11981356  0.12550353  0.12558709
  0.12690543  0.12711723  0.12858471  0.13015371  0.13189736  0.13553498
  0.13627018  0.13764075  0.13875193  0.1402165   0.14080255  0.14173619
  0.14232979  0.14241335  0.14280561  0.14378854  0.14663412  0.14680877
  0.14709194  0.14719057  0.14733216  0.14746331  0.15497878  0.15911309
  0.16008096  0.16234338  0.16322652  0.16462088  0.16520694  0.16565605
  0.16906682  0.1694457   0.17217059  0.17291041  0.1730044   0.17418988
  0.17551578  0.17683292  0.17764704  0.17837934  0.17850466  0.17895203
  0.18002899  0.18036611  0.18353374  0.18478359  0.18637059  0.18683133
  0.18721313  0.18782761  0.18830634  0.18989623  0.19084032  0.19155343
  0.1926153   0.19268377  0.19428703  0.19576204  0.19596047  0.19612759
  0.19671828  0.19675833  0.19872713  0.20338484  0.20419024  0.20458249
  0.20496721  0.20586078  0.20817835  0.20989939  0.21018429  0.21314184
  0.21457509  0.2148884   0.21526268  0.21660364  0.21691815  0.21698545
  0.21725528  0.21808447  0.22045133  0.22081688  0.22157759  0.22166987
  0.22655734  0.22768651  0.22897353  0.22998781  0.23140596  0.23142685
  0.23151504  0.23189221  0.23200711  0.23367592  0.23508246  0.2352345
  0.23539116  0.23566852  0.23591627  0.23596677  0.23602944  0.23626675
  0.23818915  0.24194165  0.24203392  0.24211748  0.24220567  0.24301687
  0.2431869   0.24349269  0.24421744  0.24470831  0.24538373  0.24555376
  0.24615199  0.24795368  0.24832042  0.25017433  0.2514004   0.25190582
  0.25192671  0.25246517  0.25305587  0.25312144  0.25469625  0.25521094
  0.25618344  0.25626528  0.25792827  0.25824452  0.25830256  0.25937659
  0.25998644  0.26344824  0.26385965  0.26418344  0.26463834  0.26632224
  0.26632807  0.26690367  0.26821911  0.26935872  0.26937671  0.27307236
  0.27679645  0.27721422  0.27834749  0.27841014  0.279584    0.27974358
  0.28085184  0.28295179  0.28314269  0.28328428  0.28361676  0.2837154
  0.28557512  0.28622385  0.28924408  0.28983477  0.29309059  0.29328149
  0.29404511  0.29419715  0.29538143  0.29554101  0.29557234  0.29563791
  0.29595415  0.29614505  0.29625821  0.29633595  0.29671776  0.29690867
  0.29826126  0.29919951  0.29938579  0.30010473  0.30041341  0.30217624
  0.30326655  0.30374235  0.30458951  0.30548891  0.30587072  0.30594965
  0.31200926  0.31486991  0.31512639  0.3153637   0.31573044  0.31852841
  0.31912956  0.31960536  0.32006611  0.32244051  0.32279389  0.32347105
  0.32598122  0.32852101  0.33308644  0.33363073  0.33423186  0.334551
  0.33459278  0.33691786  0.33968452  0.34114619  0.34602904  0.34621704
  0.34892103  0.35091654  0.3518113   0.35215596  0.35293582  0.35532531
  0.35654965  0.3585034   0.3590732   0.36078087  0.36129847  0.36180852
  0.36330442  0.36370419  0.37012183  0.37107635  0.37187129  0.37209353
  0.37354013  0.37568938  0.37589073  0.37712553  0.37859763  0.37897652
  0.38055135  0.38064534  0.38415182  0.38506456  0.38701538  0.3879699
  0.38854262  0.38892442  0.38938517  0.39159999  0.39178798  0.3921698
  0.39259919  0.39312432  0.39347478  0.39351946  0.3936657   0.39426974
  0.394832    0.39768512  0.40263064  0.40425478  0.40438765  0.40457102
  0.40499462  0.4052824   0.40801483  0.40892002  0.40896935  0.40909006
  0.40935116  0.40939876  0.41008344  0.41030568  0.4112109   0.41183291
  0.4121625   0.41278743  0.41378664  0.41462626  0.41758672  0.41877104
  0.42079786  0.42115878  0.42309625  0.42648322  0.42717081  0.42999549
  0.43054151  0.43250567  0.43340797  0.43549747  0.43586593  0.43691735
  0.43870861  0.43950355  0.44085325  0.44093678  0.44096812  0.44275183
  0.44360945  0.4454767   0.44556317  0.44603899  0.4474693   0.44781979
  0.45162742  0.45393914  0.45448052  0.46266852  0.46433442  0.46519204
  0.46897878  0.470072    0.47423766  0.47814218  0.47863889  0.4788298
  0.47975298  0.47997521  0.48016612  0.48137712  0.48148156  0.48245697
  0.48265831  0.4833906   0.48358149  0.48361574  0.48423023  0.48456735
  0.48475827  0.48533098  0.48552187  0.48779183  0.48848233  0.48874635
  0.48886416  0.4891624   0.48946819  0.48991267  0.49122811  0.49218263
  0.49239443  0.49269267  0.49332806  0.49621542  0.49625719  0.49639297
  0.49903427  0.50152649  0.50232143  0.50251235  0.50308504  0.50534457
  0.51089123  0.51143261  0.51201576  0.51270626  0.51271671  0.51853782
  0.51963104  0.52293908  0.52309865  0.52559084  0.533057    0.53393838
  0.5377878   0.54010999  0.5406827   0.54483037  0.55014435  0.55881859
  0.55978356  0.55997445  0.56816245  0.57387911  0.57886351  0.58189708
  0.58479198]

  warnings.warn(

2022-11-03 10:49:57,811:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16578022 -0.16332363 -0.15087291 -0.14971795 -0.1472931  -0.14688344
 -0.14423788 -0.14329304 -0.14028013 -0.13895736 -0.13125198 -0.13087405
 -0.12257004 -0.11672262 -0.10989862 -0.10861815 -0.10310635 -0.10009345
 -0.09899137 -0.09650305 -0.09405705 -0.09352187 -0.09292324 -0.08672961
 -0.08579535 -0.08556407 -0.08519671 -0.07123424 -0.05448874 -0.04179756
 -0.03823891 -0.03388066 -0.03239148 -0.02840059 -0.02387735 -0.0210943
 -0.00975764 -0.0072785  -0.00315435 -0.00180901  0.00344896  0.00458276
  0.00779521  0.00901505  0.00976035  0.0139282   0.01665555  0.01806436
  0.01957608  0.02089887  0.02740697  0.02753388  0.02781523  0.03083871
  0.03816615  0.03899747  0.04061216  0.04080113  0.04426601  0.04439152
  0.04981093  0.05118379  0.05134243  0.0541572   0.0547636   0.0553305
  0.05608637  0.05795771  0.0591853   0.06002578  0.06057153  0.06078167
  0.06171591  0.069104    0.07287278  0.07328245  0.07364205  0.07646317
  0.07700116  0.08024533  0.08044488  0.08083199  0.08591437  0.09079299
  0.0913599   0.09195077  0.09309515  0.09558206  0.09969563  0.10016876
  0.1002865   0.10236516  0.10307873  0.1051334   0.10591325  0.10625945
  0.11045906  0.11046962  0.11294737  0.11539336  0.11799519  0.11985457
  0.12260445  0.12437639  0.12463023  0.12525918  0.12559622  0.12744501
  0.12787582  0.12880867  0.12890104  0.12898848  0.13094937  0.13128642
  0.13269379  0.13383818  0.13528645  0.13532595  0.13815272  0.14136234
  0.14167048  0.14184886  0.14275139  0.14361938  0.14437525  0.14458818
  0.14569803  0.14803193  0.15400204  0.15469726  0.1559446   0.15600029
  0.15644945  0.1566081   0.15688169  0.15723705  0.15764672  0.15773275
  0.16018933  0.16026053  0.16402932  0.16403848  0.16437553  0.16509191
  0.16715998  0.16738985  0.16852083  0.17041051  0.17096049  0.17587364
  0.17621985  0.17646946  0.17851779  0.17927507  0.17945981  0.17980743
  0.18008242  0.18040465  0.18072337  0.18078259  0.18135231  0.18198266
  0.18228797  0.18418401  0.18455278  0.18794644  0.18804939  0.18834411
  0.18888986  0.19096852  0.1917258   0.19209951  0.19250282  0.19397225
  0.19643943  0.19721645  0.1990744   0.20021455  0.20210422  0.20288548
  0.20319997  0.20427806  0.20514887  0.20535264  0.20568828  0.20573057
  0.20592872  0.20601333  0.20609651  0.20715771  0.20776269  0.20799818
  0.20970808  0.21074669  0.2125729   0.21361433  0.21420098  0.21436598
  0.21831739  0.21926857  0.22145438  0.22190777  0.22210309  0.22312337
  0.22529861  0.22813948  0.22816346  0.22841588  0.23035844  0.23174469
  0.23455381  0.23480765  0.23499661  0.23512072  0.23516725  0.23550006
  0.23692861  0.23848407  0.2400233   0.24040124  0.24269281  0.24643763
  0.24794938  0.24822436  0.24929612  0.24980098  0.24982848  0.25059493
  0.2513508   0.25393711  0.25426218  0.25558495  0.25700009  0.25813249
  0.25832146  0.25907733  0.25916476  0.25919649  0.25939461  0.26023653
  0.26247875  0.26306045  0.26407791  0.26442131  0.26738417  0.26794824
  0.26851514  0.26957773  0.26967151  0.27011431  0.27077218  0.27095481
  0.27099428  0.27140254  0.27222469  0.27285083  0.2736067   0.27450147
  0.27492947  0.27664358  0.27684172  0.27902755  0.27963532  0.28021
  0.28039897  0.2806105   0.28074798  0.28086434  0.28248044  0.2832229
  0.28387725  0.28450618  0.28586846  0.28623723  0.28719123  0.2872152
  0.28797109  0.28853516  0.28869239  0.28944827  0.28969152  0.28989249
  0.29001517  0.29096001  0.29152691  0.29190485  0.2939835   0.29459553
  0.2967468   0.29724106  0.29730312  0.29740607  0.29989438  0.3023418
  0.30264853  0.3026866   0.30340297  0.30781129  0.30903887  0.30972352
  0.31080163  0.31128815  0.3115166   0.31212441  0.31231338  0.31250235
  0.31325962  0.31382512  0.31450836  0.31485457  0.31746065  0.31756641
  0.31831029  0.31869739  0.32158479  0.32261566  0.32509198  0.32541703
  0.32593245  0.32702395  0.32735042  0.32786444  0.32836648  0.32900883
  0.32956514  0.32976187  0.33170586  0.3318645   0.33196603  0.33412013
  0.33449948  0.33517849  0.33551553  0.3358843   0.33679742  0.33742777
  0.33867792  0.33982089  0.33983006  0.34081861  0.34313629  0.34788445
  0.34916211  0.34927987  0.35412957  0.3544666   0.35560042  0.35693237
  0.35884317  0.35886433  0.35938892  0.3605439   0.36060594  0.36107906
  0.3642612   0.36429153  0.36453337  0.3662235   0.36648652  0.3703794
  0.37084335  0.37245945  0.37452751  0.37534544  0.37842181  0.38035238
  0.38066826  0.38129722  0.38239153  0.38242045  0.38429181  0.38570836
  0.38639935  0.38733362  0.38739849  0.38752259  0.38790052  0.38793225
  0.38884535  0.38945738  0.39434658  0.39462017  0.39472451  0.39587032
  0.39773742  0.3996814   0.40000504  0.40100417  0.403178    0.40623041
  0.40625296  0.40748195  0.40830905  0.40959233  0.41229853  0.41280338
  0.41786319  0.41790693  0.42064625  0.42400395  0.42443617  0.42599941
  0.42697597  0.42803714  0.42874015  0.42895026  0.42974843  0.43033508
  0.43354753  0.43663447  0.437989    0.44155821  0.44167315  0.44206168
  0.44207224  0.44345707  0.44543277  0.45693724  0.45703244  0.45884808
  0.45939383  0.46109453  0.46222835  0.46446423  0.46467576  0.46586104
  0.46587161  0.46785507  0.47032222  0.47219077  0.47304183  0.47464733
  0.47559217  0.47615907  0.47634805  0.47748184  0.47844784  0.47936096
  0.47968743  0.48011682  0.48143962  0.48162857  0.48178721  0.48238445
  0.48247963  0.48297392  0.48446308  0.4855969   0.48788567  0.48918728
  0.49147605  0.49354411  0.49564393  0.49565449  0.49603242  0.49787981
  0.49841639  0.49919341  0.50106193  0.50128124  0.5036969   0.50632131
  0.50854663  0.51130711  0.51318619  0.51433057  0.51450899  0.51513935
  0.51589521  0.51991642  0.51998905  0.5204939   0.52095504  0.52115459
  0.52347508  0.52705489  0.52929078  0.52970043  0.53173677  0.53647156
  0.53724858  0.53794099  0.54067116  0.5417838   0.54398796  0.54797745
  0.55594582  0.56239187  0.56326267  0.56764066  0.58181325  0.61184856]

  warnings.warn(

2022-11-03 10:49:57,826:INFO:Calculating mean and std
2022-11-03 10:49:57,826:INFO:Creating metrics dataframe
2022-11-03 10:49:57,826:INFO:Uploading results into container
2022-11-03 10:49:57,826:INFO:Uploading model into container now
2022-11-03 10:49:57,826:INFO:master_model_container: 8
2022-11-03 10:49:57,826:INFO:display_container: 2
2022-11-03 10:49:57,842:INFO:ElasticNet(random_state=4411)
2022-11-03 10:49:57,842:INFO:create_model() successfully completed......................................
2022-11-03 10:49:58,093:ERROR:create_model() for ElasticNet(random_state=4411) raised an exception or returned all 0.0:
2022-11-03 10:49:58,093:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:49:58,093:INFO:Initializing Least Angle Regression
2022-11-03 10:49:58,093:INFO:Total runtime is 1.3697246273358663 minutes
2022-11-03 10:49:58,093:INFO:SubProcess create_model() called ==================================
2022-11-03 10:49:58,101:INFO:Initializing create_model()
2022-11-03 10:49:58,101:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:49:58,101:INFO:Checking exceptions
2022-11-03 10:49:58,101:INFO:Importing libraries
2022-11-03 10:49:58,101:INFO:Copying training dataset
2022-11-03 10:49:58,117:INFO:Defining folds
2022-11-03 10:49:58,117:INFO:Declaring metric variables
2022-11-03 10:49:58,117:INFO:Importing untrained model
2022-11-03 10:49:58,117:INFO:Least Angle Regression Imported successfully
2022-11-03 10:49:58,117:INFO:Starting cross validation
2022-11-03 10:49:58,117:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:50:00,995:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:01,026:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:01,042:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:01,078:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:01,095:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:01,143:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:01,143:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:01,192:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.937e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-11-03 10:50:01,192:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.899e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-11-03 10:50:01,192:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.759e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-11-03 10:50:01,192:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.758e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-11-03 10:50:01,192:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.656e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-11-03 10:50:01,192:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.513e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-11-03 10:50:01,225:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:02,160:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.22759221 -0.18281955 -0.16490057 -0.15916288 -0.15481519 -0.15037475
 -0.13391122 -0.13320558 -0.1327074  -0.13256409 -0.13095249 -0.1273081
 -0.12681571 -0.11797125 -0.11663401 -0.11609342 -0.11249005 -0.11221214
 -0.11213424 -0.10815903 -0.10812711 -0.10807845 -0.10758513 -0.10469526
 -0.0964037  -0.0916023  -0.08400326 -0.08322505 -0.08234236 -0.08132376
 -0.0720365  -0.06928709 -0.06874596 -0.06701821 -0.06437475 -0.06393701
 -0.06095805 -0.06010752 -0.05762167 -0.05535918 -0.05518988 -0.05499943
 -0.05433063 -0.05096152 -0.04819062 -0.04507993 -0.04477371 -0.04432384
 -0.04179743 -0.0401627  -0.03346355 -0.03186098 -0.03166437 -0.03161845
 -0.03058267 -0.02874367 -0.0254831  -0.02315566 -0.0230948  -0.02199682
 -0.02141183 -0.02011469 -0.02000063 -0.01965465 -0.01419046 -0.00459904
 -0.00370943  0.00344583  0.00743552  0.00922258  0.01040018  0.0104983
  0.01109955  0.01173035  0.01186176  0.01589742  0.01788842  0.01920366
  0.0209784   0.0224974   0.02332723  0.024382    0.02537359  0.02565929
  0.02701675  0.02720935  0.03319138  0.03319866  0.03585579  0.03632771
  0.0379261   0.03922978  0.04357103  0.04404904  0.04595345  0.04675752
  0.04693946  0.04733717  0.04830229  0.04927382  0.05036855  0.05162026
  0.05168833  0.0517132   0.05349545  0.05500137  0.05801273  0.059001
  0.06008231  0.06542563  0.0662237   0.06654805  0.06689119  0.06689152
  0.06715118  0.07511242  0.07638561  0.07672291  0.07921667  0.08006163
  0.0804636   0.08244954  0.08311129  0.0844589   0.08631289  0.0864061
  0.0864236   0.08902445  0.09070019  0.09162993  0.09411949  0.09867068
  0.10100462  0.10298721  0.10305137  0.10331588  0.1035254   0.10391193
  0.10476436  0.10572539  0.10633997  0.10669534  0.10720733  0.10776001
  0.11027698  0.11124508  0.11234039  0.11433397  0.11704384  0.11960591
  0.12041164  0.120981    0.12404506  0.12456287  0.12544055  0.12613948
  0.12630957  0.12776268  0.12807984  0.12896698  0.12943014  0.12993136
  0.13685346  0.13805388  0.13995182  0.14010507  0.14119334  0.14462429
  0.14526182  0.14573409  0.14937214  0.14980869  0.15156332  0.15356078
  0.15398515  0.15449947  0.15494677  0.1580319   0.16153808  0.16399483
  0.16748496  0.16863745  0.17026985  0.17176277  0.17322491  0.17497194
  0.17587985  0.17809644  0.17903139  0.17982566  0.18071381  0.18201188
  0.18243059  0.18262188  0.18650394  0.18659539  0.18701319  0.18843688
  0.18873573  0.18893998  0.18909032  0.19056766  0.19591832  0.19707374
  0.1999712   0.20048607  0.20240882  0.20259527  0.20309472  0.20359552
  0.20568186  0.20571297  0.2057855   0.20628684  0.20747989  0.20809745
  0.20832389  0.20833996  0.20874108  0.20937992  0.20957316  0.20975356
  0.21412598  0.21517826  0.21625468  0.21640577  0.21790559  0.22074496
  0.22157666  0.22342703  0.22368264  0.2251784   0.2277293   0.22968501
  0.23295245  0.23347432  0.23499852  0.23538645  0.23539654  0.23582126
  0.23603424  0.23642982  0.23659726  0.23868069  0.23978823  0.24301063
  0.24355835  0.24448087  0.24456466  0.24678786  0.24839935  0.24983298
  0.25031738  0.25078023  0.25172539  0.2555624   0.25571729  0.25607812
  0.25821318  0.26038021  0.26167455  0.26273589  0.2639788   0.26477114
  0.26688956  0.26711272  0.26794053  0.27137969  0.27519268  0.28404647
  0.28613917  0.28725794  0.28843805  0.28940204  0.28990584  0.29073417
  0.29091754  0.2914618   0.29211427  0.29349374  0.29395974  0.29503095
  0.29539312  0.29616104  0.29861094  0.30004213  0.30040813  0.30077668
  0.30216647  0.30245036  0.30297473  0.30372261  0.30444101  0.30465111
  0.30495027  0.30612323  0.30745607  0.30750639  0.30890722  0.30983346
  0.31109311  0.31270066  0.31300694  0.31419388  0.31439174  0.31497002
  0.31939761  0.31993411  0.32317877  0.32413821  0.32471538  0.32759252
  0.32829213  0.33022821  0.33043179  0.33057407  0.33143016  0.33179264
  0.33257427  0.33307936  0.33441494  0.33585127  0.33759265  0.34426155
  0.34484972  0.34530559  0.34995965  0.35152731  0.35165376  0.35177651
  0.35224241  0.35560022  0.35636658  0.35756157  0.36069095  0.36074197
  0.36146014  0.36219325  0.36570567  0.3665477   0.36753312  0.36872521
  0.36970901  0.37142196  0.37416747  0.37427992  0.37692361  0.37709295
  0.37785455  0.38199079  0.38312878  0.38400479  0.3847381   0.38541466
  0.38662125  0.39010744  0.40350929  0.40379373  0.40463114  0.40647878
  0.40855606  0.41048669  0.41094187  0.41545822  0.41792328  0.41871622
  0.42240963  0.42330054  0.42399341  0.42484933  0.43007681  0.43067912
  0.43184134  0.43184597  0.43320167  0.43446911  0.4357637   0.43740654
  0.43767834  0.43769191  0.44033269  0.44194887  0.45035741  0.45098822
  0.45137831  0.45191732  0.45274253  0.45410296  0.46303508  0.46442587
  0.46516541  0.46795817  0.46945319  0.4736979   0.47629589  0.47694102
  0.48043288  0.48055402  0.48098343  0.48103068  0.48169626  0.48461644
  0.48699976  0.48811948  0.49014283  0.49596642  0.50214313  0.50358034
  0.50415208  0.5059043   0.5108498   0.51290831  0.51356367  0.51523019
  0.51537383  0.51634971  0.51821451  0.52017281  0.526025    0.52602647
  0.52897194  0.53028964  0.530424    0.53124961  0.53548438  0.53642586
  0.53971278  0.54121636  0.54300089  0.54558479  0.54776939  0.54787775
  0.54957297  0.55174319  0.55176146  0.55359277  0.55538556  0.56015626
  0.56598114  0.56725437  0.57363187  0.57373614  0.57565645  0.57967081
  0.58276696  0.58669196  0.58790188  0.59037985  0.59214094  0.59299113
  0.59657468  0.60081719  0.60090122  0.60145224  0.60398068  0.60590994
  0.60767836  0.60788558  0.60935213  0.61303197  0.61460339  0.61995438
  0.62228099  0.62256566  0.62413935  0.62679056  0.63050591  0.63286426
  0.64401532  0.64468527  0.64572663  0.64745305  0.65767456  0.66336743
  0.66341367  0.66477482  0.66571625  0.6687756   0.67402886  0.67773434
  0.68360734  0.68472522  0.68601512  0.68642374  0.69594331  0.6964306
  0.70121238  0.71192317  0.71772971  0.71823367  0.72196     0.7238786
  0.73099813]

  warnings.warn(

2022-11-03 10:50:02,207:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.17859909 -0.17137604 -0.16883698 -0.16142804 -0.15823029 -0.15433136
 -0.14520169 -0.14441111 -0.14382384 -0.14100634 -0.14087518 -0.13970847
 -0.13939833 -0.13249522 -0.13246282 -0.13130347 -0.12968202 -0.12649065
 -0.12527451 -0.12306027 -0.11891008 -0.10731138 -0.09906699 -0.09631797
 -0.09589221 -0.09027215 -0.08319542 -0.08295828 -0.08277739 -0.08250432
 -0.08228986 -0.07808097 -0.07650528 -0.07380014 -0.07271407 -0.07262291
 -0.07233932 -0.07211808 -0.06746815 -0.06702841 -0.06594035 -0.06342977
 -0.06150551 -0.05364698 -0.05151769 -0.0501369  -0.04992765 -0.04987424
 -0.04494082 -0.04027569 -0.03818179 -0.03732028 -0.03453102 -0.03381415
 -0.02955203 -0.02864871 -0.02553229 -0.02487619 -0.02268468 -0.02252069
 -0.02037185 -0.01877173 -0.01822087 -0.0182084  -0.01770274 -0.01543315
 -0.01412414 -0.0114035  -0.00778528 -0.00710542 -0.00449067 -0.00204671
  0.00162219  0.00443329  0.00625776  0.00653145  0.00771633  0.00801741
  0.01023331  0.01061632  0.0147856   0.01687977  0.01835336  0.01943637
  0.02416442  0.02781641  0.02928354  0.02986251  0.03016783  0.03094979
  0.03379028  0.0356441   0.03925998  0.03962373  0.03985412  0.0399344
  0.04082751  0.05174638  0.05261206  0.05682378  0.06012914  0.06078503
  0.06159185  0.06564717  0.06577092  0.06666856  0.07102967  0.07256755
  0.07474553  0.07520501  0.07544624  0.0761721   0.07779038  0.08238989
  0.08386085  0.0849662   0.08584834  0.08643958  0.08680819  0.08706165
  0.09031636  0.09085284  0.09183286  0.09228593  0.09255045  0.0950769
  0.09526852  0.09544835  0.09714589  0.09834657  0.10211893  0.10374185
  0.10443462  0.10474312  0.10564904  0.10602678  0.10996826  0.11186988
  0.11197397  0.11202978  0.11359033  0.11478966  0.11549043  0.11689716
  0.11987762  0.12011251  0.120418    0.12100201  0.12263108  0.12330828
  0.12556199  0.12646656  0.12895193  0.13195149  0.13256507  0.13548904
  0.13652015  0.13769934  0.13859216  0.13906358  0.14001971  0.14274009
  0.14311831  0.14466241  0.14576557  0.14609819  0.14691511  0.14735177
  0.15345267  0.15387854  0.15460962  0.15594909  0.15669579  0.15758231
  0.1581733   0.15844873  0.16242785  0.16278127  0.16348791  0.16398071
  0.16564139  0.16606266  0.169004    0.17093509  0.17124833  0.17288807
  0.17335896  0.17355232  0.17437053  0.17473283  0.17526983  0.17527054
  0.17556138  0.17615514  0.17648087  0.17742806  0.17818027  0.17841982
  0.18097947  0.18112012  0.18208847  0.18282333  0.18437182  0.18544378
  0.18639886  0.19001275  0.1968446   0.19817807  0.19827033  0.19854711
  0.1987812   0.20017674  0.20150764  0.20176155  0.2030076   0.20403427
  0.20443419  0.20550196  0.20724564  0.20821553  0.20852778  0.20862954
  0.20890153  0.2092309   0.21137079  0.21314535  0.21396375  0.21419139
  0.2151643   0.21852511  0.21897764  0.22006489  0.22109162  0.22195463
  0.22231064  0.224986    0.22541104  0.22575761  0.2287525   0.23075427
  0.23160618  0.23546496  0.23548466  0.23573909  0.23750414  0.23869479
  0.24245129  0.24427656  0.24890434  0.25108094  0.25133329  0.25228159
  0.25435941  0.25461977  0.25813919  0.25989824  0.26381099  0.2644135
  0.2654833   0.26568439  0.26632215  0.26758752  0.27389795  0.27584366
  0.27982775  0.28013009  0.2802278   0.28081642  0.28565539  0.28809932
  0.28872465  0.28976764  0.29438848  0.29439729  0.2983794   0.298952
  0.30056516  0.30396865  0.30469516  0.30497693  0.30510832  0.3081813
  0.3087887   0.31129456  0.31191352  0.31202232  0.31205255  0.31214926
  0.313166    0.3141328   0.31855322  0.31918478  0.31972045  0.31984586
  0.32020819  0.32424274  0.3243946   0.32924798  0.33138723  0.33226646
  0.33506402  0.34231866  0.34290593  0.344175    0.34521683  0.34669929
  0.34687919  0.34791121  0.34799461  0.35228108  0.35248076  0.35273546
  0.35314949  0.35368953  0.35470757  0.35701237  0.35812474  0.3583853
  0.3596053   0.36101767  0.36270634  0.36839226  0.37608674  0.37659653
  0.37719176  0.37730064  0.37743132  0.38113479  0.38170251  0.38194438
  0.38312869  0.38468645  0.38689591  0.3903517   0.39379965  0.39383144
  0.39476262  0.39510278  0.39548872  0.39557743  0.39954061  0.40436677
  0.40909104  0.40975384  0.41720849  0.41763515  0.41920264  0.41936779
  0.42128908  0.42320586  0.42370236  0.4245139   0.43250952  0.43255183
  0.43431112  0.43611767  0.43903664  0.44193085  0.44444533  0.44481374
  0.44646204  0.44807096  0.44877735  0.45180219  0.45188271  0.45317501
  0.45476071  0.45576707  0.455824    0.45782896  0.45955087  0.45955986
  0.45970005  0.46424165  0.46591635  0.46655379  0.47076474  0.47498066
  0.47525658  0.47712874  0.47722008  0.48031336  0.48348754  0.48348842
  0.48663683  0.48929393  0.49136601  0.4921309   0.49815352  0.49820391
  0.49837914  0.50112939  0.50143085  0.50150219  0.50165706  0.50192396
  0.50549932  0.5062357   0.50665115  0.51135318  0.51287801  0.51404285
  0.51632054  0.51890148  0.51918744  0.52057425  0.52158057  0.52888547
  0.53218272  0.53260741  0.532886    0.5341401   0.53905577  0.5398203
  0.54003297  0.54101899  0.54191458  0.54329125  0.54891503  0.55005171
  0.55057397  0.55115858  0.55142052  0.55145255  0.55158167  0.55173629
  0.55192204  0.55280502  0.55319444  0.5541294   0.55763616  0.55763957
  0.56216761  0.5628237   0.56291041  0.56939642  0.56980925  0.57118016
  0.5862847   0.5877238   0.588692    0.59615516  0.59779372  0.6039677
  0.60609727  0.60803595  0.60876772  0.60899448  0.60904007  0.61031502
  0.61039874  0.61094132  0.61565771  0.61801732  0.62576125  0.62897278
  0.62922915  0.63256296  0.63393807  0.63661445  0.63812846  0.64145848
  0.6450304   0.64578623  0.64631825  0.64980956  0.65322722  0.6575099
  0.65754569  0.6580613   0.66211529  0.66728319  0.66754401  0.66778824
  0.67265571  0.67359233  0.67695794  0.68081029  0.68150292  0.68302453
  0.69239014  0.69457139  0.69503269  0.70163067  0.71779953  0.71909945
  0.72530793  0.7340377   0.75742178  0.76269903  0.76518511  0.76941151
  0.77693273]

  warnings.warn(

2022-11-03 10:50:02,256:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.91536643e-01 -1.67359420e-01 -1.50870734e-01 -1.44159664e-01
 -1.38960061e-01 -1.32436532e-01 -1.30889801e-01 -1.28575354e-01
 -1.25375597e-01 -1.19656661e-01 -1.18150440e-01 -1.16905858e-01
 -1.15855640e-01 -1.14869827e-01 -1.10000053e-01 -1.08742246e-01
 -1.02658434e-01 -1.00956932e-01 -9.73111792e-02 -9.45469244e-02
 -9.38658895e-02 -8.94110252e-02 -8.83384577e-02 -8.72819414e-02
 -8.61035590e-02 -8.50650186e-02 -8.43474356e-02 -7.10468027e-02
 -7.04816209e-02 -6.87108069e-02 -6.37387019e-02 -6.28943016e-02
 -5.85875125e-02 -5.80858280e-02 -5.60564666e-02 -4.94512165e-02
 -4.90296134e-02 -4.88748000e-02 -4.45132564e-02 -4.43538630e-02
 -4.40967398e-02 -4.26252005e-02 -4.22766378e-02 -4.20604365e-02
 -3.92637758e-02 -3.83042172e-02 -3.62594597e-02 -3.54494103e-02
 -3.39927224e-02 -3.22665592e-02 -2.88161229e-02 -2.84128299e-02
 -2.83729897e-02 -2.60828247e-02 -2.39400514e-02 -2.32741770e-02
 -2.30260760e-02 -2.20656206e-02 -2.12055933e-02 -2.07070292e-02
 -1.83971892e-02 -1.60854585e-02 -1.11865461e-02 -9.60151851e-03
 -8.56415256e-03 -7.73981647e-03 -5.00728719e-03 -4.55203497e-03
 -3.64030117e-03 -2.47441637e-03 -1.84942938e-03 -5.46357691e-04
  3.97419262e-04  1.80096008e-03  1.82125364e-03  3.06468202e-03
  4.00706692e-03  5.84919651e-03  7.08961722e-03  9.09800060e-03
  1.03393827e-02  1.05297693e-02  1.27093672e-02  1.39506779e-02
  1.81484029e-02  1.85039728e-02  2.05039797e-02  2.23952074e-02
  2.42888276e-02  2.52986602e-02  2.64413677e-02  2.92927366e-02
  3.07691202e-02  3.15242904e-02  3.34991029e-02  3.46064463e-02
  3.55142691e-02  3.75459441e-02  3.92452317e-02  4.03918660e-02
  4.09253173e-02  4.11507475e-02  4.31378714e-02  4.50620786e-02
  4.55038215e-02  5.03371588e-02  5.08658530e-02  5.19539264e-02
  5.29843303e-02  5.41493830e-02  5.70246292e-02  5.93864979e-02
  6.10636622e-02  6.13139407e-02  6.48214776e-02  6.66912225e-02
  6.80486759e-02  8.36861943e-02  8.36933787e-02  8.43881222e-02
  8.57264399e-02  8.61214207e-02  8.69781282e-02  8.76503043e-02
  9.02553172e-02  9.20395944e-02  9.21943250e-02  9.21977971e-02
  9.22061068e-02  9.22336508e-02  9.27073664e-02  9.31882116e-02
  9.60494081e-02  9.63813148e-02  9.71826885e-02  9.75017600e-02
  9.87076957e-02  1.01190009e-01  1.01393881e-01  1.01450635e-01
  1.01495022e-01  1.02324181e-01  1.05597463e-01  1.06451733e-01
  1.07266935e-01  1.07517864e-01  1.16184338e-01  1.16716781e-01
  1.16826366e-01  1.20516159e-01  1.20635318e-01  1.21128695e-01
  1.21694195e-01  1.21795334e-01  1.21834318e-01  1.24970041e-01
  1.25118558e-01  1.25189682e-01  1.27316463e-01  1.28369007e-01
  1.28692825e-01  1.29577824e-01  1.29755724e-01  1.30321635e-01
  1.31631805e-01  1.34477280e-01  1.34986612e-01  1.36280298e-01
  1.37193143e-01  1.37699771e-01  1.37755461e-01  1.37959923e-01
  1.38161964e-01  1.44475733e-01  1.46114891e-01  1.47220300e-01
  1.47758796e-01  1.48081184e-01  1.51093358e-01  1.54781716e-01
  1.55712602e-01  1.55997642e-01  1.58113010e-01  1.59763918e-01
  1.60571794e-01  1.60797117e-01  1.63693205e-01  1.66537112e-01
  1.68539896e-01  1.71726594e-01  1.73918319e-01  1.73964749e-01
  1.74545681e-01  1.77316540e-01  1.78326132e-01  1.83367154e-01
  1.85435065e-01  1.86867145e-01  1.87019805e-01  1.89839919e-01
  1.94422132e-01  1.99536814e-01  2.00489722e-01  2.03530992e-01
  2.04817151e-01  2.05491322e-01  2.05966375e-01  2.06335933e-01
  2.07720455e-01  2.08740852e-01  2.08886326e-01  2.11783441e-01
  2.12010797e-01  2.13235793e-01  2.13572110e-01  2.13855980e-01
  2.14921852e-01  2.17055062e-01  2.18418133e-01  2.20202868e-01
  2.22421645e-01  2.23203353e-01  2.23533429e-01  2.27028625e-01
  2.28473450e-01  2.29210831e-01  2.32129963e-01  2.32626870e-01
  2.34638819e-01  2.35020610e-01  2.35106903e-01  2.35314619e-01
  2.37640732e-01  2.38041173e-01  2.39601716e-01  2.39677194e-01
  2.41551143e-01  2.41884232e-01  2.42746266e-01  2.43491566e-01
  2.45788293e-01  2.46428161e-01  2.46551139e-01  2.46721802e-01
  2.47415341e-01  2.48045753e-01  2.48844830e-01  2.48992325e-01
  2.49067212e-01  2.51036084e-01  2.51925249e-01  2.52376578e-01
  2.54552440e-01  2.56807930e-01  2.58692842e-01  2.58757014e-01
  2.59946085e-01  2.61976636e-01  2.62379740e-01  2.62706582e-01
  2.62861226e-01  2.63108193e-01  2.63804758e-01  2.63823336e-01
  2.64367743e-01  2.66615005e-01  2.66951226e-01  2.70205988e-01
  2.71447682e-01  2.71783207e-01  2.72010905e-01  2.72827817e-01
  2.72907220e-01  2.74952794e-01  2.75491700e-01  2.79469991e-01
  2.81996056e-01  2.82294479e-01  2.83853631e-01  2.86007186e-01
  2.86289887e-01  2.89077274e-01  2.95115315e-01  2.99569027e-01
  3.01769414e-01  3.09999139e-01  3.12326758e-01  3.12736518e-01
  3.13670556e-01  3.14759692e-01  3.15406198e-01  3.15956362e-01
  3.17854405e-01  3.18409436e-01  3.19196082e-01  3.19772226e-01
  3.23459591e-01  3.26347833e-01  3.26486051e-01  3.29380107e-01
  3.30493962e-01  3.30914543e-01  3.31556717e-01  3.32286323e-01
  3.32563149e-01  3.33160609e-01  3.33593845e-01  3.34459624e-01
  3.37699054e-01  3.37843768e-01  3.39145169e-01  3.40315803e-01
  3.42680218e-01  3.46397073e-01  3.51344889e-01  3.56294430e-01
  3.58882193e-01  3.61634306e-01  3.63340967e-01  3.63956521e-01
  3.64008378e-01  3.65432756e-01  3.65883448e-01  3.67102814e-01
  3.69351246e-01  3.71370904e-01  3.73254827e-01  3.74506060e-01
  3.74676228e-01  3.75517968e-01  3.76805650e-01  3.78060316e-01
  3.81806214e-01  3.84192011e-01  3.86478058e-01  3.86995876e-01
  3.89424358e-01  3.92010317e-01  3.93962002e-01  3.95289046e-01
  3.96091013e-01  3.96895919e-01  3.97283908e-01  4.02794716e-01
  4.05305972e-01  4.10243379e-01  4.10622452e-01  4.12596019e-01
  4.20545426e-01  4.20863573e-01  4.22774009e-01  4.23131319e-01
  4.24839658e-01  4.26670200e-01  4.32282090e-01  4.32714529e-01
  4.32829690e-01  4.33279559e-01  4.34204057e-01  4.36694750e-01
  4.39460093e-01  4.39680338e-01  4.48311560e-01  4.49166113e-01
  4.49722148e-01  4.50418509e-01  4.51015568e-01  4.51565791e-01
  4.53510080e-01  4.57773749e-01  4.58220552e-01  4.59966294e-01
  4.60802966e-01  4.61139132e-01  4.61291091e-01  4.61805462e-01
  4.62594966e-01  4.65824252e-01  4.67167090e-01  4.67296781e-01
  4.67518541e-01  4.68380396e-01  4.69058356e-01  4.69131342e-01
  4.69619104e-01  4.69663813e-01  4.70717059e-01  4.73423934e-01
  4.74338783e-01  4.76293653e-01  4.77634827e-01  4.78615950e-01
  4.78647934e-01  4.78856741e-01  4.82089552e-01  4.82495921e-01
  4.86557177e-01  4.86757866e-01  4.88552614e-01  4.88752071e-01
  4.91410939e-01  4.91918905e-01  4.92848247e-01  4.96948599e-01
  4.97674764e-01  4.98651134e-01  5.01183617e-01  5.01354555e-01
  5.02641144e-01  5.03203411e-01  5.07567064e-01  5.08724173e-01
  5.09267201e-01  5.10605132e-01  5.11336496e-01  5.11890025e-01
  5.18909118e-01  5.20727852e-01  5.21601739e-01  5.23036185e-01
  5.28846937e-01  5.31834730e-01  5.33438523e-01  5.34981273e-01
  5.36705237e-01  5.38826779e-01  5.39623551e-01  5.39820240e-01
  5.46043864e-01  5.47859601e-01  5.53252308e-01  5.54452252e-01
  5.64841343e-01  5.65029514e-01  5.67129649e-01  5.68497847e-01
  5.69127893e-01  5.69420600e-01  5.70882558e-01  5.76068122e-01
  5.76512105e-01  5.77408426e-01  5.79633590e-01  5.83147772e-01
  5.85003364e-01  5.88695094e-01  5.88879332e-01  5.90208191e-01
  5.92126964e-01  5.92946725e-01  5.94383573e-01  6.05977211e-01
  6.07875684e-01  6.10498520e-01  6.11088281e-01  6.12979036e-01
  6.14828938e-01  6.15578813e-01  6.15786892e-01  6.16536838e-01
  6.25331307e-01  6.26666691e-01  6.30485604e-01  6.30858969e-01
  6.31475245e-01  6.32517642e-01  6.36848552e-01  6.37108377e-01
  6.41339326e-01  6.42807978e-01  6.45803629e-01  6.47796298e-01
  6.49530005e-01  6.51708312e-01  6.56994861e-01  6.57718430e-01
  6.58356301e-01  6.59537916e-01  6.63803527e-01  6.65764377e-01
  6.76686596e-01  6.82522271e-01  6.86354446e-01  6.89426665e-01
  6.89473322e-01  6.90933830e-01  6.97097964e-01  7.00935149e-01
  7.11969663e-01  7.15067857e-01  7.19851108e-01  7.20450391e-01
  7.35815757e-01]

  warnings.warn(

2022-11-03 10:50:02,278:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.83048829e-01 -1.81381742e-01 -1.58083276e-01 -1.50423314e-01
 -1.47290788e-01 -1.46354752e-01 -1.44953723e-01 -1.43663561e-01
 -1.39135457e-01 -1.39121192e-01 -1.38846414e-01 -1.36962409e-01
 -1.36295000e-01 -1.35574397e-01 -1.34490460e-01 -1.32417308e-01
 -1.32181235e-01 -1.31680899e-01 -1.30806367e-01 -1.23025449e-01
 -1.22979860e-01 -1.19803774e-01 -1.16641620e-01 -1.15912482e-01
 -1.12983592e-01 -1.09661321e-01 -1.04346591e-01 -1.02850955e-01
 -1.01394430e-01 -1.01322001e-01 -1.01019470e-01 -9.90403359e-02
 -9.86937054e-02 -9.76875809e-02 -9.35593976e-02 -9.27037889e-02
 -8.97175890e-02 -8.65876122e-02 -8.57566814e-02 -8.21570689e-02
 -8.06957057e-02 -7.59068905e-02 -7.53589118e-02 -7.45425115e-02
 -7.35776245e-02 -7.30700111e-02 -6.74251300e-02 -6.64703563e-02
 -6.55812342e-02 -6.50859608e-02 -6.38233592e-02 -6.32419485e-02
 -6.13002663e-02 -5.55202542e-02 -5.42327303e-02 -5.28575714e-02
 -5.08925434e-02 -4.97711477e-02 -4.84626563e-02 -4.82071409e-02
 -4.80388558e-02 -4.60122762e-02 -4.58537614e-02 -4.55407771e-02
 -4.44548253e-02 -4.38736815e-02 -4.29009634e-02 -4.27723018e-02
 -3.77706389e-02 -3.71000656e-02 -3.68305264e-02 -3.59785007e-02
 -3.59693976e-02 -3.44127290e-02 -3.42522932e-02 -3.18979640e-02
 -3.09344657e-02 -3.03713326e-02 -2.89210928e-02 -2.81037383e-02
 -2.65394298e-02 -2.64132184e-02 -2.31327830e-02 -2.28205878e-02
 -2.23960791e-02 -1.91505964e-02 -1.69828336e-02 -1.49059804e-02
 -5.02951294e-03 -4.53640452e-03 -3.37920074e-03 -3.17077229e-03
 -2.00576923e-03 -4.12962613e-04  3.84644342e-05  6.47029689e-04
  2.02591225e-03  3.55584178e-03  1.04510286e-02  2.00994147e-02
  2.08936594e-02  2.10963536e-02  2.11091828e-02  2.21361531e-02
  2.70294292e-02  2.76347989e-02  2.94859472e-02  3.02914946e-02
  3.12935476e-02  3.13581519e-02  3.24732444e-02  3.43543862e-02
  3.48155540e-02  3.64898033e-02  3.79243168e-02  3.91456549e-02
  4.09129045e-02  4.09775934e-02  4.20045487e-02  4.26542435e-02
  4.73489886e-02  4.84663442e-02  4.91197403e-02  5.24057005e-02
  5.47165033e-02  5.54034856e-02  5.71060289e-02  5.77064599e-02
  5.94476548e-02  5.97530493e-02  6.05337892e-02  6.33792394e-02
  6.61212095e-02  6.65641182e-02  6.69028977e-02  6.83130576e-02
  6.84685586e-02  6.85714852e-02  7.09178601e-02  7.17167882e-02
  7.63956746e-02  7.70585196e-02  7.93785708e-02  8.18951791e-02
  8.27921469e-02  8.40313354e-02  9.14757524e-02  9.17496285e-02
  9.19602769e-02  9.62260920e-02  9.91101952e-02  9.93495933e-02
  1.00828545e-01  1.01775900e-01  1.03608069e-01  1.03651973e-01
  1.05147773e-01  1.05894018e-01  1.08886375e-01  1.12213278e-01
  1.12719848e-01  1.16141000e-01  1.17206507e-01  1.18349666e-01
  1.19100314e-01  1.20058879e-01  1.20614912e-01  1.22719138e-01
  1.25039035e-01  1.26383969e-01  1.26948408e-01  1.27025164e-01
  1.28000158e-01  1.30415989e-01  1.31137466e-01  1.33192228e-01
  1.33408312e-01  1.33507087e-01  1.37176670e-01  1.38294168e-01
  1.40818776e-01  1.41242629e-01  1.42158674e-01  1.43161697e-01
  1.43245271e-01  1.43958963e-01  1.45138818e-01  1.48467920e-01
  1.51345103e-01  1.51512602e-01  1.54130883e-01  1.56234720e-01
  1.58744424e-01  1.58846398e-01  1.60969175e-01  1.61567933e-01
  1.61661569e-01  1.61747228e-01  1.62586310e-01  1.63108814e-01
  1.63538063e-01  1.63552633e-01  1.64043132e-01  1.64446052e-01
  1.65101453e-01  1.66775580e-01  1.66972373e-01  1.70403636e-01
  1.70435043e-01  1.75787594e-01  1.76835768e-01  1.78574042e-01
  1.79221174e-01  1.79703223e-01  1.79922373e-01  1.83370532e-01
  1.84991518e-01  1.89180863e-01  1.89982551e-01  1.90301442e-01
  1.91461686e-01  1.91527375e-01  1.93648357e-01  1.93836648e-01
  1.95871901e-01  1.96793778e-01  1.98576664e-01  1.98660504e-01
  2.00533306e-01  2.02476918e-01  2.04709370e-01  2.07143055e-01
  2.11647216e-01  2.11692583e-01  2.12079570e-01  2.12322905e-01
  2.12856854e-01  2.16898405e-01  2.17165559e-01  2.17697251e-01
  2.18692699e-01  2.18787521e-01  2.20028052e-01  2.22393065e-01
  2.23582531e-01  2.25880592e-01  2.27012645e-01  2.27277640e-01
  2.31205819e-01  2.31622764e-01  2.32726465e-01  2.37413119e-01
  2.41274633e-01  2.41877594e-01  2.41945433e-01  2.44910773e-01
  2.45512359e-01  2.47503643e-01  2.49174932e-01  2.51050436e-01
  2.51709938e-01  2.54775059e-01  2.54969560e-01  2.56615302e-01
  2.58952564e-01  2.59990898e-01  2.60952556e-01  2.61514434e-01
  2.61900818e-01  2.64349536e-01  2.65821207e-01  2.68014074e-01
  2.69325018e-01  2.77179933e-01  2.78069440e-01  2.78763932e-01
  2.79571529e-01  2.80084492e-01  2.82652454e-01  2.83236529e-01
  2.86614532e-01  2.90282417e-01  2.98424887e-01  2.98731174e-01
  2.99723056e-01  3.02705473e-01  3.03957206e-01  3.11260165e-01
  3.11599002e-01  3.13584598e-01  3.14167658e-01  3.15599045e-01
  3.16862258e-01  3.22630922e-01  3.24489622e-01  3.24850151e-01
  3.26619350e-01  3.26946666e-01  3.30851700e-01  3.30876534e-01
  3.32676011e-01  3.33583875e-01  3.37158134e-01  3.41818029e-01
  3.41924677e-01  3.42201580e-01  3.42227665e-01  3.44168420e-01
  3.48985329e-01  3.50466549e-01  3.50741864e-01  3.52372643e-01
  3.52742256e-01  3.55380364e-01  3.56474234e-01  3.59607566e-01
  3.62514562e-01  3.64347463e-01  3.65329354e-01  3.67540601e-01
  3.70273049e-01  3.71416331e-01  3.71434427e-01  3.76381467e-01
  3.79176821e-01  3.80211902e-01  3.81874197e-01  3.82178535e-01
  3.85433937e-01  3.85870952e-01  3.93308997e-01  3.93532140e-01
  3.93736966e-01  3.94538431e-01  3.95973191e-01  3.98033899e-01
  3.98094357e-01  3.98108755e-01  4.01248287e-01  4.03920691e-01
  4.06356776e-01  4.06858637e-01  4.07590118e-01  4.09669724e-01
  4.15119757e-01  4.19512558e-01  4.19618430e-01  4.20966547e-01
  4.26407912e-01  4.28269102e-01  4.28957450e-01  4.30061115e-01
  4.30831156e-01  4.31781771e-01  4.32584197e-01  4.33523502e-01
  4.35232843e-01  4.36262235e-01  4.37711000e-01  4.38328832e-01
  4.39340866e-01  4.40357516e-01  4.40841310e-01  4.42374534e-01
  4.42917506e-01  4.44129054e-01  4.45163818e-01  4.47874273e-01
  4.48026298e-01  4.50864512e-01  4.54480902e-01  4.55526093e-01
  4.56120028e-01  4.56874392e-01  4.57798794e-01  4.62059886e-01
  4.63103449e-01  4.64818529e-01  4.66637968e-01  4.68587963e-01
  4.69344322e-01  4.72129364e-01  4.72696067e-01  4.73231436e-01
  4.74200958e-01  4.74243080e-01  4.74599944e-01  4.75572735e-01
  4.76763100e-01  4.76951190e-01  4.80489528e-01  4.81878219e-01
  4.86524971e-01  4.88722192e-01  4.89144035e-01  4.90136945e-01
  4.90923619e-01  4.96369069e-01  4.98094177e-01  4.99480826e-01
  5.00651560e-01  5.05540388e-01  5.05639573e-01  5.05685233e-01
  5.06098778e-01  5.06239437e-01  5.11154296e-01  5.17055936e-01
  5.21085298e-01  5.23569596e-01  5.24363026e-01  5.25583556e-01
  5.27619435e-01  5.31362577e-01  5.31426996e-01  5.33732739e-01
  5.34136792e-01  5.35656287e-01  5.40202116e-01  5.40753646e-01
  5.43228897e-01  5.51621930e-01  5.53227630e-01  5.53585598e-01
  5.54284892e-01  5.54429767e-01  5.55678074e-01  5.56108209e-01
  5.56838660e-01  5.57015731e-01  5.59792622e-01  5.60404115e-01
  5.60421495e-01  5.60871878e-01  5.64127456e-01  5.66071219e-01
  5.68547211e-01  5.69393878e-01  5.69956533e-01  5.70334329e-01
  5.74668826e-01  5.75559078e-01  5.75630174e-01  5.75714462e-01
  5.76130333e-01  5.76206475e-01  5.77583699e-01  5.79698807e-01
  5.79811865e-01  5.85880168e-01  5.89544830e-01  5.91914875e-01
  5.92437699e-01  6.07017452e-01  6.07918891e-01  6.09256391e-01
  6.10704552e-01  6.12244325e-01  6.15720098e-01  6.16198873e-01
  6.16612515e-01  6.22852063e-01  6.27184999e-01  6.27389391e-01
  6.31008719e-01  6.33559077e-01  6.34504263e-01  6.36979582e-01
  6.38641839e-01  6.40299705e-01  6.47600730e-01  6.48037639e-01
  6.50575548e-01  6.50668424e-01  6.55159508e-01  6.56796814e-01
  6.59114071e-01  6.63057602e-01  6.69459020e-01  6.71326068e-01
  6.78791750e-01  6.83756634e-01  6.87845241e-01  6.92638561e-01
  6.94242623e-01  6.94265206e-01  6.97263983e-01  6.97767292e-01
  7.09144843e-01  7.16116857e-01  7.19600921e-01  7.25472248e-01
  7.34536799e-01]

  warnings.warn(

2022-11-03 10:50:02,295:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.87642799e-01 -1.62929345e-01 -1.60162427e-01 -1.58967038e-01
 -1.49996106e-01 -1.48520718e-01 -1.45839637e-01 -1.44535895e-01
 -1.36062380e-01 -1.35368127e-01 -1.31472286e-01 -1.29789551e-01
 -1.26857581e-01 -1.26821190e-01 -1.25075438e-01 -1.23499152e-01
 -1.22744739e-01 -1.20360530e-01 -1.18281998e-01 -1.17041329e-01
 -1.16388591e-01 -1.05400292e-01 -1.05175092e-01 -1.04566606e-01
 -1.02554677e-01 -1.01982367e-01 -9.95851288e-02 -9.94075324e-02
 -9.39880865e-02 -9.38074635e-02 -8.87095177e-02 -8.74564878e-02
 -8.26639326e-02 -8.24956336e-02 -8.12749238e-02 -7.85131408e-02
 -7.29077810e-02 -7.28610671e-02 -7.07008035e-02 -6.50091643e-02
 -6.25510481e-02 -6.17937901e-02 -5.97256383e-02 -5.90432096e-02
 -5.66585347e-02 -5.44045935e-02 -5.00019052e-02 -4.79105706e-02
 -4.68107225e-02 -4.45706298e-02 -4.30460656e-02 -4.22403918e-02
 -4.14605625e-02 -4.12665629e-02 -3.40479921e-02 -3.21608961e-02
 -2.93881989e-02 -2.49552349e-02 -2.36141441e-02 -2.30354425e-02
 -2.25116879e-02 -2.16528656e-02 -2.04574368e-02 -1.89296222e-02
 -1.72249146e-02 -1.50459201e-02 -1.48453114e-02 -1.39537737e-02
 -1.39176199e-02 -1.30167713e-02 -1.00186534e-02 -9.91533736e-03
 -6.50311709e-03 -6.18656153e-03 -6.04569513e-03 -5.59021719e-03
 -3.34309443e-03 -1.93572124e-04  1.92909981e-03  2.00665242e-03
  4.39905735e-03  6.29112528e-03  7.87904395e-03  8.11402176e-03
  9.22605776e-03  1.19062152e-02  1.24869587e-02  1.42590691e-02
  1.46193136e-02  1.55391651e-02  1.72981722e-02  1.93099944e-02
  2.14215947e-02  2.14729578e-02  2.24912537e-02  2.25961562e-02
  2.37122757e-02  2.44030916e-02  2.73589284e-02  2.87383692e-02
  3.22678447e-02  3.41588734e-02  3.47856752e-02  3.63089886e-02
  3.69345538e-02  3.80267976e-02  3.89539777e-02  4.02885408e-02
  4.26622098e-02  4.27317133e-02  4.42641729e-02  4.69037099e-02
  4.72883379e-02  4.74978040e-02  5.12560360e-02  5.19134922e-02
  5.27478539e-02  5.62950270e-02  5.64154038e-02  5.66689762e-02
  6.18457680e-02  6.22441594e-02  6.35577000e-02  6.73780637e-02
  7.08251965e-02  7.34343383e-02  7.35994039e-02  7.44864632e-02
  7.53012800e-02  7.88370595e-02  7.97405312e-02  8.25498576e-02
  8.27308971e-02  8.34556842e-02  8.34683224e-02  8.37209191e-02
  8.39040067e-02  8.44168425e-02  8.54589310e-02  8.62568184e-02
  9.21006012e-02  9.26420178e-02  9.80412298e-02  1.00049970e-01
  1.02768318e-01  1.04128562e-01  1.07319535e-01  1.09636651e-01
  1.09791272e-01  1.10610426e-01  1.10710670e-01  1.11126803e-01
  1.12161062e-01  1.15188573e-01  1.15492382e-01  1.16160219e-01
  1.18068923e-01  1.18096463e-01  1.18269632e-01  1.25523558e-01
  1.26145253e-01  1.29659764e-01  1.29785154e-01  1.31800847e-01
  1.33283851e-01  1.33783746e-01  1.41728349e-01  1.41858556e-01
  1.41987128e-01  1.42223266e-01  1.42877329e-01  1.44068371e-01
  1.45688391e-01  1.46052902e-01  1.46912359e-01  1.47082540e-01
  1.47170099e-01  1.50259122e-01  1.50956896e-01  1.51515407e-01
  1.52306230e-01  1.60070503e-01  1.60454134e-01  1.61536229e-01
  1.61950558e-01  1.64058083e-01  1.71369373e-01  1.72137094e-01
  1.72616002e-01  1.73361381e-01  1.73511826e-01  1.77369323e-01
  1.79201884e-01  1.82698729e-01  1.84041196e-01  1.88501020e-01
  1.89683639e-01  1.90465342e-01  1.91924466e-01  1.93211061e-01
  2.00568520e-01  2.03230297e-01  2.07605805e-01  2.08744784e-01
  2.09935556e-01  2.10091291e-01  2.11171732e-01  2.11870033e-01
  2.12353085e-01  2.14096703e-01  2.15464351e-01  2.16932966e-01
  2.18397816e-01  2.19580589e-01  2.19643224e-01  2.22603013e-01
  2.23262883e-01  2.26310869e-01  2.26601516e-01  2.26925592e-01
  2.27342885e-01  2.27554011e-01  2.29362886e-01  2.32087027e-01
  2.36546587e-01  2.36719738e-01  2.37183399e-01  2.37829328e-01
  2.42092710e-01  2.42283697e-01  2.44767261e-01  2.45238759e-01
  2.45890698e-01  2.46526682e-01  2.47457106e-01  2.54214899e-01
  2.57533783e-01  2.59262868e-01  2.59607796e-01  2.62988227e-01
  2.64221802e-01  2.64624672e-01  2.65488076e-01  2.65546625e-01
  2.67565007e-01  2.69560333e-01  2.73780238e-01  2.74356860e-01
  2.75554279e-01  2.76456389e-01  2.78960145e-01  2.79630862e-01
  2.80520280e-01  2.81188552e-01  2.81545550e-01  2.82200503e-01
  2.83567035e-01  2.84156059e-01  2.85689221e-01  2.86515096e-01
  2.91560497e-01  2.93971208e-01  2.96658162e-01  2.97045073e-01
  2.97415723e-01  2.97490849e-01  2.99586637e-01  3.02330095e-01
  3.03442333e-01  3.03667284e-01  3.06440238e-01  3.07228292e-01
  3.07415878e-01  3.09502868e-01  3.10450018e-01  3.10735826e-01
  3.14441565e-01  3.14904611e-01  3.15264740e-01  3.15378786e-01
  3.17639072e-01  3.21915960e-01  3.27243627e-01  3.27494099e-01
  3.27540728e-01  3.28617786e-01  3.29039621e-01  3.35243623e-01
  3.35611300e-01  3.35895248e-01  3.36970781e-01  3.38370223e-01
  3.41095153e-01  3.44838909e-01  3.46597488e-01  3.46845148e-01
  3.48949972e-01  3.49897400e-01  3.53388354e-01  3.53441426e-01
  3.59849772e-01  3.61070057e-01  3.62685074e-01  3.63329742e-01
  3.64969418e-01  3.66501593e-01  3.67735752e-01  3.68671071e-01
  3.68966040e-01  3.69970343e-01  3.71643888e-01  3.72578511e-01
  3.73822829e-01  3.75307688e-01  3.75681096e-01  3.76174231e-01
  3.76960013e-01  3.78681559e-01  3.79257046e-01  3.79891856e-01
  3.80488855e-01  3.80703880e-01  3.81479574e-01  3.87507114e-01
  3.87799133e-01  3.93145166e-01  3.93427179e-01  3.97857742e-01
  3.97952038e-01  3.98301441e-01  3.99426993e-01  3.99927728e-01
  4.00684371e-01  4.02450653e-01  4.02507173e-01  4.03223961e-01
  4.03567165e-01  4.05650452e-01  4.05750299e-01  4.05799645e-01
  4.06219971e-01  4.09421126e-01  4.09826486e-01  4.15508987e-01
  4.16892513e-01  4.16935117e-01  4.17495288e-01  4.18113241e-01
  4.18124656e-01  4.21524030e-01  4.28639113e-01  4.28863134e-01
  4.29919530e-01  4.33173972e-01  4.34287829e-01  4.36173773e-01
  4.38228384e-01  4.38263755e-01  4.38896454e-01  4.39478441e-01
  4.42510976e-01  4.46305032e-01  4.48621055e-01  4.51334448e-01
  4.51905507e-01  4.52684946e-01  4.54244173e-01  4.54372180e-01
  4.54625192e-01  4.55817252e-01  4.60141339e-01  4.65424431e-01
  4.66070109e-01  4.66312578e-01  4.67019058e-01  4.69648288e-01
  4.70509857e-01  4.72298648e-01  4.72394268e-01  4.73093670e-01
  4.76376316e-01  4.77929320e-01  4.78325143e-01  4.79357807e-01
  4.80371132e-01  4.80398676e-01  4.84585457e-01  4.89363787e-01
  4.90287667e-01  4.90853054e-01  4.91887013e-01  4.95000094e-01
  4.98401697e-01  4.98803715e-01  4.99975751e-01  5.01644459e-01
  5.02348304e-01  5.04147716e-01  5.07815126e-01  5.13104971e-01
  5.14794884e-01  5.15144914e-01  5.15726194e-01  5.16404778e-01
  5.17747453e-01  5.18565883e-01  5.18846024e-01  5.24585273e-01
  5.26414857e-01  5.26903758e-01  5.27293594e-01  5.30208672e-01
  5.32892440e-01  5.33415600e-01  5.33512187e-01  5.36191860e-01
  5.36280787e-01  5.36298006e-01  5.36532269e-01  5.38799937e-01
  5.41979651e-01  5.42336203e-01  5.42980468e-01  5.44971325e-01
  5.45575933e-01  5.50558429e-01  5.53972263e-01  5.56369955e-01
  5.57908059e-01  5.60959009e-01  5.61101686e-01  5.61506294e-01
  5.67428974e-01  5.70362038e-01  5.73129921e-01  5.77001002e-01
  5.78013754e-01  5.78711657e-01  5.78880363e-01  5.90001433e-01
  5.91699194e-01  5.96771265e-01  5.96801756e-01  5.99740817e-01
  6.00948590e-01  6.01157320e-01  6.01719458e-01  6.03685143e-01
  6.07256454e-01  6.07289830e-01  6.09859882e-01  6.10533165e-01
  6.12626069e-01  6.13095228e-01  6.13191443e-01  6.14471269e-01
  6.15279296e-01  6.16920322e-01  6.17933046e-01  6.20416982e-01
  6.20516285e-01  6.21530324e-01  6.22272388e-01  6.25102231e-01
  6.25284184e-01  6.26863613e-01  6.27005415e-01  6.27899985e-01
  6.28172117e-01  6.29494033e-01  6.32837213e-01  6.36623702e-01
  6.39374061e-01  6.42411313e-01  6.52281056e-01  6.60158862e-01
  6.61339925e-01  6.62504958e-01  6.63079132e-01  6.65121129e-01
  6.67425003e-01  6.68150175e-01  6.73829832e-01  6.76979878e-01
  6.84394826e-01  6.86163744e-01  6.88033416e-01  7.07335490e-01
  7.08941587e-01  7.18138843e-01  7.42895870e-01  7.43626792e-01
  7.58767626e-01]

  warnings.warn(

2022-11-03 10:50:02,327:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.25673012 -0.18928266 -0.18469016 -0.17868482 -0.17370742 -0.17248065
 -0.16972361 -0.16706492 -0.16359814 -0.15922427 -0.15815055 -0.15430371
 -0.14842358 -0.14732471 -0.14704498 -0.14090611 -0.13555668 -0.13371695
 -0.12908503 -0.12900906 -0.12315133 -0.11989664 -0.1188972  -0.11754277
 -0.11602213 -0.11105468 -0.10478617 -0.10268161 -0.10244518 -0.10172873
 -0.09720959 -0.09330662 -0.09077263 -0.09053115 -0.08599952 -0.08097342
 -0.08004602 -0.07890947 -0.07847512 -0.07798814 -0.06990207 -0.06676709
 -0.06420763 -0.06249359 -0.06174803 -0.06165133 -0.06133053 -0.05860642
 -0.05771604 -0.05748284 -0.0572407  -0.05700893 -0.05441777 -0.05432672
 -0.05395349 -0.05362728 -0.05080962 -0.04991135 -0.04947918 -0.04922091
 -0.04885126 -0.04318646 -0.04164557 -0.04158553 -0.03668416 -0.03163038
 -0.03018425 -0.02816951 -0.02737862 -0.02691031 -0.02587975 -0.02459504
 -0.02209855 -0.02157241 -0.0186596  -0.01683405 -0.01475458 -0.01210854
 -0.01100831 -0.00732126 -0.00665795 -0.00321759 -0.00151452 -0.00106906
  0.0040785   0.01008817  0.01191116  0.01484585  0.01586965  0.01789862
  0.0180467   0.02448223  0.02475673  0.02759881  0.02780438  0.02843082
  0.029373    0.03177217  0.03337059  0.03341139  0.03586602  0.0380098
  0.03821658  0.04062341  0.04193673  0.04205322  0.04288253  0.04604386
  0.04668832  0.04747835  0.04820201  0.04854782  0.0488829   0.04914914
  0.05066605  0.05114086  0.05334533  0.0575347   0.05936988  0.06018869
  0.06054156  0.06094739  0.06095357  0.06142736  0.06153681  0.06349724
  0.06429844  0.06459413  0.06551674  0.06606878  0.06695876  0.06755838
  0.06834632  0.06961911  0.06963944  0.07054962  0.07239184  0.07276533
  0.07322426  0.0789695   0.07938769  0.08035618  0.0812935   0.08137342
  0.08141526  0.0842788   0.08477142  0.08536092  0.08616549  0.08889035
  0.08948205  0.09797772  0.10075395  0.10103333  0.10145011  0.1056661
  0.10909609  0.11253068  0.11659985  0.12016317  0.12318119  0.12361628
  0.12397383  0.12678241  0.12709341  0.12779855  0.13046052  0.13110419
  0.13363277  0.13554909  0.13637862  0.1382151   0.13829778  0.13855699
  0.14067195  0.1419196   0.14255841  0.14275507  0.14507639  0.14542483
  0.14673535  0.14833425  0.14836857  0.15165524  0.15183871  0.15205198
  0.15205507  0.15545023  0.15606247  0.15667122  0.15693671  0.16137869
  0.16223435  0.16299475  0.16710537  0.16862062  0.17207842  0.17310288
  0.1751268   0.17715253  0.18096352  0.18155913  0.18359493  0.18380281
  0.18575054  0.1865692   0.18732878  0.18825684  0.18931274  0.18932054
  0.192967    0.19763313  0.19775829  0.19834042  0.19851001  0.19975211
  0.20001525  0.20069113  0.20103966  0.20145184  0.20292439  0.20442161
  0.20494687  0.20776315  0.20825897  0.20868293  0.20914442  0.20959812
  0.21135448  0.21254913  0.21540493  0.21570815  0.21583563  0.21887392
  0.22005182  0.22173136  0.22187887  0.22413114  0.22419026  0.22472057
  0.22778844  0.22828212  0.22962053  0.23075654  0.23506825  0.2358922
  0.23760413  0.23813828  0.23856332  0.23886689  0.23918204  0.24070185
  0.24182338  0.24221748  0.24415496  0.24495922  0.24677217  0.25057934
  0.25429417  0.25555079  0.25714728  0.25732679  0.25781093  0.25797321
  0.26000029  0.26542203  0.26603342  0.26716963  0.26719124  0.26838189
  0.26927854  0.2695292   0.27227746  0.27308474  0.27310862  0.27337697
  0.27497592  0.27563479  0.27637087  0.27679296  0.27691509  0.27738246
  0.27935983  0.28098919  0.28315441  0.28353379  0.28701833  0.28729985
  0.28762535  0.28902551  0.29095592  0.29130149  0.29246153  0.29251412
  0.29545432  0.30113047  0.30318882  0.30438199  0.30553951  0.30720902
  0.30808899  0.30830726  0.30929534  0.30937352  0.31007307  0.31111846
  0.3142527   0.31476028  0.31515295  0.31565444  0.32028686  0.32224238
  0.32358479  0.32543341  0.32977673  0.33500042  0.33626332  0.33637595
  0.33901478  0.34438633  0.3450429   0.34513782  0.34751978  0.34822393
  0.34949348  0.35063259  0.35273496  0.35391931  0.35436642  0.35674581
  0.35911727  0.36018839  0.3606988   0.36095977  0.36135123  0.36365786
  0.3636906   0.36396391  0.36417358  0.36832931  0.37371409  0.37401977
  0.37565311  0.37701306  0.37910456  0.38085047  0.3873719   0.38749107
  0.38925254  0.39296029  0.39888029  0.40199162  0.40357871  0.40561751
  0.40998418  0.4105892   0.4153835   0.41575026  0.41772176  0.42208825
  0.42226512  0.42667936  0.42668858  0.4282259   0.42957215  0.42976644
  0.43191849  0.43299144  0.44041252  0.44238587  0.44265275  0.44733335
  0.44766065  0.44796471  0.44796816  0.44956215  0.45079451  0.45854666
  0.46389364  0.46497244  0.47299951  0.47587334  0.47824828  0.47974949
  0.48123202  0.48167879  0.48220596  0.4835059   0.4840529   0.48565889
  0.48727044  0.48927472  0.49316616  0.49525487  0.49544985  0.49600376
  0.5007673   0.50123221  0.5038754   0.50388723  0.50532008  0.5101269
  0.51125228  0.5114937   0.51761715  0.52210849  0.52267682  0.52317162
  0.52533886  0.52639534  0.52821555  0.53051506  0.53111205  0.53252599
  0.53325164  0.53515183  0.53547     0.53689451  0.53898383  0.54103739
  0.5412498   0.54239507  0.54321818  0.54334222  0.54576232  0.54796694
  0.54835635  0.55052517  0.55102802  0.55127206  0.55416132  0.55608392
  0.55637787  0.55713576  0.55823593  0.558263    0.56013837  0.56361517
  0.57216774  0.5757967   0.57749626  0.57931301  0.58000725  0.58045791
  0.58159043  0.58791499  0.5881489   0.5887347   0.58928191  0.5913947
  0.59151773  0.59809122  0.59955705  0.60181789  0.6020408   0.60612662
  0.61319188  0.61469544  0.61616804  0.62521422  0.62550199  0.62591737
  0.62706533  0.62759294  0.62809509  0.62910247  0.62991181  0.63421216
  0.63900214  0.64095647  0.64719681  0.65099007  0.6536679   0.65597422
  0.65625755  0.66764162  0.67172357  0.67469027  0.67539615  0.67646896
  0.67822633  0.67839768  0.67846094  0.6787406   0.68030324  0.68311283
  0.68791549  0.68899136  0.6924184   0.71355539  0.71654626  0.72280334
  0.72921307]

  warnings.warn(

2022-11-03 10:50:02,343:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.76828970e-01 -1.71590919e-01 -1.60074520e-01 -1.54233416e-01
 -1.53754253e-01 -1.35760584e-01 -1.33551417e-01 -1.32461881e-01
 -1.25838367e-01 -1.17038122e-01 -1.16500262e-01 -1.15088990e-01
 -1.13429493e-01 -1.11939679e-01 -1.11383469e-01 -1.09295667e-01
 -1.08794800e-01 -1.05104520e-01 -1.02609051e-01 -1.01706842e-01
 -1.00838178e-01 -1.00060102e-01 -9.92363730e-02 -9.65399041e-02
 -9.46506116e-02 -9.17803428e-02 -9.06399124e-02 -8.03530817e-02
 -7.89968938e-02 -7.75164309e-02 -7.71067875e-02 -7.69268583e-02
 -7.57457605e-02 -6.90883016e-02 -6.83036689e-02 -6.17971968e-02
 -6.04433692e-02 -6.01744042e-02 -5.86243117e-02 -5.81042684e-02
 -5.63905720e-02 -5.55752302e-02 -5.30256916e-02 -5.25311828e-02
 -5.18352272e-02 -4.95095232e-02 -4.75784811e-02 -4.73616742e-02
 -4.57105651e-02 -4.46881460e-02 -4.37709708e-02 -4.34436666e-02
 -4.05282443e-02 -3.80974203e-02 -3.69345616e-02 -3.62506776e-02
 -3.53203570e-02 -3.46977167e-02 -3.38618021e-02 -3.31728175e-02
 -2.97655185e-02 -2.77641212e-02 -2.58381100e-02 -2.56689700e-02
 -2.51336614e-02 -2.50571707e-02 -2.48193960e-02 -2.05577396e-02
 -1.99662980e-02 -1.69089379e-02 -1.59841051e-02 -1.48097797e-02
 -1.46612823e-02 -9.85119430e-03 -8.24179321e-03 -3.09162106e-03
 -2.16165421e-03 -1.65463332e-03 -2.03126882e-04  8.15562068e-06
  4.98402147e-03  7.44600853e-03  1.02434234e-02  1.05811733e-02
  1.52127800e-02  1.58290733e-02  1.86435511e-02  1.86641338e-02
  2.14386405e-02  2.21812666e-02  2.32399765e-02  2.45357686e-02
  2.52718538e-02  2.70195193e-02  2.76674864e-02  2.77928925e-02
  3.04522416e-02  3.08062873e-02  3.09463859e-02  3.46253724e-02
  3.53403464e-02  3.58346150e-02  3.62356384e-02  3.74417980e-02
  3.82654510e-02  3.92358557e-02  4.14142745e-02  4.21686074e-02
  4.21755080e-02  4.22210887e-02  4.37041475e-02  4.45280409e-02
  4.52454903e-02  5.01641867e-02  5.14701976e-02  5.31305104e-02
  5.49214926e-02  5.56424330e-02  5.59853385e-02  5.60559178e-02
  5.71679814e-02  5.84685098e-02  5.98297980e-02  6.24660123e-02
  6.46230377e-02  6.59403227e-02  6.73136198e-02  6.79505493e-02
  6.87671958e-02  6.91527335e-02  6.93247992e-02  6.99943193e-02
  7.06540423e-02  7.40828575e-02  7.69243052e-02  7.74713345e-02
  7.75698252e-02  7.84758769e-02  8.07963095e-02  8.14315408e-02
  8.27171309e-02  8.30835894e-02  8.56577914e-02  8.61202990e-02
  8.74514238e-02  8.92051393e-02  9.11343239e-02  9.13090491e-02
  9.18111157e-02  9.21068953e-02  9.51817366e-02  9.57133179e-02
  9.71698666e-02  9.95515120e-02  1.00874940e-01  1.02105049e-01
  1.04753719e-01  1.05660935e-01  1.06040994e-01  1.06668271e-01
  1.06862947e-01  1.08689657e-01  1.10108758e-01  1.13412228e-01
  1.14168775e-01  1.14886257e-01  1.16700158e-01  1.17372249e-01
  1.21480659e-01  1.24704294e-01  1.27825327e-01  1.28326517e-01
  1.29905645e-01  1.31903977e-01  1.33956037e-01  1.39452550e-01
  1.40565854e-01  1.49006685e-01  1.52111030e-01  1.53052374e-01
  1.55638544e-01  1.55852230e-01  1.56153701e-01  1.58908051e-01
  1.58916757e-01  1.59320338e-01  1.59632253e-01  1.61553977e-01
  1.62308063e-01  1.62631600e-01  1.63239273e-01  1.63970966e-01
  1.65400834e-01  1.67744732e-01  1.69409182e-01  1.70200607e-01
  1.71577613e-01  1.72650654e-01  1.74332081e-01  1.76668793e-01
  1.78249035e-01  1.81248698e-01  1.81459875e-01  1.82512820e-01
  1.83982105e-01  1.88441880e-01  1.88655686e-01  1.89848852e-01
  1.90736604e-01  1.92298419e-01  1.94421113e-01  1.98386300e-01
  1.99683331e-01  2.00619640e-01  2.00989311e-01  2.02818361e-01
  2.04494302e-01  2.05432037e-01  2.06751453e-01  2.06929719e-01
  2.07346486e-01  2.07809537e-01  2.07992895e-01  2.10484206e-01
  2.11319619e-01  2.11459960e-01  2.15608003e-01  2.19177550e-01
  2.20660602e-01  2.23231563e-01  2.23328837e-01  2.27690324e-01
  2.28084914e-01  2.28258642e-01  2.28906586e-01  2.29709847e-01
  2.29876168e-01  2.34117328e-01  2.36648358e-01  2.37783622e-01
  2.39450639e-01  2.40454788e-01  2.41435331e-01  2.43926043e-01
  2.44454053e-01  2.47078210e-01  2.47522202e-01  2.49226023e-01
  2.49489784e-01  2.53536602e-01  2.56838799e-01  2.57268689e-01
  2.59456426e-01  2.60866807e-01  2.61045491e-01  2.63134590e-01
  2.64950540e-01  2.66969778e-01  2.67653795e-01  2.68719169e-01
  2.69415381e-01  2.69599170e-01  2.70587445e-01  2.72027198e-01
  2.75375763e-01  2.77384029e-01  2.78114262e-01  2.79386590e-01
  2.80013012e-01  2.80238803e-01  2.84934635e-01  2.86328533e-01
  2.87644820e-01  2.88524457e-01  2.92390551e-01  2.92742897e-01
  2.93208408e-01  2.93312143e-01  2.96445530e-01  2.98256748e-01
  2.98365610e-01  2.98767273e-01  2.99033117e-01  2.99419398e-01
  2.99778438e-01  3.03531510e-01  3.04870281e-01  3.06160536e-01
  3.06819081e-01  3.07362572e-01  3.07481063e-01  3.07593984e-01
  3.07769271e-01  3.10477076e-01  3.13067822e-01  3.14376786e-01
  3.14624999e-01  3.15849516e-01  3.18379503e-01  3.18554257e-01
  3.19027010e-01  3.20464702e-01  3.22056721e-01  3.22808893e-01
  3.22809392e-01  3.28589192e-01  3.29107710e-01  3.30089319e-01
  3.31001306e-01  3.31920809e-01  3.33944077e-01  3.36306619e-01
  3.39043529e-01  3.41649017e-01  3.45053127e-01  3.47811905e-01
  3.49203531e-01  3.50015594e-01  3.51526776e-01  3.53123103e-01
  3.55200558e-01  3.55388683e-01  3.57168020e-01  3.58612946e-01
  3.58989094e-01  3.60087481e-01  3.60592265e-01  3.63474439e-01
  3.63763798e-01  3.66045287e-01  3.66621314e-01  3.67057851e-01
  3.68296817e-01  3.69975278e-01  3.70732555e-01  3.72438024e-01
  3.73466860e-01  3.75312022e-01  3.75977138e-01  3.81285658e-01
  3.83788074e-01  3.84157003e-01  3.84493599e-01  3.88150338e-01
  3.90226797e-01  3.93561955e-01  3.96175629e-01  4.01164172e-01
  4.04266362e-01  4.05352072e-01  4.12868229e-01  4.12932899e-01
  4.13247697e-01  4.13609466e-01  4.14089768e-01  4.14667631e-01
  4.15573631e-01  4.19887153e-01  4.21804021e-01  4.23553221e-01
  4.24276146e-01  4.26017007e-01  4.26203539e-01  4.26262530e-01
  4.34462828e-01  4.36573217e-01  4.37353302e-01  4.38545434e-01
  4.41785737e-01  4.44099947e-01  4.46395743e-01  4.49317434e-01
  4.49329431e-01  4.49447897e-01  4.50491964e-01  4.51289664e-01
  4.55347155e-01  4.56609081e-01  4.58432099e-01  4.59064721e-01
  4.59356465e-01  4.60865732e-01  4.60949787e-01  4.63882765e-01
  4.68299103e-01  4.68738903e-01  4.69909047e-01  4.73102315e-01
  4.76480006e-01  4.76760631e-01  4.77177494e-01  4.78797523e-01
  4.81139207e-01  4.85327175e-01  4.89884066e-01  4.91808739e-01
  4.92500030e-01  4.93823502e-01  4.96253001e-01  4.97267539e-01
  4.99699543e-01  5.01555007e-01  5.03398937e-01  5.06210835e-01
  5.09613397e-01  5.13528798e-01  5.13587699e-01  5.13869720e-01
  5.19318546e-01  5.22035845e-01  5.22507502e-01  5.25244696e-01
  5.28032313e-01  5.28759864e-01  5.30013458e-01  5.30127594e-01
  5.30508558e-01  5.35903881e-01  5.39752239e-01  5.45661764e-01
  5.45890323e-01  5.47712762e-01  5.47891902e-01  5.50044906e-01
  5.52096380e-01  5.53671158e-01  5.60032958e-01  5.61498711e-01
  5.68278403e-01  5.74696319e-01  5.78761926e-01  5.83139858e-01
  5.85206103e-01  5.85641166e-01  5.86640733e-01  5.93164100e-01
  5.94548245e-01  5.95417966e-01  5.96593657e-01  5.97208020e-01
  5.98201852e-01  5.98325363e-01  6.00104142e-01  6.02111061e-01
  6.03521359e-01  6.03553934e-01  6.05935122e-01  6.06261823e-01
  6.06507172e-01  6.06953774e-01  6.12001248e-01  6.12560296e-01
  6.12849245e-01  6.13269654e-01  6.15091948e-01  6.17064177e-01
  6.18273199e-01  6.19377878e-01  6.24037071e-01  6.27114018e-01
  6.27962170e-01  6.29657402e-01  6.32783546e-01  6.35194715e-01
  6.38056403e-01  6.41347368e-01  6.42440633e-01  6.46976870e-01
  6.49446479e-01  6.53948993e-01  6.54646347e-01  6.57946539e-01
  6.58463372e-01  6.58914496e-01  6.61215993e-01  6.61245463e-01
  6.65931179e-01  6.66746575e-01  6.66840200e-01  6.73361811e-01
  6.84028019e-01  6.84917464e-01  7.02234432e-01  7.08033739e-01
  7.09314070e-01  7.12785068e-01  7.21896567e-01  7.22050583e-01
  7.22933656e-01  7.22964407e-01  7.26397217e-01  7.64551607e-01]

  warnings.warn(

2022-11-03 10:50:02,407:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.21995375 -0.20156224 -0.19517494 -0.17532196 -0.17526044 -0.17190587
 -0.16542606 -0.16485416 -0.16473621 -0.16382731 -0.15718968 -0.1567626
 -0.15370434 -0.15249719 -0.14336911 -0.13846612 -0.13716038 -0.13617887
 -0.13612657 -0.12946446 -0.12688151 -0.11988778 -0.11883769 -0.11591722
 -0.11344735 -0.11168056 -0.11134392 -0.11102781 -0.10922339 -0.10921945
 -0.10552838 -0.09637341 -0.09617047 -0.09268386 -0.09246421 -0.09106299
 -0.09049436 -0.08483331 -0.08423254 -0.08395837 -0.08332911 -0.08278316
 -0.08254977 -0.08042081 -0.07882946 -0.07486019 -0.06887859 -0.06816356
 -0.06648795 -0.06282146 -0.06253692 -0.06240397 -0.06055665 -0.0561319
 -0.05561258 -0.05423256 -0.05390765 -0.0529403  -0.05242863 -0.04733807
 -0.04646914 -0.04635257 -0.04583403 -0.04499208 -0.04491378 -0.04151733
 -0.03846217 -0.03759608 -0.03595636 -0.03356973 -0.03300069 -0.03244074
 -0.02533164 -0.01986503 -0.01693559 -0.01556187 -0.01546881 -0.01449584
 -0.01336515 -0.01099972 -0.01066485 -0.00811392 -0.00693457 -0.00489688
 -0.00175242  0.00122484  0.00182201  0.00604517  0.0096298   0.01149551
  0.01206894  0.01391786  0.01686972  0.01817437  0.01833533  0.02119341
  0.02133919  0.02310854  0.02466553  0.02953944  0.03039273  0.03070166
  0.03143429  0.03695     0.03903229  0.03980566  0.04090561  0.04462778
  0.05041741  0.050648    0.05099537  0.05499355  0.05512309  0.05532132
  0.05673146  0.05906322  0.05919481  0.06075396  0.06105576  0.06563767
  0.06850039  0.06963128  0.07166113  0.07637359  0.07687042  0.07828668
  0.07857365  0.0807738   0.08452663  0.08629059  0.08664293  0.08746303
  0.09033173  0.09243359  0.09263015  0.09350479  0.09474971  0.0978683
  0.09975243  0.10090523  0.1027711   0.10537573  0.10595945  0.10804259
  0.10932383  0.11028343  0.11029149  0.11100749  0.11376965  0.1172224
  0.12107135  0.12406677  0.12412644  0.1243029   0.12777708  0.12947169
  0.13038808  0.13263488  0.13294959  0.13309386  0.13636565  0.1377309
  0.13850561  0.13903155  0.14279615  0.14347773  0.14434866  0.1462418
  0.14681006  0.14896265  0.1550299   0.15634256  0.15789115  0.15820876
  0.15889219  0.15946524  0.16078833  0.16362414  0.16471883  0.16584496
  0.16971421  0.1720216   0.17376434  0.17603084  0.18207719  0.18220787
  0.18256065  0.18358076  0.18529167  0.18637677  0.19150296  0.192734
  0.19293644  0.19298236  0.19393749  0.19637504  0.1967431   0.19709383
  0.19759569  0.19800886  0.19844034  0.19845084  0.20106684  0.20250968
  0.20418742  0.2055052   0.20589124  0.20734537  0.20737285  0.21048647
  0.21133808  0.21171044  0.21825507  0.21903192  0.21981598  0.22053948
  0.22075728  0.22134801  0.22257426  0.22345965  0.22424499  0.22493684
  0.2253667   0.22542378  0.22548906  0.23096236  0.23248797  0.23663138
  0.24005468  0.24022641  0.24166679  0.24249524  0.24364751  0.24532052
  0.24730132  0.2476877   0.24929943  0.25248203  0.25406676  0.25471994
  0.25474042  0.25522692  0.2554401   0.25686827  0.25690413  0.25717003
  0.26223076  0.26396139  0.26407468  0.26540716  0.26647102  0.26658703
  0.27034574  0.2706378   0.27387613  0.27402494  0.27479193  0.27557641
  0.27621449  0.27737494  0.2777618   0.27838998  0.28099577  0.28187683
  0.28401214  0.2844458   0.28529793  0.28580957  0.28663927  0.28682262
  0.28850114  0.28851251  0.28989243  0.28997739  0.29233565  0.29494311
  0.29846965  0.29876389  0.30701713  0.31155038  0.31236627  0.31319372
  0.31334198  0.31425054  0.31583074  0.31657161  0.31722067  0.31729027
  0.3173681   0.31820598  0.32144875  0.32197988  0.32299161  0.32400343
  0.32463548  0.32531887  0.32605303  0.32634561  0.32803443  0.32852128
  0.32980238  0.33104302  0.33282144  0.33466423  0.34188786  0.34218012
  0.34652675  0.34665565  0.35022167  0.35136931  0.35162105  0.3524659
  0.35280341  0.35389056  0.3555991   0.35628712  0.35736158  0.35760006
  0.35801337  0.36007729  0.3623172   0.36423307  0.3665527   0.36909796
  0.37117461  0.37315326  0.37334409  0.37745377  0.37860499  0.38003817
  0.38471284  0.38692133  0.38791566  0.38908373  0.38993828  0.39022611
  0.39029691  0.39032836  0.39141929  0.3915439   0.39312261  0.39569472
  0.39693782  0.39920439  0.40293068  0.40632054  0.40723917  0.4127757
  0.41464425  0.41635175  0.41636185  0.41972     0.42007464  0.42231677
  0.42249514  0.4239188   0.42465395  0.42979767  0.43118197  0.43225241
  0.43263666  0.4328125   0.43445567  0.44102219  0.44105451  0.44132504
  0.44151715  0.44247983  0.44289565  0.44375305  0.44558489  0.44572274
  0.44706832  0.44790358  0.44878909  0.44988428  0.45407443  0.45600448
  0.4630452   0.46444079  0.4660532   0.46811018  0.46973204  0.47142993
  0.4729564   0.47480479  0.47871436  0.4807098   0.48099816  0.48452403
  0.48531511  0.48765297  0.48803672  0.48902529  0.48996073  0.49022132
  0.49195976  0.49254325  0.494444    0.49510512  0.49814578  0.49938263
  0.50053628  0.50286795  0.50304959  0.50337797  0.50373531  0.50579938
  0.50614557  0.50715503  0.50971351  0.51410136  0.51556894  0.51611288
  0.51937683  0.52121574  0.52411329  0.53216051  0.53281377  0.53415622
  0.53609685  0.53611704  0.53653496  0.53725898  0.53925269  0.54315348
  0.54456024  0.54486036  0.549304    0.55417157  0.55764519  0.56153977
  0.56160231  0.56248144  0.5641628   0.56581901  0.56804971  0.57039778
  0.5735457   0.57504472  0.5758139   0.57661747  0.57937371  0.57949028
  0.58038332  0.58094584  0.58804753  0.59242457  0.59480299  0.59621177
  0.59741719  0.59807833  0.60348283  0.60895934  0.60994314  0.61016638
  0.61197567  0.61222734  0.61263175  0.61504288  0.61590109  0.62467272
  0.62727327  0.62780469  0.63030568  0.63690553  0.642505    0.64462021
  0.64474076  0.64492361  0.64642337  0.64751352  0.64930491  0.65005229
  0.65053256  0.65497884  0.6617082   0.66282176  0.66337642  0.66470706
  0.66494939  0.66512453  0.66537536  0.67329833  0.67536626  0.69493366
  0.7048751   0.71299944  0.73657167  0.74688423  0.75921636  0.76776268
  0.76948426]

  warnings.warn(

2022-11-03 10:50:04,029:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:04,045:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:04,709:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.13808844e-01 -2.09173753e-01 -2.07443349e-01 -1.90821735e-01
 -1.86030049e-01 -1.71647435e-01 -1.68266034e-01 -1.51747971e-01
 -1.51216949e-01 -1.51137418e-01 -1.49946208e-01 -1.49577891e-01
 -1.39052724e-01 -1.32655682e-01 -1.32163734e-01 -1.31465927e-01
 -1.28257956e-01 -1.27042841e-01 -1.26413752e-01 -1.25099816e-01
 -1.24888447e-01 -1.24662051e-01 -1.23411706e-01 -1.21309139e-01
 -1.20499269e-01 -1.18221138e-01 -1.06674896e-01 -1.05730834e-01
 -1.01341223e-01 -9.91349153e-02 -9.71267462e-02 -9.55788703e-02
 -9.35183332e-02 -9.26751555e-02 -9.25446485e-02 -9.21121973e-02
 -9.10499744e-02 -8.91215239e-02 -8.37524877e-02 -7.67716698e-02
 -7.62168319e-02 -7.38828992e-02 -7.28941466e-02 -7.08876713e-02
 -7.07886706e-02 -6.48864922e-02 -6.00769827e-02 -5.93176617e-02
 -5.84790659e-02 -5.73694148e-02 -5.41268508e-02 -5.28297489e-02
 -4.98184212e-02 -4.88872161e-02 -4.70962933e-02 -4.70477156e-02
 -4.42225111e-02 -4.30382082e-02 -4.27737502e-02 -4.06245407e-02
 -3.98367651e-02 -3.90618497e-02 -3.76747594e-02 -3.53186776e-02
 -3.40138254e-02 -3.37692002e-02 -2.97115328e-02 -2.76807581e-02
 -2.71841268e-02 -2.26024178e-02 -2.14616189e-02 -2.03326901e-02
 -1.89302133e-02 -1.82186680e-02 -1.58550345e-02 -1.51932204e-02
 -1.42069042e-02 -1.24354957e-02 -1.04429321e-02 -9.84225601e-03
 -9.47102266e-03 -5.57915397e-03 -4.06564157e-03 -4.06219680e-03
 -1.71271556e-03 -1.35901473e-03 -1.64804971e-04  1.13630778e-03
  2.81053173e-03  2.98279769e-03  5.34608796e-03  6.23609956e-03
  6.99276017e-03  8.66732567e-03  9.12070682e-03  1.55780243e-02
  2.04126261e-02  2.20247119e-02  2.26399565e-02  2.38025116e-02
  2.42864367e-02  2.98602848e-02  2.99523029e-02  3.21878503e-02
  3.23974065e-02  3.34262184e-02  3.52450005e-02  3.64047101e-02
  4.08066264e-02  4.34728325e-02  4.76591241e-02  4.85309113e-02
  4.91113118e-02  4.95568938e-02  5.32121521e-02  5.60030061e-02
  5.86330113e-02  5.95480324e-02  6.01138897e-02  6.11387418e-02
  6.28092946e-02  6.44947333e-02  6.49225072e-02  6.54623712e-02
  6.62700548e-02  6.84301359e-02  6.85575174e-02  7.02576279e-02
  7.14701277e-02  7.20266155e-02  7.37930795e-02  7.47211380e-02
  7.49301377e-02  7.54888294e-02  8.03495251e-02  8.41906934e-02
  8.52403698e-02  8.54353496e-02  8.71562391e-02  8.72230915e-02
  8.72522669e-02  8.77712202e-02  8.79049847e-02  9.19845458e-02
  9.26359473e-02  9.35275422e-02  9.55800594e-02  9.61981687e-02
  9.80993609e-02  1.01326888e-01  1.01379761e-01  1.02150401e-01
  1.03862949e-01  1.05245777e-01  1.09135275e-01  1.11602020e-01
  1.12327002e-01  1.12375769e-01  1.12690676e-01  1.13478792e-01
  1.13647890e-01  1.16199187e-01  1.16646952e-01  1.21482893e-01
  1.21566378e-01  1.22385368e-01  1.22586609e-01  1.22908380e-01
  1.23641196e-01  1.25467003e-01  1.25939344e-01  1.26463328e-01
  1.28662109e-01  1.31436952e-01  1.31971491e-01  1.33557354e-01
  1.35755288e-01  1.39277032e-01  1.39380372e-01  1.40015679e-01
  1.40442969e-01  1.41247015e-01  1.41321471e-01  1.42406469e-01
  1.42553363e-01  1.45331022e-01  1.48157813e-01  1.53653170e-01
  1.53914943e-01  1.54648976e-01  1.55411547e-01  1.55493257e-01
  1.56873935e-01  1.58485337e-01  1.59468259e-01  1.61962540e-01
  1.63196848e-01  1.64283153e-01  1.64836651e-01  1.65822618e-01
  1.65828972e-01  1.67998539e-01  1.70842505e-01  1.73517748e-01
  1.75996485e-01  1.76553807e-01  1.78440602e-01  1.78522057e-01
  1.81072047e-01  1.81362642e-01  1.83209427e-01  1.84015134e-01
  1.84492719e-01  1.89159068e-01  1.89449419e-01  1.91881875e-01
  1.93597891e-01  1.94607487e-01  1.95508371e-01  1.96911188e-01
  2.01110297e-01  2.02157095e-01  2.02283618e-01  2.02754127e-01
  2.02967545e-01  2.03276532e-01  2.03431101e-01  2.04808617e-01
  2.04944351e-01  2.10152747e-01  2.13086500e-01  2.14789513e-01
  2.20916272e-01  2.24292164e-01  2.25903939e-01  2.26346975e-01
  2.26590945e-01  2.30847592e-01  2.31407936e-01  2.32745969e-01
  2.32872781e-01  2.33180475e-01  2.34352402e-01  2.34510389e-01
  2.36650533e-01  2.38437111e-01  2.38814057e-01  2.40450276e-01
  2.43376916e-01  2.44331706e-01  2.46421728e-01  2.48293216e-01
  2.49406172e-01  2.50916534e-01  2.54196815e-01  2.59047286e-01
  2.59170262e-01  2.63747523e-01  2.64566535e-01  2.65438429e-01
  2.65456883e-01  2.67219232e-01  2.71140403e-01  2.73352796e-01
  2.80369657e-01  2.81218396e-01  2.81890963e-01  2.82532060e-01
  2.85913129e-01  2.87387377e-01  2.89071082e-01  2.92742157e-01
  2.93385913e-01  2.94401844e-01  2.95786774e-01  2.96022781e-01
  2.96373416e-01  2.96708133e-01  2.97156914e-01  2.97603129e-01
  2.98665260e-01  2.98782636e-01  2.99539381e-01  3.01233927e-01
  3.01786419e-01  3.02490254e-01  3.02665091e-01  3.06900083e-01
  3.07321522e-01  3.09579416e-01  3.12008883e-01  3.14856474e-01
  3.20370569e-01  3.23071474e-01  3.26139729e-01  3.26681600e-01
  3.27542360e-01  3.27734089e-01  3.28185879e-01  3.28211300e-01
  3.28496129e-01  3.29135310e-01  3.29726160e-01  3.30192559e-01
  3.33955802e-01  3.39569255e-01  3.40340182e-01  3.41220302e-01
  3.42363645e-01  3.45405152e-01  3.47015863e-01  3.52102259e-01
  3.53883476e-01  3.54112185e-01  3.55651029e-01  3.56076815e-01
  3.59260748e-01  3.60720008e-01  3.60878626e-01  3.62679372e-01
  3.62865799e-01  3.64790794e-01  3.65054422e-01  3.67225841e-01
  3.67709329e-01  3.68547060e-01  3.70889601e-01  3.71235160e-01
  3.72822835e-01  3.72948267e-01  3.78279150e-01  3.81116836e-01
  3.83182599e-01  3.86533218e-01  3.92319014e-01  3.93500952e-01
  3.95810935e-01  3.96484314e-01  3.97800334e-01  3.99712586e-01
  4.00830665e-01  4.01555282e-01  4.02176136e-01  4.03412045e-01
  4.03560211e-01  4.05581825e-01  4.05645234e-01  4.05886903e-01
  4.08174661e-01  4.09063228e-01  4.11614884e-01  4.15675585e-01
  4.16746175e-01  4.16877740e-01  4.18935483e-01  4.19832094e-01
  4.21344981e-01  4.22765367e-01  4.28365472e-01  4.28468383e-01
  4.29070004e-01  4.30957046e-01  4.34426134e-01  4.34801624e-01
  4.36953329e-01  4.37502610e-01  4.38630548e-01  4.38886638e-01
  4.40077151e-01  4.40494529e-01  4.47110453e-01  4.47782274e-01
  4.49100743e-01  4.52154928e-01  4.52976624e-01  4.53226038e-01
  4.55844351e-01  4.55900398e-01  4.58553743e-01  4.59575991e-01
  4.60726428e-01  4.63824397e-01  4.64081750e-01  4.67481095e-01
  4.74954071e-01  4.79180975e-01  4.81097062e-01  4.82182753e-01
  4.82436848e-01  4.83125116e-01  4.84771357e-01  4.87865412e-01
  4.88593489e-01  4.88945050e-01  4.90300419e-01  4.92203269e-01
  4.96293917e-01  4.96711286e-01  4.98616414e-01  4.99202896e-01
  5.00534115e-01  5.01877557e-01  5.03415304e-01  5.07635951e-01
  5.08117992e-01  5.09927175e-01  5.10277890e-01  5.14009217e-01
  5.14856356e-01  5.15529418e-01  5.17128279e-01  5.18926774e-01
  5.21676836e-01  5.23349949e-01  5.24210273e-01  5.24719021e-01
  5.27072811e-01  5.29270402e-01  5.29454778e-01  5.29555710e-01
  5.29986320e-01  5.30952820e-01  5.31748171e-01  5.32716472e-01
  5.33411574e-01  5.33418829e-01  5.35428162e-01  5.41209878e-01
  5.41835609e-01  5.47198486e-01  5.48030342e-01  5.51476416e-01
  5.52107598e-01  5.54393060e-01  5.54561878e-01  5.59259888e-01
  5.64725473e-01  5.64895775e-01  5.74208127e-01  5.74941720e-01
  5.76497442e-01  5.77721314e-01  5.86299935e-01  5.87372260e-01
  5.88078149e-01  5.89483849e-01  5.92350769e-01  5.92693255e-01
  5.93175016e-01  5.94906585e-01  5.97409262e-01  6.02261132e-01
  6.02651612e-01  6.02988262e-01  6.03190021e-01  6.14590807e-01
  6.18052881e-01  6.19551354e-01  6.19935930e-01  6.20634447e-01
  6.22975487e-01  6.25623671e-01  6.26173328e-01  6.26444585e-01
  6.27638656e-01  6.28177573e-01  6.28229711e-01  6.30938548e-01
  6.32527957e-01  6.37082199e-01  6.41447573e-01  6.42862080e-01
  6.50009861e-01  6.51269545e-01  6.51723063e-01  6.58075050e-01
  6.60717942e-01  6.63822374e-01  6.67209940e-01  6.80513386e-01
  6.87751210e-01  6.88374859e-01  6.94661092e-01  7.02351090e-01
  7.06838203e-01  7.11749803e-01  7.13185848e-01  7.13612622e-01
  7.16448531e-01  7.20580983e-01  7.21338357e-01  7.27856171e-01
  7.60407830e-01]

  warnings.warn(

2022-11-03 10:50:04,868:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.26451712e+08 -1.16244480e+08 -1.14819072e+08 -1.11280128e+08
 -9.03741440e+07 -8.57374720e+07 -8.47216640e+07 -8.37222400e+07
 -8.37058560e+07 -8.29030400e+07 -8.21821440e+07 -8.07239680e+07
 -8.06912000e+07 -8.05601280e+07 -7.68409600e+07 -7.62675200e+07
 -7.22862080e+07 -7.13195520e+07 -7.01071360e+07 -6.58636800e+07
 -6.42744320e+07 -6.12106240e+07 -6.03914240e+07 -5.83270400e+07
 -5.82451200e+07 -5.73931520e+07 -5.67050240e+07 -5.64592640e+07
 -5.44112640e+07 -5.21666560e+07 -5.12983040e+07 -5.11180800e+07
 -5.09214720e+07 -5.06101760e+07 -5.02333440e+07 -4.99548160e+07
 -4.93813760e+07 -4.90045440e+07 -4.88407040e+07 -4.81689600e+07
 -4.80378880e+07 -4.79232000e+07 -4.78085120e+07 -4.77757440e+07
 -4.76774400e+07 -4.71040000e+07 -4.62028800e+07 -4.56622080e+07
 -4.52526080e+07 -4.46300160e+07 -4.45644800e+07 -4.43187200e+07
 -4.35978240e+07 -4.33520640e+07 -4.30407680e+07 -4.30243840e+07
 -4.27458560e+07 -4.25000960e+07 -4.20413440e+07 -4.19594240e+07
 -4.19430400e+07 -4.09436160e+07 -4.01735680e+07 -3.99933440e+07
 -3.99769600e+07 -3.98295040e+07 -3.97967360e+07 -3.96001280e+07
 -3.87645440e+07 -3.86007040e+07 -3.77815040e+07 -3.77323520e+07
 -3.75848960e+07 -3.74865920e+07 -3.71752960e+07 -3.65035520e+07
 -3.62250240e+07 -3.61758720e+07 -3.60939520e+07 -3.59956480e+07
 -3.54713600e+07 -3.50617600e+07 -3.45047040e+07 -3.39968000e+07
 -3.38165760e+07 -3.34888960e+07 -3.28663040e+07 -3.25222400e+07
 -3.20634880e+07 -3.20143360e+07 -3.18341120e+07 -3.10476800e+07
 -3.08346880e+07 -3.02776320e+07 -2.99663360e+07 -2.95895040e+07
 -2.88522240e+07 -2.87375360e+07 -2.87211520e+07 -2.82951680e+07
 -2.79838720e+07 -2.78036480e+07 -2.77381120e+07 -2.75742720e+07
 -2.75578880e+07 -2.74104320e+07 -2.73285120e+07 -2.67550720e+07
 -2.59358720e+07 -2.55590400e+07 -2.52641280e+07 -2.49692160e+07
 -2.49036800e+07 -2.48545280e+07 -2.44940800e+07 -2.44776960e+07
 -2.41500160e+07 -2.40025600e+07 -2.37240320e+07 -2.35438080e+07
 -2.33144320e+07 -2.31669760e+07 -2.28065280e+07 -2.24952320e+07
 -2.24460800e+07 -2.22003200e+07 -2.15121920e+07 -2.14302720e+07
 -2.13975040e+07 -2.12664320e+07 -2.10862080e+07 -2.08732160e+07
 -2.02014720e+07 -2.00704000e+07 -1.97263360e+07 -1.96935680e+07
 -1.93658880e+07 -1.93331200e+07 -1.92020480e+07 -1.91528960e+07
 -1.91201280e+07 -1.82845440e+07 -1.82681600e+07 -1.78585600e+07
 -1.77930240e+07 -1.73670400e+07 -1.73342720e+07 -1.73178880e+07
 -1.72523520e+07 -1.71212800e+07 -1.70557440e+07 -1.67280640e+07
 -1.66461440e+07 -1.65969920e+07 -1.64495360e+07 -1.61218560e+07
 -1.60727040e+07 -1.58597120e+07 -1.57450240e+07 -1.56958720e+07
 -1.56139520e+07 -1.54992640e+07 -1.53190400e+07 -1.51388160e+07
 -1.47456000e+07 -1.47128320e+07 -1.46964480e+07 -1.43523840e+07
 -1.35004160e+07 -1.33365760e+07 -1.31727360e+07 -1.29269760e+07
 -1.27303680e+07 -1.26648320e+07 -1.26484480e+07 -1.24846080e+07
 -1.23371520e+07 -1.23043840e+07 -1.21405440e+07 -1.20094720e+07
 -1.17309440e+07 -1.16817920e+07 -1.16490240e+07 -1.12066560e+07
 -1.10919680e+07 -1.08134400e+07 -1.06496000e+07 -1.05185280e+07
 -1.03710720e+07 -9.92870400e+06 -9.78124800e+06 -9.46995200e+06
 -9.33888000e+06 -9.28972800e+06 -9.19142400e+06 -8.86374400e+06
 -8.84736000e+06 -8.56883200e+06 -8.48691200e+06 -8.42137600e+06
 -8.20838400e+06 -8.04454400e+06 -8.01177600e+06 -7.78240000e+06
 -7.63494400e+06 -7.45472000e+06 -7.40556800e+06 -7.38918400e+06
 -7.14342400e+06 -7.07788800e+06 -7.02873600e+06 -6.97958400e+06
 -6.65190400e+06 -6.61913600e+06 -6.12761600e+06 -5.76716800e+06
 -5.65248000e+06 -5.53779200e+06 -5.34118400e+06 -5.27564800e+06
 -5.17734400e+06 -5.06265600e+06 -4.88243200e+06 -4.73497600e+06
 -4.70220800e+06 -4.12876800e+06 -3.83385600e+06 -3.67001600e+06
 -3.62086400e+06 -3.19488000e+06 -2.71974400e+06 -2.63782400e+06
 -2.62144000e+06 -2.53952000e+06 -2.37568000e+06 -2.29376000e+06
 -2.22822400e+06 -1.93331200e+06 -1.65478400e+06 -1.24518400e+06
 -1.22880000e+06 -1.08134400e+06 -8.84736000e+05 -8.19200000e+05
 -6.71744000e+05 -4.91520000e+05 -3.44064000e+05 -3.27680000e+05
 -2.78528000e+05 -1.47456000e+05  2.62144000e+05  7.70048000e+05
  8.19200000e+05  9.83040000e+05  1.17964800e+06  1.21241600e+06
  1.99884800e+06  2.29376000e+06  2.45760000e+06  2.96550400e+06
  3.08019200e+06  3.30956800e+06  3.44064000e+06  3.70278400e+06
  3.73555200e+06  4.01408000e+06  4.25984000e+06  4.53836800e+06
  4.70220800e+06  4.73497600e+06  4.78412800e+06  4.89881600e+06
  5.01350400e+06  5.06265600e+06  5.09542400e+06  5.17734400e+06
  5.37395200e+06  5.53779200e+06  6.12761600e+06  6.14400000e+06
  6.42252800e+06  6.52083200e+06  6.71744000e+06  6.89766400e+06
  6.94681600e+06  7.34003200e+06  7.43833600e+06  7.70048000e+06
  7.79878400e+06  7.81516800e+06  7.96262400e+06  8.40499200e+06
  8.53606400e+06  8.55244800e+06  8.81459200e+06  8.84736000e+06
  8.99481600e+06  9.40441600e+06  9.56825600e+06  9.89593600e+06
  1.02727680e+07  1.05021440e+07  1.06004480e+07  1.14360320e+07
  1.16817920e+07  1.18620160e+07  1.19439360e+07  1.23043840e+07
  1.29105920e+07  1.33365760e+07  1.33857280e+07  1.34840320e+07
  1.43196160e+07  1.43687680e+07  1.44179200e+07  1.46145280e+07
  1.47128320e+07  1.48439040e+07  1.54992640e+07  1.56631040e+07
  1.60235520e+07  1.61382400e+07  1.63020800e+07  1.65478400e+07
  1.73015040e+07  1.73178880e+07  1.73342720e+07  1.76128000e+07
  1.79404800e+07  1.79568640e+07  1.84811520e+07  1.87760640e+07
  1.88252160e+07  1.92839680e+07  1.93003520e+07  1.93986560e+07
  1.95624960e+07  1.95952640e+07  1.96116480e+07  1.96771840e+07
  1.98901760e+07  1.99884800e+07  2.02014720e+07  2.05783040e+07
  2.11517440e+07  2.13319680e+07  2.14630400e+07  2.17088000e+07
  2.17251840e+07  2.18726400e+07  2.19709440e+07  2.29048320e+07
  2.32161280e+07  2.33308160e+07  2.37568000e+07  2.43302400e+07
  2.46251520e+07  2.47070720e+07  2.47726080e+07  2.48053760e+07
  2.49692160e+07  2.49856000e+07  2.53132800e+07  2.56409600e+07
  2.60833280e+07  2.62471680e+07  2.63782400e+07  2.69025280e+07
  2.75087360e+07  2.78036480e+07  2.82460160e+07  2.85736960e+07
  2.87375360e+07  2.90160640e+07  2.92618240e+07  3.03431680e+07
  3.04414720e+07  3.05889280e+07  3.06708480e+07  3.09329920e+07
  3.16538880e+07  3.17685760e+07  3.21126400e+07  3.21617920e+07
  3.23092480e+07  3.30137600e+07  3.33414400e+07  3.35708160e+07
  3.41934080e+07  3.43736320e+07  3.46685440e+07  3.47668480e+07
  3.48160000e+07  3.49962240e+07  3.52747520e+07  3.56515840e+07
  3.61758720e+07  3.63397120e+07  3.72244480e+07  3.72408320e+07
  3.88300800e+07  3.93871360e+07  4.04193280e+07  4.06978560e+07
  4.08453120e+07  4.16808960e+07  4.19102720e+07  4.19921920e+07
  4.21068800e+07  4.26311680e+07  4.31882240e+07  4.45480960e+07
  4.49576960e+07  4.56949760e+07  4.58424320e+07  4.58752000e+07
  4.71531520e+07  4.79395840e+07  4.82181120e+07  4.84638720e+07
  4.87915520e+07  5.04627200e+07  5.06757120e+07  5.07412480e+07
  5.08559360e+07  5.12819200e+07  5.14129920e+07  5.20192000e+07
  5.29367040e+07  5.37231360e+07  5.37722880e+07  5.47717120e+07
  5.48372480e+07  5.69180160e+07  5.77372160e+07  6.00145920e+07
  6.02767360e+07  6.15383040e+07  6.15874560e+07  6.18332160e+07
  6.22100480e+07  6.24885760e+07  6.37009920e+07  6.50936320e+07
  6.62077440e+07  6.62896640e+07  6.70433280e+07  6.87800320e+07
  6.88128000e+07  6.92715520e+07  7.19093760e+07  7.33839360e+07
  7.44652800e+07  7.51370240e+07  7.71850240e+07  7.83482880e+07
  7.98720000e+07  8.02652160e+07  8.32634880e+07  8.35584000e+07
  8.40499200e+07  8.80967680e+07  8.92928000e+07  8.99809280e+07
  9.01611520e+07  9.07018240e+07  9.07673600e+07  9.26515200e+07
  9.43718400e+07  9.57153280e+07  9.78288640e+07  9.83040000e+07
  9.92542720e+07  1.03546880e+08  1.08822528e+08  1.10379008e+08
  1.28647168e+08]

  warnings.warn(

2022-11-03 10:50:04,868:INFO:Calculating mean and std
2022-11-03 10:50:04,868:INFO:Creating metrics dataframe
2022-11-03 10:50:04,887:INFO:Uploading results into container
2022-11-03 10:50:04,887:INFO:Uploading model into container now
2022-11-03 10:50:04,895:INFO:master_model_container: 9
2022-11-03 10:50:04,896:INFO:display_container: 2
2022-11-03 10:50:04,898:INFO:Lars(random_state=4411)
2022-11-03 10:50:04,898:INFO:create_model() successfully completed......................................
2022-11-03 10:50:05,175:WARNING:create_model() for Lars(random_state=4411) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:50:05,175:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:50:05,175:INFO:Initializing create_model()
2022-11-03 10:50:05,175:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:50:05,175:INFO:Checking exceptions
2022-11-03 10:50:05,175:INFO:Importing libraries
2022-11-03 10:50:05,175:INFO:Copying training dataset
2022-11-03 10:50:05,191:INFO:Defining folds
2022-11-03 10:50:05,191:INFO:Declaring metric variables
2022-11-03 10:50:05,191:INFO:Importing untrained model
2022-11-03 10:50:05,191:INFO:Least Angle Regression Imported successfully
2022-11-03 10:50:05,191:INFO:Starting cross validation
2022-11-03 10:50:05,206:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:50:07,959:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:08,024:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:08,040:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:08,112:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:08,223:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:08,313:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:08,313:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:08,330:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:08,346:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.937e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-11-03 10:50:08,346:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.899e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-11-03 10:50:08,346:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.759e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-11-03 10:50:08,346:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.758e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-11-03 10:50:08,346:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.656e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-11-03 10:50:08,346:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.513e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2022-11-03 10:50:09,143:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.17859909 -0.17137604 -0.16883698 -0.16142804 -0.15823029 -0.15433136
 -0.14520169 -0.14441111 -0.14382384 -0.14100634 -0.14087518 -0.13970847
 -0.13939833 -0.13249522 -0.13246282 -0.13130347 -0.12968202 -0.12649065
 -0.12527451 -0.12306027 -0.11891008 -0.10731138 -0.09906699 -0.09631797
 -0.09589221 -0.09027215 -0.08319542 -0.08295828 -0.08277739 -0.08250432
 -0.08228986 -0.07808097 -0.07650528 -0.07380014 -0.07271407 -0.07262291
 -0.07233932 -0.07211808 -0.06746815 -0.06702841 -0.06594035 -0.06342977
 -0.06150551 -0.05364698 -0.05151769 -0.0501369  -0.04992765 -0.04987424
 -0.04494082 -0.04027569 -0.03818179 -0.03732028 -0.03453102 -0.03381415
 -0.02955203 -0.02864871 -0.02553229 -0.02487619 -0.02268468 -0.02252069
 -0.02037185 -0.01877173 -0.01822087 -0.0182084  -0.01770274 -0.01543315
 -0.01412414 -0.0114035  -0.00778528 -0.00710542 -0.00449067 -0.00204671
  0.00162219  0.00443329  0.00625776  0.00653145  0.00771633  0.00801741
  0.01023331  0.01061632  0.0147856   0.01687977  0.01835336  0.01943637
  0.02416442  0.02781641  0.02928354  0.02986251  0.03016783  0.03094979
  0.03379028  0.0356441   0.03925998  0.03962373  0.03985412  0.0399344
  0.04082751  0.05174638  0.05261206  0.05682378  0.06012914  0.06078503
  0.06159185  0.06564717  0.06577092  0.06666856  0.07102967  0.07256755
  0.07474553  0.07520501  0.07544624  0.0761721   0.07779038  0.08238989
  0.08386085  0.0849662   0.08584834  0.08643958  0.08680819  0.08706165
  0.09031636  0.09085284  0.09183286  0.09228593  0.09255045  0.0950769
  0.09526852  0.09544835  0.09714589  0.09834657  0.10211893  0.10374185
  0.10443462  0.10474312  0.10564904  0.10602678  0.10996826  0.11186988
  0.11197397  0.11202978  0.11359033  0.11478966  0.11549043  0.11689716
  0.11987762  0.12011251  0.120418    0.12100201  0.12263108  0.12330828
  0.12556199  0.12646656  0.12895193  0.13195149  0.13256507  0.13548904
  0.13652015  0.13769934  0.13859216  0.13906358  0.14001971  0.14274009
  0.14311831  0.14466241  0.14576557  0.14609819  0.14691511  0.14735177
  0.15345267  0.15387854  0.15460962  0.15594909  0.15669579  0.15758231
  0.1581733   0.15844873  0.16242785  0.16278127  0.16348791  0.16398071
  0.16564139  0.16606266  0.169004    0.17093509  0.17124833  0.17288807
  0.17335896  0.17355232  0.17437053  0.17473283  0.17526983  0.17527054
  0.17556138  0.17615514  0.17648087  0.17742806  0.17818027  0.17841982
  0.18097947  0.18112012  0.18208847  0.18282333  0.18437182  0.18544378
  0.18639886  0.19001275  0.1968446   0.19817807  0.19827033  0.19854711
  0.1987812   0.20017674  0.20150764  0.20176155  0.2030076   0.20403427
  0.20443419  0.20550196  0.20724564  0.20821553  0.20852778  0.20862954
  0.20890153  0.2092309   0.21137079  0.21314535  0.21396375  0.21419139
  0.2151643   0.21852511  0.21897764  0.22006489  0.22109162  0.22195463
  0.22231064  0.224986    0.22541104  0.22575761  0.2287525   0.23075427
  0.23160618  0.23546496  0.23548466  0.23573909  0.23750414  0.23869479
  0.24245129  0.24427656  0.24890434  0.25108094  0.25133329  0.25228159
  0.25435941  0.25461977  0.25813919  0.25989824  0.26381099  0.2644135
  0.2654833   0.26568439  0.26632215  0.26758752  0.27389795  0.27584366
  0.27982775  0.28013009  0.2802278   0.28081642  0.28565539  0.28809932
  0.28872465  0.28976764  0.29438848  0.29439729  0.2983794   0.298952
  0.30056516  0.30396865  0.30469516  0.30497693  0.30510832  0.3081813
  0.3087887   0.31129456  0.31191352  0.31202232  0.31205255  0.31214926
  0.313166    0.3141328   0.31855322  0.31918478  0.31972045  0.31984586
  0.32020819  0.32424274  0.3243946   0.32924798  0.33138723  0.33226646
  0.33506402  0.34231866  0.34290593  0.344175    0.34521683  0.34669929
  0.34687919  0.34791121  0.34799461  0.35228108  0.35248076  0.35273546
  0.35314949  0.35368953  0.35470757  0.35701237  0.35812474  0.3583853
  0.3596053   0.36101767  0.36270634  0.36839226  0.37608674  0.37659653
  0.37719176  0.37730064  0.37743132  0.38113479  0.38170251  0.38194438
  0.38312869  0.38468645  0.38689591  0.3903517   0.39379965  0.39383144
  0.39476262  0.39510278  0.39548872  0.39557743  0.39954061  0.40436677
  0.40909104  0.40975384  0.41720849  0.41763515  0.41920264  0.41936779
  0.42128908  0.42320586  0.42370236  0.4245139   0.43250952  0.43255183
  0.43431112  0.43611767  0.43903664  0.44193085  0.44444533  0.44481374
  0.44646204  0.44807096  0.44877735  0.45180219  0.45188271  0.45317501
  0.45476071  0.45576707  0.455824    0.45782896  0.45955087  0.45955986
  0.45970005  0.46424165  0.46591635  0.46655379  0.47076474  0.47498066
  0.47525658  0.47712874  0.47722008  0.48031336  0.48348754  0.48348842
  0.48663683  0.48929393  0.49136601  0.4921309   0.49815352  0.49820391
  0.49837914  0.50112939  0.50143085  0.50150219  0.50165706  0.50192396
  0.50549932  0.5062357   0.50665115  0.51135318  0.51287801  0.51404285
  0.51632054  0.51890148  0.51918744  0.52057425  0.52158057  0.52888547
  0.53218272  0.53260741  0.532886    0.5341401   0.53905577  0.5398203
  0.54003297  0.54101899  0.54191458  0.54329125  0.54891503  0.55005171
  0.55057397  0.55115858  0.55142052  0.55145255  0.55158167  0.55173629
  0.55192204  0.55280502  0.55319444  0.5541294   0.55763616  0.55763957
  0.56216761  0.5628237   0.56291041  0.56939642  0.56980925  0.57118016
  0.5862847   0.5877238   0.588692    0.59615516  0.59779372  0.6039677
  0.60609727  0.60803595  0.60876772  0.60899448  0.60904007  0.61031502
  0.61039874  0.61094132  0.61565771  0.61801732  0.62576125  0.62897278
  0.62922915  0.63256296  0.63393807  0.63661445  0.63812846  0.64145848
  0.6450304   0.64578623  0.64631825  0.64980956  0.65322722  0.6575099
  0.65754569  0.6580613   0.66211529  0.66728319  0.66754401  0.66778824
  0.67265571  0.67359233  0.67695794  0.68081029  0.68150292  0.68302453
  0.69239014  0.69457139  0.69503269  0.70163067  0.71779953  0.71909945
  0.72530793  0.7340377   0.75742178  0.76269903  0.76518511  0.76941151
  0.77693273]

  warnings.warn(

2022-11-03 10:50:09,174:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.22759221 -0.18281955 -0.16490057 -0.15916288 -0.15481519 -0.15037475
 -0.13391122 -0.13320558 -0.1327074  -0.13256409 -0.13095249 -0.1273081
 -0.12681571 -0.11797125 -0.11663401 -0.11609342 -0.11249005 -0.11221214
 -0.11213424 -0.10815903 -0.10812711 -0.10807845 -0.10758513 -0.10469526
 -0.0964037  -0.0916023  -0.08400326 -0.08322505 -0.08234236 -0.08132376
 -0.0720365  -0.06928709 -0.06874596 -0.06701821 -0.06437475 -0.06393701
 -0.06095805 -0.06010752 -0.05762167 -0.05535918 -0.05518988 -0.05499943
 -0.05433063 -0.05096152 -0.04819062 -0.04507993 -0.04477371 -0.04432384
 -0.04179743 -0.0401627  -0.03346355 -0.03186098 -0.03166437 -0.03161845
 -0.03058267 -0.02874367 -0.0254831  -0.02315566 -0.0230948  -0.02199682
 -0.02141183 -0.02011469 -0.02000063 -0.01965465 -0.01419046 -0.00459904
 -0.00370943  0.00344583  0.00743552  0.00922258  0.01040018  0.0104983
  0.01109955  0.01173035  0.01186176  0.01589742  0.01788842  0.01920366
  0.0209784   0.0224974   0.02332723  0.024382    0.02537359  0.02565929
  0.02701675  0.02720935  0.03319138  0.03319866  0.03585579  0.03632771
  0.0379261   0.03922978  0.04357103  0.04404904  0.04595345  0.04675752
  0.04693946  0.04733717  0.04830229  0.04927382  0.05036855  0.05162026
  0.05168833  0.0517132   0.05349545  0.05500137  0.05801273  0.059001
  0.06008231  0.06542563  0.0662237   0.06654805  0.06689119  0.06689152
  0.06715118  0.07511242  0.07638561  0.07672291  0.07921667  0.08006163
  0.0804636   0.08244954  0.08311129  0.0844589   0.08631289  0.0864061
  0.0864236   0.08902445  0.09070019  0.09162993  0.09411949  0.09867068
  0.10100462  0.10298721  0.10305137  0.10331588  0.1035254   0.10391193
  0.10476436  0.10572539  0.10633997  0.10669534  0.10720733  0.10776001
  0.11027698  0.11124508  0.11234039  0.11433397  0.11704384  0.11960591
  0.12041164  0.120981    0.12404506  0.12456287  0.12544055  0.12613948
  0.12630957  0.12776268  0.12807984  0.12896698  0.12943014  0.12993136
  0.13685346  0.13805388  0.13995182  0.14010507  0.14119334  0.14462429
  0.14526182  0.14573409  0.14937214  0.14980869  0.15156332  0.15356078
  0.15398515  0.15449947  0.15494677  0.1580319   0.16153808  0.16399483
  0.16748496  0.16863745  0.17026985  0.17176277  0.17322491  0.17497194
  0.17587985  0.17809644  0.17903139  0.17982566  0.18071381  0.18201188
  0.18243059  0.18262188  0.18650394  0.18659539  0.18701319  0.18843688
  0.18873573  0.18893998  0.18909032  0.19056766  0.19591832  0.19707374
  0.1999712   0.20048607  0.20240882  0.20259527  0.20309472  0.20359552
  0.20568186  0.20571297  0.2057855   0.20628684  0.20747989  0.20809745
  0.20832389  0.20833996  0.20874108  0.20937992  0.20957316  0.20975356
  0.21412598  0.21517826  0.21625468  0.21640577  0.21790559  0.22074496
  0.22157666  0.22342703  0.22368264  0.2251784   0.2277293   0.22968501
  0.23295245  0.23347432  0.23499852  0.23538645  0.23539654  0.23582126
  0.23603424  0.23642982  0.23659726  0.23868069  0.23978823  0.24301063
  0.24355835  0.24448087  0.24456466  0.24678786  0.24839935  0.24983298
  0.25031738  0.25078023  0.25172539  0.2555624   0.25571729  0.25607812
  0.25821318  0.26038021  0.26167455  0.26273589  0.2639788   0.26477114
  0.26688956  0.26711272  0.26794053  0.27137969  0.27519268  0.28404647
  0.28613917  0.28725794  0.28843805  0.28940204  0.28990584  0.29073417
  0.29091754  0.2914618   0.29211427  0.29349374  0.29395974  0.29503095
  0.29539312  0.29616104  0.29861094  0.30004213  0.30040813  0.30077668
  0.30216647  0.30245036  0.30297473  0.30372261  0.30444101  0.30465111
  0.30495027  0.30612323  0.30745607  0.30750639  0.30890722  0.30983346
  0.31109311  0.31270066  0.31300694  0.31419388  0.31439174  0.31497002
  0.31939761  0.31993411  0.32317877  0.32413821  0.32471538  0.32759252
  0.32829213  0.33022821  0.33043179  0.33057407  0.33143016  0.33179264
  0.33257427  0.33307936  0.33441494  0.33585127  0.33759265  0.34426155
  0.34484972  0.34530559  0.34995965  0.35152731  0.35165376  0.35177651
  0.35224241  0.35560022  0.35636658  0.35756157  0.36069095  0.36074197
  0.36146014  0.36219325  0.36570567  0.3665477   0.36753312  0.36872521
  0.36970901  0.37142196  0.37416747  0.37427992  0.37692361  0.37709295
  0.37785455  0.38199079  0.38312878  0.38400479  0.3847381   0.38541466
  0.38662125  0.39010744  0.40350929  0.40379373  0.40463114  0.40647878
  0.40855606  0.41048669  0.41094187  0.41545822  0.41792328  0.41871622
  0.42240963  0.42330054  0.42399341  0.42484933  0.43007681  0.43067912
  0.43184134  0.43184597  0.43320167  0.43446911  0.4357637   0.43740654
  0.43767834  0.43769191  0.44033269  0.44194887  0.45035741  0.45098822
  0.45137831  0.45191732  0.45274253  0.45410296  0.46303508  0.46442587
  0.46516541  0.46795817  0.46945319  0.4736979   0.47629589  0.47694102
  0.48043288  0.48055402  0.48098343  0.48103068  0.48169626  0.48461644
  0.48699976  0.48811948  0.49014283  0.49596642  0.50214313  0.50358034
  0.50415208  0.5059043   0.5108498   0.51290831  0.51356367  0.51523019
  0.51537383  0.51634971  0.51821451  0.52017281  0.526025    0.52602647
  0.52897194  0.53028964  0.530424    0.53124961  0.53548438  0.53642586
  0.53971278  0.54121636  0.54300089  0.54558479  0.54776939  0.54787775
  0.54957297  0.55174319  0.55176146  0.55359277  0.55538556  0.56015626
  0.56598114  0.56725437  0.57363187  0.57373614  0.57565645  0.57967081
  0.58276696  0.58669196  0.58790188  0.59037985  0.59214094  0.59299113
  0.59657468  0.60081719  0.60090122  0.60145224  0.60398068  0.60590994
  0.60767836  0.60788558  0.60935213  0.61303197  0.61460339  0.61995438
  0.62228099  0.62256566  0.62413935  0.62679056  0.63050591  0.63286426
  0.64401532  0.64468527  0.64572663  0.64745305  0.65767456  0.66336743
  0.66341367  0.66477482  0.66571625  0.6687756   0.67402886  0.67773434
  0.68360734  0.68472522  0.68601512  0.68642374  0.69594331  0.6964306
  0.70121238  0.71192317  0.71772971  0.71823367  0.72196     0.7238786
  0.73099813]

  warnings.warn(

2022-11-03 10:50:09,233:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.91536643e-01 -1.67359420e-01 -1.50870734e-01 -1.44159664e-01
 -1.38960061e-01 -1.32436532e-01 -1.30889801e-01 -1.28575354e-01
 -1.25375597e-01 -1.19656661e-01 -1.18150440e-01 -1.16905858e-01
 -1.15855640e-01 -1.14869827e-01 -1.10000053e-01 -1.08742246e-01
 -1.02658434e-01 -1.00956932e-01 -9.73111792e-02 -9.45469244e-02
 -9.38658895e-02 -8.94110252e-02 -8.83384577e-02 -8.72819414e-02
 -8.61035590e-02 -8.50650186e-02 -8.43474356e-02 -7.10468027e-02
 -7.04816209e-02 -6.87108069e-02 -6.37387019e-02 -6.28943016e-02
 -5.85875125e-02 -5.80858280e-02 -5.60564666e-02 -4.94512165e-02
 -4.90296134e-02 -4.88748000e-02 -4.45132564e-02 -4.43538630e-02
 -4.40967398e-02 -4.26252005e-02 -4.22766378e-02 -4.20604365e-02
 -3.92637758e-02 -3.83042172e-02 -3.62594597e-02 -3.54494103e-02
 -3.39927224e-02 -3.22665592e-02 -2.88161229e-02 -2.84128299e-02
 -2.83729897e-02 -2.60828247e-02 -2.39400514e-02 -2.32741770e-02
 -2.30260760e-02 -2.20656206e-02 -2.12055933e-02 -2.07070292e-02
 -1.83971892e-02 -1.60854585e-02 -1.11865461e-02 -9.60151851e-03
 -8.56415256e-03 -7.73981647e-03 -5.00728719e-03 -4.55203497e-03
 -3.64030117e-03 -2.47441637e-03 -1.84942938e-03 -5.46357691e-04
  3.97419262e-04  1.80096008e-03  1.82125364e-03  3.06468202e-03
  4.00706692e-03  5.84919651e-03  7.08961722e-03  9.09800060e-03
  1.03393827e-02  1.05297693e-02  1.27093672e-02  1.39506779e-02
  1.81484029e-02  1.85039728e-02  2.05039797e-02  2.23952074e-02
  2.42888276e-02  2.52986602e-02  2.64413677e-02  2.92927366e-02
  3.07691202e-02  3.15242904e-02  3.34991029e-02  3.46064463e-02
  3.55142691e-02  3.75459441e-02  3.92452317e-02  4.03918660e-02
  4.09253173e-02  4.11507475e-02  4.31378714e-02  4.50620786e-02
  4.55038215e-02  5.03371588e-02  5.08658530e-02  5.19539264e-02
  5.29843303e-02  5.41493830e-02  5.70246292e-02  5.93864979e-02
  6.10636622e-02  6.13139407e-02  6.48214776e-02  6.66912225e-02
  6.80486759e-02  8.36861943e-02  8.36933787e-02  8.43881222e-02
  8.57264399e-02  8.61214207e-02  8.69781282e-02  8.76503043e-02
  9.02553172e-02  9.20395944e-02  9.21943250e-02  9.21977971e-02
  9.22061068e-02  9.22336508e-02  9.27073664e-02  9.31882116e-02
  9.60494081e-02  9.63813148e-02  9.71826885e-02  9.75017600e-02
  9.87076957e-02  1.01190009e-01  1.01393881e-01  1.01450635e-01
  1.01495022e-01  1.02324181e-01  1.05597463e-01  1.06451733e-01
  1.07266935e-01  1.07517864e-01  1.16184338e-01  1.16716781e-01
  1.16826366e-01  1.20516159e-01  1.20635318e-01  1.21128695e-01
  1.21694195e-01  1.21795334e-01  1.21834318e-01  1.24970041e-01
  1.25118558e-01  1.25189682e-01  1.27316463e-01  1.28369007e-01
  1.28692825e-01  1.29577824e-01  1.29755724e-01  1.30321635e-01
  1.31631805e-01  1.34477280e-01  1.34986612e-01  1.36280298e-01
  1.37193143e-01  1.37699771e-01  1.37755461e-01  1.37959923e-01
  1.38161964e-01  1.44475733e-01  1.46114891e-01  1.47220300e-01
  1.47758796e-01  1.48081184e-01  1.51093358e-01  1.54781716e-01
  1.55712602e-01  1.55997642e-01  1.58113010e-01  1.59763918e-01
  1.60571794e-01  1.60797117e-01  1.63693205e-01  1.66537112e-01
  1.68539896e-01  1.71726594e-01  1.73918319e-01  1.73964749e-01
  1.74545681e-01  1.77316540e-01  1.78326132e-01  1.83367154e-01
  1.85435065e-01  1.86867145e-01  1.87019805e-01  1.89839919e-01
  1.94422132e-01  1.99536814e-01  2.00489722e-01  2.03530992e-01
  2.04817151e-01  2.05491322e-01  2.05966375e-01  2.06335933e-01
  2.07720455e-01  2.08740852e-01  2.08886326e-01  2.11783441e-01
  2.12010797e-01  2.13235793e-01  2.13572110e-01  2.13855980e-01
  2.14921852e-01  2.17055062e-01  2.18418133e-01  2.20202868e-01
  2.22421645e-01  2.23203353e-01  2.23533429e-01  2.27028625e-01
  2.28473450e-01  2.29210831e-01  2.32129963e-01  2.32626870e-01
  2.34638819e-01  2.35020610e-01  2.35106903e-01  2.35314619e-01
  2.37640732e-01  2.38041173e-01  2.39601716e-01  2.39677194e-01
  2.41551143e-01  2.41884232e-01  2.42746266e-01  2.43491566e-01
  2.45788293e-01  2.46428161e-01  2.46551139e-01  2.46721802e-01
  2.47415341e-01  2.48045753e-01  2.48844830e-01  2.48992325e-01
  2.49067212e-01  2.51036084e-01  2.51925249e-01  2.52376578e-01
  2.54552440e-01  2.56807930e-01  2.58692842e-01  2.58757014e-01
  2.59946085e-01  2.61976636e-01  2.62379740e-01  2.62706582e-01
  2.62861226e-01  2.63108193e-01  2.63804758e-01  2.63823336e-01
  2.64367743e-01  2.66615005e-01  2.66951226e-01  2.70205988e-01
  2.71447682e-01  2.71783207e-01  2.72010905e-01  2.72827817e-01
  2.72907220e-01  2.74952794e-01  2.75491700e-01  2.79469991e-01
  2.81996056e-01  2.82294479e-01  2.83853631e-01  2.86007186e-01
  2.86289887e-01  2.89077274e-01  2.95115315e-01  2.99569027e-01
  3.01769414e-01  3.09999139e-01  3.12326758e-01  3.12736518e-01
  3.13670556e-01  3.14759692e-01  3.15406198e-01  3.15956362e-01
  3.17854405e-01  3.18409436e-01  3.19196082e-01  3.19772226e-01
  3.23459591e-01  3.26347833e-01  3.26486051e-01  3.29380107e-01
  3.30493962e-01  3.30914543e-01  3.31556717e-01  3.32286323e-01
  3.32563149e-01  3.33160609e-01  3.33593845e-01  3.34459624e-01
  3.37699054e-01  3.37843768e-01  3.39145169e-01  3.40315803e-01
  3.42680218e-01  3.46397073e-01  3.51344889e-01  3.56294430e-01
  3.58882193e-01  3.61634306e-01  3.63340967e-01  3.63956521e-01
  3.64008378e-01  3.65432756e-01  3.65883448e-01  3.67102814e-01
  3.69351246e-01  3.71370904e-01  3.73254827e-01  3.74506060e-01
  3.74676228e-01  3.75517968e-01  3.76805650e-01  3.78060316e-01
  3.81806214e-01  3.84192011e-01  3.86478058e-01  3.86995876e-01
  3.89424358e-01  3.92010317e-01  3.93962002e-01  3.95289046e-01
  3.96091013e-01  3.96895919e-01  3.97283908e-01  4.02794716e-01
  4.05305972e-01  4.10243379e-01  4.10622452e-01  4.12596019e-01
  4.20545426e-01  4.20863573e-01  4.22774009e-01  4.23131319e-01
  4.24839658e-01  4.26670200e-01  4.32282090e-01  4.32714529e-01
  4.32829690e-01  4.33279559e-01  4.34204057e-01  4.36694750e-01
  4.39460093e-01  4.39680338e-01  4.48311560e-01  4.49166113e-01
  4.49722148e-01  4.50418509e-01  4.51015568e-01  4.51565791e-01
  4.53510080e-01  4.57773749e-01  4.58220552e-01  4.59966294e-01
  4.60802966e-01  4.61139132e-01  4.61291091e-01  4.61805462e-01
  4.62594966e-01  4.65824252e-01  4.67167090e-01  4.67296781e-01
  4.67518541e-01  4.68380396e-01  4.69058356e-01  4.69131342e-01
  4.69619104e-01  4.69663813e-01  4.70717059e-01  4.73423934e-01
  4.74338783e-01  4.76293653e-01  4.77634827e-01  4.78615950e-01
  4.78647934e-01  4.78856741e-01  4.82089552e-01  4.82495921e-01
  4.86557177e-01  4.86757866e-01  4.88552614e-01  4.88752071e-01
  4.91410939e-01  4.91918905e-01  4.92848247e-01  4.96948599e-01
  4.97674764e-01  4.98651134e-01  5.01183617e-01  5.01354555e-01
  5.02641144e-01  5.03203411e-01  5.07567064e-01  5.08724173e-01
  5.09267201e-01  5.10605132e-01  5.11336496e-01  5.11890025e-01
  5.18909118e-01  5.20727852e-01  5.21601739e-01  5.23036185e-01
  5.28846937e-01  5.31834730e-01  5.33438523e-01  5.34981273e-01
  5.36705237e-01  5.38826779e-01  5.39623551e-01  5.39820240e-01
  5.46043864e-01  5.47859601e-01  5.53252308e-01  5.54452252e-01
  5.64841343e-01  5.65029514e-01  5.67129649e-01  5.68497847e-01
  5.69127893e-01  5.69420600e-01  5.70882558e-01  5.76068122e-01
  5.76512105e-01  5.77408426e-01  5.79633590e-01  5.83147772e-01
  5.85003364e-01  5.88695094e-01  5.88879332e-01  5.90208191e-01
  5.92126964e-01  5.92946725e-01  5.94383573e-01  6.05977211e-01
  6.07875684e-01  6.10498520e-01  6.11088281e-01  6.12979036e-01
  6.14828938e-01  6.15578813e-01  6.15786892e-01  6.16536838e-01
  6.25331307e-01  6.26666691e-01  6.30485604e-01  6.30858969e-01
  6.31475245e-01  6.32517642e-01  6.36848552e-01  6.37108377e-01
  6.41339326e-01  6.42807978e-01  6.45803629e-01  6.47796298e-01
  6.49530005e-01  6.51708312e-01  6.56994861e-01  6.57718430e-01
  6.58356301e-01  6.59537916e-01  6.63803527e-01  6.65764377e-01
  6.76686596e-01  6.82522271e-01  6.86354446e-01  6.89426665e-01
  6.89473322e-01  6.90933830e-01  6.97097964e-01  7.00935149e-01
  7.11969663e-01  7.15067857e-01  7.19851108e-01  7.20450391e-01
  7.35815757e-01]

  warnings.warn(

2022-11-03 10:50:09,296:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.83048829e-01 -1.81381742e-01 -1.58083276e-01 -1.50423314e-01
 -1.47290788e-01 -1.46354752e-01 -1.44953723e-01 -1.43663561e-01
 -1.39135457e-01 -1.39121192e-01 -1.38846414e-01 -1.36962409e-01
 -1.36295000e-01 -1.35574397e-01 -1.34490460e-01 -1.32417308e-01
 -1.32181235e-01 -1.31680899e-01 -1.30806367e-01 -1.23025449e-01
 -1.22979860e-01 -1.19803774e-01 -1.16641620e-01 -1.15912482e-01
 -1.12983592e-01 -1.09661321e-01 -1.04346591e-01 -1.02850955e-01
 -1.01394430e-01 -1.01322001e-01 -1.01019470e-01 -9.90403359e-02
 -9.86937054e-02 -9.76875809e-02 -9.35593976e-02 -9.27037889e-02
 -8.97175890e-02 -8.65876122e-02 -8.57566814e-02 -8.21570689e-02
 -8.06957057e-02 -7.59068905e-02 -7.53589118e-02 -7.45425115e-02
 -7.35776245e-02 -7.30700111e-02 -6.74251300e-02 -6.64703563e-02
 -6.55812342e-02 -6.50859608e-02 -6.38233592e-02 -6.32419485e-02
 -6.13002663e-02 -5.55202542e-02 -5.42327303e-02 -5.28575714e-02
 -5.08925434e-02 -4.97711477e-02 -4.84626563e-02 -4.82071409e-02
 -4.80388558e-02 -4.60122762e-02 -4.58537614e-02 -4.55407771e-02
 -4.44548253e-02 -4.38736815e-02 -4.29009634e-02 -4.27723018e-02
 -3.77706389e-02 -3.71000656e-02 -3.68305264e-02 -3.59785007e-02
 -3.59693976e-02 -3.44127290e-02 -3.42522932e-02 -3.18979640e-02
 -3.09344657e-02 -3.03713326e-02 -2.89210928e-02 -2.81037383e-02
 -2.65394298e-02 -2.64132184e-02 -2.31327830e-02 -2.28205878e-02
 -2.23960791e-02 -1.91505964e-02 -1.69828336e-02 -1.49059804e-02
 -5.02951294e-03 -4.53640452e-03 -3.37920074e-03 -3.17077229e-03
 -2.00576923e-03 -4.12962613e-04  3.84644342e-05  6.47029689e-04
  2.02591225e-03  3.55584178e-03  1.04510286e-02  2.00994147e-02
  2.08936594e-02  2.10963536e-02  2.11091828e-02  2.21361531e-02
  2.70294292e-02  2.76347989e-02  2.94859472e-02  3.02914946e-02
  3.12935476e-02  3.13581519e-02  3.24732444e-02  3.43543862e-02
  3.48155540e-02  3.64898033e-02  3.79243168e-02  3.91456549e-02
  4.09129045e-02  4.09775934e-02  4.20045487e-02  4.26542435e-02
  4.73489886e-02  4.84663442e-02  4.91197403e-02  5.24057005e-02
  5.47165033e-02  5.54034856e-02  5.71060289e-02  5.77064599e-02
  5.94476548e-02  5.97530493e-02  6.05337892e-02  6.33792394e-02
  6.61212095e-02  6.65641182e-02  6.69028977e-02  6.83130576e-02
  6.84685586e-02  6.85714852e-02  7.09178601e-02  7.17167882e-02
  7.63956746e-02  7.70585196e-02  7.93785708e-02  8.18951791e-02
  8.27921469e-02  8.40313354e-02  9.14757524e-02  9.17496285e-02
  9.19602769e-02  9.62260920e-02  9.91101952e-02  9.93495933e-02
  1.00828545e-01  1.01775900e-01  1.03608069e-01  1.03651973e-01
  1.05147773e-01  1.05894018e-01  1.08886375e-01  1.12213278e-01
  1.12719848e-01  1.16141000e-01  1.17206507e-01  1.18349666e-01
  1.19100314e-01  1.20058879e-01  1.20614912e-01  1.22719138e-01
  1.25039035e-01  1.26383969e-01  1.26948408e-01  1.27025164e-01
  1.28000158e-01  1.30415989e-01  1.31137466e-01  1.33192228e-01
  1.33408312e-01  1.33507087e-01  1.37176670e-01  1.38294168e-01
  1.40818776e-01  1.41242629e-01  1.42158674e-01  1.43161697e-01
  1.43245271e-01  1.43958963e-01  1.45138818e-01  1.48467920e-01
  1.51345103e-01  1.51512602e-01  1.54130883e-01  1.56234720e-01
  1.58744424e-01  1.58846398e-01  1.60969175e-01  1.61567933e-01
  1.61661569e-01  1.61747228e-01  1.62586310e-01  1.63108814e-01
  1.63538063e-01  1.63552633e-01  1.64043132e-01  1.64446052e-01
  1.65101453e-01  1.66775580e-01  1.66972373e-01  1.70403636e-01
  1.70435043e-01  1.75787594e-01  1.76835768e-01  1.78574042e-01
  1.79221174e-01  1.79703223e-01  1.79922373e-01  1.83370532e-01
  1.84991518e-01  1.89180863e-01  1.89982551e-01  1.90301442e-01
  1.91461686e-01  1.91527375e-01  1.93648357e-01  1.93836648e-01
  1.95871901e-01  1.96793778e-01  1.98576664e-01  1.98660504e-01
  2.00533306e-01  2.02476918e-01  2.04709370e-01  2.07143055e-01
  2.11647216e-01  2.11692583e-01  2.12079570e-01  2.12322905e-01
  2.12856854e-01  2.16898405e-01  2.17165559e-01  2.17697251e-01
  2.18692699e-01  2.18787521e-01  2.20028052e-01  2.22393065e-01
  2.23582531e-01  2.25880592e-01  2.27012645e-01  2.27277640e-01
  2.31205819e-01  2.31622764e-01  2.32726465e-01  2.37413119e-01
  2.41274633e-01  2.41877594e-01  2.41945433e-01  2.44910773e-01
  2.45512359e-01  2.47503643e-01  2.49174932e-01  2.51050436e-01
  2.51709938e-01  2.54775059e-01  2.54969560e-01  2.56615302e-01
  2.58952564e-01  2.59990898e-01  2.60952556e-01  2.61514434e-01
  2.61900818e-01  2.64349536e-01  2.65821207e-01  2.68014074e-01
  2.69325018e-01  2.77179933e-01  2.78069440e-01  2.78763932e-01
  2.79571529e-01  2.80084492e-01  2.82652454e-01  2.83236529e-01
  2.86614532e-01  2.90282417e-01  2.98424887e-01  2.98731174e-01
  2.99723056e-01  3.02705473e-01  3.03957206e-01  3.11260165e-01
  3.11599002e-01  3.13584598e-01  3.14167658e-01  3.15599045e-01
  3.16862258e-01  3.22630922e-01  3.24489622e-01  3.24850151e-01
  3.26619350e-01  3.26946666e-01  3.30851700e-01  3.30876534e-01
  3.32676011e-01  3.33583875e-01  3.37158134e-01  3.41818029e-01
  3.41924677e-01  3.42201580e-01  3.42227665e-01  3.44168420e-01
  3.48985329e-01  3.50466549e-01  3.50741864e-01  3.52372643e-01
  3.52742256e-01  3.55380364e-01  3.56474234e-01  3.59607566e-01
  3.62514562e-01  3.64347463e-01  3.65329354e-01  3.67540601e-01
  3.70273049e-01  3.71416331e-01  3.71434427e-01  3.76381467e-01
  3.79176821e-01  3.80211902e-01  3.81874197e-01  3.82178535e-01
  3.85433937e-01  3.85870952e-01  3.93308997e-01  3.93532140e-01
  3.93736966e-01  3.94538431e-01  3.95973191e-01  3.98033899e-01
  3.98094357e-01  3.98108755e-01  4.01248287e-01  4.03920691e-01
  4.06356776e-01  4.06858637e-01  4.07590118e-01  4.09669724e-01
  4.15119757e-01  4.19512558e-01  4.19618430e-01  4.20966547e-01
  4.26407912e-01  4.28269102e-01  4.28957450e-01  4.30061115e-01
  4.30831156e-01  4.31781771e-01  4.32584197e-01  4.33523502e-01
  4.35232843e-01  4.36262235e-01  4.37711000e-01  4.38328832e-01
  4.39340866e-01  4.40357516e-01  4.40841310e-01  4.42374534e-01
  4.42917506e-01  4.44129054e-01  4.45163818e-01  4.47874273e-01
  4.48026298e-01  4.50864512e-01  4.54480902e-01  4.55526093e-01
  4.56120028e-01  4.56874392e-01  4.57798794e-01  4.62059886e-01
  4.63103449e-01  4.64818529e-01  4.66637968e-01  4.68587963e-01
  4.69344322e-01  4.72129364e-01  4.72696067e-01  4.73231436e-01
  4.74200958e-01  4.74243080e-01  4.74599944e-01  4.75572735e-01
  4.76763100e-01  4.76951190e-01  4.80489528e-01  4.81878219e-01
  4.86524971e-01  4.88722192e-01  4.89144035e-01  4.90136945e-01
  4.90923619e-01  4.96369069e-01  4.98094177e-01  4.99480826e-01
  5.00651560e-01  5.05540388e-01  5.05639573e-01  5.05685233e-01
  5.06098778e-01  5.06239437e-01  5.11154296e-01  5.17055936e-01
  5.21085298e-01  5.23569596e-01  5.24363026e-01  5.25583556e-01
  5.27619435e-01  5.31362577e-01  5.31426996e-01  5.33732739e-01
  5.34136792e-01  5.35656287e-01  5.40202116e-01  5.40753646e-01
  5.43228897e-01  5.51621930e-01  5.53227630e-01  5.53585598e-01
  5.54284892e-01  5.54429767e-01  5.55678074e-01  5.56108209e-01
  5.56838660e-01  5.57015731e-01  5.59792622e-01  5.60404115e-01
  5.60421495e-01  5.60871878e-01  5.64127456e-01  5.66071219e-01
  5.68547211e-01  5.69393878e-01  5.69956533e-01  5.70334329e-01
  5.74668826e-01  5.75559078e-01  5.75630174e-01  5.75714462e-01
  5.76130333e-01  5.76206475e-01  5.77583699e-01  5.79698807e-01
  5.79811865e-01  5.85880168e-01  5.89544830e-01  5.91914875e-01
  5.92437699e-01  6.07017452e-01  6.07918891e-01  6.09256391e-01
  6.10704552e-01  6.12244325e-01  6.15720098e-01  6.16198873e-01
  6.16612515e-01  6.22852063e-01  6.27184999e-01  6.27389391e-01
  6.31008719e-01  6.33559077e-01  6.34504263e-01  6.36979582e-01
  6.38641839e-01  6.40299705e-01  6.47600730e-01  6.48037639e-01
  6.50575548e-01  6.50668424e-01  6.55159508e-01  6.56796814e-01
  6.59114071e-01  6.63057602e-01  6.69459020e-01  6.71326068e-01
  6.78791750e-01  6.83756634e-01  6.87845241e-01  6.92638561e-01
  6.94242623e-01  6.94265206e-01  6.97263983e-01  6.97767292e-01
  7.09144843e-01  7.16116857e-01  7.19600921e-01  7.25472248e-01
  7.34536799e-01]

  warnings.warn(

2022-11-03 10:50:09,439:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.87642799e-01 -1.62929345e-01 -1.60162427e-01 -1.58967038e-01
 -1.49996106e-01 -1.48520718e-01 -1.45839637e-01 -1.44535895e-01
 -1.36062380e-01 -1.35368127e-01 -1.31472286e-01 -1.29789551e-01
 -1.26857581e-01 -1.26821190e-01 -1.25075438e-01 -1.23499152e-01
 -1.22744739e-01 -1.20360530e-01 -1.18281998e-01 -1.17041329e-01
 -1.16388591e-01 -1.05400292e-01 -1.05175092e-01 -1.04566606e-01
 -1.02554677e-01 -1.01982367e-01 -9.95851288e-02 -9.94075324e-02
 -9.39880865e-02 -9.38074635e-02 -8.87095177e-02 -8.74564878e-02
 -8.26639326e-02 -8.24956336e-02 -8.12749238e-02 -7.85131408e-02
 -7.29077810e-02 -7.28610671e-02 -7.07008035e-02 -6.50091643e-02
 -6.25510481e-02 -6.17937901e-02 -5.97256383e-02 -5.90432096e-02
 -5.66585347e-02 -5.44045935e-02 -5.00019052e-02 -4.79105706e-02
 -4.68107225e-02 -4.45706298e-02 -4.30460656e-02 -4.22403918e-02
 -4.14605625e-02 -4.12665629e-02 -3.40479921e-02 -3.21608961e-02
 -2.93881989e-02 -2.49552349e-02 -2.36141441e-02 -2.30354425e-02
 -2.25116879e-02 -2.16528656e-02 -2.04574368e-02 -1.89296222e-02
 -1.72249146e-02 -1.50459201e-02 -1.48453114e-02 -1.39537737e-02
 -1.39176199e-02 -1.30167713e-02 -1.00186534e-02 -9.91533736e-03
 -6.50311709e-03 -6.18656153e-03 -6.04569513e-03 -5.59021719e-03
 -3.34309443e-03 -1.93572124e-04  1.92909981e-03  2.00665242e-03
  4.39905735e-03  6.29112528e-03  7.87904395e-03  8.11402176e-03
  9.22605776e-03  1.19062152e-02  1.24869587e-02  1.42590691e-02
  1.46193136e-02  1.55391651e-02  1.72981722e-02  1.93099944e-02
  2.14215947e-02  2.14729578e-02  2.24912537e-02  2.25961562e-02
  2.37122757e-02  2.44030916e-02  2.73589284e-02  2.87383692e-02
  3.22678447e-02  3.41588734e-02  3.47856752e-02  3.63089886e-02
  3.69345538e-02  3.80267976e-02  3.89539777e-02  4.02885408e-02
  4.26622098e-02  4.27317133e-02  4.42641729e-02  4.69037099e-02
  4.72883379e-02  4.74978040e-02  5.12560360e-02  5.19134922e-02
  5.27478539e-02  5.62950270e-02  5.64154038e-02  5.66689762e-02
  6.18457680e-02  6.22441594e-02  6.35577000e-02  6.73780637e-02
  7.08251965e-02  7.34343383e-02  7.35994039e-02  7.44864632e-02
  7.53012800e-02  7.88370595e-02  7.97405312e-02  8.25498576e-02
  8.27308971e-02  8.34556842e-02  8.34683224e-02  8.37209191e-02
  8.39040067e-02  8.44168425e-02  8.54589310e-02  8.62568184e-02
  9.21006012e-02  9.26420178e-02  9.80412298e-02  1.00049970e-01
  1.02768318e-01  1.04128562e-01  1.07319535e-01  1.09636651e-01
  1.09791272e-01  1.10610426e-01  1.10710670e-01  1.11126803e-01
  1.12161062e-01  1.15188573e-01  1.15492382e-01  1.16160219e-01
  1.18068923e-01  1.18096463e-01  1.18269632e-01  1.25523558e-01
  1.26145253e-01  1.29659764e-01  1.29785154e-01  1.31800847e-01
  1.33283851e-01  1.33783746e-01  1.41728349e-01  1.41858556e-01
  1.41987128e-01  1.42223266e-01  1.42877329e-01  1.44068371e-01
  1.45688391e-01  1.46052902e-01  1.46912359e-01  1.47082540e-01
  1.47170099e-01  1.50259122e-01  1.50956896e-01  1.51515407e-01
  1.52306230e-01  1.60070503e-01  1.60454134e-01  1.61536229e-01
  1.61950558e-01  1.64058083e-01  1.71369373e-01  1.72137094e-01
  1.72616002e-01  1.73361381e-01  1.73511826e-01  1.77369323e-01
  1.79201884e-01  1.82698729e-01  1.84041196e-01  1.88501020e-01
  1.89683639e-01  1.90465342e-01  1.91924466e-01  1.93211061e-01
  2.00568520e-01  2.03230297e-01  2.07605805e-01  2.08744784e-01
  2.09935556e-01  2.10091291e-01  2.11171732e-01  2.11870033e-01
  2.12353085e-01  2.14096703e-01  2.15464351e-01  2.16932966e-01
  2.18397816e-01  2.19580589e-01  2.19643224e-01  2.22603013e-01
  2.23262883e-01  2.26310869e-01  2.26601516e-01  2.26925592e-01
  2.27342885e-01  2.27554011e-01  2.29362886e-01  2.32087027e-01
  2.36546587e-01  2.36719738e-01  2.37183399e-01  2.37829328e-01
  2.42092710e-01  2.42283697e-01  2.44767261e-01  2.45238759e-01
  2.45890698e-01  2.46526682e-01  2.47457106e-01  2.54214899e-01
  2.57533783e-01  2.59262868e-01  2.59607796e-01  2.62988227e-01
  2.64221802e-01  2.64624672e-01  2.65488076e-01  2.65546625e-01
  2.67565007e-01  2.69560333e-01  2.73780238e-01  2.74356860e-01
  2.75554279e-01  2.76456389e-01  2.78960145e-01  2.79630862e-01
  2.80520280e-01  2.81188552e-01  2.81545550e-01  2.82200503e-01
  2.83567035e-01  2.84156059e-01  2.85689221e-01  2.86515096e-01
  2.91560497e-01  2.93971208e-01  2.96658162e-01  2.97045073e-01
  2.97415723e-01  2.97490849e-01  2.99586637e-01  3.02330095e-01
  3.03442333e-01  3.03667284e-01  3.06440238e-01  3.07228292e-01
  3.07415878e-01  3.09502868e-01  3.10450018e-01  3.10735826e-01
  3.14441565e-01  3.14904611e-01  3.15264740e-01  3.15378786e-01
  3.17639072e-01  3.21915960e-01  3.27243627e-01  3.27494099e-01
  3.27540728e-01  3.28617786e-01  3.29039621e-01  3.35243623e-01
  3.35611300e-01  3.35895248e-01  3.36970781e-01  3.38370223e-01
  3.41095153e-01  3.44838909e-01  3.46597488e-01  3.46845148e-01
  3.48949972e-01  3.49897400e-01  3.53388354e-01  3.53441426e-01
  3.59849772e-01  3.61070057e-01  3.62685074e-01  3.63329742e-01
  3.64969418e-01  3.66501593e-01  3.67735752e-01  3.68671071e-01
  3.68966040e-01  3.69970343e-01  3.71643888e-01  3.72578511e-01
  3.73822829e-01  3.75307688e-01  3.75681096e-01  3.76174231e-01
  3.76960013e-01  3.78681559e-01  3.79257046e-01  3.79891856e-01
  3.80488855e-01  3.80703880e-01  3.81479574e-01  3.87507114e-01
  3.87799133e-01  3.93145166e-01  3.93427179e-01  3.97857742e-01
  3.97952038e-01  3.98301441e-01  3.99426993e-01  3.99927728e-01
  4.00684371e-01  4.02450653e-01  4.02507173e-01  4.03223961e-01
  4.03567165e-01  4.05650452e-01  4.05750299e-01  4.05799645e-01
  4.06219971e-01  4.09421126e-01  4.09826486e-01  4.15508987e-01
  4.16892513e-01  4.16935117e-01  4.17495288e-01  4.18113241e-01
  4.18124656e-01  4.21524030e-01  4.28639113e-01  4.28863134e-01
  4.29919530e-01  4.33173972e-01  4.34287829e-01  4.36173773e-01
  4.38228384e-01  4.38263755e-01  4.38896454e-01  4.39478441e-01
  4.42510976e-01  4.46305032e-01  4.48621055e-01  4.51334448e-01
  4.51905507e-01  4.52684946e-01  4.54244173e-01  4.54372180e-01
  4.54625192e-01  4.55817252e-01  4.60141339e-01  4.65424431e-01
  4.66070109e-01  4.66312578e-01  4.67019058e-01  4.69648288e-01
  4.70509857e-01  4.72298648e-01  4.72394268e-01  4.73093670e-01
  4.76376316e-01  4.77929320e-01  4.78325143e-01  4.79357807e-01
  4.80371132e-01  4.80398676e-01  4.84585457e-01  4.89363787e-01
  4.90287667e-01  4.90853054e-01  4.91887013e-01  4.95000094e-01
  4.98401697e-01  4.98803715e-01  4.99975751e-01  5.01644459e-01
  5.02348304e-01  5.04147716e-01  5.07815126e-01  5.13104971e-01
  5.14794884e-01  5.15144914e-01  5.15726194e-01  5.16404778e-01
  5.17747453e-01  5.18565883e-01  5.18846024e-01  5.24585273e-01
  5.26414857e-01  5.26903758e-01  5.27293594e-01  5.30208672e-01
  5.32892440e-01  5.33415600e-01  5.33512187e-01  5.36191860e-01
  5.36280787e-01  5.36298006e-01  5.36532269e-01  5.38799937e-01
  5.41979651e-01  5.42336203e-01  5.42980468e-01  5.44971325e-01
  5.45575933e-01  5.50558429e-01  5.53972263e-01  5.56369955e-01
  5.57908059e-01  5.60959009e-01  5.61101686e-01  5.61506294e-01
  5.67428974e-01  5.70362038e-01  5.73129921e-01  5.77001002e-01
  5.78013754e-01  5.78711657e-01  5.78880363e-01  5.90001433e-01
  5.91699194e-01  5.96771265e-01  5.96801756e-01  5.99740817e-01
  6.00948590e-01  6.01157320e-01  6.01719458e-01  6.03685143e-01
  6.07256454e-01  6.07289830e-01  6.09859882e-01  6.10533165e-01
  6.12626069e-01  6.13095228e-01  6.13191443e-01  6.14471269e-01
  6.15279296e-01  6.16920322e-01  6.17933046e-01  6.20416982e-01
  6.20516285e-01  6.21530324e-01  6.22272388e-01  6.25102231e-01
  6.25284184e-01  6.26863613e-01  6.27005415e-01  6.27899985e-01
  6.28172117e-01  6.29494033e-01  6.32837213e-01  6.36623702e-01
  6.39374061e-01  6.42411313e-01  6.52281056e-01  6.60158862e-01
  6.61339925e-01  6.62504958e-01  6.63079132e-01  6.65121129e-01
  6.67425003e-01  6.68150175e-01  6.73829832e-01  6.76979878e-01
  6.84394826e-01  6.86163744e-01  6.88033416e-01  7.07335490e-01
  7.08941587e-01  7.18138843e-01  7.42895870e-01  7.43626792e-01
  7.58767626e-01]

  warnings.warn(

2022-11-03 10:50:09,457:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.25673012 -0.18928266 -0.18469016 -0.17868482 -0.17370742 -0.17248065
 -0.16972361 -0.16706492 -0.16359814 -0.15922427 -0.15815055 -0.15430371
 -0.14842358 -0.14732471 -0.14704498 -0.14090611 -0.13555668 -0.13371695
 -0.12908503 -0.12900906 -0.12315133 -0.11989664 -0.1188972  -0.11754277
 -0.11602213 -0.11105468 -0.10478617 -0.10268161 -0.10244518 -0.10172873
 -0.09720959 -0.09330662 -0.09077263 -0.09053115 -0.08599952 -0.08097342
 -0.08004602 -0.07890947 -0.07847512 -0.07798814 -0.06990207 -0.06676709
 -0.06420763 -0.06249359 -0.06174803 -0.06165133 -0.06133053 -0.05860642
 -0.05771604 -0.05748284 -0.0572407  -0.05700893 -0.05441777 -0.05432672
 -0.05395349 -0.05362728 -0.05080962 -0.04991135 -0.04947918 -0.04922091
 -0.04885126 -0.04318646 -0.04164557 -0.04158553 -0.03668416 -0.03163038
 -0.03018425 -0.02816951 -0.02737862 -0.02691031 -0.02587975 -0.02459504
 -0.02209855 -0.02157241 -0.0186596  -0.01683405 -0.01475458 -0.01210854
 -0.01100831 -0.00732126 -0.00665795 -0.00321759 -0.00151452 -0.00106906
  0.0040785   0.01008817  0.01191116  0.01484585  0.01586965  0.01789862
  0.0180467   0.02448223  0.02475673  0.02759881  0.02780438  0.02843082
  0.029373    0.03177217  0.03337059  0.03341139  0.03586602  0.0380098
  0.03821658  0.04062341  0.04193673  0.04205322  0.04288253  0.04604386
  0.04668832  0.04747835  0.04820201  0.04854782  0.0488829   0.04914914
  0.05066605  0.05114086  0.05334533  0.0575347   0.05936988  0.06018869
  0.06054156  0.06094739  0.06095357  0.06142736  0.06153681  0.06349724
  0.06429844  0.06459413  0.06551674  0.06606878  0.06695876  0.06755838
  0.06834632  0.06961911  0.06963944  0.07054962  0.07239184  0.07276533
  0.07322426  0.0789695   0.07938769  0.08035618  0.0812935   0.08137342
  0.08141526  0.0842788   0.08477142  0.08536092  0.08616549  0.08889035
  0.08948205  0.09797772  0.10075395  0.10103333  0.10145011  0.1056661
  0.10909609  0.11253068  0.11659985  0.12016317  0.12318119  0.12361628
  0.12397383  0.12678241  0.12709341  0.12779855  0.13046052  0.13110419
  0.13363277  0.13554909  0.13637862  0.1382151   0.13829778  0.13855699
  0.14067195  0.1419196   0.14255841  0.14275507  0.14507639  0.14542483
  0.14673535  0.14833425  0.14836857  0.15165524  0.15183871  0.15205198
  0.15205507  0.15545023  0.15606247  0.15667122  0.15693671  0.16137869
  0.16223435  0.16299475  0.16710537  0.16862062  0.17207842  0.17310288
  0.1751268   0.17715253  0.18096352  0.18155913  0.18359493  0.18380281
  0.18575054  0.1865692   0.18732878  0.18825684  0.18931274  0.18932054
  0.192967    0.19763313  0.19775829  0.19834042  0.19851001  0.19975211
  0.20001525  0.20069113  0.20103966  0.20145184  0.20292439  0.20442161
  0.20494687  0.20776315  0.20825897  0.20868293  0.20914442  0.20959812
  0.21135448  0.21254913  0.21540493  0.21570815  0.21583563  0.21887392
  0.22005182  0.22173136  0.22187887  0.22413114  0.22419026  0.22472057
  0.22778844  0.22828212  0.22962053  0.23075654  0.23506825  0.2358922
  0.23760413  0.23813828  0.23856332  0.23886689  0.23918204  0.24070185
  0.24182338  0.24221748  0.24415496  0.24495922  0.24677217  0.25057934
  0.25429417  0.25555079  0.25714728  0.25732679  0.25781093  0.25797321
  0.26000029  0.26542203  0.26603342  0.26716963  0.26719124  0.26838189
  0.26927854  0.2695292   0.27227746  0.27308474  0.27310862  0.27337697
  0.27497592  0.27563479  0.27637087  0.27679296  0.27691509  0.27738246
  0.27935983  0.28098919  0.28315441  0.28353379  0.28701833  0.28729985
  0.28762535  0.28902551  0.29095592  0.29130149  0.29246153  0.29251412
  0.29545432  0.30113047  0.30318882  0.30438199  0.30553951  0.30720902
  0.30808899  0.30830726  0.30929534  0.30937352  0.31007307  0.31111846
  0.3142527   0.31476028  0.31515295  0.31565444  0.32028686  0.32224238
  0.32358479  0.32543341  0.32977673  0.33500042  0.33626332  0.33637595
  0.33901478  0.34438633  0.3450429   0.34513782  0.34751978  0.34822393
  0.34949348  0.35063259  0.35273496  0.35391931  0.35436642  0.35674581
  0.35911727  0.36018839  0.3606988   0.36095977  0.36135123  0.36365786
  0.3636906   0.36396391  0.36417358  0.36832931  0.37371409  0.37401977
  0.37565311  0.37701306  0.37910456  0.38085047  0.3873719   0.38749107
  0.38925254  0.39296029  0.39888029  0.40199162  0.40357871  0.40561751
  0.40998418  0.4105892   0.4153835   0.41575026  0.41772176  0.42208825
  0.42226512  0.42667936  0.42668858  0.4282259   0.42957215  0.42976644
  0.43191849  0.43299144  0.44041252  0.44238587  0.44265275  0.44733335
  0.44766065  0.44796471  0.44796816  0.44956215  0.45079451  0.45854666
  0.46389364  0.46497244  0.47299951  0.47587334  0.47824828  0.47974949
  0.48123202  0.48167879  0.48220596  0.4835059   0.4840529   0.48565889
  0.48727044  0.48927472  0.49316616  0.49525487  0.49544985  0.49600376
  0.5007673   0.50123221  0.5038754   0.50388723  0.50532008  0.5101269
  0.51125228  0.5114937   0.51761715  0.52210849  0.52267682  0.52317162
  0.52533886  0.52639534  0.52821555  0.53051506  0.53111205  0.53252599
  0.53325164  0.53515183  0.53547     0.53689451  0.53898383  0.54103739
  0.5412498   0.54239507  0.54321818  0.54334222  0.54576232  0.54796694
  0.54835635  0.55052517  0.55102802  0.55127206  0.55416132  0.55608392
  0.55637787  0.55713576  0.55823593  0.558263    0.56013837  0.56361517
  0.57216774  0.5757967   0.57749626  0.57931301  0.58000725  0.58045791
  0.58159043  0.58791499  0.5881489   0.5887347   0.58928191  0.5913947
  0.59151773  0.59809122  0.59955705  0.60181789  0.6020408   0.60612662
  0.61319188  0.61469544  0.61616804  0.62521422  0.62550199  0.62591737
  0.62706533  0.62759294  0.62809509  0.62910247  0.62991181  0.63421216
  0.63900214  0.64095647  0.64719681  0.65099007  0.6536679   0.65597422
  0.65625755  0.66764162  0.67172357  0.67469027  0.67539615  0.67646896
  0.67822633  0.67839768  0.67846094  0.6787406   0.68030324  0.68311283
  0.68791549  0.68899136  0.6924184   0.71355539  0.71654626  0.72280334
  0.72921307]

  warnings.warn(

2022-11-03 10:50:09,474:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.76828970e-01 -1.71590919e-01 -1.60074520e-01 -1.54233416e-01
 -1.53754253e-01 -1.35760584e-01 -1.33551417e-01 -1.32461881e-01
 -1.25838367e-01 -1.17038122e-01 -1.16500262e-01 -1.15088990e-01
 -1.13429493e-01 -1.11939679e-01 -1.11383469e-01 -1.09295667e-01
 -1.08794800e-01 -1.05104520e-01 -1.02609051e-01 -1.01706842e-01
 -1.00838178e-01 -1.00060102e-01 -9.92363730e-02 -9.65399041e-02
 -9.46506116e-02 -9.17803428e-02 -9.06399124e-02 -8.03530817e-02
 -7.89968938e-02 -7.75164309e-02 -7.71067875e-02 -7.69268583e-02
 -7.57457605e-02 -6.90883016e-02 -6.83036689e-02 -6.17971968e-02
 -6.04433692e-02 -6.01744042e-02 -5.86243117e-02 -5.81042684e-02
 -5.63905720e-02 -5.55752302e-02 -5.30256916e-02 -5.25311828e-02
 -5.18352272e-02 -4.95095232e-02 -4.75784811e-02 -4.73616742e-02
 -4.57105651e-02 -4.46881460e-02 -4.37709708e-02 -4.34436666e-02
 -4.05282443e-02 -3.80974203e-02 -3.69345616e-02 -3.62506776e-02
 -3.53203570e-02 -3.46977167e-02 -3.38618021e-02 -3.31728175e-02
 -2.97655185e-02 -2.77641212e-02 -2.58381100e-02 -2.56689700e-02
 -2.51336614e-02 -2.50571707e-02 -2.48193960e-02 -2.05577396e-02
 -1.99662980e-02 -1.69089379e-02 -1.59841051e-02 -1.48097797e-02
 -1.46612823e-02 -9.85119430e-03 -8.24179321e-03 -3.09162106e-03
 -2.16165421e-03 -1.65463332e-03 -2.03126882e-04  8.15562068e-06
  4.98402147e-03  7.44600853e-03  1.02434234e-02  1.05811733e-02
  1.52127800e-02  1.58290733e-02  1.86435511e-02  1.86641338e-02
  2.14386405e-02  2.21812666e-02  2.32399765e-02  2.45357686e-02
  2.52718538e-02  2.70195193e-02  2.76674864e-02  2.77928925e-02
  3.04522416e-02  3.08062873e-02  3.09463859e-02  3.46253724e-02
  3.53403464e-02  3.58346150e-02  3.62356384e-02  3.74417980e-02
  3.82654510e-02  3.92358557e-02  4.14142745e-02  4.21686074e-02
  4.21755080e-02  4.22210887e-02  4.37041475e-02  4.45280409e-02
  4.52454903e-02  5.01641867e-02  5.14701976e-02  5.31305104e-02
  5.49214926e-02  5.56424330e-02  5.59853385e-02  5.60559178e-02
  5.71679814e-02  5.84685098e-02  5.98297980e-02  6.24660123e-02
  6.46230377e-02  6.59403227e-02  6.73136198e-02  6.79505493e-02
  6.87671958e-02  6.91527335e-02  6.93247992e-02  6.99943193e-02
  7.06540423e-02  7.40828575e-02  7.69243052e-02  7.74713345e-02
  7.75698252e-02  7.84758769e-02  8.07963095e-02  8.14315408e-02
  8.27171309e-02  8.30835894e-02  8.56577914e-02  8.61202990e-02
  8.74514238e-02  8.92051393e-02  9.11343239e-02  9.13090491e-02
  9.18111157e-02  9.21068953e-02  9.51817366e-02  9.57133179e-02
  9.71698666e-02  9.95515120e-02  1.00874940e-01  1.02105049e-01
  1.04753719e-01  1.05660935e-01  1.06040994e-01  1.06668271e-01
  1.06862947e-01  1.08689657e-01  1.10108758e-01  1.13412228e-01
  1.14168775e-01  1.14886257e-01  1.16700158e-01  1.17372249e-01
  1.21480659e-01  1.24704294e-01  1.27825327e-01  1.28326517e-01
  1.29905645e-01  1.31903977e-01  1.33956037e-01  1.39452550e-01
  1.40565854e-01  1.49006685e-01  1.52111030e-01  1.53052374e-01
  1.55638544e-01  1.55852230e-01  1.56153701e-01  1.58908051e-01
  1.58916757e-01  1.59320338e-01  1.59632253e-01  1.61553977e-01
  1.62308063e-01  1.62631600e-01  1.63239273e-01  1.63970966e-01
  1.65400834e-01  1.67744732e-01  1.69409182e-01  1.70200607e-01
  1.71577613e-01  1.72650654e-01  1.74332081e-01  1.76668793e-01
  1.78249035e-01  1.81248698e-01  1.81459875e-01  1.82512820e-01
  1.83982105e-01  1.88441880e-01  1.88655686e-01  1.89848852e-01
  1.90736604e-01  1.92298419e-01  1.94421113e-01  1.98386300e-01
  1.99683331e-01  2.00619640e-01  2.00989311e-01  2.02818361e-01
  2.04494302e-01  2.05432037e-01  2.06751453e-01  2.06929719e-01
  2.07346486e-01  2.07809537e-01  2.07992895e-01  2.10484206e-01
  2.11319619e-01  2.11459960e-01  2.15608003e-01  2.19177550e-01
  2.20660602e-01  2.23231563e-01  2.23328837e-01  2.27690324e-01
  2.28084914e-01  2.28258642e-01  2.28906586e-01  2.29709847e-01
  2.29876168e-01  2.34117328e-01  2.36648358e-01  2.37783622e-01
  2.39450639e-01  2.40454788e-01  2.41435331e-01  2.43926043e-01
  2.44454053e-01  2.47078210e-01  2.47522202e-01  2.49226023e-01
  2.49489784e-01  2.53536602e-01  2.56838799e-01  2.57268689e-01
  2.59456426e-01  2.60866807e-01  2.61045491e-01  2.63134590e-01
  2.64950540e-01  2.66969778e-01  2.67653795e-01  2.68719169e-01
  2.69415381e-01  2.69599170e-01  2.70587445e-01  2.72027198e-01
  2.75375763e-01  2.77384029e-01  2.78114262e-01  2.79386590e-01
  2.80013012e-01  2.80238803e-01  2.84934635e-01  2.86328533e-01
  2.87644820e-01  2.88524457e-01  2.92390551e-01  2.92742897e-01
  2.93208408e-01  2.93312143e-01  2.96445530e-01  2.98256748e-01
  2.98365610e-01  2.98767273e-01  2.99033117e-01  2.99419398e-01
  2.99778438e-01  3.03531510e-01  3.04870281e-01  3.06160536e-01
  3.06819081e-01  3.07362572e-01  3.07481063e-01  3.07593984e-01
  3.07769271e-01  3.10477076e-01  3.13067822e-01  3.14376786e-01
  3.14624999e-01  3.15849516e-01  3.18379503e-01  3.18554257e-01
  3.19027010e-01  3.20464702e-01  3.22056721e-01  3.22808893e-01
  3.22809392e-01  3.28589192e-01  3.29107710e-01  3.30089319e-01
  3.31001306e-01  3.31920809e-01  3.33944077e-01  3.36306619e-01
  3.39043529e-01  3.41649017e-01  3.45053127e-01  3.47811905e-01
  3.49203531e-01  3.50015594e-01  3.51526776e-01  3.53123103e-01
  3.55200558e-01  3.55388683e-01  3.57168020e-01  3.58612946e-01
  3.58989094e-01  3.60087481e-01  3.60592265e-01  3.63474439e-01
  3.63763798e-01  3.66045287e-01  3.66621314e-01  3.67057851e-01
  3.68296817e-01  3.69975278e-01  3.70732555e-01  3.72438024e-01
  3.73466860e-01  3.75312022e-01  3.75977138e-01  3.81285658e-01
  3.83788074e-01  3.84157003e-01  3.84493599e-01  3.88150338e-01
  3.90226797e-01  3.93561955e-01  3.96175629e-01  4.01164172e-01
  4.04266362e-01  4.05352072e-01  4.12868229e-01  4.12932899e-01
  4.13247697e-01  4.13609466e-01  4.14089768e-01  4.14667631e-01
  4.15573631e-01  4.19887153e-01  4.21804021e-01  4.23553221e-01
  4.24276146e-01  4.26017007e-01  4.26203539e-01  4.26262530e-01
  4.34462828e-01  4.36573217e-01  4.37353302e-01  4.38545434e-01
  4.41785737e-01  4.44099947e-01  4.46395743e-01  4.49317434e-01
  4.49329431e-01  4.49447897e-01  4.50491964e-01  4.51289664e-01
  4.55347155e-01  4.56609081e-01  4.58432099e-01  4.59064721e-01
  4.59356465e-01  4.60865732e-01  4.60949787e-01  4.63882765e-01
  4.68299103e-01  4.68738903e-01  4.69909047e-01  4.73102315e-01
  4.76480006e-01  4.76760631e-01  4.77177494e-01  4.78797523e-01
  4.81139207e-01  4.85327175e-01  4.89884066e-01  4.91808739e-01
  4.92500030e-01  4.93823502e-01  4.96253001e-01  4.97267539e-01
  4.99699543e-01  5.01555007e-01  5.03398937e-01  5.06210835e-01
  5.09613397e-01  5.13528798e-01  5.13587699e-01  5.13869720e-01
  5.19318546e-01  5.22035845e-01  5.22507502e-01  5.25244696e-01
  5.28032313e-01  5.28759864e-01  5.30013458e-01  5.30127594e-01
  5.30508558e-01  5.35903881e-01  5.39752239e-01  5.45661764e-01
  5.45890323e-01  5.47712762e-01  5.47891902e-01  5.50044906e-01
  5.52096380e-01  5.53671158e-01  5.60032958e-01  5.61498711e-01
  5.68278403e-01  5.74696319e-01  5.78761926e-01  5.83139858e-01
  5.85206103e-01  5.85641166e-01  5.86640733e-01  5.93164100e-01
  5.94548245e-01  5.95417966e-01  5.96593657e-01  5.97208020e-01
  5.98201852e-01  5.98325363e-01  6.00104142e-01  6.02111061e-01
  6.03521359e-01  6.03553934e-01  6.05935122e-01  6.06261823e-01
  6.06507172e-01  6.06953774e-01  6.12001248e-01  6.12560296e-01
  6.12849245e-01  6.13269654e-01  6.15091948e-01  6.17064177e-01
  6.18273199e-01  6.19377878e-01  6.24037071e-01  6.27114018e-01
  6.27962170e-01  6.29657402e-01  6.32783546e-01  6.35194715e-01
  6.38056403e-01  6.41347368e-01  6.42440633e-01  6.46976870e-01
  6.49446479e-01  6.53948993e-01  6.54646347e-01  6.57946539e-01
  6.58463372e-01  6.58914496e-01  6.61215993e-01  6.61245463e-01
  6.65931179e-01  6.66746575e-01  6.66840200e-01  6.73361811e-01
  6.84028019e-01  6.84917464e-01  7.02234432e-01  7.08033739e-01
  7.09314070e-01  7.12785068e-01  7.21896567e-01  7.22050583e-01
  7.22933656e-01  7.22964407e-01  7.26397217e-01  7.64551607e-01]

  warnings.warn(

2022-11-03 10:50:09,513:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.21995375 -0.20156224 -0.19517494 -0.17532196 -0.17526044 -0.17190587
 -0.16542606 -0.16485416 -0.16473621 -0.16382731 -0.15718968 -0.1567626
 -0.15370434 -0.15249719 -0.14336911 -0.13846612 -0.13716038 -0.13617887
 -0.13612657 -0.12946446 -0.12688151 -0.11988778 -0.11883769 -0.11591722
 -0.11344735 -0.11168056 -0.11134392 -0.11102781 -0.10922339 -0.10921945
 -0.10552838 -0.09637341 -0.09617047 -0.09268386 -0.09246421 -0.09106299
 -0.09049436 -0.08483331 -0.08423254 -0.08395837 -0.08332911 -0.08278316
 -0.08254977 -0.08042081 -0.07882946 -0.07486019 -0.06887859 -0.06816356
 -0.06648795 -0.06282146 -0.06253692 -0.06240397 -0.06055665 -0.0561319
 -0.05561258 -0.05423256 -0.05390765 -0.0529403  -0.05242863 -0.04733807
 -0.04646914 -0.04635257 -0.04583403 -0.04499208 -0.04491378 -0.04151733
 -0.03846217 -0.03759608 -0.03595636 -0.03356973 -0.03300069 -0.03244074
 -0.02533164 -0.01986503 -0.01693559 -0.01556187 -0.01546881 -0.01449584
 -0.01336515 -0.01099972 -0.01066485 -0.00811392 -0.00693457 -0.00489688
 -0.00175242  0.00122484  0.00182201  0.00604517  0.0096298   0.01149551
  0.01206894  0.01391786  0.01686972  0.01817437  0.01833533  0.02119341
  0.02133919  0.02310854  0.02466553  0.02953944  0.03039273  0.03070166
  0.03143429  0.03695     0.03903229  0.03980566  0.04090561  0.04462778
  0.05041741  0.050648    0.05099537  0.05499355  0.05512309  0.05532132
  0.05673146  0.05906322  0.05919481  0.06075396  0.06105576  0.06563767
  0.06850039  0.06963128  0.07166113  0.07637359  0.07687042  0.07828668
  0.07857365  0.0807738   0.08452663  0.08629059  0.08664293  0.08746303
  0.09033173  0.09243359  0.09263015  0.09350479  0.09474971  0.0978683
  0.09975243  0.10090523  0.1027711   0.10537573  0.10595945  0.10804259
  0.10932383  0.11028343  0.11029149  0.11100749  0.11376965  0.1172224
  0.12107135  0.12406677  0.12412644  0.1243029   0.12777708  0.12947169
  0.13038808  0.13263488  0.13294959  0.13309386  0.13636565  0.1377309
  0.13850561  0.13903155  0.14279615  0.14347773  0.14434866  0.1462418
  0.14681006  0.14896265  0.1550299   0.15634256  0.15789115  0.15820876
  0.15889219  0.15946524  0.16078833  0.16362414  0.16471883  0.16584496
  0.16971421  0.1720216   0.17376434  0.17603084  0.18207719  0.18220787
  0.18256065  0.18358076  0.18529167  0.18637677  0.19150296  0.192734
  0.19293644  0.19298236  0.19393749  0.19637504  0.1967431   0.19709383
  0.19759569  0.19800886  0.19844034  0.19845084  0.20106684  0.20250968
  0.20418742  0.2055052   0.20589124  0.20734537  0.20737285  0.21048647
  0.21133808  0.21171044  0.21825507  0.21903192  0.21981598  0.22053948
  0.22075728  0.22134801  0.22257426  0.22345965  0.22424499  0.22493684
  0.2253667   0.22542378  0.22548906  0.23096236  0.23248797  0.23663138
  0.24005468  0.24022641  0.24166679  0.24249524  0.24364751  0.24532052
  0.24730132  0.2476877   0.24929943  0.25248203  0.25406676  0.25471994
  0.25474042  0.25522692  0.2554401   0.25686827  0.25690413  0.25717003
  0.26223076  0.26396139  0.26407468  0.26540716  0.26647102  0.26658703
  0.27034574  0.2706378   0.27387613  0.27402494  0.27479193  0.27557641
  0.27621449  0.27737494  0.2777618   0.27838998  0.28099577  0.28187683
  0.28401214  0.2844458   0.28529793  0.28580957  0.28663927  0.28682262
  0.28850114  0.28851251  0.28989243  0.28997739  0.29233565  0.29494311
  0.29846965  0.29876389  0.30701713  0.31155038  0.31236627  0.31319372
  0.31334198  0.31425054  0.31583074  0.31657161  0.31722067  0.31729027
  0.3173681   0.31820598  0.32144875  0.32197988  0.32299161  0.32400343
  0.32463548  0.32531887  0.32605303  0.32634561  0.32803443  0.32852128
  0.32980238  0.33104302  0.33282144  0.33466423  0.34188786  0.34218012
  0.34652675  0.34665565  0.35022167  0.35136931  0.35162105  0.3524659
  0.35280341  0.35389056  0.3555991   0.35628712  0.35736158  0.35760006
  0.35801337  0.36007729  0.3623172   0.36423307  0.3665527   0.36909796
  0.37117461  0.37315326  0.37334409  0.37745377  0.37860499  0.38003817
  0.38471284  0.38692133  0.38791566  0.38908373  0.38993828  0.39022611
  0.39029691  0.39032836  0.39141929  0.3915439   0.39312261  0.39569472
  0.39693782  0.39920439  0.40293068  0.40632054  0.40723917  0.4127757
  0.41464425  0.41635175  0.41636185  0.41972     0.42007464  0.42231677
  0.42249514  0.4239188   0.42465395  0.42979767  0.43118197  0.43225241
  0.43263666  0.4328125   0.43445567  0.44102219  0.44105451  0.44132504
  0.44151715  0.44247983  0.44289565  0.44375305  0.44558489  0.44572274
  0.44706832  0.44790358  0.44878909  0.44988428  0.45407443  0.45600448
  0.4630452   0.46444079  0.4660532   0.46811018  0.46973204  0.47142993
  0.4729564   0.47480479  0.47871436  0.4807098   0.48099816  0.48452403
  0.48531511  0.48765297  0.48803672  0.48902529  0.48996073  0.49022132
  0.49195976  0.49254325  0.494444    0.49510512  0.49814578  0.49938263
  0.50053628  0.50286795  0.50304959  0.50337797  0.50373531  0.50579938
  0.50614557  0.50715503  0.50971351  0.51410136  0.51556894  0.51611288
  0.51937683  0.52121574  0.52411329  0.53216051  0.53281377  0.53415622
  0.53609685  0.53611704  0.53653496  0.53725898  0.53925269  0.54315348
  0.54456024  0.54486036  0.549304    0.55417157  0.55764519  0.56153977
  0.56160231  0.56248144  0.5641628   0.56581901  0.56804971  0.57039778
  0.5735457   0.57504472  0.5758139   0.57661747  0.57937371  0.57949028
  0.58038332  0.58094584  0.58804753  0.59242457  0.59480299  0.59621177
  0.59741719  0.59807833  0.60348283  0.60895934  0.60994314  0.61016638
  0.61197567  0.61222734  0.61263175  0.61504288  0.61590109  0.62467272
  0.62727327  0.62780469  0.63030568  0.63690553  0.642505    0.64462021
  0.64474076  0.64492361  0.64642337  0.64751352  0.64930491  0.65005229
  0.65053256  0.65497884  0.6617082   0.66282176  0.66337642  0.66470706
  0.66494939  0.66512453  0.66537536  0.67329833  0.67536626  0.69493366
  0.7048751   0.71299944  0.73657167  0.74688423  0.75921636  0.76776268
  0.76948426]

  warnings.warn(

2022-11-03 10:50:10,998:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:11,014:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:11,735:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.13808844e-01 -2.09173753e-01 -2.07443349e-01 -1.90821735e-01
 -1.86030049e-01 -1.71647435e-01 -1.68266034e-01 -1.51747971e-01
 -1.51216949e-01 -1.51137418e-01 -1.49946208e-01 -1.49577891e-01
 -1.39052724e-01 -1.32655682e-01 -1.32163734e-01 -1.31465927e-01
 -1.28257956e-01 -1.27042841e-01 -1.26413752e-01 -1.25099816e-01
 -1.24888447e-01 -1.24662051e-01 -1.23411706e-01 -1.21309139e-01
 -1.20499269e-01 -1.18221138e-01 -1.06674896e-01 -1.05730834e-01
 -1.01341223e-01 -9.91349153e-02 -9.71267462e-02 -9.55788703e-02
 -9.35183332e-02 -9.26751555e-02 -9.25446485e-02 -9.21121973e-02
 -9.10499744e-02 -8.91215239e-02 -8.37524877e-02 -7.67716698e-02
 -7.62168319e-02 -7.38828992e-02 -7.28941466e-02 -7.08876713e-02
 -7.07886706e-02 -6.48864922e-02 -6.00769827e-02 -5.93176617e-02
 -5.84790659e-02 -5.73694148e-02 -5.41268508e-02 -5.28297489e-02
 -4.98184212e-02 -4.88872161e-02 -4.70962933e-02 -4.70477156e-02
 -4.42225111e-02 -4.30382082e-02 -4.27737502e-02 -4.06245407e-02
 -3.98367651e-02 -3.90618497e-02 -3.76747594e-02 -3.53186776e-02
 -3.40138254e-02 -3.37692002e-02 -2.97115328e-02 -2.76807581e-02
 -2.71841268e-02 -2.26024178e-02 -2.14616189e-02 -2.03326901e-02
 -1.89302133e-02 -1.82186680e-02 -1.58550345e-02 -1.51932204e-02
 -1.42069042e-02 -1.24354957e-02 -1.04429321e-02 -9.84225601e-03
 -9.47102266e-03 -5.57915397e-03 -4.06564157e-03 -4.06219680e-03
 -1.71271556e-03 -1.35901473e-03 -1.64804971e-04  1.13630778e-03
  2.81053173e-03  2.98279769e-03  5.34608796e-03  6.23609956e-03
  6.99276017e-03  8.66732567e-03  9.12070682e-03  1.55780243e-02
  2.04126261e-02  2.20247119e-02  2.26399565e-02  2.38025116e-02
  2.42864367e-02  2.98602848e-02  2.99523029e-02  3.21878503e-02
  3.23974065e-02  3.34262184e-02  3.52450005e-02  3.64047101e-02
  4.08066264e-02  4.34728325e-02  4.76591241e-02  4.85309113e-02
  4.91113118e-02  4.95568938e-02  5.32121521e-02  5.60030061e-02
  5.86330113e-02  5.95480324e-02  6.01138897e-02  6.11387418e-02
  6.28092946e-02  6.44947333e-02  6.49225072e-02  6.54623712e-02
  6.62700548e-02  6.84301359e-02  6.85575174e-02  7.02576279e-02
  7.14701277e-02  7.20266155e-02  7.37930795e-02  7.47211380e-02
  7.49301377e-02  7.54888294e-02  8.03495251e-02  8.41906934e-02
  8.52403698e-02  8.54353496e-02  8.71562391e-02  8.72230915e-02
  8.72522669e-02  8.77712202e-02  8.79049847e-02  9.19845458e-02
  9.26359473e-02  9.35275422e-02  9.55800594e-02  9.61981687e-02
  9.80993609e-02  1.01326888e-01  1.01379761e-01  1.02150401e-01
  1.03862949e-01  1.05245777e-01  1.09135275e-01  1.11602020e-01
  1.12327002e-01  1.12375769e-01  1.12690676e-01  1.13478792e-01
  1.13647890e-01  1.16199187e-01  1.16646952e-01  1.21482893e-01
  1.21566378e-01  1.22385368e-01  1.22586609e-01  1.22908380e-01
  1.23641196e-01  1.25467003e-01  1.25939344e-01  1.26463328e-01
  1.28662109e-01  1.31436952e-01  1.31971491e-01  1.33557354e-01
  1.35755288e-01  1.39277032e-01  1.39380372e-01  1.40015679e-01
  1.40442969e-01  1.41247015e-01  1.41321471e-01  1.42406469e-01
  1.42553363e-01  1.45331022e-01  1.48157813e-01  1.53653170e-01
  1.53914943e-01  1.54648976e-01  1.55411547e-01  1.55493257e-01
  1.56873935e-01  1.58485337e-01  1.59468259e-01  1.61962540e-01
  1.63196848e-01  1.64283153e-01  1.64836651e-01  1.65822618e-01
  1.65828972e-01  1.67998539e-01  1.70842505e-01  1.73517748e-01
  1.75996485e-01  1.76553807e-01  1.78440602e-01  1.78522057e-01
  1.81072047e-01  1.81362642e-01  1.83209427e-01  1.84015134e-01
  1.84492719e-01  1.89159068e-01  1.89449419e-01  1.91881875e-01
  1.93597891e-01  1.94607487e-01  1.95508371e-01  1.96911188e-01
  2.01110297e-01  2.02157095e-01  2.02283618e-01  2.02754127e-01
  2.02967545e-01  2.03276532e-01  2.03431101e-01  2.04808617e-01
  2.04944351e-01  2.10152747e-01  2.13086500e-01  2.14789513e-01
  2.20916272e-01  2.24292164e-01  2.25903939e-01  2.26346975e-01
  2.26590945e-01  2.30847592e-01  2.31407936e-01  2.32745969e-01
  2.32872781e-01  2.33180475e-01  2.34352402e-01  2.34510389e-01
  2.36650533e-01  2.38437111e-01  2.38814057e-01  2.40450276e-01
  2.43376916e-01  2.44331706e-01  2.46421728e-01  2.48293216e-01
  2.49406172e-01  2.50916534e-01  2.54196815e-01  2.59047286e-01
  2.59170262e-01  2.63747523e-01  2.64566535e-01  2.65438429e-01
  2.65456883e-01  2.67219232e-01  2.71140403e-01  2.73352796e-01
  2.80369657e-01  2.81218396e-01  2.81890963e-01  2.82532060e-01
  2.85913129e-01  2.87387377e-01  2.89071082e-01  2.92742157e-01
  2.93385913e-01  2.94401844e-01  2.95786774e-01  2.96022781e-01
  2.96373416e-01  2.96708133e-01  2.97156914e-01  2.97603129e-01
  2.98665260e-01  2.98782636e-01  2.99539381e-01  3.01233927e-01
  3.01786419e-01  3.02490254e-01  3.02665091e-01  3.06900083e-01
  3.07321522e-01  3.09579416e-01  3.12008883e-01  3.14856474e-01
  3.20370569e-01  3.23071474e-01  3.26139729e-01  3.26681600e-01
  3.27542360e-01  3.27734089e-01  3.28185879e-01  3.28211300e-01
  3.28496129e-01  3.29135310e-01  3.29726160e-01  3.30192559e-01
  3.33955802e-01  3.39569255e-01  3.40340182e-01  3.41220302e-01
  3.42363645e-01  3.45405152e-01  3.47015863e-01  3.52102259e-01
  3.53883476e-01  3.54112185e-01  3.55651029e-01  3.56076815e-01
  3.59260748e-01  3.60720008e-01  3.60878626e-01  3.62679372e-01
  3.62865799e-01  3.64790794e-01  3.65054422e-01  3.67225841e-01
  3.67709329e-01  3.68547060e-01  3.70889601e-01  3.71235160e-01
  3.72822835e-01  3.72948267e-01  3.78279150e-01  3.81116836e-01
  3.83182599e-01  3.86533218e-01  3.92319014e-01  3.93500952e-01
  3.95810935e-01  3.96484314e-01  3.97800334e-01  3.99712586e-01
  4.00830665e-01  4.01555282e-01  4.02176136e-01  4.03412045e-01
  4.03560211e-01  4.05581825e-01  4.05645234e-01  4.05886903e-01
  4.08174661e-01  4.09063228e-01  4.11614884e-01  4.15675585e-01
  4.16746175e-01  4.16877740e-01  4.18935483e-01  4.19832094e-01
  4.21344981e-01  4.22765367e-01  4.28365472e-01  4.28468383e-01
  4.29070004e-01  4.30957046e-01  4.34426134e-01  4.34801624e-01
  4.36953329e-01  4.37502610e-01  4.38630548e-01  4.38886638e-01
  4.40077151e-01  4.40494529e-01  4.47110453e-01  4.47782274e-01
  4.49100743e-01  4.52154928e-01  4.52976624e-01  4.53226038e-01
  4.55844351e-01  4.55900398e-01  4.58553743e-01  4.59575991e-01
  4.60726428e-01  4.63824397e-01  4.64081750e-01  4.67481095e-01
  4.74954071e-01  4.79180975e-01  4.81097062e-01  4.82182753e-01
  4.82436848e-01  4.83125116e-01  4.84771357e-01  4.87865412e-01
  4.88593489e-01  4.88945050e-01  4.90300419e-01  4.92203269e-01
  4.96293917e-01  4.96711286e-01  4.98616414e-01  4.99202896e-01
  5.00534115e-01  5.01877557e-01  5.03415304e-01  5.07635951e-01
  5.08117992e-01  5.09927175e-01  5.10277890e-01  5.14009217e-01
  5.14856356e-01  5.15529418e-01  5.17128279e-01  5.18926774e-01
  5.21676836e-01  5.23349949e-01  5.24210273e-01  5.24719021e-01
  5.27072811e-01  5.29270402e-01  5.29454778e-01  5.29555710e-01
  5.29986320e-01  5.30952820e-01  5.31748171e-01  5.32716472e-01
  5.33411574e-01  5.33418829e-01  5.35428162e-01  5.41209878e-01
  5.41835609e-01  5.47198486e-01  5.48030342e-01  5.51476416e-01
  5.52107598e-01  5.54393060e-01  5.54561878e-01  5.59259888e-01
  5.64725473e-01  5.64895775e-01  5.74208127e-01  5.74941720e-01
  5.76497442e-01  5.77721314e-01  5.86299935e-01  5.87372260e-01
  5.88078149e-01  5.89483849e-01  5.92350769e-01  5.92693255e-01
  5.93175016e-01  5.94906585e-01  5.97409262e-01  6.02261132e-01
  6.02651612e-01  6.02988262e-01  6.03190021e-01  6.14590807e-01
  6.18052881e-01  6.19551354e-01  6.19935930e-01  6.20634447e-01
  6.22975487e-01  6.25623671e-01  6.26173328e-01  6.26444585e-01
  6.27638656e-01  6.28177573e-01  6.28229711e-01  6.30938548e-01
  6.32527957e-01  6.37082199e-01  6.41447573e-01  6.42862080e-01
  6.50009861e-01  6.51269545e-01  6.51723063e-01  6.58075050e-01
  6.60717942e-01  6.63822374e-01  6.67209940e-01  6.80513386e-01
  6.87751210e-01  6.88374859e-01  6.94661092e-01  7.02351090e-01
  7.06838203e-01  7.11749803e-01  7.13185848e-01  7.13612622e-01
  7.16448531e-01  7.20580983e-01  7.21338357e-01  7.27856171e-01
  7.60407830e-01]

  warnings.warn(

2022-11-03 10:50:11,751:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.26451712e+08 -1.16244480e+08 -1.14819072e+08 -1.11280128e+08
 -9.03741440e+07 -8.57374720e+07 -8.47216640e+07 -8.37222400e+07
 -8.37058560e+07 -8.29030400e+07 -8.21821440e+07 -8.07239680e+07
 -8.06912000e+07 -8.05601280e+07 -7.68409600e+07 -7.62675200e+07
 -7.22862080e+07 -7.13195520e+07 -7.01071360e+07 -6.58636800e+07
 -6.42744320e+07 -6.12106240e+07 -6.03914240e+07 -5.83270400e+07
 -5.82451200e+07 -5.73931520e+07 -5.67050240e+07 -5.64592640e+07
 -5.44112640e+07 -5.21666560e+07 -5.12983040e+07 -5.11180800e+07
 -5.09214720e+07 -5.06101760e+07 -5.02333440e+07 -4.99548160e+07
 -4.93813760e+07 -4.90045440e+07 -4.88407040e+07 -4.81689600e+07
 -4.80378880e+07 -4.79232000e+07 -4.78085120e+07 -4.77757440e+07
 -4.76774400e+07 -4.71040000e+07 -4.62028800e+07 -4.56622080e+07
 -4.52526080e+07 -4.46300160e+07 -4.45644800e+07 -4.43187200e+07
 -4.35978240e+07 -4.33520640e+07 -4.30407680e+07 -4.30243840e+07
 -4.27458560e+07 -4.25000960e+07 -4.20413440e+07 -4.19594240e+07
 -4.19430400e+07 -4.09436160e+07 -4.01735680e+07 -3.99933440e+07
 -3.99769600e+07 -3.98295040e+07 -3.97967360e+07 -3.96001280e+07
 -3.87645440e+07 -3.86007040e+07 -3.77815040e+07 -3.77323520e+07
 -3.75848960e+07 -3.74865920e+07 -3.71752960e+07 -3.65035520e+07
 -3.62250240e+07 -3.61758720e+07 -3.60939520e+07 -3.59956480e+07
 -3.54713600e+07 -3.50617600e+07 -3.45047040e+07 -3.39968000e+07
 -3.38165760e+07 -3.34888960e+07 -3.28663040e+07 -3.25222400e+07
 -3.20634880e+07 -3.20143360e+07 -3.18341120e+07 -3.10476800e+07
 -3.08346880e+07 -3.02776320e+07 -2.99663360e+07 -2.95895040e+07
 -2.88522240e+07 -2.87375360e+07 -2.87211520e+07 -2.82951680e+07
 -2.79838720e+07 -2.78036480e+07 -2.77381120e+07 -2.75742720e+07
 -2.75578880e+07 -2.74104320e+07 -2.73285120e+07 -2.67550720e+07
 -2.59358720e+07 -2.55590400e+07 -2.52641280e+07 -2.49692160e+07
 -2.49036800e+07 -2.48545280e+07 -2.44940800e+07 -2.44776960e+07
 -2.41500160e+07 -2.40025600e+07 -2.37240320e+07 -2.35438080e+07
 -2.33144320e+07 -2.31669760e+07 -2.28065280e+07 -2.24952320e+07
 -2.24460800e+07 -2.22003200e+07 -2.15121920e+07 -2.14302720e+07
 -2.13975040e+07 -2.12664320e+07 -2.10862080e+07 -2.08732160e+07
 -2.02014720e+07 -2.00704000e+07 -1.97263360e+07 -1.96935680e+07
 -1.93658880e+07 -1.93331200e+07 -1.92020480e+07 -1.91528960e+07
 -1.91201280e+07 -1.82845440e+07 -1.82681600e+07 -1.78585600e+07
 -1.77930240e+07 -1.73670400e+07 -1.73342720e+07 -1.73178880e+07
 -1.72523520e+07 -1.71212800e+07 -1.70557440e+07 -1.67280640e+07
 -1.66461440e+07 -1.65969920e+07 -1.64495360e+07 -1.61218560e+07
 -1.60727040e+07 -1.58597120e+07 -1.57450240e+07 -1.56958720e+07
 -1.56139520e+07 -1.54992640e+07 -1.53190400e+07 -1.51388160e+07
 -1.47456000e+07 -1.47128320e+07 -1.46964480e+07 -1.43523840e+07
 -1.35004160e+07 -1.33365760e+07 -1.31727360e+07 -1.29269760e+07
 -1.27303680e+07 -1.26648320e+07 -1.26484480e+07 -1.24846080e+07
 -1.23371520e+07 -1.23043840e+07 -1.21405440e+07 -1.20094720e+07
 -1.17309440e+07 -1.16817920e+07 -1.16490240e+07 -1.12066560e+07
 -1.10919680e+07 -1.08134400e+07 -1.06496000e+07 -1.05185280e+07
 -1.03710720e+07 -9.92870400e+06 -9.78124800e+06 -9.46995200e+06
 -9.33888000e+06 -9.28972800e+06 -9.19142400e+06 -8.86374400e+06
 -8.84736000e+06 -8.56883200e+06 -8.48691200e+06 -8.42137600e+06
 -8.20838400e+06 -8.04454400e+06 -8.01177600e+06 -7.78240000e+06
 -7.63494400e+06 -7.45472000e+06 -7.40556800e+06 -7.38918400e+06
 -7.14342400e+06 -7.07788800e+06 -7.02873600e+06 -6.97958400e+06
 -6.65190400e+06 -6.61913600e+06 -6.12761600e+06 -5.76716800e+06
 -5.65248000e+06 -5.53779200e+06 -5.34118400e+06 -5.27564800e+06
 -5.17734400e+06 -5.06265600e+06 -4.88243200e+06 -4.73497600e+06
 -4.70220800e+06 -4.12876800e+06 -3.83385600e+06 -3.67001600e+06
 -3.62086400e+06 -3.19488000e+06 -2.71974400e+06 -2.63782400e+06
 -2.62144000e+06 -2.53952000e+06 -2.37568000e+06 -2.29376000e+06
 -2.22822400e+06 -1.93331200e+06 -1.65478400e+06 -1.24518400e+06
 -1.22880000e+06 -1.08134400e+06 -8.84736000e+05 -8.19200000e+05
 -6.71744000e+05 -4.91520000e+05 -3.44064000e+05 -3.27680000e+05
 -2.78528000e+05 -1.47456000e+05  2.62144000e+05  7.70048000e+05
  8.19200000e+05  9.83040000e+05  1.17964800e+06  1.21241600e+06
  1.99884800e+06  2.29376000e+06  2.45760000e+06  2.96550400e+06
  3.08019200e+06  3.30956800e+06  3.44064000e+06  3.70278400e+06
  3.73555200e+06  4.01408000e+06  4.25984000e+06  4.53836800e+06
  4.70220800e+06  4.73497600e+06  4.78412800e+06  4.89881600e+06
  5.01350400e+06  5.06265600e+06  5.09542400e+06  5.17734400e+06
  5.37395200e+06  5.53779200e+06  6.12761600e+06  6.14400000e+06
  6.42252800e+06  6.52083200e+06  6.71744000e+06  6.89766400e+06
  6.94681600e+06  7.34003200e+06  7.43833600e+06  7.70048000e+06
  7.79878400e+06  7.81516800e+06  7.96262400e+06  8.40499200e+06
  8.53606400e+06  8.55244800e+06  8.81459200e+06  8.84736000e+06
  8.99481600e+06  9.40441600e+06  9.56825600e+06  9.89593600e+06
  1.02727680e+07  1.05021440e+07  1.06004480e+07  1.14360320e+07
  1.16817920e+07  1.18620160e+07  1.19439360e+07  1.23043840e+07
  1.29105920e+07  1.33365760e+07  1.33857280e+07  1.34840320e+07
  1.43196160e+07  1.43687680e+07  1.44179200e+07  1.46145280e+07
  1.47128320e+07  1.48439040e+07  1.54992640e+07  1.56631040e+07
  1.60235520e+07  1.61382400e+07  1.63020800e+07  1.65478400e+07
  1.73015040e+07  1.73178880e+07  1.73342720e+07  1.76128000e+07
  1.79404800e+07  1.79568640e+07  1.84811520e+07  1.87760640e+07
  1.88252160e+07  1.92839680e+07  1.93003520e+07  1.93986560e+07
  1.95624960e+07  1.95952640e+07  1.96116480e+07  1.96771840e+07
  1.98901760e+07  1.99884800e+07  2.02014720e+07  2.05783040e+07
  2.11517440e+07  2.13319680e+07  2.14630400e+07  2.17088000e+07
  2.17251840e+07  2.18726400e+07  2.19709440e+07  2.29048320e+07
  2.32161280e+07  2.33308160e+07  2.37568000e+07  2.43302400e+07
  2.46251520e+07  2.47070720e+07  2.47726080e+07  2.48053760e+07
  2.49692160e+07  2.49856000e+07  2.53132800e+07  2.56409600e+07
  2.60833280e+07  2.62471680e+07  2.63782400e+07  2.69025280e+07
  2.75087360e+07  2.78036480e+07  2.82460160e+07  2.85736960e+07
  2.87375360e+07  2.90160640e+07  2.92618240e+07  3.03431680e+07
  3.04414720e+07  3.05889280e+07  3.06708480e+07  3.09329920e+07
  3.16538880e+07  3.17685760e+07  3.21126400e+07  3.21617920e+07
  3.23092480e+07  3.30137600e+07  3.33414400e+07  3.35708160e+07
  3.41934080e+07  3.43736320e+07  3.46685440e+07  3.47668480e+07
  3.48160000e+07  3.49962240e+07  3.52747520e+07  3.56515840e+07
  3.61758720e+07  3.63397120e+07  3.72244480e+07  3.72408320e+07
  3.88300800e+07  3.93871360e+07  4.04193280e+07  4.06978560e+07
  4.08453120e+07  4.16808960e+07  4.19102720e+07  4.19921920e+07
  4.21068800e+07  4.26311680e+07  4.31882240e+07  4.45480960e+07
  4.49576960e+07  4.56949760e+07  4.58424320e+07  4.58752000e+07
  4.71531520e+07  4.79395840e+07  4.82181120e+07  4.84638720e+07
  4.87915520e+07  5.04627200e+07  5.06757120e+07  5.07412480e+07
  5.08559360e+07  5.12819200e+07  5.14129920e+07  5.20192000e+07
  5.29367040e+07  5.37231360e+07  5.37722880e+07  5.47717120e+07
  5.48372480e+07  5.69180160e+07  5.77372160e+07  6.00145920e+07
  6.02767360e+07  6.15383040e+07  6.15874560e+07  6.18332160e+07
  6.22100480e+07  6.24885760e+07  6.37009920e+07  6.50936320e+07
  6.62077440e+07  6.62896640e+07  6.70433280e+07  6.87800320e+07
  6.88128000e+07  6.92715520e+07  7.19093760e+07  7.33839360e+07
  7.44652800e+07  7.51370240e+07  7.71850240e+07  7.83482880e+07
  7.98720000e+07  8.02652160e+07  8.32634880e+07  8.35584000e+07
  8.40499200e+07  8.80967680e+07  8.92928000e+07  8.99809280e+07
  9.01611520e+07  9.07018240e+07  9.07673600e+07  9.26515200e+07
  9.43718400e+07  9.57153280e+07  9.78288640e+07  9.83040000e+07
  9.92542720e+07  1.03546880e+08  1.08822528e+08  1.10379008e+08
  1.28647168e+08]

  warnings.warn(

2022-11-03 10:50:11,751:INFO:Calculating mean and std
2022-11-03 10:50:11,751:INFO:Creating metrics dataframe
2022-11-03 10:50:11,767:INFO:Uploading results into container
2022-11-03 10:50:11,767:INFO:Uploading model into container now
2022-11-03 10:50:11,767:INFO:master_model_container: 10
2022-11-03 10:50:11,767:INFO:display_container: 2
2022-11-03 10:50:11,767:INFO:Lars(random_state=4411)
2022-11-03 10:50:11,767:INFO:create_model() successfully completed......................................
2022-11-03 10:50:12,030:ERROR:create_model() for Lars(random_state=4411) raised an exception or returned all 0.0:
2022-11-03 10:50:12,030:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:50:12,030:INFO:Initializing Lasso Least Angle Regression
2022-11-03 10:50:12,030:INFO:Total runtime is 1.602007528146108 minutes
2022-11-03 10:50:12,030:INFO:SubProcess create_model() called ==================================
2022-11-03 10:50:12,030:INFO:Initializing create_model()
2022-11-03 10:50:12,030:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:50:12,030:INFO:Checking exceptions
2022-11-03 10:50:12,047:INFO:Importing libraries
2022-11-03 10:50:12,047:INFO:Copying training dataset
2022-11-03 10:50:12,055:INFO:Defining folds
2022-11-03 10:50:12,055:INFO:Declaring metric variables
2022-11-03 10:50:12,055:INFO:Importing untrained model
2022-11-03 10:50:12,055:INFO:Lasso Least Angle Regression Imported successfully
2022-11-03 10:50:12,055:INFO:Starting cross validation
2022-11-03 10:50:12,063:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:50:14,868:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:14,978:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:14,994:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:15,010:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:15,043:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:15,145:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:15,145:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:15,145:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:16,051:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26031102]

  warnings.warn(

2022-11-03 10:50:16,150:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25986027]

  warnings.warn(

2022-11-03 10:50:16,166:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26031102]

  warnings.warn(

2022-11-03 10:50:16,166:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2625648]

  warnings.warn(

2022-11-03 10:50:16,198:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26324093]

  warnings.warn(

2022-11-03 10:50:16,245:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26076178]

  warnings.warn(

2022-11-03 10:50:16,277:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25918413]

  warnings.warn(

2022-11-03 10:50:16,317:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26121253]

  warnings.warn(

2022-11-03 10:50:18,014:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:18,046:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:18,718:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26143791]

  warnings.warn(

2022-11-03 10:50:18,734:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25963489]

  warnings.warn(

2022-11-03 10:50:18,750:INFO:Calculating mean and std
2022-11-03 10:50:18,750:INFO:Creating metrics dataframe
2022-11-03 10:50:18,750:INFO:Uploading results into container
2022-11-03 10:50:18,750:INFO:Uploading model into container now
2022-11-03 10:50:18,750:INFO:master_model_container: 11
2022-11-03 10:50:18,750:INFO:display_container: 2
2022-11-03 10:50:18,765:INFO:LassoLars(random_state=4411)
2022-11-03 10:50:18,765:INFO:create_model() successfully completed......................................
2022-11-03 10:50:19,023:WARNING:create_model() for LassoLars(random_state=4411) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:50:19,039:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:50:19,039:INFO:Initializing create_model()
2022-11-03 10:50:19,039:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:50:19,039:INFO:Checking exceptions
2022-11-03 10:50:19,039:INFO:Importing libraries
2022-11-03 10:50:19,039:INFO:Copying training dataset
2022-11-03 10:50:19,055:INFO:Defining folds
2022-11-03 10:50:19,055:INFO:Declaring metric variables
2022-11-03 10:50:19,055:INFO:Importing untrained model
2022-11-03 10:50:19,055:INFO:Lasso Least Angle Regression Imported successfully
2022-11-03 10:50:19,055:INFO:Starting cross validation
2022-11-03 10:50:19,063:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:50:21,897:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:21,995:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:22,013:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:22,029:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:22,066:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:22,068:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:22,089:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:22,163:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:23,147:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26031102]

  warnings.warn(

2022-11-03 10:50:23,194:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26031102]

  warnings.warn(

2022-11-03 10:50:23,194:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25986027]

  warnings.warn(

2022-11-03 10:50:23,245:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26076178]

  warnings.warn(

2022-11-03 10:50:23,255:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26121253]

  warnings.warn(

2022-11-03 10:50:23,320:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25918413]

  warnings.warn(

2022-11-03 10:50:23,320:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2625648]

  warnings.warn(

2022-11-03 10:50:25,063:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:25,145:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2022-11-03 10:50:25,812:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25963489]

  warnings.warn(

2022-11-03 10:50:25,922:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26143791]

  warnings.warn(

2022-11-03 10:50:25,922:INFO:Calculating mean and std
2022-11-03 10:50:25,922:INFO:Creating metrics dataframe
2022-11-03 10:50:25,938:INFO:Uploading results into container
2022-11-03 10:50:25,938:INFO:Uploading model into container now
2022-11-03 10:50:25,938:INFO:master_model_container: 12
2022-11-03 10:50:25,938:INFO:display_container: 2
2022-11-03 10:50:25,938:INFO:LassoLars(random_state=4411)
2022-11-03 10:50:25,938:INFO:create_model() successfully completed......................................
2022-11-03 10:50:26,192:ERROR:create_model() for LassoLars(random_state=4411) raised an exception or returned all 0.0:
2022-11-03 10:50:26,208:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:50:26,208:INFO:Initializing Orthogonal Matching Pursuit
2022-11-03 10:50:26,208:INFO:Total runtime is 1.83830193678538 minutes
2022-11-03 10:50:26,208:INFO:SubProcess create_model() called ==================================
2022-11-03 10:50:26,208:INFO:Initializing create_model()
2022-11-03 10:50:26,208:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:50:26,208:INFO:Checking exceptions
2022-11-03 10:50:26,208:INFO:Importing libraries
2022-11-03 10:50:26,208:INFO:Copying training dataset
2022-11-03 10:50:26,223:INFO:Defining folds
2022-11-03 10:50:26,223:INFO:Declaring metric variables
2022-11-03 10:50:26,223:INFO:Importing untrained model
2022-11-03 10:50:26,223:INFO:Orthogonal Matching Pursuit Imported successfully
2022-11-03 10:50:26,223:INFO:Starting cross validation
2022-11-03 10:50:26,223:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:50:28,978:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:28,978:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:29,059:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:29,115:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:29,182:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:29,213:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:29,264:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:29,330:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:30,162:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.10715742 -0.10296704 -0.09877666 -0.09458628 -0.0903959  -0.08620553
 -0.08201515 -0.07782477 -0.07363439 -0.06944402 -0.06525364 -0.06106326
 -0.05687288 -0.0526825  -0.04849213 -0.04430175 -0.04011137 -0.03592099
 -0.03173061 -0.02754024 -0.02334986 -0.0149691  -0.00658835 -0.00239797
  0.00179241  0.00598279  0.01270985  0.01436354  0.01690023  0.01855392
  0.0227443   0.02693468  0.02947136  0.03112505  0.03704332  0.03950581
  0.04623288  0.04788656  0.05207694  0.05626732  0.0604577   0.06299439
  0.06464808  0.06718477  0.06883845  0.07302883  0.07721921  0.08559996
  0.08979034  0.09151823  0.09398072  0.09651741  0.0981711   0.10236148
  0.10313004  0.10489817  0.10655185  0.10732042  0.10827974  0.11151079
  0.11493261  0.11570117  0.1166605   0.11912299  0.12331337  0.12408193
  0.1282723   0.12923163  0.13169412  0.13423081  0.1358845   0.13665306
  0.14007488  0.14084344  0.14426525  0.14503382  0.14599314  0.14680194
  0.14922419  0.15264601  0.15341457  0.15683639  0.16102677  0.16110096
  0.16179533  0.16694503  0.16940752  0.17855684  0.18197866  0.18370654
  0.18693759  0.18789692  0.19035941  0.1920873   0.20046805  0.20369911
  0.20465843  0.21303919  0.21384799  0.21722956  0.21976625  0.22141994
  0.22299731  0.22465099  0.22561032  0.22718768  0.2298007   0.23137806
  0.23303175  0.23475964  0.23479988  0.23722213  0.23818145  0.24237183
  0.24656221  0.24733077  0.24813957  0.24979326  0.25075259  0.25232995
  0.25494296  0.25652033  0.25913334  0.2599019   0.26167003  0.2675141
  0.26909146  0.27074515  0.27170448  0.27247304  0.27328184  0.27493553
  0.27589485  0.27666341  0.27747222  0.28089404  0.28262192  0.28331628
  0.28427561  0.28585297  0.28750666  0.28846599  0.29004335  0.29265636
  0.29342493  0.29346517  0.29684674  0.2976153   0.30103712  0.30261449
  0.30426817  0.3052275   0.30599606  0.30845855  0.30941788  0.31018644
  0.31099524  0.31360825  0.31518562  0.31614494  0.31779863  0.32356637
  0.32694795  0.3353287   0.33951908  0.34789984  0.34870864  0.35881728
  0.36300766  0.36642948  0.36719804  0.36885173  0.3730421   0.3755046
  0.37723248  0.37900061  0.38142286  0.38319099  0.38738137  0.38980362
  0.39157174  0.39653068  0.39818437  0.3999525   0.40737393  0.40833326
  0.41494588  0.41575469  0.41671401  0.41913626  0.41994507  0.42090439
  0.42509477  0.42751702  0.43347552  0.43424408  0.4376659   0.43843446
  0.44008815  0.44508733  0.44846891  0.4510056   0.45684966  0.46104004
  0.4694208   0.47614786  0.47780155  0.48199193  0.48618231  0.49290937
  0.49456306  0.49709975  0.49875344  0.50129013  0.50294382  0.5071342
  0.50967089  0.51551495  0.51805164  0.51970533  0.52224202  0.52389571
  0.5264324   0.52808609  0.53481315  0.53900353  0.54319391  0.54738429
  0.55157466  0.56833618  0.57252655  0.57671693  0.58090731  0.58509769
  0.58928806  0.59347844  0.59766882  0.6018592   0.60604958  0.61443033
  0.62700147  0.63119184  0.63538222  0.6395726   0.64376298  0.64795335]

  warnings.warn(

2022-11-03 10:50:30,260:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.10089816 -0.09690002 -0.09290188 -0.08890374 -0.0849056  -0.08090746
 -0.07690933 -0.07291119 -0.06891305 -0.06091677 -0.05691863 -0.05292049
 -0.04892236 -0.04492422 -0.03692794 -0.0329298  -0.02493352 -0.02093539
 -0.01693725 -0.01293911 -0.00894097 -0.00494283 -0.00094469  0.00305345
  0.01104972  0.01504786  0.019046    0.03201568  0.03503855  0.03903669
  0.04303483  0.04401009  0.04703297  0.04800823  0.05103111  0.05502925
  0.05902739  0.06302552  0.06702366  0.0710218   0.07501994  0.07901808
  0.08301622  0.08701436  0.08800486  0.09101249  0.09198776  0.09501063
  0.09900877  0.10382101  0.10700505  0.10781915  0.10798031  0.11181729
  0.11197845  0.11199369  0.11500133  0.11581543  0.11597659  0.11899947
  0.11981356  0.1229976   0.1238117   0.12699574  0.12780984  0.12798625
  0.13099388  0.13180798  0.13198439  0.13499202  0.13580612  0.13596728
  0.13899016  0.13980426  0.14396356  0.14698644  0.15179867  0.15195983
  0.15197508  0.15898085  0.15979495  0.16779123  0.17195053  0.18378378
  0.18696782  0.19977634  0.19995274  0.20395088  0.20794902  0.20892428
  0.21177075  0.21193191  0.21194716  0.21976703  0.21992819  0.22873857
  0.23193785  0.23273671  0.23593599  0.23673485  0.23975772  0.24391702
  0.24393227  0.24473112  0.247754    0.24793041  0.25192854  0.2527274
  0.25672554  0.25674078  0.25990958  0.25992482  0.26072368  0.26090008
  0.26374655  0.26390772  0.26774469  0.2679211   0.26871995  0.2687352
  0.27174283  0.27191924  0.27271809  0.27591738  0.27671623  0.27673148
  0.27973911  0.27991551  0.28072961  0.28389841  0.28791179  0.29190993
  0.29270879  0.29272403  0.29590807  0.29672217  0.29688333  0.29990621
  0.30390435  0.3047032   0.30487961  0.30790248  0.30887775  0.31190062
  0.31271472  0.31589876  0.31669762  0.32470914  0.3248703   0.32870728
  0.33269017  0.33286658  0.33670355  0.33971119  0.34070169  0.34086285
  0.34469983  0.34868273  0.34885913  0.35268086  0.35269611  0.35669425
  0.35685541  0.36069239  0.36868866  0.36884982  0.3726868   0.3766697
  0.38068308  0.38084424  0.38466597  0.38468122  0.38484238  0.38565648
  0.38884052  0.39283866  0.39365276  0.39667563  0.40067377  0.40083493
  0.4086548   0.40883121  0.40964531  0.41282935  0.41665108  0.41682749
  0.41764159  0.42066447  0.42482376  0.42563786  0.42866074  0.4288219
  0.429636    0.43265888  0.43282004  0.43665702  0.43681818  0.44065516
  0.44081632  0.45264957  0.46064585  0.46464399  0.46561925  0.46864213
  0.47264027  0.47663841  0.47761367  0.48063654  0.48161181  0.48463468
  0.48863282  0.48960808  0.49360622  0.4966291   0.50062724  0.5016025
  0.50462538  0.51759505  0.52061793  0.52159319  0.52559133  0.52958947
  0.53358761  0.54558202  0.5535783   0.56157458  0.56557272  0.56957085
  0.57756713  0.58156527  0.58556341  0.58956155  0.59355969  0.59755782
  0.60155596  0.60955224  0.61355038  0.61754852  0.62154666  0.62554479
  0.62954293  0.63354107  0.63753921  0.64153735  0.64553549]

  warnings.warn(

2022-11-03 10:50:30,330:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.07793377e-01 -1.03660050e-01 -9.95267225e-02 -9.53933953e-02
 -9.12600680e-02 -8.71267408e-02 -8.29934136e-02 -7.88600864e-02
 -7.47267592e-02 -7.05934320e-02 -6.64601048e-02 -6.23267776e-02
 -5.81934503e-02 -5.40601231e-02 -4.99267959e-02 -3.75268143e-02
 -3.33934871e-02 -2.92601599e-02 -2.51268326e-02 -2.09935054e-02
 -1.68601782e-02 -1.27268510e-02 -8.59352379e-03 -4.46019657e-03
 -3.26869361e-04  7.93978506e-03  1.20731123e-02  1.49920164e-02
  1.91253436e-02  2.03397667e-02  2.44730939e-02  2.86064211e-02
  3.56586525e-02  3.68730756e-02  4.51397300e-02  4.80586341e-02
  4.92730572e-02  5.21919613e-02  5.34063844e-02  5.75397116e-02
  6.16730388e-02  6.58063660e-02  6.99396933e-02  7.69919246e-02
  7.82063477e-02  8.52585790e-02  8.83383528e-02  9.06063293e-02
  9.24716800e-02  9.47396565e-02  9.73612391e-02  9.88729837e-02
  1.00738334e-01  1.01494566e-01  1.03006311e-01  1.05627894e-01
  1.07139638e-01  1.09761221e-01  1.10058542e-01  1.11272965e-01
  1.13894548e-01  1.14191870e-01  1.15406293e-01  1.18027875e-01
  1.19539620e-01  1.21404971e-01  1.22161202e-01  1.23672947e-01
  1.26591851e-01  1.27806274e-01  1.30725178e-01  1.31939601e-01
  1.34561184e-01  1.34858506e-01  1.36072929e-01  1.38694511e-01
  1.42071607e-01  1.42827838e-01  1.44339583e-01  1.52606238e-01
  1.54471588e-01  1.56739565e-01  1.60872892e-01  1.63494475e-01
  1.65006219e-01  1.69139546e-01  1.71761129e-01  1.73272874e-01
  1.79271551e-01  1.81539528e-01  1.83404879e-01  1.87538206e-01
  1.89806182e-01  1.92427765e-01  1.94590437e-01  1.95804860e-01
  1.96561092e-01  1.96858414e-01  2.00694419e-01  2.08204842e-01
  2.11123746e-01  2.20146632e-01  2.20604824e-01  2.24279960e-01
  2.24738151e-01  2.28413287e-01  2.29627710e-01  2.31790382e-01
  2.33004805e-01  2.41271460e-01  2.44946596e-01  2.52159697e-01
  2.53213250e-01  2.54724995e-01  2.57804769e-01  2.61479905e-01
  2.61938096e-01  2.66071423e-01  2.69746559e-01  2.70204750e-01
  2.73879886e-01  2.74338077e-01  2.77256982e-01  2.78471405e-01
  2.82604732e-01  2.83658285e-01  2.86738059e-01  2.90871386e-01
  2.94546522e-01  2.98679849e-01  2.99138041e-01  3.03271368e-01
  3.06190272e-01  3.07404695e-01  3.10323599e-01  3.11079831e-01
  3.11538022e-01  3.15671350e-01  3.19804677e-01  3.22723581e-01
  3.30692914e-01  3.40013122e-01  3.45360872e-01  3.47226223e-01
  3.47523544e-01  3.51359550e-01  3.51656871e-01  3.55790199e-01
  3.59626204e-01  3.59923526e-01  3.63759531e-01  3.73079739e-01
  3.80292840e-01  3.80590162e-01  3.81346394e-01  3.84426168e-01
  3.84723489e-01  3.88559495e-01  3.88856816e-01  3.89613048e-01
  3.92990144e-01  3.96826149e-01  3.99745053e-01  4.00959476e-01
  4.01256798e-01  4.05092804e-01  4.08011708e-01  4.09523452e-01
  4.10279684e-01  4.13656780e-01  4.17492785e-01  4.20411689e-01
  4.28678344e-01  4.30190088e-01  4.34026094e-01  4.34323416e-01
  4.38159421e-01  4.38456743e-01  4.41078325e-01  4.42590070e-01
  4.46426076e-01  4.54692730e-01  4.57611634e-01  4.58826057e-01
  4.62959385e-01  4.65878289e-01  4.67092712e-01  4.70011616e-01
  4.78278270e-01  4.79492693e-01  4.82411598e-01  4.86544925e-01
  4.87759348e-01  4.90678252e-01  4.98944906e-01  5.04292657e-01
  5.07211561e-01  5.08425984e-01  5.11344888e-01  5.12559311e-01
  5.19611542e-01  5.20825966e-01  5.23744870e-01  5.24959293e-01
  5.27878197e-01  5.36144851e-01  5.44411506e-01  5.48544833e-01
  5.52678160e-01  5.56811487e-01  5.60944815e-01  5.69211469e-01
  5.73344796e-01  5.77478123e-01  5.81611451e-01  5.85744778e-01
  5.89878105e-01  5.94011432e-01  6.10544741e-01  6.14678068e-01
  6.18811396e-01  6.22944723e-01  6.27078050e-01  6.31211377e-01
  6.35344704e-01  6.39478032e-01  6.43611359e-01  6.47744686e-01]

  warnings.warn(

2022-11-03 10:50:30,345:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.10130613 -0.09736673 -0.09342733 -0.08948794 -0.08554854 -0.08160914
 -0.07766974 -0.06979094 -0.06585154 -0.06191214 -0.05797274 -0.05403334
 -0.05009394 -0.04615454 -0.04221514 -0.03827574 -0.03039694 -0.02645754
 -0.02251814 -0.01857874 -0.01463934 -0.01069994 -0.00282114  0.00111826
  0.00505766  0.00899706  0.01293646  0.01586188  0.01980128  0.02475465
  0.02869405  0.03161948  0.03263345  0.03555888  0.03657285  0.04445165
  0.04839105  0.05233045  0.05919528  0.06020925  0.06414865  0.06808805
  0.07101348  0.07596685  0.08283167  0.08384565  0.08677107  0.09172445
  0.09566385  0.10252867  0.10347977  0.10354265  0.10748205  0.11040747
  0.11142145  0.11529797  0.11536085  0.11923737  0.12317677  0.12711617
  0.12717904  0.13010447  0.13105556  0.13404387  0.13505784  0.13798327
  0.13899724  0.1392213   0.14287376  0.14293664  0.14586207  0.15075256
  0.15081544  0.1510395   0.15469196  0.1589183   0.16257076  0.16263364
  0.16651016  0.16657304  0.17438896  0.17467589  0.17832836  0.17839124
  0.18255469  0.18525606  0.19014656  0.19043349  0.19408596  0.19831229
  0.20101366  0.20196476  0.20225169  0.20619109  0.21406989  0.21772236
  0.22064778  0.22194869  0.22458718  0.22588809  0.22852658  0.22982749
  0.23347996  0.23376689  0.23770629  0.24034478  0.24164569  0.24529815
  0.24822358  0.24923755  0.24952449  0.25216298  0.25317695  0.25340101
  0.25346389  0.25740329  0.26032871  0.26134269  0.26426811  0.26528209
  0.26922149  0.27185998  0.27309801  0.27316089  0.27681335  0.27710028
  0.27973878  0.28075275  0.28103968  0.28469215  0.28790451  0.28891848
  0.29285788  0.29679728  0.29972271  0.30073668  0.30337517  0.3046132
  0.30467608  0.3085526   0.31226795  0.31255488  0.3164314   0.32043368
  0.32307217  0.32335911  0.32437308  0.3282496   0.33095097  0.33123791
  0.3430561   0.3479466   0.351886    0.35852677  0.3597648   0.3666925
  0.37034497  0.371583    0.37428437  0.3755224   0.3785107   0.3834012
  0.3863895   0.3873406   0.39128     0.39420542  0.3942683   0.39814482
  0.3982077   0.39915879  0.40309819  0.40602362  0.4060865   0.40703759
  0.4100259   0.41097699  0.41390242  0.4139653   0.41491639  0.41784182
  0.4179047   0.41885579  0.4218441   0.4257835   0.42673459  0.42966002
  0.43067399  0.4336623   0.43461339  0.4376017   0.43855279  0.44154109
  0.44249219  0.46117521  0.46218919  0.46511461  0.46612859  0.46905401
  0.47006799  0.47400739  0.47693281  0.47794679  0.48087221  0.48188619
  0.48481161  0.48582559  0.48852696  0.48875101  0.48976499  0.49370439
  0.49662981  0.49764378  0.50056921  0.50450861  0.50552258  0.50844801
  0.50946198  0.51238741  0.51340138  0.51632681  0.52026621  0.52128018
  0.52420561  0.52521958  0.52814501  0.52915898  0.53208441  0.53602381
  0.53996321  0.54784201  0.55178141  0.55572081  0.5832966   0.5911754
  0.5951148   0.5990542   0.6029936   0.606933    0.6108724   0.6148118
  0.6187512   0.6226906   0.62663     0.6305694   0.6345088   0.6384482
  0.6423876   0.646327  ]

  warnings.warn(

2022-11-03 10:50:30,375:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.104898   -0.10074519 -0.09659238 -0.09243957 -0.08828675 -0.08413394
 -0.07998113 -0.07582832 -0.07167551 -0.06752269 -0.06336988 -0.05921707
 -0.05506426 -0.05091145 -0.04675863 -0.04260582 -0.03845301 -0.02184176
 -0.01768895 -0.00938333 -0.00523051  0.00307511  0.00722792  0.01138073
  0.01391203  0.01806484  0.01968636  0.02383917  0.02799198  0.03052328
  0.03467609  0.03629761  0.04045042  0.04713452  0.04875604  0.05128734
  0.05290885  0.05544015  0.05706167  0.06121448  0.06536729  0.0695201
  0.0720514   0.07782573  0.07809893  0.08197854  0.08613135  0.09028416
  0.09055737  0.09443697  0.09696827  0.09726529  0.09858979  0.10141811
  0.1027426   0.10689541  0.10972373  0.11104822  0.11520103  0.11547424
  0.11802935  0.11962705  0.12218217  0.12350666  0.12377986  0.12633498
  0.12765947  0.13048779  0.13181228  0.13434358  0.1346406   0.13849639
  0.13879341  0.14427072  0.14454392  0.14869673  0.15095482  0.15257634
  0.15672915  0.16341326  0.16503478  0.16530798  0.1678631   0.16918759
  0.17201591  0.17749321  0.18032153  0.18164603  0.18191923  0.18579884
  0.18995165  0.19248294  0.19277997  0.19437767  0.19690896  0.20106177
  0.20523841  0.20939122  0.213247    0.21354403  0.21514173  0.21607532
  0.22022814  0.22438095  0.22760016  0.22853376  0.23015528  0.23175297
  0.23428427  0.23430809  0.23590579  0.23683938  0.2384609   0.2400586
  0.24288692  0.24514501  0.24836422  0.25119254  0.25251703  0.25534535
  0.25666985  0.25949817  0.26082266  0.26365098  0.26497547  0.26590907
  0.26780379  0.26912828  0.27328109  0.27421469  0.27743391  0.2799652
  0.27998902  0.28158672  0.28252032  0.28573953  0.28667313  0.28856785
  0.28989234  0.29082594  0.29244746  0.29272066  0.29404515  0.29819797
  0.30072926  0.30328438  0.30650359  0.30905871  0.3106564   0.31348472
  0.31480921  0.31763753  0.31896203  0.32311484  0.32564613  0.33009597
  0.33235406  0.33840159  0.34255441  0.34670722  0.35056301  0.35471582
  0.37132707  0.37162409  0.3757769   0.3783082   0.37963269  0.37992971
  0.38793831  0.38823534  0.39238815  0.39654096  0.39907226  0.40322507
  0.40454956  0.40484659  0.4089994   0.41153069  0.41285519  0.417008
  0.41983632  0.42145783  0.42531362  0.42561065  0.42814194  0.42946643
  0.43361925  0.43391627  0.43644757  0.43777206  0.43806908  0.44060038
  0.44192487  0.44222189  0.44637471  0.45052752  0.45883314  0.46136444
  0.46298595  0.46713877  0.46967006  0.47354967  0.47382287  0.47544439
  0.47797569  0.4795972   0.48628131  0.48790283  0.49043412  0.49205564
  0.49458693  0.49620845  0.50289256  0.50704537  0.50866689  0.5128197
  0.51535099  0.51697251  0.51950381  0.52112532  0.52365662  0.52527813
  0.53196224  0.53611505  0.54442068  0.54857349  0.5527263   0.55687911
  0.56103192  0.56518474  0.56933755  0.57349036  0.58179598  0.59010161
  0.59425442  0.59840723  0.60671286  0.61086567  0.61501848  0.61917129
  0.6233241   0.62747692  0.63162973  0.63578254  0.63993535  0.64408816]

  warnings.warn(

2022-11-03 10:50:30,386:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.07103654e-01 -1.02990651e-01 -9.88776471e-02 -9.47646434e-02
 -9.06516397e-02 -8.65386360e-02 -8.24256323e-02 -7.41996249e-02
 -7.00866213e-02 -6.59736176e-02 -6.18606139e-02 -5.77476102e-02
 -5.36346065e-02 -4.95216028e-02 -4.54085991e-02 -4.12955954e-02
 -3.71825918e-02 -2.89565844e-02 -2.48435807e-02 -2.07305770e-02
 -1.66175733e-02 -1.25045696e-02 -8.39156595e-03 -4.27856226e-03
 -1.65558571e-04  1.17029510e-02  1.21734525e-02  1.58159547e-02
  1.62864562e-02  1.99289584e-02  2.03994599e-02  2.45124636e-02
  2.86254672e-02  3.27384709e-02  3.68514746e-02  4.09644783e-02
  4.87199842e-02  5.33034894e-02  5.74164931e-02  5.96948089e-02
  6.15294967e-02  6.92850026e-02  6.97555041e-02  7.38685078e-02
  7.75110100e-02  7.79815115e-02  8.02598273e-02  8.20945152e-02
  8.43728310e-02  8.62075189e-02  9.03205226e-02  9.44335262e-02
  9.85465299e-02  1.02659534e-01  1.02752697e-01  1.06772537e-01
  1.06865701e-01  1.09050853e-01  1.10885541e-01  1.10978704e-01
  1.15091708e-01  1.19111548e-01  1.19204712e-01  1.23224552e-01
  1.27337556e-01  1.27430719e-01  1.29615872e-01  1.31450559e-01
  1.35563563e-01  1.35656726e-01  1.39206065e-01  1.39676567e-01
  1.43789570e-01  1.43882734e-01  1.46067886e-01  1.52108741e-01
  1.54293894e-01  1.56128582e-01  1.56221745e-01  1.58406897e-01
  1.60334749e-01  1.62519901e-01  1.64447752e-01  1.70745908e-01
  1.74858912e-01  1.76693600e-01  1.80899767e-01  1.83084920e-01
  1.85012771e-01  1.89032611e-01  1.89125774e-01  1.93238778e-01
  1.95423931e-01  2.01464785e-01  2.03649938e-01  2.07292440e-01
  2.09690793e-01  2.11405444e-01  2.15988949e-01  2.21559302e-01
  2.22029804e-01  2.23744455e-01  2.24214956e-01  2.32440964e-01
  2.33898313e-01  2.38011317e-01  2.42124321e-01  2.42594822e-01
  2.44779975e-01  2.44873138e-01  2.46237325e-01  2.46707826e-01
  2.48892979e-01  2.50350328e-01  2.53005982e-01  2.53099146e-01
  2.54933833e-01  2.58483172e-01  2.61231990e-01  2.62596176e-01
  2.65344993e-01  2.65438157e-01  2.69457997e-01  2.69551160e-01
  2.73571001e-01  2.73664164e-01  2.75028350e-01  2.77684004e-01
  2.81797008e-01  2.85910012e-01  2.87367361e-01  2.87837863e-01
  2.90023015e-01  2.94136019e-01  2.95593369e-01  2.98249023e-01
  3.01891525e-01  3.02362026e-01  3.04289878e-01  3.06475030e-01
  3.06568194e-01  3.10588034e-01  3.14701038e-01  3.18814041e-01
  3.18907205e-01  3.22927045e-01  3.24384395e-01  3.27133212e-01
  3.31246216e-01  3.34795554e-01  3.39472223e-01  3.43021562e-01
  3.47698230e-01  3.51811234e-01  3.55924238e-01  3.59473577e-01
  3.61401428e-01  3.67699584e-01  3.71812588e-01  3.72376253e-01
  3.73740439e-01  3.76018755e-01  3.76489256e-01  3.80038595e-01
  3.86079450e-01  3.92377606e-01  3.92470769e-01  3.92941271e-01
  3.96490610e-01  3.96583773e-01  3.97054275e-01  4.01167278e-01
  4.04809781e-01  4.05280282e-01  4.12942625e-01  4.17148792e-01
  4.17619293e-01  4.21732297e-01  4.25374799e-01  4.25845300e-01
  4.29394639e-01  4.29487803e-01  4.29958304e-01  4.33507643e-01
  4.33600806e-01  4.37620647e-01  4.41733650e-01  4.45939817e-01
  4.46410319e-01  4.50052821e-01  4.50523323e-01  4.54165825e-01
  4.54636326e-01  4.62391832e-01  4.62862334e-01  4.66504836e-01
  4.66975337e-01  4.71088341e-01  4.79314348e-01  4.83427352e-01
  4.87069854e-01  4.87540356e-01  4.91182858e-01  4.95295862e-01
  4.99879367e-01  5.03521869e-01  5.07634873e-01  5.08105374e-01
  5.11747876e-01  5.16331382e-01  5.19973884e-01  5.24086887e-01
  5.24557389e-01  5.32312895e-01  5.32783396e-01  5.48764910e-01
  5.52877913e-01  5.56990917e-01  5.61103921e-01  5.69329928e-01
  5.73442932e-01  5.77555935e-01  5.81668939e-01  5.85781943e-01
  5.89894946e-01  5.94007950e-01  5.98120954e-01  6.02233958e-01
  6.06346961e-01  6.10459965e-01  6.14572969e-01  6.18685972e-01
  6.22798976e-01  6.26911980e-01  6.35137987e-01  6.39250991e-01
  6.43363994e-01  6.47476998e-01  6.51590002e-01]

  warnings.warn(

2022-11-03 10:50:30,515:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.10126501 -0.09729867 -0.09333233 -0.08539965 -0.08143331 -0.07746697
 -0.07350063 -0.06953429 -0.06556795 -0.06160161 -0.05763527 -0.05366893
 -0.04970259 -0.04573625 -0.04176991 -0.03780357 -0.03383723 -0.02987089
 -0.02590455 -0.02193821 -0.01797187 -0.01400553 -0.00607285 -0.00210651
  0.00185983  0.00582617  0.01375885  0.01772519  0.02169153  0.02962421
  0.03359055  0.03755689  0.04152323  0.04460687  0.04548957  0.04945591
  0.05342225  0.05738859  0.06443857  0.06532126  0.0692876   0.07633759
  0.07722028  0.08118662  0.08515296  0.09308564  0.09616929  0.09705198
  0.10013563  0.10101832  0.10328671  0.10348425  0.10745059  0.108951
  0.11141693  0.11291734  0.11518573  0.11538327  0.11688368  0.11915207
  0.11934961  0.12085002  0.12331595  0.12481636  0.12708475  0.12728229
  0.1287827   0.13124863  0.13274904  0.13671538  0.13918131  0.13979902
  0.14068172  0.14376536  0.14464806  0.14711399  0.1486144   0.15088278
  0.15484912  0.15504667  0.15654708  0.15881546  0.15901301  0.15963072
  0.1627818   0.16694569  0.1684461   0.17549608  0.17637878  0.17884471
  0.18034512  0.1826135   0.18342876  0.18431146  0.18677739  0.19054618
  0.19074373  0.19532778  0.19867641  0.19929412  0.20949518  0.21057543
  0.21434422  0.21831056  0.22247445  0.22309216  0.22536054  0.22555809
  0.22644079  0.2270585   0.23020958  0.23040713  0.23349077  0.23437347
  0.23725956  0.24538979  0.24915858  0.25004128  0.25400762  0.25647355
  0.25728881  0.25797396  0.25878922  0.2619403   0.26440623  0.26522149
  0.26590664  0.26918783  0.27007052  0.27383932  0.28027159  0.28108685
  0.281772    0.28505319  0.28573834  0.28882198  0.28901953  0.28970468
  0.29217061  0.29367102  0.29763736  0.30010329  0.30091855  0.3016037
  0.30180124  0.30406963  0.30557004  0.30953638  0.31200231  0.31281757
  0.31350272  0.31746906  0.32868292  0.33641806  0.3403844   0.34058194
  0.34146464  0.34435074  0.34769936  0.34831708  0.35563204  0.35624976
  0.3564473   0.37546374  0.37943008  0.3800478   0.38339642  0.38401414
  0.39194681  0.39529544  0.39591315  0.39926178  0.39987949  0.40384583
  0.40719446  0.40781217  0.4111608   0.41177851  0.41197606  0.41424444
  0.41512714  0.41574485  0.41909348  0.41971119  0.42217712  0.42305982
  0.42367753  0.42614346  0.42702616  0.42764387  0.4301098   0.4309925
  0.43161021  0.43407614  0.43557655  0.43804248  0.43954289  0.44200882
  0.44370678  0.44685786  0.4499415   0.4508242   0.45390784  0.45479054
  0.45875688  0.46184052  0.46272322  0.46668956  0.4706559   0.47462224
  0.47858858  0.48255492  0.48563856  0.48652126  0.49445394  0.50150392
  0.50635296  0.5094366   0.5103193   0.51340294  0.51428564  0.51736928
  0.51825198  0.52133562  0.52221832  0.52530196  0.5292683   0.53720098
  0.54116732  0.54513366  0.5491      0.55306634  0.55703268  0.56099902
  0.56496536  0.57686438  0.58083072  0.58479706  0.5887634   0.59272974
  0.59669607  0.60066241  0.60462875  0.61256143  0.61652777  0.62049411
  0.62446045  0.62842679  0.63239313  0.63635947  0.64032581  0.64429215]

  warnings.warn(

2022-11-03 10:50:30,577:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.10793065 -0.10380839 -0.09968613 -0.09556388 -0.09144162 -0.08731936
 -0.08319711 -0.07907485 -0.07495259 -0.07083034 -0.06670808 -0.05434131
 -0.05021905 -0.0460968  -0.03785229 -0.03373003 -0.02960777 -0.02548552
 -0.02136326 -0.017241   -0.01311875 -0.00899649 -0.00487423 -0.00075198
  0.00337028  0.00749254  0.01161479  0.01223495  0.01635721  0.01985931
  0.02398156  0.02810382  0.03222607  0.03634833  0.04047059  0.04459284
  0.0487151   0.05283736  0.05345752  0.06108187  0.06170203  0.06406401
  0.06520413  0.06932638  0.07344864  0.08169315  0.08581541  0.08643557
  0.08993767  0.09055783  0.09291981  0.09405992  0.09818218  0.10116432
  0.10230443  0.10349808  0.10642669  0.10762034  0.1117426   0.11529137
  0.11586485  0.11879346  0.11998711  0.12410937  0.13116023  0.13528249
  0.13590265  0.13647614  0.13940474  0.14059839  0.143527    0.14472065
  0.14650914  0.14764926  0.15177151  0.15296516  0.15589377  0.16001603
  0.16120967  0.16413828  0.16712042  0.16945419  0.17536494  0.17948719
  0.18182096  0.18360945  0.18536973  0.18594321  0.18887182  0.18949198
  0.19006547  0.19185396  0.19418773  0.19597622  0.20243224  0.20422073
  0.20834299  0.21010326  0.21067675  0.21246525  0.21479901  0.22070976
  0.22304352  0.22366368  0.22483201  0.22545218  0.22716578  0.23307653
  0.23541029  0.24132104  0.24427497  0.2454433   0.24956555  0.25251948
  0.25602157  0.25781007  0.26076399  0.26193232  0.26312597  0.26488625
  0.26605458  0.26667474  0.27137048  0.27193711  0.2725106   0.27429909
  0.27491925  0.27725302  0.27842135  0.279615    0.28018162  0.28254361
  0.28373725  0.28487737  0.28666586  0.28961979  0.29078812  0.29198177
  0.29491037  0.29553054  0.29610402  0.29667065  0.29903263  0.30198656
  0.30315489  0.30377505  0.30434854  0.30548865  0.30610881  0.30727714
  0.30789731  0.30847079  0.3113994   0.31552166  0.31614182  0.31671531
  0.32026407  0.32259784  0.32495982  0.32850859  0.32908208  0.33320433
  0.33732659  0.34144884  0.34499761  0.35381561  0.35557589  0.35736438
  0.35793787  0.35969815  0.36148664  0.3656089   0.36794266  0.3709248
  0.37385341  0.37618717  0.37854915  0.38329157  0.38443169  0.38679367
  0.39153608  0.39446469  0.39503818  0.39916044  0.3997806   0.4027092
  0.40740495  0.4115272   0.41214737  0.41564946  0.41919823  0.41977172
  0.42039188  0.42332049  0.42389397  0.42451414  0.42744274  0.42801623
  0.42863639  0.43213849  0.43275865  0.43389876  0.43568726  0.440383
  0.44100316  0.45274977  0.45336993  0.45687203  0.45749219  0.46161444
  0.46511654  0.4692388   0.47336105  0.47810347  0.48336584  0.48572782
  0.48634798  0.48985008  0.49397233  0.49809459  0.50221685  0.50283701
  0.5063391   0.50695927  0.51046136  0.51108152  0.51458362  0.51520378
  0.51870587  0.51932603  0.52282813  0.52344829  0.52695039  0.52757055
  0.5316928   0.53581506  0.53993732  0.54405957  0.55230409  0.55642634
  0.5605486   0.56879311  0.57291537  0.57703763  0.58115988  0.58528214
  0.58940439  0.59352665  0.59764891  0.60177116  0.60589342  0.61001568
  0.61413793  0.61826019  0.62238245  0.6265047   0.63474922  0.63887147
  0.64299373  0.64711599]

  warnings.warn(

2022-11-03 10:50:32,008:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:32,098:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:32,843:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.04070850e-01 -1.00085122e-01 -9.60993939e-02 -9.21136658e-02
 -8.81279376e-02 -8.41422095e-02 -8.01564814e-02 -7.61707532e-02
 -7.21850251e-02 -6.81992969e-02 -6.42135688e-02 -6.02278406e-02
 -5.62421125e-02 -5.22563843e-02 -4.82706562e-02 -4.02991999e-02
 -3.63134718e-02 -3.23277436e-02 -2.83420155e-02 -2.43562873e-02
 -2.03705592e-02 -8.41337477e-03 -4.42764662e-03 -4.41918481e-04
  3.54380966e-03  7.52953781e-03  1.15152660e-02  1.94867222e-02
  2.34724504e-02  2.39290009e-02  2.74581785e-02  2.79147290e-02
  3.14439067e-02  3.19004571e-02  3.54296348e-02  3.94153630e-02
  4.34010911e-02  4.46488820e-02  4.73868192e-02  5.53582755e-02
  6.33297318e-02  7.13011881e-02  7.17577386e-02  7.52869163e-02
  7.92726444e-02  8.32583725e-02  8.72441007e-02  9.12298288e-02
  9.52155570e-02  9.56721074e-02  9.92012851e-02  1.02742169e-01
  1.03643564e-01  1.04434804e-01  1.06727897e-01  1.07172741e-01
  1.10713625e-01  1.11158470e-01  1.14699354e-01  1.15600748e-01
  1.18685082e-01  1.19129926e-01  1.22670810e-01  1.23115654e-01
  1.26656538e-01  1.27101382e-01  1.27557933e-01  1.30642266e-01
  1.31087110e-01  1.34627994e-01  1.35072838e-01  1.35529389e-01
  1.39058567e-01  1.42599451e-01  1.43044295e-01  1.44292086e-01
  1.47030023e-01  1.51015751e-01  1.52263542e-01  1.56249270e-01
  1.58542363e-01  1.58987207e-01  1.60234998e-01  1.62528091e-01
  1.64220726e-01  1.66513819e-01  1.66958664e-01  1.70499548e-01
  1.70944392e-01  1.71400942e-01  1.74930120e-01  1.75386670e-01
  1.78471004e-01  1.78915848e-01  1.80163639e-01  1.82456732e-01
  1.84149367e-01  1.86442460e-01  1.88135095e-01  1.91329583e-01
  2.00092280e-01  2.02385373e-01  2.04078008e-01  2.06371101e-01
  2.12049464e-01  2.15243952e-01  2.16035192e-01  2.18328285e-01
  2.19229680e-01  2.20020920e-01  2.24463199e-01  2.30742020e-01
  2.32434655e-01  2.34727748e-01  2.35963833e-01  2.38713477e-01
  2.39949561e-01  2.42699205e-01  2.43935289e-01  2.47921017e-01
  2.51115505e-01  2.54199839e-01  2.55892474e-01  2.58642117e-01
  2.63863930e-01  2.67849658e-01  2.70599302e-01  2.71835386e-01
  2.72291937e-01  2.75821114e-01  2.76277665e-01  2.78570758e-01
  2.79806843e-01  2.82099936e-01  2.83347727e-01  2.83792571e-01
  2.84249121e-01  2.86085664e-01  2.87333455e-01  2.87778299e-01
  2.91764027e-01  2.92220577e-01  2.94513671e-01  2.95749755e-01
  2.98499399e-01  2.99735483e-01  3.03276367e-01  3.03721211e-01
  3.04177762e-01  3.07262095e-01  3.07706940e-01  3.10456583e-01
  3.11247824e-01  3.11692668e-01  3.14442311e-01  3.23205008e-01
  3.30385224e-01  3.34370952e-01  3.35162192e-01  3.39147921e-01
  3.48020771e-01  3.50313865e-01  3.52006500e-01  3.54299593e-01
  3.58285321e-01  3.59076561e-01  3.62271049e-01  3.63963684e-01
  3.71033746e-01  3.71935140e-01  3.79461752e-01  3.82199690e-01
  3.82990930e-01  3.86976658e-01  3.87433209e-01  3.87878053e-01
  3.90962386e-01  3.91418937e-01  3.94948115e-01  3.95849509e-01
  3.98933843e-01  3.99390393e-01  3.99835237e-01  4.02919571e-01
  4.07361850e-01  4.11347578e-01  4.19763878e-01  4.22848212e-01
  4.26833940e-01  4.30819668e-01  4.31721062e-01  4.39692519e-01
  4.42776852e-01  4.43233403e-01  4.50748309e-01  4.51204859e-01
  4.54734037e-01  4.55190587e-01  4.58719765e-01  4.63162044e-01
  4.67147772e-01  4.70676949e-01  4.71133500e-01  4.74662677e-01
  4.75119228e-01  4.79104956e-01  4.83090684e-01  4.86619862e-01
  4.87076412e-01  4.90605590e-01  4.91062141e-01  4.94591318e-01
  4.99033597e-01  5.06548503e-01  5.10534231e-01  5.14519959e-01
  5.14976509e-01  5.18505687e-01  5.18962238e-01  5.22947966e-01
  5.30919422e-01  5.34905150e-01  5.46862335e-01  5.50848063e-01
  5.54833791e-01  5.58819519e-01  5.62805247e-01  5.66790975e-01
  5.70776703e-01  5.78748160e-01  5.86719616e-01  5.90705344e-01
  5.94691072e-01  5.98676800e-01  6.02662529e-01  6.06648257e-01
  6.10633985e-01  6.14619713e-01  6.18605441e-01  6.22591169e-01
  6.26576897e-01  6.30562626e-01  6.34548354e-01  6.38534082e-01
  6.42519810e-01  6.46505538e-01]

  warnings.warn(

2022-11-03 10:50:32,964:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.10370324 -0.09974813 -0.09579303 -0.09183792 -0.08788281 -0.08392771
 -0.0799726  -0.07601749 -0.06810728 -0.06415217 -0.06019706 -0.05624195
 -0.05228685 -0.04833174 -0.04437663 -0.04042152 -0.03646642 -0.03251131
 -0.0285562  -0.0246011  -0.02064599 -0.01669088 -0.01273577 -0.00878067
 -0.00482556 -0.00087045  0.00308466  0.00703976  0.01494998  0.0167319
  0.01890508  0.02068701  0.02286019  0.0268153   0.03077041  0.03255233
  0.03472551  0.03868062  0.04046254  0.04263573  0.04659084  0.04837276
  0.05054594  0.05845616  0.06023808  0.06163437  0.06419319  0.06636637
  0.07427659  0.07605851  0.0774548   0.07823169  0.08140991  0.08614191
  0.09009702  0.09583405  0.09800723  0.10196234  0.10415379  0.10514055
  0.10591745  0.1081089   0.112064    0.11382766  0.11560958  0.11601911
  0.11778277  0.11997422  0.12392933  0.12964809  0.13183954  0.1336032
  0.13934023  0.13974976  0.14073652  0.14151341  0.14370486  0.14469162
  0.14765997  0.15161508  0.15260184  0.15337873  0.15655695  0.15733384
  0.15952529  0.16702598  0.16743551  0.17315427  0.1741593   0.17534572
  0.1788913   0.18206951  0.1842427   0.1881978   0.18997973  0.20006313
  0.20303147  0.20401823  0.20797334  0.21094169  0.21192845  0.21489679
  0.21588356  0.21766548  0.22379377  0.22458893  0.22774888  0.22854404
  0.2295308   0.23645425  0.24040936  0.24436447  0.24576076  0.24752441
  0.24831957  0.25049276  0.25147952  0.25543463  0.25622979  0.25721655
  0.25762608  0.26729995  0.26809511  0.2694914   0.27125506  0.27205022
  0.27344651  0.27521017  0.27916527  0.27996043  0.28312038  0.28707549
  0.28787065  0.29103059  0.29281252  0.29322205  0.2949857   0.29578086
  0.29676762  0.29894081  0.30289592  0.30369108  0.30467784  0.30685102
  0.30863295  0.30904248  0.30981937  0.31080613  0.31377448  0.31476124
  0.31871635  0.32445337  0.3248629   0.32881801  0.33137683  0.33236359
  0.3363187   0.33672823  0.3402738   0.34068333  0.34422891  0.34463844
  0.35254866  0.35609423  0.36400445  0.36441398  0.37015101  0.37191466
  0.38377998  0.38418951  0.3916902   0.39209973  0.39388165  0.39960041
  0.40000994  0.40355552  0.40396505  0.40970208  0.41187527  0.41761229
  0.42333106  0.42374059  0.42552251  0.42728616  0.42769569  0.43124127
  0.43343272  0.43519638  0.43560591  0.43738783  0.43915149  0.43956102
  0.44529805  0.44925315  0.45142634  0.45538145  0.45716337  0.45933655
  0.46189537  0.46507358  0.46724677  0.47120187  0.47515698  0.47911209
  0.48089401  0.4870223   0.49097741  0.49493252  0.49888763  0.50284273
  0.50679784  0.50857976  0.51075295  0.51253487  0.51470806  0.51648998
  0.51866316  0.52044508  0.52261827  0.52440019  0.52657338  0.5283553
  0.53231041  0.54417573  0.54813084  0.55208594  0.55604105  0.55999616
  0.56395126  0.56790637  0.57186148  0.57581659  0.57977169  0.59163702
  0.59559212  0.59954723  0.60350234  0.60745745  0.61141255  0.62327787
  0.63118809  0.6351432   0.6390983   0.64305341  0.64700852]

  warnings.warn(

2022-11-03 10:50:32,964:INFO:Calculating mean and std
2022-11-03 10:50:32,964:INFO:Creating metrics dataframe
2022-11-03 10:50:32,964:INFO:Uploading results into container
2022-11-03 10:50:32,980:INFO:Uploading model into container now
2022-11-03 10:50:32,980:INFO:master_model_container: 13
2022-11-03 10:50:32,980:INFO:display_container: 2
2022-11-03 10:50:32,980:INFO:OrthogonalMatchingPursuit()
2022-11-03 10:50:32,980:INFO:create_model() successfully completed......................................
2022-11-03 10:50:33,241:WARNING:create_model() for OrthogonalMatchingPursuit() raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:50:33,241:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:50:33,241:INFO:Initializing create_model()
2022-11-03 10:50:33,241:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:50:33,241:INFO:Checking exceptions
2022-11-03 10:50:33,241:INFO:Importing libraries
2022-11-03 10:50:33,257:INFO:Copying training dataset
2022-11-03 10:50:33,267:INFO:Defining folds
2022-11-03 10:50:33,267:INFO:Declaring metric variables
2022-11-03 10:50:33,267:INFO:Importing untrained model
2022-11-03 10:50:33,267:INFO:Orthogonal Matching Pursuit Imported successfully
2022-11-03 10:50:33,267:INFO:Starting cross validation
2022-11-03 10:50:33,283:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:50:36,131:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:36,383:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:36,383:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:36,400:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:36,418:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:36,482:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:36,563:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:36,626:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:37,348:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.10715742 -0.10296704 -0.09877666 -0.09458628 -0.0903959  -0.08620553
 -0.08201515 -0.07782477 -0.07363439 -0.06944402 -0.06525364 -0.06106326
 -0.05687288 -0.0526825  -0.04849213 -0.04430175 -0.04011137 -0.03592099
 -0.03173061 -0.02754024 -0.02334986 -0.0149691  -0.00658835 -0.00239797
  0.00179241  0.00598279  0.01270985  0.01436354  0.01690023  0.01855392
  0.0227443   0.02693468  0.02947136  0.03112505  0.03704332  0.03950581
  0.04623288  0.04788656  0.05207694  0.05626732  0.0604577   0.06299439
  0.06464808  0.06718477  0.06883845  0.07302883  0.07721921  0.08559996
  0.08979034  0.09151823  0.09398072  0.09651741  0.0981711   0.10236148
  0.10313004  0.10489817  0.10655185  0.10732042  0.10827974  0.11151079
  0.11493261  0.11570117  0.1166605   0.11912299  0.12331337  0.12408193
  0.1282723   0.12923163  0.13169412  0.13423081  0.1358845   0.13665306
  0.14007488  0.14084344  0.14426525  0.14503382  0.14599314  0.14680194
  0.14922419  0.15264601  0.15341457  0.15683639  0.16102677  0.16110096
  0.16179533  0.16694503  0.16940752  0.17855684  0.18197866  0.18370654
  0.18693759  0.18789692  0.19035941  0.1920873   0.20046805  0.20369911
  0.20465843  0.21303919  0.21384799  0.21722956  0.21976625  0.22141994
  0.22299731  0.22465099  0.22561032  0.22718768  0.2298007   0.23137806
  0.23303175  0.23475964  0.23479988  0.23722213  0.23818145  0.24237183
  0.24656221  0.24733077  0.24813957  0.24979326  0.25075259  0.25232995
  0.25494296  0.25652033  0.25913334  0.2599019   0.26167003  0.2675141
  0.26909146  0.27074515  0.27170448  0.27247304  0.27328184  0.27493553
  0.27589485  0.27666341  0.27747222  0.28089404  0.28262192  0.28331628
  0.28427561  0.28585297  0.28750666  0.28846599  0.29004335  0.29265636
  0.29342493  0.29346517  0.29684674  0.2976153   0.30103712  0.30261449
  0.30426817  0.3052275   0.30599606  0.30845855  0.30941788  0.31018644
  0.31099524  0.31360825  0.31518562  0.31614494  0.31779863  0.32356637
  0.32694795  0.3353287   0.33951908  0.34789984  0.34870864  0.35881728
  0.36300766  0.36642948  0.36719804  0.36885173  0.3730421   0.3755046
  0.37723248  0.37900061  0.38142286  0.38319099  0.38738137  0.38980362
  0.39157174  0.39653068  0.39818437  0.3999525   0.40737393  0.40833326
  0.41494588  0.41575469  0.41671401  0.41913626  0.41994507  0.42090439
  0.42509477  0.42751702  0.43347552  0.43424408  0.4376659   0.43843446
  0.44008815  0.44508733  0.44846891  0.4510056   0.45684966  0.46104004
  0.4694208   0.47614786  0.47780155  0.48199193  0.48618231  0.49290937
  0.49456306  0.49709975  0.49875344  0.50129013  0.50294382  0.5071342
  0.50967089  0.51551495  0.51805164  0.51970533  0.52224202  0.52389571
  0.5264324   0.52808609  0.53481315  0.53900353  0.54319391  0.54738429
  0.55157466  0.56833618  0.57252655  0.57671693  0.58090731  0.58509769
  0.58928806  0.59347844  0.59766882  0.6018592   0.60604958  0.61443033
  0.62700147  0.63119184  0.63538222  0.6395726   0.64376298  0.64795335]

  warnings.warn(

2022-11-03 10:50:37,564:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.10089816 -0.09690002 -0.09290188 -0.08890374 -0.0849056  -0.08090746
 -0.07690933 -0.07291119 -0.06891305 -0.06091677 -0.05691863 -0.05292049
 -0.04892236 -0.04492422 -0.03692794 -0.0329298  -0.02493352 -0.02093539
 -0.01693725 -0.01293911 -0.00894097 -0.00494283 -0.00094469  0.00305345
  0.01104972  0.01504786  0.019046    0.03201568  0.03503855  0.03903669
  0.04303483  0.04401009  0.04703297  0.04800823  0.05103111  0.05502925
  0.05902739  0.06302552  0.06702366  0.0710218   0.07501994  0.07901808
  0.08301622  0.08701436  0.08800486  0.09101249  0.09198776  0.09501063
  0.09900877  0.10382101  0.10700505  0.10781915  0.10798031  0.11181729
  0.11197845  0.11199369  0.11500133  0.11581543  0.11597659  0.11899947
  0.11981356  0.1229976   0.1238117   0.12699574  0.12780984  0.12798625
  0.13099388  0.13180798  0.13198439  0.13499202  0.13580612  0.13596728
  0.13899016  0.13980426  0.14396356  0.14698644  0.15179867  0.15195983
  0.15197508  0.15898085  0.15979495  0.16779123  0.17195053  0.18378378
  0.18696782  0.19977634  0.19995274  0.20395088  0.20794902  0.20892428
  0.21177075  0.21193191  0.21194716  0.21976703  0.21992819  0.22873857
  0.23193785  0.23273671  0.23593599  0.23673485  0.23975772  0.24391702
  0.24393227  0.24473112  0.247754    0.24793041  0.25192854  0.2527274
  0.25672554  0.25674078  0.25990958  0.25992482  0.26072368  0.26090008
  0.26374655  0.26390772  0.26774469  0.2679211   0.26871995  0.2687352
  0.27174283  0.27191924  0.27271809  0.27591738  0.27671623  0.27673148
  0.27973911  0.27991551  0.28072961  0.28389841  0.28791179  0.29190993
  0.29270879  0.29272403  0.29590807  0.29672217  0.29688333  0.29990621
  0.30390435  0.3047032   0.30487961  0.30790248  0.30887775  0.31190062
  0.31271472  0.31589876  0.31669762  0.32470914  0.3248703   0.32870728
  0.33269017  0.33286658  0.33670355  0.33971119  0.34070169  0.34086285
  0.34469983  0.34868273  0.34885913  0.35268086  0.35269611  0.35669425
  0.35685541  0.36069239  0.36868866  0.36884982  0.3726868   0.3766697
  0.38068308  0.38084424  0.38466597  0.38468122  0.38484238  0.38565648
  0.38884052  0.39283866  0.39365276  0.39667563  0.40067377  0.40083493
  0.4086548   0.40883121  0.40964531  0.41282935  0.41665108  0.41682749
  0.41764159  0.42066447  0.42482376  0.42563786  0.42866074  0.4288219
  0.429636    0.43265888  0.43282004  0.43665702  0.43681818  0.44065516
  0.44081632  0.45264957  0.46064585  0.46464399  0.46561925  0.46864213
  0.47264027  0.47663841  0.47761367  0.48063654  0.48161181  0.48463468
  0.48863282  0.48960808  0.49360622  0.4966291   0.50062724  0.5016025
  0.50462538  0.51759505  0.52061793  0.52159319  0.52559133  0.52958947
  0.53358761  0.54558202  0.5535783   0.56157458  0.56557272  0.56957085
  0.57756713  0.58156527  0.58556341  0.58956155  0.59355969  0.59755782
  0.60155596  0.60955224  0.61355038  0.61754852  0.62154666  0.62554479
  0.62954293  0.63354107  0.63753921  0.64153735  0.64553549]

  warnings.warn(

2022-11-03 10:50:37,627:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.07103654e-01 -1.02990651e-01 -9.88776471e-02 -9.47646434e-02
 -9.06516397e-02 -8.65386360e-02 -8.24256323e-02 -7.41996249e-02
 -7.00866213e-02 -6.59736176e-02 -6.18606139e-02 -5.77476102e-02
 -5.36346065e-02 -4.95216028e-02 -4.54085991e-02 -4.12955954e-02
 -3.71825918e-02 -2.89565844e-02 -2.48435807e-02 -2.07305770e-02
 -1.66175733e-02 -1.25045696e-02 -8.39156595e-03 -4.27856226e-03
 -1.65558571e-04  1.17029510e-02  1.21734525e-02  1.58159547e-02
  1.62864562e-02  1.99289584e-02  2.03994599e-02  2.45124636e-02
  2.86254672e-02  3.27384709e-02  3.68514746e-02  4.09644783e-02
  4.87199842e-02  5.33034894e-02  5.74164931e-02  5.96948089e-02
  6.15294967e-02  6.92850026e-02  6.97555041e-02  7.38685078e-02
  7.75110100e-02  7.79815115e-02  8.02598273e-02  8.20945152e-02
  8.43728310e-02  8.62075189e-02  9.03205226e-02  9.44335262e-02
  9.85465299e-02  1.02659534e-01  1.02752697e-01  1.06772537e-01
  1.06865701e-01  1.09050853e-01  1.10885541e-01  1.10978704e-01
  1.15091708e-01  1.19111548e-01  1.19204712e-01  1.23224552e-01
  1.27337556e-01  1.27430719e-01  1.29615872e-01  1.31450559e-01
  1.35563563e-01  1.35656726e-01  1.39206065e-01  1.39676567e-01
  1.43789570e-01  1.43882734e-01  1.46067886e-01  1.52108741e-01
  1.54293894e-01  1.56128582e-01  1.56221745e-01  1.58406897e-01
  1.60334749e-01  1.62519901e-01  1.64447752e-01  1.70745908e-01
  1.74858912e-01  1.76693600e-01  1.80899767e-01  1.83084920e-01
  1.85012771e-01  1.89032611e-01  1.89125774e-01  1.93238778e-01
  1.95423931e-01  2.01464785e-01  2.03649938e-01  2.07292440e-01
  2.09690793e-01  2.11405444e-01  2.15988949e-01  2.21559302e-01
  2.22029804e-01  2.23744455e-01  2.24214956e-01  2.32440964e-01
  2.33898313e-01  2.38011317e-01  2.42124321e-01  2.42594822e-01
  2.44779975e-01  2.44873138e-01  2.46237325e-01  2.46707826e-01
  2.48892979e-01  2.50350328e-01  2.53005982e-01  2.53099146e-01
  2.54933833e-01  2.58483172e-01  2.61231990e-01  2.62596176e-01
  2.65344993e-01  2.65438157e-01  2.69457997e-01  2.69551160e-01
  2.73571001e-01  2.73664164e-01  2.75028350e-01  2.77684004e-01
  2.81797008e-01  2.85910012e-01  2.87367361e-01  2.87837863e-01
  2.90023015e-01  2.94136019e-01  2.95593369e-01  2.98249023e-01
  3.01891525e-01  3.02362026e-01  3.04289878e-01  3.06475030e-01
  3.06568194e-01  3.10588034e-01  3.14701038e-01  3.18814041e-01
  3.18907205e-01  3.22927045e-01  3.24384395e-01  3.27133212e-01
  3.31246216e-01  3.34795554e-01  3.39472223e-01  3.43021562e-01
  3.47698230e-01  3.51811234e-01  3.55924238e-01  3.59473577e-01
  3.61401428e-01  3.67699584e-01  3.71812588e-01  3.72376253e-01
  3.73740439e-01  3.76018755e-01  3.76489256e-01  3.80038595e-01
  3.86079450e-01  3.92377606e-01  3.92470769e-01  3.92941271e-01
  3.96490610e-01  3.96583773e-01  3.97054275e-01  4.01167278e-01
  4.04809781e-01  4.05280282e-01  4.12942625e-01  4.17148792e-01
  4.17619293e-01  4.21732297e-01  4.25374799e-01  4.25845300e-01
  4.29394639e-01  4.29487803e-01  4.29958304e-01  4.33507643e-01
  4.33600806e-01  4.37620647e-01  4.41733650e-01  4.45939817e-01
  4.46410319e-01  4.50052821e-01  4.50523323e-01  4.54165825e-01
  4.54636326e-01  4.62391832e-01  4.62862334e-01  4.66504836e-01
  4.66975337e-01  4.71088341e-01  4.79314348e-01  4.83427352e-01
  4.87069854e-01  4.87540356e-01  4.91182858e-01  4.95295862e-01
  4.99879367e-01  5.03521869e-01  5.07634873e-01  5.08105374e-01
  5.11747876e-01  5.16331382e-01  5.19973884e-01  5.24086887e-01
  5.24557389e-01  5.32312895e-01  5.32783396e-01  5.48764910e-01
  5.52877913e-01  5.56990917e-01  5.61103921e-01  5.69329928e-01
  5.73442932e-01  5.77555935e-01  5.81668939e-01  5.85781943e-01
  5.89894946e-01  5.94007950e-01  5.98120954e-01  6.02233958e-01
  6.06346961e-01  6.10459965e-01  6.14572969e-01  6.18685972e-01
  6.22798976e-01  6.26911980e-01  6.35137987e-01  6.39250991e-01
  6.43363994e-01  6.47476998e-01  6.51590002e-01]

  warnings.warn(

2022-11-03 10:50:37,642:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.10126501 -0.09729867 -0.09333233 -0.08539965 -0.08143331 -0.07746697
 -0.07350063 -0.06953429 -0.06556795 -0.06160161 -0.05763527 -0.05366893
 -0.04970259 -0.04573625 -0.04176991 -0.03780357 -0.03383723 -0.02987089
 -0.02590455 -0.02193821 -0.01797187 -0.01400553 -0.00607285 -0.00210651
  0.00185983  0.00582617  0.01375885  0.01772519  0.02169153  0.02962421
  0.03359055  0.03755689  0.04152323  0.04460687  0.04548957  0.04945591
  0.05342225  0.05738859  0.06443857  0.06532126  0.0692876   0.07633759
  0.07722028  0.08118662  0.08515296  0.09308564  0.09616929  0.09705198
  0.10013563  0.10101832  0.10328671  0.10348425  0.10745059  0.108951
  0.11141693  0.11291734  0.11518573  0.11538327  0.11688368  0.11915207
  0.11934961  0.12085002  0.12331595  0.12481636  0.12708475  0.12728229
  0.1287827   0.13124863  0.13274904  0.13671538  0.13918131  0.13979902
  0.14068172  0.14376536  0.14464806  0.14711399  0.1486144   0.15088278
  0.15484912  0.15504667  0.15654708  0.15881546  0.15901301  0.15963072
  0.1627818   0.16694569  0.1684461   0.17549608  0.17637878  0.17884471
  0.18034512  0.1826135   0.18342876  0.18431146  0.18677739  0.19054618
  0.19074373  0.19532778  0.19867641  0.19929412  0.20949518  0.21057543
  0.21434422  0.21831056  0.22247445  0.22309216  0.22536054  0.22555809
  0.22644079  0.2270585   0.23020958  0.23040713  0.23349077  0.23437347
  0.23725956  0.24538979  0.24915858  0.25004128  0.25400762  0.25647355
  0.25728881  0.25797396  0.25878922  0.2619403   0.26440623  0.26522149
  0.26590664  0.26918783  0.27007052  0.27383932  0.28027159  0.28108685
  0.281772    0.28505319  0.28573834  0.28882198  0.28901953  0.28970468
  0.29217061  0.29367102  0.29763736  0.30010329  0.30091855  0.3016037
  0.30180124  0.30406963  0.30557004  0.30953638  0.31200231  0.31281757
  0.31350272  0.31746906  0.32868292  0.33641806  0.3403844   0.34058194
  0.34146464  0.34435074  0.34769936  0.34831708  0.35563204  0.35624976
  0.3564473   0.37546374  0.37943008  0.3800478   0.38339642  0.38401414
  0.39194681  0.39529544  0.39591315  0.39926178  0.39987949  0.40384583
  0.40719446  0.40781217  0.4111608   0.41177851  0.41197606  0.41424444
  0.41512714  0.41574485  0.41909348  0.41971119  0.42217712  0.42305982
  0.42367753  0.42614346  0.42702616  0.42764387  0.4301098   0.4309925
  0.43161021  0.43407614  0.43557655  0.43804248  0.43954289  0.44200882
  0.44370678  0.44685786  0.4499415   0.4508242   0.45390784  0.45479054
  0.45875688  0.46184052  0.46272322  0.46668956  0.4706559   0.47462224
  0.47858858  0.48255492  0.48563856  0.48652126  0.49445394  0.50150392
  0.50635296  0.5094366   0.5103193   0.51340294  0.51428564  0.51736928
  0.51825198  0.52133562  0.52221832  0.52530196  0.5292683   0.53720098
  0.54116732  0.54513366  0.5491      0.55306634  0.55703268  0.56099902
  0.56496536  0.57686438  0.58083072  0.58479706  0.5887634   0.59272974
  0.59669607  0.60066241  0.60462875  0.61256143  0.61652777  0.62049411
  0.62446045  0.62842679  0.63239313  0.63635947  0.64032581  0.64429215]

  warnings.warn(

2022-11-03 10:50:37,674:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.10130613 -0.09736673 -0.09342733 -0.08948794 -0.08554854 -0.08160914
 -0.07766974 -0.06979094 -0.06585154 -0.06191214 -0.05797274 -0.05403334
 -0.05009394 -0.04615454 -0.04221514 -0.03827574 -0.03039694 -0.02645754
 -0.02251814 -0.01857874 -0.01463934 -0.01069994 -0.00282114  0.00111826
  0.00505766  0.00899706  0.01293646  0.01586188  0.01980128  0.02475465
  0.02869405  0.03161948  0.03263345  0.03555888  0.03657285  0.04445165
  0.04839105  0.05233045  0.05919528  0.06020925  0.06414865  0.06808805
  0.07101348  0.07596685  0.08283167  0.08384565  0.08677107  0.09172445
  0.09566385  0.10252867  0.10347977  0.10354265  0.10748205  0.11040747
  0.11142145  0.11529797  0.11536085  0.11923737  0.12317677  0.12711617
  0.12717904  0.13010447  0.13105556  0.13404387  0.13505784  0.13798327
  0.13899724  0.1392213   0.14287376  0.14293664  0.14586207  0.15075256
  0.15081544  0.1510395   0.15469196  0.1589183   0.16257076  0.16263364
  0.16651016  0.16657304  0.17438896  0.17467589  0.17832836  0.17839124
  0.18255469  0.18525606  0.19014656  0.19043349  0.19408596  0.19831229
  0.20101366  0.20196476  0.20225169  0.20619109  0.21406989  0.21772236
  0.22064778  0.22194869  0.22458718  0.22588809  0.22852658  0.22982749
  0.23347996  0.23376689  0.23770629  0.24034478  0.24164569  0.24529815
  0.24822358  0.24923755  0.24952449  0.25216298  0.25317695  0.25340101
  0.25346389  0.25740329  0.26032871  0.26134269  0.26426811  0.26528209
  0.26922149  0.27185998  0.27309801  0.27316089  0.27681335  0.27710028
  0.27973878  0.28075275  0.28103968  0.28469215  0.28790451  0.28891848
  0.29285788  0.29679728  0.29972271  0.30073668  0.30337517  0.3046132
  0.30467608  0.3085526   0.31226795  0.31255488  0.3164314   0.32043368
  0.32307217  0.32335911  0.32437308  0.3282496   0.33095097  0.33123791
  0.3430561   0.3479466   0.351886    0.35852677  0.3597648   0.3666925
  0.37034497  0.371583    0.37428437  0.3755224   0.3785107   0.3834012
  0.3863895   0.3873406   0.39128     0.39420542  0.3942683   0.39814482
  0.3982077   0.39915879  0.40309819  0.40602362  0.4060865   0.40703759
  0.4100259   0.41097699  0.41390242  0.4139653   0.41491639  0.41784182
  0.4179047   0.41885579  0.4218441   0.4257835   0.42673459  0.42966002
  0.43067399  0.4336623   0.43461339  0.4376017   0.43855279  0.44154109
  0.44249219  0.46117521  0.46218919  0.46511461  0.46612859  0.46905401
  0.47006799  0.47400739  0.47693281  0.47794679  0.48087221  0.48188619
  0.48481161  0.48582559  0.48852696  0.48875101  0.48976499  0.49370439
  0.49662981  0.49764378  0.50056921  0.50450861  0.50552258  0.50844801
  0.50946198  0.51238741  0.51340138  0.51632681  0.52026621  0.52128018
  0.52420561  0.52521958  0.52814501  0.52915898  0.53208441  0.53602381
  0.53996321  0.54784201  0.55178141  0.55572081  0.5832966   0.5911754
  0.5951148   0.5990542   0.6029936   0.606933    0.6108724   0.6148118
  0.6187512   0.6226906   0.62663     0.6305694   0.6345088   0.6384482
  0.6423876   0.646327  ]

  warnings.warn(

2022-11-03 10:50:37,720:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.104898   -0.10074519 -0.09659238 -0.09243957 -0.08828675 -0.08413394
 -0.07998113 -0.07582832 -0.07167551 -0.06752269 -0.06336988 -0.05921707
 -0.05506426 -0.05091145 -0.04675863 -0.04260582 -0.03845301 -0.02184176
 -0.01768895 -0.00938333 -0.00523051  0.00307511  0.00722792  0.01138073
  0.01391203  0.01806484  0.01968636  0.02383917  0.02799198  0.03052328
  0.03467609  0.03629761  0.04045042  0.04713452  0.04875604  0.05128734
  0.05290885  0.05544015  0.05706167  0.06121448  0.06536729  0.0695201
  0.0720514   0.07782573  0.07809893  0.08197854  0.08613135  0.09028416
  0.09055737  0.09443697  0.09696827  0.09726529  0.09858979  0.10141811
  0.1027426   0.10689541  0.10972373  0.11104822  0.11520103  0.11547424
  0.11802935  0.11962705  0.12218217  0.12350666  0.12377986  0.12633498
  0.12765947  0.13048779  0.13181228  0.13434358  0.1346406   0.13849639
  0.13879341  0.14427072  0.14454392  0.14869673  0.15095482  0.15257634
  0.15672915  0.16341326  0.16503478  0.16530798  0.1678631   0.16918759
  0.17201591  0.17749321  0.18032153  0.18164603  0.18191923  0.18579884
  0.18995165  0.19248294  0.19277997  0.19437767  0.19690896  0.20106177
  0.20523841  0.20939122  0.213247    0.21354403  0.21514173  0.21607532
  0.22022814  0.22438095  0.22760016  0.22853376  0.23015528  0.23175297
  0.23428427  0.23430809  0.23590579  0.23683938  0.2384609   0.2400586
  0.24288692  0.24514501  0.24836422  0.25119254  0.25251703  0.25534535
  0.25666985  0.25949817  0.26082266  0.26365098  0.26497547  0.26590907
  0.26780379  0.26912828  0.27328109  0.27421469  0.27743391  0.2799652
  0.27998902  0.28158672  0.28252032  0.28573953  0.28667313  0.28856785
  0.28989234  0.29082594  0.29244746  0.29272066  0.29404515  0.29819797
  0.30072926  0.30328438  0.30650359  0.30905871  0.3106564   0.31348472
  0.31480921  0.31763753  0.31896203  0.32311484  0.32564613  0.33009597
  0.33235406  0.33840159  0.34255441  0.34670722  0.35056301  0.35471582
  0.37132707  0.37162409  0.3757769   0.3783082   0.37963269  0.37992971
  0.38793831  0.38823534  0.39238815  0.39654096  0.39907226  0.40322507
  0.40454956  0.40484659  0.4089994   0.41153069  0.41285519  0.417008
  0.41983632  0.42145783  0.42531362  0.42561065  0.42814194  0.42946643
  0.43361925  0.43391627  0.43644757  0.43777206  0.43806908  0.44060038
  0.44192487  0.44222189  0.44637471  0.45052752  0.45883314  0.46136444
  0.46298595  0.46713877  0.46967006  0.47354967  0.47382287  0.47544439
  0.47797569  0.4795972   0.48628131  0.48790283  0.49043412  0.49205564
  0.49458693  0.49620845  0.50289256  0.50704537  0.50866689  0.5128197
  0.51535099  0.51697251  0.51950381  0.52112532  0.52365662  0.52527813
  0.53196224  0.53611505  0.54442068  0.54857349  0.5527263   0.55687911
  0.56103192  0.56518474  0.56933755  0.57349036  0.58179598  0.59010161
  0.59425442  0.59840723  0.60671286  0.61086567  0.61501848  0.61917129
  0.6233241   0.62747692  0.63162973  0.63578254  0.63993535  0.64408816]

  warnings.warn(

2022-11-03 10:50:37,760:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.07793377e-01 -1.03660050e-01 -9.95267225e-02 -9.53933953e-02
 -9.12600680e-02 -8.71267408e-02 -8.29934136e-02 -7.88600864e-02
 -7.47267592e-02 -7.05934320e-02 -6.64601048e-02 -6.23267776e-02
 -5.81934503e-02 -5.40601231e-02 -4.99267959e-02 -3.75268143e-02
 -3.33934871e-02 -2.92601599e-02 -2.51268326e-02 -2.09935054e-02
 -1.68601782e-02 -1.27268510e-02 -8.59352379e-03 -4.46019657e-03
 -3.26869361e-04  7.93978506e-03  1.20731123e-02  1.49920164e-02
  1.91253436e-02  2.03397667e-02  2.44730939e-02  2.86064211e-02
  3.56586525e-02  3.68730756e-02  4.51397300e-02  4.80586341e-02
  4.92730572e-02  5.21919613e-02  5.34063844e-02  5.75397116e-02
  6.16730388e-02  6.58063660e-02  6.99396933e-02  7.69919246e-02
  7.82063477e-02  8.52585790e-02  8.83383528e-02  9.06063293e-02
  9.24716800e-02  9.47396565e-02  9.73612391e-02  9.88729837e-02
  1.00738334e-01  1.01494566e-01  1.03006311e-01  1.05627894e-01
  1.07139638e-01  1.09761221e-01  1.10058542e-01  1.11272965e-01
  1.13894548e-01  1.14191870e-01  1.15406293e-01  1.18027875e-01
  1.19539620e-01  1.21404971e-01  1.22161202e-01  1.23672947e-01
  1.26591851e-01  1.27806274e-01  1.30725178e-01  1.31939601e-01
  1.34561184e-01  1.34858506e-01  1.36072929e-01  1.38694511e-01
  1.42071607e-01  1.42827838e-01  1.44339583e-01  1.52606238e-01
  1.54471588e-01  1.56739565e-01  1.60872892e-01  1.63494475e-01
  1.65006219e-01  1.69139546e-01  1.71761129e-01  1.73272874e-01
  1.79271551e-01  1.81539528e-01  1.83404879e-01  1.87538206e-01
  1.89806182e-01  1.92427765e-01  1.94590437e-01  1.95804860e-01
  1.96561092e-01  1.96858414e-01  2.00694419e-01  2.08204842e-01
  2.11123746e-01  2.20146632e-01  2.20604824e-01  2.24279960e-01
  2.24738151e-01  2.28413287e-01  2.29627710e-01  2.31790382e-01
  2.33004805e-01  2.41271460e-01  2.44946596e-01  2.52159697e-01
  2.53213250e-01  2.54724995e-01  2.57804769e-01  2.61479905e-01
  2.61938096e-01  2.66071423e-01  2.69746559e-01  2.70204750e-01
  2.73879886e-01  2.74338077e-01  2.77256982e-01  2.78471405e-01
  2.82604732e-01  2.83658285e-01  2.86738059e-01  2.90871386e-01
  2.94546522e-01  2.98679849e-01  2.99138041e-01  3.03271368e-01
  3.06190272e-01  3.07404695e-01  3.10323599e-01  3.11079831e-01
  3.11538022e-01  3.15671350e-01  3.19804677e-01  3.22723581e-01
  3.30692914e-01  3.40013122e-01  3.45360872e-01  3.47226223e-01
  3.47523544e-01  3.51359550e-01  3.51656871e-01  3.55790199e-01
  3.59626204e-01  3.59923526e-01  3.63759531e-01  3.73079739e-01
  3.80292840e-01  3.80590162e-01  3.81346394e-01  3.84426168e-01
  3.84723489e-01  3.88559495e-01  3.88856816e-01  3.89613048e-01
  3.92990144e-01  3.96826149e-01  3.99745053e-01  4.00959476e-01
  4.01256798e-01  4.05092804e-01  4.08011708e-01  4.09523452e-01
  4.10279684e-01  4.13656780e-01  4.17492785e-01  4.20411689e-01
  4.28678344e-01  4.30190088e-01  4.34026094e-01  4.34323416e-01
  4.38159421e-01  4.38456743e-01  4.41078325e-01  4.42590070e-01
  4.46426076e-01  4.54692730e-01  4.57611634e-01  4.58826057e-01
  4.62959385e-01  4.65878289e-01  4.67092712e-01  4.70011616e-01
  4.78278270e-01  4.79492693e-01  4.82411598e-01  4.86544925e-01
  4.87759348e-01  4.90678252e-01  4.98944906e-01  5.04292657e-01
  5.07211561e-01  5.08425984e-01  5.11344888e-01  5.12559311e-01
  5.19611542e-01  5.20825966e-01  5.23744870e-01  5.24959293e-01
  5.27878197e-01  5.36144851e-01  5.44411506e-01  5.48544833e-01
  5.52678160e-01  5.56811487e-01  5.60944815e-01  5.69211469e-01
  5.73344796e-01  5.77478123e-01  5.81611451e-01  5.85744778e-01
  5.89878105e-01  5.94011432e-01  6.10544741e-01  6.14678068e-01
  6.18811396e-01  6.22944723e-01  6.27078050e-01  6.31211377e-01
  6.35344704e-01  6.39478032e-01  6.43611359e-01  6.47744686e-01]

  warnings.warn(

2022-11-03 10:50:37,808:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.10793065 -0.10380839 -0.09968613 -0.09556388 -0.09144162 -0.08731936
 -0.08319711 -0.07907485 -0.07495259 -0.07083034 -0.06670808 -0.05434131
 -0.05021905 -0.0460968  -0.03785229 -0.03373003 -0.02960777 -0.02548552
 -0.02136326 -0.017241   -0.01311875 -0.00899649 -0.00487423 -0.00075198
  0.00337028  0.00749254  0.01161479  0.01223495  0.01635721  0.01985931
  0.02398156  0.02810382  0.03222607  0.03634833  0.04047059  0.04459284
  0.0487151   0.05283736  0.05345752  0.06108187  0.06170203  0.06406401
  0.06520413  0.06932638  0.07344864  0.08169315  0.08581541  0.08643557
  0.08993767  0.09055783  0.09291981  0.09405992  0.09818218  0.10116432
  0.10230443  0.10349808  0.10642669  0.10762034  0.1117426   0.11529137
  0.11586485  0.11879346  0.11998711  0.12410937  0.13116023  0.13528249
  0.13590265  0.13647614  0.13940474  0.14059839  0.143527    0.14472065
  0.14650914  0.14764926  0.15177151  0.15296516  0.15589377  0.16001603
  0.16120967  0.16413828  0.16712042  0.16945419  0.17536494  0.17948719
  0.18182096  0.18360945  0.18536973  0.18594321  0.18887182  0.18949198
  0.19006547  0.19185396  0.19418773  0.19597622  0.20243224  0.20422073
  0.20834299  0.21010326  0.21067675  0.21246525  0.21479901  0.22070976
  0.22304352  0.22366368  0.22483201  0.22545218  0.22716578  0.23307653
  0.23541029  0.24132104  0.24427497  0.2454433   0.24956555  0.25251948
  0.25602157  0.25781007  0.26076399  0.26193232  0.26312597  0.26488625
  0.26605458  0.26667474  0.27137048  0.27193711  0.2725106   0.27429909
  0.27491925  0.27725302  0.27842135  0.279615    0.28018162  0.28254361
  0.28373725  0.28487737  0.28666586  0.28961979  0.29078812  0.29198177
  0.29491037  0.29553054  0.29610402  0.29667065  0.29903263  0.30198656
  0.30315489  0.30377505  0.30434854  0.30548865  0.30610881  0.30727714
  0.30789731  0.30847079  0.3113994   0.31552166  0.31614182  0.31671531
  0.32026407  0.32259784  0.32495982  0.32850859  0.32908208  0.33320433
  0.33732659  0.34144884  0.34499761  0.35381561  0.35557589  0.35736438
  0.35793787  0.35969815  0.36148664  0.3656089   0.36794266  0.3709248
  0.37385341  0.37618717  0.37854915  0.38329157  0.38443169  0.38679367
  0.39153608  0.39446469  0.39503818  0.39916044  0.3997806   0.4027092
  0.40740495  0.4115272   0.41214737  0.41564946  0.41919823  0.41977172
  0.42039188  0.42332049  0.42389397  0.42451414  0.42744274  0.42801623
  0.42863639  0.43213849  0.43275865  0.43389876  0.43568726  0.440383
  0.44100316  0.45274977  0.45336993  0.45687203  0.45749219  0.46161444
  0.46511654  0.4692388   0.47336105  0.47810347  0.48336584  0.48572782
  0.48634798  0.48985008  0.49397233  0.49809459  0.50221685  0.50283701
  0.5063391   0.50695927  0.51046136  0.51108152  0.51458362  0.51520378
  0.51870587  0.51932603  0.52282813  0.52344829  0.52695039  0.52757055
  0.5316928   0.53581506  0.53993732  0.54405957  0.55230409  0.55642634
  0.5605486   0.56879311  0.57291537  0.57703763  0.58115988  0.58528214
  0.58940439  0.59352665  0.59764891  0.60177116  0.60589342  0.61001568
  0.61413793  0.61826019  0.62238245  0.6265047   0.63474922  0.63887147
  0.64299373  0.64711599]

  warnings.warn(

2022-11-03 10:50:39,365:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:39,611:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2022-11-03 10:50:40,283:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.04070850e-01 -1.00085122e-01 -9.60993939e-02 -9.21136658e-02
 -8.81279376e-02 -8.41422095e-02 -8.01564814e-02 -7.61707532e-02
 -7.21850251e-02 -6.81992969e-02 -6.42135688e-02 -6.02278406e-02
 -5.62421125e-02 -5.22563843e-02 -4.82706562e-02 -4.02991999e-02
 -3.63134718e-02 -3.23277436e-02 -2.83420155e-02 -2.43562873e-02
 -2.03705592e-02 -8.41337477e-03 -4.42764662e-03 -4.41918481e-04
  3.54380966e-03  7.52953781e-03  1.15152660e-02  1.94867222e-02
  2.34724504e-02  2.39290009e-02  2.74581785e-02  2.79147290e-02
  3.14439067e-02  3.19004571e-02  3.54296348e-02  3.94153630e-02
  4.34010911e-02  4.46488820e-02  4.73868192e-02  5.53582755e-02
  6.33297318e-02  7.13011881e-02  7.17577386e-02  7.52869163e-02
  7.92726444e-02  8.32583725e-02  8.72441007e-02  9.12298288e-02
  9.52155570e-02  9.56721074e-02  9.92012851e-02  1.02742169e-01
  1.03643564e-01  1.04434804e-01  1.06727897e-01  1.07172741e-01
  1.10713625e-01  1.11158470e-01  1.14699354e-01  1.15600748e-01
  1.18685082e-01  1.19129926e-01  1.22670810e-01  1.23115654e-01
  1.26656538e-01  1.27101382e-01  1.27557933e-01  1.30642266e-01
  1.31087110e-01  1.34627994e-01  1.35072838e-01  1.35529389e-01
  1.39058567e-01  1.42599451e-01  1.43044295e-01  1.44292086e-01
  1.47030023e-01  1.51015751e-01  1.52263542e-01  1.56249270e-01
  1.58542363e-01  1.58987207e-01  1.60234998e-01  1.62528091e-01
  1.64220726e-01  1.66513819e-01  1.66958664e-01  1.70499548e-01
  1.70944392e-01  1.71400942e-01  1.74930120e-01  1.75386670e-01
  1.78471004e-01  1.78915848e-01  1.80163639e-01  1.82456732e-01
  1.84149367e-01  1.86442460e-01  1.88135095e-01  1.91329583e-01
  2.00092280e-01  2.02385373e-01  2.04078008e-01  2.06371101e-01
  2.12049464e-01  2.15243952e-01  2.16035192e-01  2.18328285e-01
  2.19229680e-01  2.20020920e-01  2.24463199e-01  2.30742020e-01
  2.32434655e-01  2.34727748e-01  2.35963833e-01  2.38713477e-01
  2.39949561e-01  2.42699205e-01  2.43935289e-01  2.47921017e-01
  2.51115505e-01  2.54199839e-01  2.55892474e-01  2.58642117e-01
  2.63863930e-01  2.67849658e-01  2.70599302e-01  2.71835386e-01
  2.72291937e-01  2.75821114e-01  2.76277665e-01  2.78570758e-01
  2.79806843e-01  2.82099936e-01  2.83347727e-01  2.83792571e-01
  2.84249121e-01  2.86085664e-01  2.87333455e-01  2.87778299e-01
  2.91764027e-01  2.92220577e-01  2.94513671e-01  2.95749755e-01
  2.98499399e-01  2.99735483e-01  3.03276367e-01  3.03721211e-01
  3.04177762e-01  3.07262095e-01  3.07706940e-01  3.10456583e-01
  3.11247824e-01  3.11692668e-01  3.14442311e-01  3.23205008e-01
  3.30385224e-01  3.34370952e-01  3.35162192e-01  3.39147921e-01
  3.48020771e-01  3.50313865e-01  3.52006500e-01  3.54299593e-01
  3.58285321e-01  3.59076561e-01  3.62271049e-01  3.63963684e-01
  3.71033746e-01  3.71935140e-01  3.79461752e-01  3.82199690e-01
  3.82990930e-01  3.86976658e-01  3.87433209e-01  3.87878053e-01
  3.90962386e-01  3.91418937e-01  3.94948115e-01  3.95849509e-01
  3.98933843e-01  3.99390393e-01  3.99835237e-01  4.02919571e-01
  4.07361850e-01  4.11347578e-01  4.19763878e-01  4.22848212e-01
  4.26833940e-01  4.30819668e-01  4.31721062e-01  4.39692519e-01
  4.42776852e-01  4.43233403e-01  4.50748309e-01  4.51204859e-01
  4.54734037e-01  4.55190587e-01  4.58719765e-01  4.63162044e-01
  4.67147772e-01  4.70676949e-01  4.71133500e-01  4.74662677e-01
  4.75119228e-01  4.79104956e-01  4.83090684e-01  4.86619862e-01
  4.87076412e-01  4.90605590e-01  4.91062141e-01  4.94591318e-01
  4.99033597e-01  5.06548503e-01  5.10534231e-01  5.14519959e-01
  5.14976509e-01  5.18505687e-01  5.18962238e-01  5.22947966e-01
  5.30919422e-01  5.34905150e-01  5.46862335e-01  5.50848063e-01
  5.54833791e-01  5.58819519e-01  5.62805247e-01  5.66790975e-01
  5.70776703e-01  5.78748160e-01  5.86719616e-01  5.90705344e-01
  5.94691072e-01  5.98676800e-01  6.02662529e-01  6.06648257e-01
  6.10633985e-01  6.14619713e-01  6.18605441e-01  6.22591169e-01
  6.26576897e-01  6.30562626e-01  6.34548354e-01  6.38534082e-01
  6.42519810e-01  6.46505538e-01]

  warnings.warn(

2022-11-03 10:50:40,463:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.10370324 -0.09974813 -0.09579303 -0.09183792 -0.08788281 -0.08392771
 -0.0799726  -0.07601749 -0.06810728 -0.06415217 -0.06019706 -0.05624195
 -0.05228685 -0.04833174 -0.04437663 -0.04042152 -0.03646642 -0.03251131
 -0.0285562  -0.0246011  -0.02064599 -0.01669088 -0.01273577 -0.00878067
 -0.00482556 -0.00087045  0.00308466  0.00703976  0.01494998  0.0167319
  0.01890508  0.02068701  0.02286019  0.0268153   0.03077041  0.03255233
  0.03472551  0.03868062  0.04046254  0.04263573  0.04659084  0.04837276
  0.05054594  0.05845616  0.06023808  0.06163437  0.06419319  0.06636637
  0.07427659  0.07605851  0.0774548   0.07823169  0.08140991  0.08614191
  0.09009702  0.09583405  0.09800723  0.10196234  0.10415379  0.10514055
  0.10591745  0.1081089   0.112064    0.11382766  0.11560958  0.11601911
  0.11778277  0.11997422  0.12392933  0.12964809  0.13183954  0.1336032
  0.13934023  0.13974976  0.14073652  0.14151341  0.14370486  0.14469162
  0.14765997  0.15161508  0.15260184  0.15337873  0.15655695  0.15733384
  0.15952529  0.16702598  0.16743551  0.17315427  0.1741593   0.17534572
  0.1788913   0.18206951  0.1842427   0.1881978   0.18997973  0.20006313
  0.20303147  0.20401823  0.20797334  0.21094169  0.21192845  0.21489679
  0.21588356  0.21766548  0.22379377  0.22458893  0.22774888  0.22854404
  0.2295308   0.23645425  0.24040936  0.24436447  0.24576076  0.24752441
  0.24831957  0.25049276  0.25147952  0.25543463  0.25622979  0.25721655
  0.25762608  0.26729995  0.26809511  0.2694914   0.27125506  0.27205022
  0.27344651  0.27521017  0.27916527  0.27996043  0.28312038  0.28707549
  0.28787065  0.29103059  0.29281252  0.29322205  0.2949857   0.29578086
  0.29676762  0.29894081  0.30289592  0.30369108  0.30467784  0.30685102
  0.30863295  0.30904248  0.30981937  0.31080613  0.31377448  0.31476124
  0.31871635  0.32445337  0.3248629   0.32881801  0.33137683  0.33236359
  0.3363187   0.33672823  0.3402738   0.34068333  0.34422891  0.34463844
  0.35254866  0.35609423  0.36400445  0.36441398  0.37015101  0.37191466
  0.38377998  0.38418951  0.3916902   0.39209973  0.39388165  0.39960041
  0.40000994  0.40355552  0.40396505  0.40970208  0.41187527  0.41761229
  0.42333106  0.42374059  0.42552251  0.42728616  0.42769569  0.43124127
  0.43343272  0.43519638  0.43560591  0.43738783  0.43915149  0.43956102
  0.44529805  0.44925315  0.45142634  0.45538145  0.45716337  0.45933655
  0.46189537  0.46507358  0.46724677  0.47120187  0.47515698  0.47911209
  0.48089401  0.4870223   0.49097741  0.49493252  0.49888763  0.50284273
  0.50679784  0.50857976  0.51075295  0.51253487  0.51470806  0.51648998
  0.51866316  0.52044508  0.52261827  0.52440019  0.52657338  0.5283553
  0.53231041  0.54417573  0.54813084  0.55208594  0.55604105  0.55999616
  0.56395126  0.56790637  0.57186148  0.57581659  0.57977169  0.59163702
  0.59559212  0.59954723  0.60350234  0.60745745  0.61141255  0.62327787
  0.63118809  0.6351432   0.6390983   0.64305341  0.64700852]

  warnings.warn(

2022-11-03 10:50:40,471:INFO:Calculating mean and std
2022-11-03 10:50:40,471:INFO:Creating metrics dataframe
2022-11-03 10:50:40,488:INFO:Uploading results into container
2022-11-03 10:50:40,488:INFO:Uploading model into container now
2022-11-03 10:50:40,488:INFO:master_model_container: 14
2022-11-03 10:50:40,488:INFO:display_container: 2
2022-11-03 10:50:40,488:INFO:OrthogonalMatchingPursuit()
2022-11-03 10:50:40,488:INFO:create_model() successfully completed......................................
2022-11-03 10:50:40,767:ERROR:create_model() for OrthogonalMatchingPursuit() raised an exception or returned all 0.0:
2022-11-03 10:50:40,767:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:50:40,767:INFO:Initializing Bayesian Ridge
2022-11-03 10:50:40,767:INFO:Total runtime is 2.0809565424919128 minutes
2022-11-03 10:50:40,767:INFO:SubProcess create_model() called ==================================
2022-11-03 10:50:40,767:INFO:Initializing create_model()
2022-11-03 10:50:40,767:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:50:40,767:INFO:Checking exceptions
2022-11-03 10:50:40,782:INFO:Importing libraries
2022-11-03 10:50:40,782:INFO:Copying training dataset
2022-11-03 10:50:40,782:INFO:Defining folds
2022-11-03 10:50:40,782:INFO:Declaring metric variables
2022-11-03 10:50:40,782:INFO:Importing untrained model
2022-11-03 10:50:40,798:INFO:Bayesian Ridge Imported successfully
2022-11-03 10:50:40,798:INFO:Starting cross validation
2022-11-03 10:50:40,798:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:50:44,964:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.21227549 -0.16979526 -0.16248269 -0.15814182 -0.148695   -0.14604559
 -0.1354599  -0.13495242 -0.13269035 -0.12969945 -0.12648436 -0.12565762
 -0.12504077 -0.11881392 -0.1138547  -0.11259228 -0.10968298 -0.10940607
 -0.10903423 -0.10761337 -0.10424113 -0.10103071 -0.10076058 -0.09936239
 -0.09883419 -0.09246691 -0.08635161 -0.07957852 -0.07738912 -0.07113382
 -0.06989331 -0.0648136  -0.06219308 -0.06019977 -0.05859338 -0.05752304
 -0.05441505 -0.05098917 -0.05092668 -0.04957223 -0.04911774 -0.04892642
 -0.04791445 -0.04715452 -0.04535142 -0.04439641 -0.04244158 -0.0418049
 -0.03945525 -0.03569171 -0.0355503  -0.03515793 -0.03215923 -0.03127173
 -0.02871115 -0.02866525 -0.02810685 -0.02388449 -0.0195057  -0.01917767
 -0.01813805 -0.01480004 -0.01362827 -0.01270768 -0.00971886 -0.00604656
  0.00384204  0.00578375  0.00668997  0.00768284  0.00884903  0.00895651
  0.00908771  0.01079894  0.01357178  0.01428505  0.01915865  0.01962805
  0.02200355  0.02511208  0.03007116  0.03155088  0.03158432  0.03222175
  0.03288548  0.0354411   0.03675283  0.03680052  0.03731022  0.03939888
  0.04100079  0.04182097  0.04494379  0.04638717  0.04671887  0.0490721
  0.05043847  0.05153484  0.05225532  0.05278701  0.05426552  0.05430581
  0.0572586   0.05909782  0.05987035  0.06031321  0.06521979  0.06589879
  0.06880847  0.06937248  0.07147313  0.07412608  0.07434124  0.07640248
  0.07711312  0.07744809  0.07758923  0.08120435  0.08261978  0.08300438
  0.08354662  0.08457828  0.0860562   0.08734639  0.08735955  0.08843292
  0.09155853  0.09316794  0.09333239  0.09601867  0.09625417  0.10003828
  0.10198587  0.10204447  0.10213357  0.10273507  0.10607644  0.10711661
  0.10733271  0.11044111  0.11051617  0.11101661  0.11302553  0.113189
  0.11457264  0.11648155  0.11727939  0.11803736  0.11863654  0.1209784
  0.12163569  0.12227456  0.12332219  0.12423448  0.12479538  0.12504823
  0.12607588  0.12805689  0.12842838  0.13003677  0.13390383  0.1371035
  0.13741414  0.13937935  0.14137834  0.14273202  0.14515222  0.14666545
  0.1470479   0.14707757  0.14946365  0.15115219  0.1531069   0.16060792
  0.16156291  0.16161266  0.16219954  0.16399858  0.16615832  0.17019499
  0.17120681  0.17349182  0.17413455  0.17433969  0.17605175  0.17605773
  0.1761313   0.17863581  0.17943517  0.18003697  0.18206248  0.18361307
  0.18601351  0.18808998  0.18830953  0.19089839  0.19091415  0.19122052
  0.19127922  0.19256462  0.19508117  0.19526381  0.19531284  0.19565106
  0.19835076  0.19928476  0.20035601  0.20131256  0.20192973  0.20390034
  0.20540372  0.20586786  0.20701077  0.20870813  0.20971621  0.21058104
  0.21085682  0.21162333  0.21273819  0.21555208  0.21557347  0.21579827
  0.21754291  0.21852374  0.22105868  0.22117604  0.22265782  0.22288097
  0.22583205  0.22879933  0.22890848  0.22935483  0.22968502  0.22980367
  0.2301059   0.23110363  0.23512584  0.23545152  0.23575327  0.23578831
  0.23579905  0.23632061  0.23686058  0.23950006  0.24184539  0.24444117
  0.24468332  0.2448084   0.24889804  0.25049182  0.25055685  0.25199083
  0.25364804  0.25385666  0.25448129  0.25538101  0.25912945  0.26045476
  0.26135476  0.26174372  0.26308825  0.26627056  0.26755318  0.26848043
  0.26874107  0.27086697  0.27295107  0.27684645  0.28011716  0.2855781
  0.28579244  0.28654433  0.28946583  0.29068927  0.29248845  0.29305778
  0.29519955  0.29658275  0.29662423  0.29687053  0.29738207  0.29887859
  0.30023371  0.30156307  0.30185497  0.3026742   0.3032752   0.3039306
  0.30439509  0.30466558  0.3049635   0.3050262   0.30558109  0.30602684
  0.30642785  0.30722117  0.30857096  0.30913474  0.30973441  0.3102648
  0.31163725  0.31202904  0.3140027   0.31452345  0.31591837  0.31696479
  0.31715321  0.31758768  0.31890605  0.32103467  0.32380583  0.32496005
  0.32555169  0.32713488  0.32789116  0.32793856  0.32879383  0.32882821
  0.32932467  0.33087285  0.33459198  0.33493423  0.33856507  0.34303474
  0.34309424  0.34360619  0.34457122  0.34983562  0.35053284  0.3525288
  0.35360078  0.35779929  0.35902956  0.36023525  0.36129358  0.36135767
  0.36248011  0.36390292  0.36607118  0.36907766  0.36982366  0.37197622
  0.37303057  0.37414033  0.37656402  0.37830399  0.37939631  0.38058976
  0.38231489  0.38262851  0.38311279  0.38405768  0.38496046  0.38558245
  0.38836869  0.39280362  0.39558421  0.39586417  0.40039495  0.40142032
  0.41276225  0.41356116  0.41627792  0.4174557   0.41866115  0.41984949
  0.42149484  0.42293609  0.42434459  0.4250872   0.42514113  0.42576175
  0.43053102  0.43269689  0.43395389  0.43596281  0.43727266  0.43966142
  0.44099411  0.4413308   0.44461153  0.44561589  0.44649677  0.44722692
  0.45279064  0.45364476  0.45516736  0.45988353  0.46307708  0.46556885
  0.46738412  0.46794694  0.47096677  0.47102897  0.47137947  0.47715607
  0.4775179   0.48259746  0.48417377  0.485767    0.4858794   0.48806939
  0.49111502  0.49249743  0.49373593  0.49622478  0.49685666  0.49939075
  0.50058323  0.50101597  0.50327041  0.5098589   0.51393917  0.5145728
  0.51569245  0.51599532  0.51969675  0.52146914  0.52407424  0.52607035
  0.52639091  0.52685654  0.5302436   0.53084244  0.53210153  0.53230131
  0.5346022   0.53616177  0.53678108  0.53690728  0.54265959  0.54413429
  0.54466358  0.5479149   0.54856617  0.55137984  0.55552336  0.56035264
  0.5625363   0.5627953   0.56459123  0.57474261  0.57617284  0.57655665
  0.57965737  0.58034524  0.58779389  0.59068084  0.59120077  0.59120527
  0.59835441  0.60013328  0.60146908  0.60155715  0.6034981   0.60471034
  0.60575079  0.60697887  0.6074538   0.60835725  0.60993852  0.61248278
  0.61302397  0.61814374  0.62091729  0.6230532   0.6280799   0.63018289
  0.633361    0.63507376  0.64259726  0.65226757  0.6526865   0.65484616
  0.66232078  0.66382788  0.66559962  0.66733172  0.66886297  0.67039306
  0.67224147  0.68126469  0.68133896  0.6830735   0.68403825  0.68801664
  0.69440353  0.69713229  0.69733077  0.70753831  0.72335201  0.72363208
  0.72742759]

  warnings.warn(

2022-11-03 10:50:45,011:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.79233620e-01 -1.68329052e-01 -1.54274179e-01 -1.45957929e-01
 -1.41834897e-01 -1.32772610e-01 -1.32711678e-01 -1.28974592e-01
 -1.25313361e-01 -1.22675210e-01 -1.22113020e-01 -1.16012272e-01
 -1.11064926e-01 -1.09458701e-01 -1.09428103e-01 -1.08376003e-01
 -1.07115103e-01 -1.05626526e-01 -1.02276763e-01 -1.01569563e-01
 -1.00185287e-01 -9.72084671e-02 -9.65286399e-02 -9.22939490e-02
 -8.80735878e-02 -8.17552274e-02 -7.79674279e-02 -7.41384639e-02
 -7.10039912e-02 -7.08647545e-02 -6.57540004e-02 -5.76474888e-02
 -5.63198394e-02 -5.53994459e-02 -5.25800450e-02 -5.20759258e-02
 -5.09082925e-02 -5.05572045e-02 -4.92485390e-02 -4.69210306e-02
 -4.51347833e-02 -4.38562935e-02 -4.31120705e-02 -4.20006692e-02
 -4.12551731e-02 -3.77891101e-02 -3.58042157e-02 -3.57870899e-02
 -3.30971546e-02 -3.19525714e-02 -3.10890001e-02 -2.72577646e-02
 -2.61387256e-02 -2.58566434e-02 -2.56697198e-02 -2.45050663e-02
 -2.36075365e-02 -2.33109485e-02 -1.69199157e-02 -1.02265643e-02
 -9.66789127e-03 -9.53531642e-03 -9.31751657e-03 -9.23878554e-03
 -9.02536177e-03 -7.19359361e-03 -2.91507177e-03 -1.82272351e-04
  3.53969686e-04  8.07507755e-04  8.65501875e-04  2.34937507e-03
  3.07885836e-03  3.34011447e-03  3.87878545e-03  4.12142180e-03
  6.61946548e-03  6.88693971e-03  1.30949882e-02  1.40684891e-02
  1.58868848e-02  1.92879410e-02  2.19907022e-02  2.26062662e-02
  2.53501598e-02  2.74698884e-02  2.83611609e-02  2.89317944e-02
  2.90077450e-02  3.00912398e-02  3.09446413e-02  3.15501684e-02
  3.18362960e-02  3.22806780e-02  3.30880142e-02  3.43096979e-02
  3.50700054e-02  3.79923819e-02  3.83993829e-02  3.90831961e-02
  4.07144962e-02  4.12725739e-02  4.26442821e-02  4.41777820e-02
  4.51483133e-02  4.62491237e-02  4.67418685e-02  5.69645398e-02
  5.77095540e-02  6.31560374e-02  6.32972679e-02  6.36754474e-02
  6.52919392e-02  6.79070205e-02  7.11195628e-02  7.34203305e-02
  7.58984538e-02  7.89706905e-02  7.91055053e-02  8.10904495e-02
  8.28640694e-02  8.37821128e-02  8.87088990e-02  9.04638615e-02
  9.24896029e-02  9.29328193e-02  9.38980768e-02  9.43751463e-02
  9.64198678e-02  9.78557086e-02  9.84871040e-02  9.95650000e-02
  1.00353932e-01  1.00839598e-01  1.01140121e-01  1.01882689e-01
  1.02567425e-01  1.03148895e-01  1.03213329e-01  1.05449780e-01
  1.06704233e-01  1.06892758e-01  1.07117391e-01  1.08960070e-01
  1.09932242e-01  1.10678065e-01  1.10945733e-01  1.13870888e-01
  1.14120856e-01  1.14657588e-01  1.17293764e-01  1.17702753e-01
  1.17748210e-01  1.18541374e-01  1.18769406e-01  1.21743775e-01
  1.25266920e-01  1.25711872e-01  1.25910432e-01  1.27318433e-01
  1.28354793e-01  1.29262811e-01  1.30146482e-01  1.33967904e-01
  1.34337424e-01  1.35172827e-01  1.37096376e-01  1.37952907e-01
  1.41051114e-01  1.41833530e-01  1.41986758e-01  1.42346611e-01
  1.45731764e-01  1.47182310e-01  1.47461404e-01  1.51214607e-01
  1.53150951e-01  1.54459924e-01  1.56829821e-01  1.57407722e-01
  1.64098519e-01  1.68195529e-01  1.68405562e-01  1.68480189e-01
  1.68987789e-01  1.69506623e-01  1.69954956e-01  1.74768427e-01
  1.79091698e-01  1.79879260e-01  1.82053746e-01  1.84376510e-01
  1.84802909e-01  1.85414593e-01  1.85445574e-01  1.86114882e-01
  1.87300866e-01  1.90303443e-01  1.91353366e-01  1.91845656e-01
  1.94297246e-01  1.94948169e-01  1.95679089e-01  1.96848847e-01
  1.98238144e-01  2.00205536e-01  2.03384025e-01  2.08550931e-01
  2.08688114e-01  2.09203644e-01  2.12834616e-01  2.13192795e-01
  2.13537225e-01  2.14090186e-01  2.15388585e-01  2.15691316e-01
  2.15792603e-01  2.17556950e-01  2.20552600e-01  2.20843682e-01
  2.20896206e-01  2.26526138e-01  2.26935005e-01  2.26939866e-01
  2.27495279e-01  2.27812255e-01  2.28359815e-01  2.32865710e-01
  2.33349617e-01  2.35839724e-01  2.35951354e-01  2.36771236e-01
  2.37870282e-01  2.38927259e-01  2.39468533e-01  2.42203227e-01
  2.42597497e-01  2.42746710e-01  2.42758645e-01  2.46603267e-01
  2.46834948e-01  2.48513425e-01  2.48529489e-01  2.49670987e-01
  2.49695743e-01  2.50427088e-01  2.51037328e-01  2.51122570e-01
  2.52353235e-01  2.53239097e-01  2.54012309e-01  2.55689346e-01
  2.55770341e-01  2.56169275e-01  2.57572719e-01  2.57583069e-01
  2.60487115e-01  2.60564075e-01  2.61697003e-01  2.67379860e-01
  2.70247332e-01  2.71955553e-01  2.73494216e-01  2.73537267e-01
  2.73558388e-01  2.74429543e-01  2.75011764e-01  2.75864462e-01
  2.76194627e-01  2.76635589e-01  2.77333342e-01  2.78373310e-01
  2.78554387e-01  2.80029010e-01  2.80162390e-01  2.81041280e-01
  2.81321336e-01  2.81325898e-01  2.81650143e-01  2.84617432e-01
  2.86859108e-01  2.91063329e-01  2.95778491e-01  3.02385239e-01
  3.03132947e-01  3.04011661e-01  3.09793139e-01  3.10939388e-01
  3.15618569e-01  3.18477814e-01  3.18617285e-01  3.18688995e-01
  3.19735021e-01  3.21408654e-01  3.22566924e-01  3.22922671e-01
  3.23351697e-01  3.24949585e-01  3.25058880e-01  3.25886587e-01
  3.26086690e-01  3.28878529e-01  3.30455430e-01  3.31122676e-01
  3.31289105e-01  3.31481053e-01  3.33556348e-01  3.34664779e-01
  3.36356002e-01  3.38787436e-01  3.39022535e-01  3.39295380e-01
  3.40217988e-01  3.48657687e-01  3.49709897e-01  3.57784464e-01
  3.58149622e-01  3.61824304e-01  3.63453892e-01  3.64437916e-01
  3.66664433e-01  3.67484524e-01  3.69610081e-01  3.70095642e-01
  3.72824837e-01  3.73192637e-01  3.74968134e-01  3.78395091e-01
  3.79462786e-01  3.80339660e-01  3.80490836e-01  3.81200719e-01
  3.82761653e-01  3.85223907e-01  3.85859998e-01  3.88960239e-01
  3.89015726e-01  3.89929377e-01  3.96656373e-01  3.99354343e-01
  4.01591965e-01  4.01778280e-01  4.03479221e-01  4.06005007e-01
  4.06495488e-01  4.06941655e-01  4.08558109e-01  4.11200246e-01
  4.13515650e-01  4.14634802e-01  4.27224206e-01  4.27679299e-01
  4.29169719e-01  4.29217666e-01  4.30357033e-01  4.32925917e-01
  4.33205549e-01  4.34999128e-01  4.35297041e-01  4.36803393e-01
  4.40956826e-01  4.41200914e-01  4.41528511e-01  4.42413833e-01
  4.45356606e-01  4.48124279e-01  4.49433600e-01  4.52408297e-01
  4.53436648e-01  4.55615255e-01  4.55945874e-01  4.56962058e-01
  4.57167681e-01  4.60715753e-01  4.61916486e-01  4.62227012e-01
  4.62389394e-01  4.62446891e-01  4.62578772e-01  4.63269194e-01
  4.64840070e-01  4.65397378e-01  4.72382991e-01  4.73839355e-01
  4.74080221e-01  4.74515190e-01  4.74627659e-01  4.75118088e-01
  4.75842551e-01  4.75999408e-01  4.76377466e-01  4.77265471e-01
  4.81254545e-01  4.81634325e-01  4.83175769e-01  4.84467999e-01
  4.85346916e-01  4.88120421e-01  4.88228966e-01  4.92638055e-01
  4.93284084e-01  4.93613031e-01  4.93998237e-01  4.94911000e-01
  4.95008319e-01  4.99550996e-01  5.00390876e-01  5.01053866e-01
  5.01632576e-01  5.05455369e-01  5.07944315e-01  5.11482782e-01
  5.11990662e-01  5.14923208e-01  5.18686451e-01  5.19017994e-01
  5.19037398e-01  5.19470228e-01  5.24268788e-01  5.25572158e-01
  5.26746900e-01  5.33796013e-01  5.34678456e-01  5.40593803e-01
  5.42169473e-01  5.42231130e-01  5.46435927e-01  5.48097891e-01
  5.49940677e-01  5.51145805e-01  5.54210776e-01  5.55771301e-01
  5.58873859e-01  5.59696308e-01  5.62022668e-01  5.64533499e-01
  5.65253305e-01  5.68619584e-01  5.69860354e-01  5.71482825e-01
  5.71959159e-01  5.76037025e-01  5.79603581e-01  5.80707385e-01
  5.81720708e-01  5.81901599e-01  5.85153673e-01  5.85376233e-01
  5.90833850e-01  5.95153402e-01  5.97229616e-01  6.01590805e-01
  6.03254278e-01  6.03888247e-01  6.03899632e-01  6.06716167e-01
  6.11884871e-01  6.15049692e-01  6.21463516e-01  6.21699639e-01
  6.23669024e-01  6.25224540e-01  6.25407842e-01  6.26182434e-01
  6.27792002e-01  6.28878364e-01  6.29666411e-01  6.31897656e-01
  6.35570001e-01  6.36603522e-01  6.37210482e-01  6.39226681e-01
  6.41710709e-01  6.48064784e-01  6.48995189e-01  6.49289914e-01
  6.55736918e-01  6.59022266e-01  6.59270163e-01  6.61604942e-01
  6.64936937e-01  6.66875825e-01  6.78041392e-01  6.88857191e-01
  6.89091652e-01  6.90332520e-01  6.92332535e-01  7.00516966e-01
  7.03533247e-01  7.07922954e-01  7.08363818e-01  7.18854641e-01
  7.23286973e-01]

  warnings.warn(

2022-11-03 10:50:45,066:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.88495778e-01 -1.66805387e-01 -1.61900028e-01 -1.53058841e-01
 -1.47654750e-01 -1.46804812e-01 -1.40839819e-01 -1.40593167e-01
 -1.36269524e-01 -1.32319327e-01 -1.30119507e-01 -1.29588137e-01
 -1.28062164e-01 -1.26991294e-01 -1.23053514e-01 -1.22652936e-01
 -1.22631866e-01 -1.22611980e-01 -1.19581866e-01 -1.17957475e-01
 -1.16525025e-01 -1.09054156e-01 -1.06846124e-01 -1.03743465e-01
 -1.02427926e-01 -1.02046317e-01 -1.01573370e-01 -9.74013606e-02
 -9.61498525e-02 -9.16701525e-02 -8.93057131e-02 -8.77472950e-02
 -8.36351213e-02 -8.31841863e-02 -8.31058779e-02 -7.84478357e-02
 -7.76074990e-02 -7.31073731e-02 -7.02994763e-02 -6.56931530e-02
 -6.55008599e-02 -6.35502420e-02 -6.24709695e-02 -6.13923177e-02
 -6.09276834e-02 -6.03511939e-02 -5.44062199e-02 -5.26429508e-02
 -4.91882564e-02 -4.69756875e-02 -4.22441715e-02 -4.16740410e-02
 -3.96175094e-02 -3.46551047e-02 -2.86699278e-02 -2.82325280e-02
 -2.70246552e-02 -2.69906461e-02 -2.50836095e-02 -2.31652189e-02
 -2.31639746e-02 -2.29808774e-02 -1.98781773e-02 -1.70759493e-02
 -1.66011666e-02 -1.64957891e-02 -1.58299597e-02 -1.54588243e-02
 -1.47080043e-02 -1.41677345e-02 -1.29732611e-02 -1.25733848e-02
 -1.04311880e-02 -7.16517922e-03 -4.56934350e-03 -4.38412224e-03
 -4.06656498e-03 -2.71446964e-03  7.28875968e-04  7.74424108e-04
  6.24842361e-03  8.78651037e-03  1.06989580e-02  1.21910175e-02
  1.42756649e-02  1.63382250e-02  1.63813263e-02  1.72761111e-02
  1.93387746e-02  2.04036604e-02  2.04916963e-02  2.06029078e-02
  2.09091007e-02  2.21639717e-02  2.32413973e-02  2.41758887e-02
  2.55169625e-02  2.55491735e-02  2.94057260e-02  3.25167021e-02
  3.25642904e-02  3.49252228e-02  4.05844732e-02  4.13316163e-02
  4.15996888e-02  4.47156887e-02  4.52388187e-02  4.55989950e-02
  4.57531106e-02  4.57647607e-02  4.81357138e-02  5.20609806e-02
  5.33029279e-02  5.36792198e-02  5.45853697e-02  5.66465683e-02
  5.72973037e-02  5.79601281e-02  5.96867428e-02  6.21958098e-02
  6.30843259e-02  6.37866069e-02  6.82121944e-02  6.99073856e-02
  7.39348219e-02  7.45558724e-02  7.63020563e-02  7.64360208e-02
  7.75427362e-02  8.13282223e-02  8.21181466e-02  8.44756833e-02
  8.45145498e-02  8.59005307e-02  8.71213827e-02  8.77824447e-02
  8.78021343e-02  8.78051084e-02  8.80490426e-02  8.89803484e-02
  9.29849253e-02  9.64772879e-02  9.80229443e-02  1.02919550e-01
  1.06547988e-01  1.07923117e-01  1.09833815e-01  1.11413952e-01
  1.15327534e-01  1.16865848e-01  1.17792478e-01  1.19794892e-01
  1.20389225e-01  1.22594101e-01  1.25508539e-01  1.27351498e-01
  1.28352698e-01  1.29893617e-01  1.30017880e-01  1.30880932e-01
  1.31300751e-01  1.32600492e-01  1.34848803e-01  1.37290761e-01
  1.37667266e-01  1.37795980e-01  1.37870597e-01  1.38309178e-01
  1.39467221e-01  1.40798951e-01  1.40955182e-01  1.42920466e-01
  1.44596732e-01  1.45659974e-01  1.49127250e-01  1.51262980e-01
  1.53160029e-01  1.56444997e-01  1.56450469e-01  1.56975965e-01
  1.59458053e-01  1.60835801e-01  1.61399748e-01  1.64892501e-01
  1.65688178e-01  1.69213025e-01  1.69301379e-01  1.70749349e-01
  1.73325326e-01  1.73425516e-01  1.77274451e-01  1.78966451e-01
  1.83614639e-01  1.86219430e-01  1.86450042e-01  1.88616662e-01
  1.91200063e-01  1.95563118e-01  1.96504196e-01  1.96713929e-01
  1.99148977e-01  2.09154421e-01  2.09504689e-01  2.11112516e-01
  2.12123869e-01  2.13065854e-01  2.13291263e-01  2.16793507e-01
  2.18440508e-01  2.18441714e-01  2.19839062e-01  2.20302820e-01
  2.20425501e-01  2.21782852e-01  2.22096237e-01  2.22205387e-01
  2.23463538e-01  2.23754109e-01  2.24768080e-01  2.25725629e-01
  2.28497457e-01  2.30756431e-01  2.31734622e-01  2.36213818e-01
  2.37635207e-01  2.38259751e-01  2.40422211e-01  2.42199396e-01
  2.42711571e-01  2.42903759e-01  2.43659717e-01  2.44269709e-01
  2.44552077e-01  2.46594965e-01  2.50283956e-01  2.51194141e-01
  2.54794760e-01  2.58289662e-01  2.61197219e-01  2.62309791e-01
  2.62339188e-01  2.64909772e-01  2.65645985e-01  2.65918776e-01
  2.65949818e-01  2.67067412e-01  2.67634687e-01  2.72638128e-01
  2.74901066e-01  2.76184443e-01  2.76233020e-01  2.80032694e-01
  2.81155376e-01  2.82560096e-01  2.83412732e-01  2.83951187e-01
  2.86530220e-01  2.88470908e-01  2.89736477e-01  2.92537621e-01
  2.93069822e-01  2.94956504e-01  2.95983727e-01  2.97023882e-01
  2.97411658e-01  2.97646956e-01  2.98737691e-01  3.00785443e-01
  3.00875691e-01  3.04513074e-01  3.09387423e-01  3.10627185e-01
  3.10740817e-01  3.11967554e-01  3.12968688e-01  3.13741911e-01
  3.18513746e-01  3.19074135e-01  3.20430442e-01  3.21098849e-01
  3.22750774e-01  3.22914027e-01  3.24301157e-01  3.27783185e-01
  3.29319850e-01  3.30482169e-01  3.33933411e-01  3.34372500e-01
  3.34555183e-01  3.34934464e-01  3.35282743e-01  3.40770646e-01
  3.40965455e-01  3.41202225e-01  3.45959545e-01  3.46450738e-01
  3.48185007e-01  3.48335659e-01  3.49126313e-01  3.51311496e-01
  3.53820930e-01  3.56222514e-01  3.56651129e-01  3.59856563e-01
  3.60542213e-01  3.61764166e-01  3.63581607e-01  3.65463755e-01
  3.66804488e-01  3.67830966e-01  3.71790068e-01  3.71903765e-01
  3.72906516e-01  3.73352539e-01  3.73994395e-01  3.75811600e-01
  3.76414900e-01  3.78919284e-01  3.79898587e-01  3.80330461e-01
  3.81738700e-01  3.82778747e-01  3.84278199e-01  3.86292191e-01
  3.87131309e-01  3.90082437e-01  3.91253270e-01  3.91483996e-01
  3.91534452e-01  3.93353806e-01  3.93582926e-01  3.94597378e-01
  3.96089107e-01  4.02757605e-01  4.05141001e-01  4.06788520e-01
  4.08162148e-01  4.08390411e-01  4.09992452e-01  4.10685298e-01
  4.11032788e-01  4.12747049e-01  4.12907310e-01  4.13302521e-01
  4.15400625e-01  4.16413668e-01  4.20938765e-01  4.21034672e-01
  4.21902803e-01  4.26272625e-01  4.27643005e-01  4.28215802e-01
  4.28314174e-01  4.32075922e-01  4.32835615e-01  4.41658871e-01
  4.42279181e-01  4.42281658e-01  4.43672809e-01  4.47236604e-01
  4.48352645e-01  4.48792954e-01  4.49909887e-01  4.49972645e-01
  4.51715483e-01  4.53469052e-01  4.53960016e-01  4.54606580e-01
  4.59086792e-01  4.59259738e-01  4.60862820e-01  4.63460342e-01
  4.67661784e-01  4.68761267e-01  4.69727712e-01  4.72157316e-01
  4.72865710e-01  4.73432370e-01  4.73645745e-01  4.73963072e-01
  4.75771794e-01  4.77066979e-01  4.79068255e-01  4.79300190e-01
  4.80747803e-01  4.82337324e-01  4.83708916e-01  4.93374941e-01
  4.93599977e-01  4.93624438e-01  4.94577986e-01  4.94759716e-01
  4.94793079e-01  4.98202015e-01  4.99330120e-01  5.03714280e-01
  5.07793631e-01  5.09470259e-01  5.09832894e-01  5.10769960e-01
  5.12850551e-01  5.16122071e-01  5.17158314e-01  5.17275371e-01
  5.20258448e-01  5.20415462e-01  5.21162258e-01  5.22114483e-01
  5.22587739e-01  5.26063820e-01  5.26627026e-01  5.26852482e-01
  5.30334940e-01  5.34127675e-01  5.34827051e-01  5.35311819e-01
  5.35530486e-01  5.36184152e-01  5.36378670e-01  5.36396146e-01
  5.36537118e-01  5.36966493e-01  5.39970126e-01  5.43221276e-01
  5.44169953e-01  5.44818715e-01  5.49032696e-01  5.52353731e-01
  5.57584122e-01  5.59537174e-01  5.60107509e-01  5.62863049e-01
  5.63577156e-01  5.66089214e-01  5.68375039e-01  5.72757534e-01
  5.73742888e-01  5.79044786e-01  5.79994561e-01  5.82778411e-01
  5.82843485e-01  5.89090454e-01  5.90929878e-01  5.93647268e-01
  5.94794635e-01  5.96815198e-01  5.98197896e-01  6.02976139e-01
  6.05393997e-01  6.06623601e-01  6.08427648e-01  6.08454350e-01
  6.09536389e-01  6.10774002e-01  6.13240473e-01  6.13398987e-01
  6.16101557e-01  6.16437932e-01  6.16882449e-01  6.17405889e-01
  6.18167871e-01  6.21101745e-01  6.23073774e-01  6.26498996e-01
  6.27189984e-01  6.27282229e-01  6.28053144e-01  6.29613823e-01
  6.30394683e-01  6.32515819e-01  6.33690078e-01  6.36395658e-01
  6.36640190e-01  6.44390285e-01  6.46414850e-01  6.54867178e-01
  6.54877980e-01  6.59219556e-01  6.65929644e-01  6.69964522e-01
  6.70409470e-01  6.71810851e-01  6.74661176e-01  6.78749214e-01
  6.79034912e-01  6.81431603e-01  6.85593641e-01  7.02015623e-01
  7.08000439e-01  7.12136130e-01  7.22105155e-01  7.35327976e-01
  7.50577087e-01]

  warnings.warn(

2022-11-03 10:50:45,101:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.73664340e-01 -1.70814324e-01 -1.64020826e-01 -1.59568552e-01
 -1.56759485e-01 -1.56572998e-01 -1.47049558e-01 -1.46140434e-01
 -1.42938694e-01 -1.42757081e-01 -1.42585535e-01 -1.40252852e-01
 -1.39719816e-01 -1.33749383e-01 -1.31846706e-01 -1.31097741e-01
 -1.29082928e-01 -1.27627764e-01 -1.18358645e-01 -1.16094971e-01
 -1.15913370e-01 -1.07697129e-01 -1.03855689e-01 -9.54559122e-02
 -9.46609175e-02 -9.04481547e-02 -8.97997858e-02 -8.80244018e-02
 -8.59615299e-02 -8.25919885e-02 -7.96822202e-02 -7.96802980e-02
 -7.67308565e-02 -7.52825226e-02 -7.30284558e-02 -6.93137885e-02
 -6.88601055e-02 -6.68759931e-02 -6.61213514e-02 -6.11377306e-02
 -6.03764605e-02 -5.90491011e-02 -5.74285069e-02 -5.49990001e-02
 -5.49952877e-02 -5.27649771e-02 -5.01681689e-02 -4.92270592e-02
 -4.51885557e-02 -4.31539107e-02 -3.49390051e-02 -3.30461694e-02
 -2.76527110e-02 -2.72452254e-02 -2.56473823e-02 -2.54819364e-02
 -2.51270713e-02 -2.40759493e-02 -2.28232803e-02 -2.24889732e-02
 -1.93119378e-02 -1.83636635e-02 -1.66590051e-02 -1.59320623e-02
 -1.54651989e-02 -1.24381326e-02 -9.56092298e-03 -7.58825968e-03
 -6.86525024e-03 -1.44451808e-03  7.12783208e-04  2.00566164e-03
  2.97567738e-03  3.35389953e-03  5.89305645e-03  8.16187350e-03
  8.43528645e-03  9.72709864e-03  1.13950541e-02  1.15813759e-02
  1.28305232e-02  1.59379270e-02  2.23512793e-02  2.50469032e-02
  2.79394314e-02  3.04146143e-02  3.12690544e-02  3.22761134e-02
  3.46237698e-02  4.02981601e-02  4.08030714e-02  4.12989732e-02
  4.26661612e-02  4.26952350e-02  4.27179821e-02  4.38874147e-02
  4.85875044e-02  4.86571118e-02  4.92956227e-02  6.04244893e-02
  6.37634180e-02  6.45494855e-02  6.66332900e-02  6.94312375e-02
  6.95010857e-02  7.01901692e-02  7.02003670e-02  7.02091033e-02
  7.62026555e-02  7.81522911e-02  8.24135138e-02  8.42242464e-02
  8.49920619e-02  8.54027009e-02  8.63269035e-02  8.63440386e-02
  8.63875718e-02  8.67068806e-02  8.93274056e-02  9.02433464e-02
  9.04727073e-02  9.13807467e-02  9.25175889e-02  9.32394510e-02
  9.39121285e-02  9.44242883e-02  9.70278546e-02  9.71646602e-02
  9.89138626e-02  1.01614350e-01  1.02337100e-01  1.04625680e-01
  1.06636233e-01  1.07414176e-01  1.09422165e-01  1.11438254e-01
  1.12853650e-01  1.15166407e-01  1.16008269e-01  1.17284138e-01
  1.19893063e-01  1.20116687e-01  1.22217773e-01  1.23280518e-01
  1.24887338e-01  1.25038144e-01  1.26711473e-01  1.27604340e-01
  1.29218519e-01  1.31150451e-01  1.31669622e-01  1.31879320e-01
  1.32132083e-01  1.36153407e-01  1.38277800e-01  1.39081316e-01
  1.39254159e-01  1.39422949e-01  1.40648417e-01  1.41725232e-01
  1.42666521e-01  1.42969416e-01  1.46241767e-01  1.47185136e-01
  1.48946999e-01  1.49613594e-01  1.49689292e-01  1.49836484e-01
  1.50095474e-01  1.57620776e-01  1.58647016e-01  1.60715940e-01
  1.61019400e-01  1.62508017e-01  1.65696432e-01  1.69040988e-01
  1.69054588e-01  1.69184008e-01  1.69730305e-01  1.70159999e-01
  1.71993608e-01  1.72169218e-01  1.72251082e-01  1.72569404e-01
  1.73480676e-01  1.73866977e-01  1.74206081e-01  1.74529911e-01
  1.76022340e-01  1.78308432e-01  1.78399059e-01  1.78403133e-01
  1.78454186e-01  1.79815124e-01  1.80684780e-01  1.80881324e-01
  1.81898720e-01  1.81978623e-01  1.82153297e-01  1.83483411e-01
  1.87163822e-01  1.87253913e-01  1.89018899e-01  1.89761977e-01
  1.91673814e-01  1.93428665e-01  1.97741652e-01  1.99335688e-01
  1.99799623e-01  2.00200437e-01  2.00931069e-01  2.01070696e-01
  2.03223901e-01  2.03506860e-01  2.03601179e-01  2.05512409e-01
  2.05958243e-01  2.06413689e-01  2.08590385e-01  2.10706166e-01
  2.11472660e-01  2.11509192e-01  2.12120966e-01  2.14424954e-01
  2.14937532e-01  2.16160286e-01  2.16535790e-01  2.17926320e-01
  2.18793366e-01  2.20242337e-01  2.21781876e-01  2.26152259e-01
  2.26450205e-01  2.26573621e-01  2.27672212e-01  2.28283999e-01
  2.30910168e-01  2.31628948e-01  2.32452302e-01  2.33750344e-01
  2.36506226e-01  2.40471673e-01  2.40815148e-01  2.41228668e-01
  2.43096265e-01  2.43115528e-01  2.44190218e-01  2.44364651e-01
  2.48835018e-01  2.50998917e-01  2.51693541e-01  2.54980363e-01
  2.55461847e-01  2.57261341e-01  2.59759196e-01  2.60939638e-01
  2.63320793e-01  2.64500570e-01  2.64923153e-01  2.66443381e-01
  2.71127899e-01  2.71412954e-01  2.71507005e-01  2.78891134e-01
  2.82904186e-01  2.82972540e-01  2.83579225e-01  2.86360307e-01
  2.87622010e-01  2.89048011e-01  2.90011278e-01  2.90083582e-01
  2.90129236e-01  2.90291005e-01  2.92312943e-01  2.92970766e-01
  2.95717346e-01  2.96713829e-01  2.97909565e-01  3.03623371e-01
  3.06269859e-01  3.08033476e-01  3.08414389e-01  3.09678594e-01
  3.11147198e-01  3.11911543e-01  3.11978779e-01  3.14439619e-01
  3.15789894e-01  3.16962333e-01  3.17437982e-01  3.18344364e-01
  3.19797806e-01  3.20262619e-01  3.23179165e-01  3.23597402e-01
  3.23604841e-01  3.24595690e-01  3.26166655e-01  3.27703576e-01
  3.27844042e-01  3.28312553e-01  3.28538634e-01  3.36840292e-01
  3.40879322e-01  3.41471057e-01  3.41712914e-01  3.44557111e-01
  3.44854788e-01  3.45894123e-01  3.47768317e-01  3.50961851e-01
  3.53015833e-01  3.53270621e-01  3.54228133e-01  3.54497888e-01
  3.55586085e-01  3.56683778e-01  3.61235928e-01  3.64737063e-01
  3.68653879e-01  3.70375216e-01  3.71027969e-01  3.72025231e-01
  3.73412362e-01  3.77953542e-01  3.77976188e-01  3.79042023e-01
  3.79251730e-01  3.80580394e-01  3.82860784e-01  3.86085218e-01
  3.86578398e-01  3.90502521e-01  3.92715963e-01  3.93769557e-01
  3.94425710e-01  3.95402423e-01  3.97439536e-01  3.98523129e-01
  3.99368323e-01  4.06660130e-01  4.07075187e-01  4.07218316e-01
  4.10882697e-01  4.11674588e-01  4.12997254e-01  4.16209248e-01
  4.19683425e-01  4.20529721e-01  4.24225152e-01  4.25360128e-01
  4.27188909e-01  4.28191740e-01  4.28830327e-01  4.29675973e-01
  4.33646170e-01  4.35925129e-01  4.42518125e-01  4.47068303e-01
  4.47759284e-01  4.48971355e-01  4.51832767e-01  4.53350163e-01
  4.53919635e-01  4.54193477e-01  4.54316284e-01  4.54694917e-01
  4.54747044e-01  4.56774334e-01  4.61276057e-01  4.61767933e-01
  4.62710475e-01  4.62769360e-01  4.64372754e-01  4.65271044e-01
  4.67811229e-01  4.68413908e-01  4.68847018e-01  4.70804184e-01
  4.71687890e-01  4.75996372e-01  4.76981520e-01  4.79693878e-01
  4.81212287e-01  4.83405009e-01  4.83553060e-01  4.92084656e-01
  4.92382027e-01  4.94307109e-01  4.95095438e-01  4.96445832e-01
  4.97117625e-01  4.98663668e-01  4.99299358e-01  5.02243974e-01
  5.05060329e-01  5.07697288e-01  5.08021960e-01  5.08828665e-01
  5.10455391e-01  5.14816295e-01  5.18129191e-01  5.18502293e-01
  5.18598148e-01  5.18630966e-01  5.24662218e-01  5.26072281e-01
  5.29597816e-01  5.33008599e-01  5.34063547e-01  5.34660287e-01
  5.35280998e-01  5.38345078e-01  5.40659644e-01  5.41566907e-01
  5.42146354e-01  5.42512548e-01  5.42669563e-01  5.47751097e-01
  5.47843433e-01  5.47843655e-01  5.49228796e-01  5.50553394e-01
  5.51726552e-01  5.51888538e-01  5.54087474e-01  5.54761183e-01
  5.54986847e-01  5.56343235e-01  5.56453971e-01  5.56511045e-01
  5.56524507e-01  5.63802173e-01  5.64028442e-01  5.66338069e-01
  5.68109125e-01  5.70919882e-01  5.79981358e-01  5.80880682e-01
  5.82176785e-01  5.86822278e-01  5.89466211e-01  5.99225196e-01
  6.00752809e-01  6.03120296e-01  6.05171867e-01  6.06132333e-01
  6.06907697e-01  6.08126415e-01  6.09400639e-01  6.11355450e-01
  6.12567356e-01  6.17257506e-01  6.18719097e-01  6.22064595e-01
  6.26608232e-01  6.27199248e-01  6.30656050e-01  6.31989246e-01
  6.33124324e-01  6.37990714e-01  6.39894833e-01  6.42047097e-01
  6.43866281e-01  6.44417223e-01  6.47033056e-01  6.49105745e-01
  6.54532021e-01  6.55644469e-01  6.57614511e-01  6.61456105e-01
  6.61907096e-01  6.69360983e-01  6.74256889e-01  6.74448058e-01
  6.74541296e-01  6.76865112e-01  6.77968106e-01  6.79128799e-01
  6.86025224e-01  6.89161844e-01  6.89633444e-01  7.00518518e-01
  7.13533633e-01  7.14766633e-01  7.19807607e-01  7.22392720e-01
  7.53981484e-01  7.55879948e-01  7.59369222e-01  7.60767498e-01
  7.70543243e-01]

  warnings.warn(

2022-11-03 10:50:45,146:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.1858178  -0.18523791 -0.14937594 -0.14914649 -0.14573196 -0.1404447
 -0.13721105 -0.13453853 -0.13436871 -0.13354472 -0.13127526 -0.13119366
 -0.13092752 -0.12969674 -0.12910107 -0.12776109 -0.12684199 -0.12548519
 -0.12522214 -0.12282995 -0.12207035 -0.12143129 -0.11759893 -0.11664442
 -0.11486781 -0.1145079  -0.11110178 -0.10076064 -0.09968907 -0.09941117
 -0.09692984 -0.09563822 -0.09393579 -0.09089733 -0.09025844 -0.08929462
 -0.08852075 -0.08538528 -0.0830378  -0.08065005 -0.07972137 -0.07933722
 -0.07760436 -0.07552354 -0.07509455 -0.07482642 -0.07262352 -0.07261618
 -0.07057725 -0.06823385 -0.06271807 -0.06265511 -0.05528378 -0.05447057
 -0.05349532 -0.05319307 -0.05242617 -0.05065778 -0.05026382 -0.04936213
 -0.04843971 -0.04544454 -0.04462167 -0.04402641 -0.039013   -0.03891393
 -0.03839533 -0.03804097 -0.03738363 -0.03726992 -0.03452361 -0.03412122
 -0.03265722 -0.03216237 -0.03207732 -0.03148624 -0.02605154 -0.02464608
 -0.0244059  -0.02228761 -0.02124923 -0.02084609 -0.01678861 -0.0159481
 -0.01584593 -0.01114945 -0.01106202 -0.0097737  -0.00926966 -0.00607298
 -0.0032489  -0.00311556  0.0011966   0.00347783  0.00481911  0.00622837
  0.00715118  0.0088141   0.01042052  0.01683472  0.02110244  0.02343226
  0.02361128  0.02582905  0.02632247  0.02895879  0.03231646  0.03418443
  0.03718327  0.03914202  0.03931088  0.04008805  0.04092982  0.04102043
  0.04128496  0.041493    0.04230755  0.04284057  0.04645434  0.0486542
  0.04980712  0.05096086  0.05532804  0.05604611  0.05633605  0.05672889
  0.06047408  0.06110628  0.06275225  0.0631849   0.06329674  0.06343779
  0.06426895  0.06624191  0.06730016  0.07051501  0.07376407  0.07524547
  0.07744494  0.07858493  0.07996451  0.08045754  0.08049691  0.08104448
  0.08248635  0.08316045  0.08656741  0.08929494  0.09055454  0.09249305
  0.0980747   0.10032469  0.10084151  0.10569561  0.10585757  0.10938076
  0.10992067  0.11262224  0.11270431  0.11423128  0.11550078  0.11590901
  0.1162997   0.11840178  0.1187062   0.12052569  0.12122989  0.12323168
  0.12546425  0.12818721  0.1297677   0.13030672  0.13262424  0.13425498
  0.13430294  0.13573982  0.13641558  0.14036364  0.14105778  0.14257581
  0.14443208  0.1451438   0.14554668  0.14751808  0.1478098   0.14947895
  0.14968408  0.14980309  0.15063599  0.1518148   0.15257096  0.15426288
  0.15463609  0.15716176  0.15784578  0.16001129  0.16393243  0.16571392
  0.16722751  0.16838325  0.16998471  0.1703517   0.17117556  0.17217903
  0.17240748  0.17625492  0.17684216  0.17690839  0.18030446  0.18113094
  0.18331327  0.18589158  0.18614192  0.18642136  0.18691874  0.18774717
  0.18811042  0.18836771  0.18953461  0.19010601  0.19041402  0.19430857
  0.19543482  0.19719302  0.19785586  0.19908377  0.20207388  0.20313803
  0.20514579  0.20527324  0.20800751  0.20897829  0.21019296  0.21204339
  0.21243681  0.21633156  0.21832407  0.21940134  0.2202374   0.22042219
  0.22048616  0.2231556   0.22338566  0.22366325  0.22466017  0.22509349
  0.22668439  0.22741206  0.22800616  0.22878716  0.2298672   0.23143313
  0.23915472  0.23937037  0.24034758  0.24161179  0.2418309   0.24354022
  0.24462789  0.24514249  0.24666569  0.24999759  0.25087147  0.25140641
  0.25789084  0.25891233  0.26135589  0.26138788  0.26239196  0.26350975
  0.26474987  0.26603216  0.26892679  0.27016483  0.27213607  0.27400293
  0.27688352  0.27945097  0.28012725  0.2812499   0.28377739  0.28456471
  0.28550041  0.286015    0.2889251   0.30098539  0.30146453  0.30343566
  0.30499397  0.30767389  0.30790123  0.30800252  0.30980109  0.31651589
  0.31668931  0.32741008  0.32823004  0.33054109  0.33612764  0.33616774
  0.338628    0.33949338  0.34603906  0.34660968  0.34819335  0.35031108
  0.35054113  0.35170571  0.35222331  0.35237258  0.3532793   0.35415048
  0.35460197  0.35561029  0.35767238  0.35774991  0.35855347  0.35988266
  0.36611719  0.36856364  0.37045724  0.37256177  0.3726342   0.37574512
  0.37963047  0.38161112  0.38183544  0.3821105   0.38326428  0.38425593
  0.38680349  0.38858591  0.39306669  0.39646322  0.39771011  0.39860081
  0.39959852  0.4021048   0.40227521  0.40405581  0.40507262  0.40515814
  0.40603919  0.40764424  0.40884633  0.41361352  0.41446585  0.4163235
  0.42071407  0.42512721  0.42574813  0.42826042  0.4311314   0.43225811
  0.43276196  0.43432739  0.43744571  0.43966707  0.44217951  0.44235384
  0.44273951  0.44390442  0.44576281  0.4462287   0.44632612  0.44926386
  0.45131508  0.45199419  0.45758674  0.46072136  0.46130056  0.46214463
  0.46323438  0.46340749  0.46382518  0.46425572  0.46509822  0.4659724
  0.46827697  0.46834908  0.46853706  0.46962183  0.47206802  0.47255736
  0.47379159  0.47402877  0.47435098  0.47438967  0.47517591  0.47745281
  0.48000922  0.48104341  0.48415926  0.48609407  0.48738378  0.49336065
  0.49558294  0.4971712   0.5001828   0.50048054  0.50084028  0.50265703
  0.50504821  0.50884653  0.50938956  0.51257332  0.51413014  0.51881469
  0.5208183   0.52187063  0.52283678  0.52429197  0.52672069  0.52837517
  0.53076148  0.53234528  0.53418693  0.53720535  0.54115434  0.54283856
  0.54318081  0.54873985  0.55020396  0.55067402  0.5510734   0.55119183
  0.55238949  0.55350371  0.55777865  0.56033847  0.56129932  0.56280916
  0.56301779  0.563764    0.5665566   0.56681553  0.5670815   0.56716693
  0.56843611  0.56944455  0.56984676  0.57163116  0.57246815  0.57264103
  0.57367222  0.57590034  0.57641405  0.58037232  0.58061255  0.58347663
  0.58579257  0.58998954  0.59138033  0.59633475  0.6058484   0.6064259
  0.6067866   0.60890367  0.61384315  0.61526394  0.61554502  0.61569841
  0.6181153   0.6198661   0.62346744  0.62476079  0.62496768  0.64207761
  0.64245791  0.64394416  0.64445997  0.6467781   0.64915811  0.65026686
  0.65882938  0.66207575  0.66597326  0.66694228  0.66745187  0.66798766
  0.6714852   0.67192078  0.6759338   0.68018889  0.68349563  0.68527653
  0.68757428  0.69048485  0.6984434   0.70033552  0.70173735  0.70884698
  0.71720177]

  warnings.warn(

2022-11-03 10:50:45,163:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.24341827 -0.17857274 -0.17691416 -0.17296837 -0.17041277 -0.16946066
 -0.16230322 -0.16206649 -0.15652817 -0.15579386 -0.15540803 -0.15415235
 -0.14943975 -0.14843484 -0.14017239 -0.13828496 -0.13267954 -0.13085668
 -0.12639932 -0.12604202 -0.12534788 -0.12351528 -0.12272938 -0.11795286
 -0.11779562 -0.11059087 -0.10808982 -0.10743168 -0.10345447 -0.1027219
 -0.09406842 -0.09327054 -0.09157913 -0.08962934 -0.08664557 -0.08489692
 -0.08380001 -0.08202565 -0.07454646 -0.07216751 -0.07041507 -0.0677622
 -0.06678023 -0.06544485 -0.06351672 -0.06235327 -0.06053988 -0.05745078
 -0.05555982 -0.05532408 -0.05244157 -0.05241235 -0.05127813 -0.05116598
 -0.05019253 -0.04825883 -0.04806802 -0.04804899 -0.04473671 -0.04151826
 -0.04125333 -0.03972189 -0.03930853 -0.03619789 -0.03565441 -0.03415583
 -0.03379147 -0.03346172 -0.03023254 -0.02871629 -0.02709492 -0.02278221
 -0.01838077 -0.01786413 -0.01352395 -0.01327602 -0.01258126 -0.0077164
 -0.00704583 -0.00587445 -0.00326077 -0.00323934  0.00316148  0.00354514
  0.00656073  0.01016739  0.01034605  0.01224721  0.01366668  0.01955242
  0.02218427  0.02672927  0.02782716  0.02859259  0.02938517  0.02992787
  0.03145424  0.03371752  0.03595197  0.03789856  0.04081397  0.04339737
  0.04584697  0.04717551  0.04742938  0.04744578  0.04889587  0.05197236
  0.05204182  0.05292942  0.05330371  0.05357465  0.05369261  0.05419398
  0.05513246  0.05666909  0.05680072  0.05738952  0.05839861  0.05851547
  0.05902295  0.06239672  0.06659095  0.06888096  0.06917746  0.06926927
  0.06955376  0.06956457  0.0702645   0.07055472  0.07089214  0.07116453
  0.07176201  0.07218687  0.07274392  0.07320609  0.07474528  0.0765455
  0.07675567  0.07675624  0.07985074  0.08024774  0.08295455  0.08353611
  0.08630196  0.08984459  0.09029219  0.09054224  0.09171683  0.09211266
  0.09527122  0.10130786  0.10340408  0.10540832  0.11043713  0.11101627
  0.11166852  0.11539553  0.11697138  0.11809251  0.1221655   0.12279067
  0.12896673  0.12990009  0.13263173  0.13321074  0.13332362  0.13408737
  0.13689454  0.13708158  0.13834912  0.13841725  0.13979776  0.13996184
  0.14016191  0.14049813  0.14457071  0.1472537   0.14897582  0.15188122
  0.15195076  0.15214638  0.1524689   0.15261798  0.15293119  0.15802265
  0.1600626   0.16268301  0.1629951   0.16309666  0.16528761  0.16613374
  0.17166689  0.1727096   0.17464145  0.17683044  0.17731236  0.17736216
  0.17779424  0.1778632   0.17819033  0.17928408  0.18221498  0.18508222
  0.18588063  0.18709483  0.18798859  0.19526883  0.19628527  0.19636986
  0.19847567  0.19953521  0.1997558   0.20268487  0.20394079  0.20414217
  0.20473609  0.20540704  0.20592035  0.20603405  0.20624673  0.20635347
  0.20768989  0.20956902  0.21080574  0.2108063   0.21105623  0.21258711
  0.21380959  0.21619389  0.21635982  0.21835221  0.21911812  0.21928013
  0.22122099  0.22155096  0.22256921  0.22311224  0.22426822  0.22674128
  0.22684224  0.23450131  0.23641747  0.23849723  0.23965201  0.2404463
  0.24083858  0.24115209  0.24310162  0.24359744  0.24469098  0.24715571
  0.24824945  0.24843104  0.24966106  0.25039933  0.25144884  0.25269432
  0.25435177  0.25506322  0.25614344  0.26008616  0.26126226  0.26517352
  0.2655254   0.26557802  0.2661027   0.26673275  0.26678856  0.26810129
  0.26842268  0.2690573   0.26930937  0.26937425  0.27158676  0.27291417
  0.27638116  0.27758467  0.27839363  0.27841963  0.27999801  0.28012718
  0.28061606  0.28192455  0.28460257  0.28468273  0.28506099  0.28562105
  0.28694334  0.28982725  0.29106683  0.29138247  0.29173124  0.29244465
  0.29322011  0.2954334   0.30095961  0.30243457  0.30302068  0.30503722
  0.30737711  0.30956401  0.30986667  0.31008035  0.31033098  0.31457813
  0.3148277   0.31495756  0.31501574  0.31922058  0.32022169  0.32413846
  0.32612237  0.32738928  0.32745323  0.33057253  0.33427674  0.33692715
  0.34077875  0.3441087   0.34478076  0.34479921  0.34596927  0.34711692
  0.34940075  0.34994697  0.35120043  0.3550579   0.35532722  0.35592073
  0.35673576  0.35836227  0.36054504  0.36227272  0.36288888  0.36429238
  0.36447991  0.36615836  0.37156229  0.37256825  0.37350013  0.37858519
  0.38006703  0.38142006  0.38220097  0.38599077  0.38702664  0.3874502
  0.38783333  0.38914722  0.39689771  0.40143571  0.40567552  0.40653986
  0.41007382  0.41068931  0.41193087  0.41377615  0.41478424  0.41831145
  0.41989406  0.42196793  0.42296325  0.42955869  0.43039047  0.43165277
  0.43166389  0.43258145  0.43418799  0.43473818  0.43647483  0.44274012
  0.443638    0.44635952  0.44803714  0.448427    0.44922935  0.45069648
  0.45513792  0.46329084  0.46388692  0.46547578  0.46917014  0.47060979
  0.4726968   0.47521323  0.47531699  0.4798648   0.48363239  0.48493778
  0.48825399  0.48864638  0.49432752  0.49591075  0.49881925  0.5011637
  0.50144266  0.50633985  0.50747354  0.509804    0.51137555  0.51208025
  0.51381117  0.51436067  0.51730338  0.51973923  0.52093371  0.52448721
  0.52480561  0.52634389  0.52885265  0.52904535  0.53045638  0.53287799
  0.53361561  0.53398836  0.53772133  0.53879401  0.53889749  0.53953279
  0.53983079  0.54220275  0.54353963  0.54419764  0.54445086  0.5444928
  0.54461154  0.54510733  0.54656711  0.5483301   0.54930363  0.55099118
  0.55106648  0.55419236  0.56041537  0.56095522  0.56335283  0.56396084
  0.56565586  0.56854818  0.57284392  0.57540136  0.57759762  0.57860446
  0.58050241  0.58053298  0.58281587  0.58505469  0.58527595  0.58692457
  0.58766867  0.59428736  0.59904795  0.59946096  0.60005483  0.60564731
  0.61259266  0.61393944  0.61604566  0.62325665  0.62353888  0.62473852
  0.62497758  0.62599269  0.62649274  0.62762447  0.62943826  0.63101225
  0.63659309  0.6404849   0.64347814  0.64775558  0.6509795   0.6522847
  0.65230974  0.65761434  0.66642852  0.66992967  0.67248999  0.67362881
  0.67399656  0.67516973  0.67660994  0.67780165  0.67883188  0.68053812
  0.68135968  0.68215385  0.6828771   0.69423607  0.70693165  0.7071685
  0.71985015]

  warnings.warn(

2022-11-03 10:50:45,163:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.17673864 -0.17048987 -0.15979402 -0.14818382 -0.14114884 -0.13395027
 -0.13146779 -0.13069872 -0.12550519 -0.12074727 -0.11960249 -0.11510166
 -0.11378273 -0.10821942 -0.107764   -0.10750714 -0.10667065 -0.10333323
 -0.10248055 -0.10042046 -0.10022143 -0.09855319 -0.0947004  -0.09346623
 -0.08509704 -0.08466409 -0.08269339 -0.08225847 -0.07959583 -0.07870752
 -0.07573521 -0.07465144 -0.07119171 -0.06377678 -0.06008879 -0.05932318
 -0.05880952 -0.05740793 -0.05565138 -0.05132321 -0.04933676 -0.04908029
 -0.04830076 -0.04815382 -0.04631891 -0.04528172 -0.04466669 -0.04343109
 -0.04295808 -0.04281641 -0.04261954 -0.04047501 -0.0400243  -0.0375289
 -0.0356903  -0.03401872 -0.03318291 -0.03279815 -0.03191314 -0.03176281
 -0.03167602 -0.03071845 -0.02670288 -0.02668581 -0.02422929 -0.02277454
 -0.02146864 -0.02065124 -0.01764508 -0.01537311 -0.01194772 -0.01173088
 -0.01091289 -0.00462435 -0.0043224   0.00085299  0.00125697  0.0012765
  0.00374117  0.00630151  0.00668372  0.01130411  0.01323219  0.01331636
  0.0142217   0.01482961  0.01768813  0.0206917   0.02357945  0.02372196
  0.02528396  0.0275215   0.02855484  0.03142226  0.03474338  0.03488561
  0.03528377  0.03676884  0.03790629  0.03796427  0.03877159  0.03884832
  0.0396781   0.0411765   0.0413205   0.04265428  0.04491629  0.04502302
  0.04576918  0.05053583  0.05082958  0.05180741  0.05317945  0.05331836
  0.05776487  0.05946863  0.05970295  0.05974101  0.06009296  0.06046698
  0.06190057  0.0640654   0.06476261  0.0657423   0.06576706  0.066214
  0.07018646  0.07170608  0.07362751  0.07640989  0.07684782  0.07825005
  0.08188373  0.08198395  0.08205556  0.08415505  0.08671672  0.08679543
  0.08891593  0.08928032  0.09032495  0.092826    0.09303742  0.09374227
  0.09384968  0.0958171   0.09650753  0.09705736  0.09736851  0.09920963
  0.0997628   0.10225252  0.10234481  0.10354589  0.10432811  0.10441412
  0.10555475  0.10705929  0.10754077  0.11034501  0.11146433  0.11204439
  0.11205803  0.11504288  0.11760678  0.11939862  0.12247297  0.12597549
  0.12778198  0.1294546   0.13023853  0.13270507  0.13387242  0.13533148
  0.14035033  0.14295321  0.1491367   0.14965372  0.15018005  0.15080086
  0.15081726  0.1559557   0.15680042  0.15815967  0.15952175  0.16292546
  0.16324038  0.16527766  0.16662175  0.16824801  0.17056607  0.17315002
  0.17366391  0.1737516   0.17550195  0.17631458  0.17911883  0.18031237
  0.18049773  0.18238479  0.18272284  0.18604101  0.18764568  0.18799956
  0.18811974  0.18953683  0.19098649  0.19212812  0.19444593  0.19722912
  0.19955403  0.20435073  0.20473535  0.20677439  0.20758481  0.20879928
  0.21052873  0.21202095  0.21451216  0.21502424  0.21727802  0.2178271
  0.21792036  0.22053641  0.22135087  0.22155306  0.22429026  0.22510362
  0.22598149  0.22620516  0.22716548  0.2274459   0.22758391  0.23126792
  0.23134577  0.23322296  0.23540193  0.23575423  0.23626687  0.23654402
  0.23833924  0.23887358  0.23925945  0.24068727  0.24070252  0.24101557
  0.24792496  0.24840681  0.24921926  0.24964179  0.25069618  0.25236852
  0.25334362  0.2540027   0.25822897  0.25961938  0.26355655  0.26394538
  0.26578524  0.26594001  0.26902092  0.26924172  0.26967597  0.27208989
  0.27520465  0.27546944  0.2773874   0.27768585  0.27774989  0.27838714
  0.28408154  0.28749907  0.28797705  0.28932275  0.29367966  0.29385556
  0.29399141  0.30004505  0.30121362  0.30135131  0.30309898  0.30335515
  0.3037312   0.30488083  0.30652383  0.30707589  0.30714959  0.30715846
  0.30813035  0.30864128  0.30949616  0.31017159  0.31092885  0.31122608
  0.31245411  0.31276688  0.31407169  0.31474349  0.315524    0.31683357
  0.31858021  0.32147788  0.32267583  0.32603576  0.32647189  0.32669164
  0.3271636   0.32737605  0.3280575   0.33250438  0.33264428  0.33466383
  0.33504552  0.34032129  0.34497357  0.34512568  0.34665641  0.34742946
  0.35116424  0.35424921  0.35460347  0.35563995  0.35671905  0.35679426
  0.35822992  0.35937929  0.36110086  0.36199554  0.36317348  0.364781
  0.3684344   0.36960356  0.36998044  0.37131105  0.37438389  0.37539415
  0.37745983  0.37874135  0.38004769  0.38092131  0.38143833  0.38299865
  0.38756152  0.38816498  0.38844925  0.39726331  0.39927256  0.40054198
  0.41002524  0.41130947  0.41131104  0.41215558  0.41254449  0.41443596
  0.41501904  0.41521829  0.41734495  0.41779583  0.42113133  0.42298968
  0.42855862  0.4301076   0.43109988  0.43392404  0.43429456  0.43508867
  0.43600278  0.43858651  0.43964053  0.44208689  0.44265201  0.44539012
  0.44571865  0.4487335   0.44893671  0.45363935  0.4564092   0.45655088
  0.45808413  0.46150486  0.46210155  0.46260002  0.4631179   0.46507746
  0.46573712  0.46881886  0.47133271  0.47222066  0.4736494   0.47395454
  0.47507529  0.48079908  0.48719231  0.48758816  0.48983572  0.49250933
  0.49276883  0.49321688  0.4959082   0.49763666  0.49815368  0.5000148
  0.5035305   0.50378698  0.50531122  0.5065384   0.50757457  0.51353238
  0.51854303  0.51965416  0.5214473   0.52576862  0.52771022  0.52801351
  0.52881894  0.52925872  0.53154334  0.53414487  0.53731443  0.54057488
  0.54321129  0.54537587  0.54998832  0.55181748  0.55210803  0.55486806
  0.55517333  0.55811348  0.56529834  0.56873361  0.57139518  0.57625014
  0.57720566  0.57801386  0.58004371  0.58653295  0.58655271  0.59418016
  0.59446167  0.59620959  0.59734587  0.5977213   0.59798903  0.59829372
  0.59863986  0.60014084  0.60356568  0.60494488  0.60516143  0.60743673
  0.60827295  0.60983745  0.61203596  0.61312995  0.61364221  0.61503698
  0.61509186  0.61584937  0.61955444  0.62120696  0.62285977  0.62792964
  0.63001315  0.63132215  0.63183862  0.64609191  0.64949575  0.65132336
  0.65194794  0.65286652  0.65346175  0.65406206  0.65750559  0.6607066
  0.6625536   0.66375611  0.6646906   0.66690442  0.66793696  0.67307952
  0.67425893  0.67628405  0.69402936  0.69958648  0.7010376   0.70964483
  0.70982924  0.71159708  0.71427134  0.7196269   0.72709859  0.75332583]

  warnings.warn(

2022-11-03 10:50:45,282:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.04564258e-01 -1.89722738e-01 -1.76508404e-01 -1.75014545e-01
 -1.68699999e-01 -1.68251831e-01 -1.67374944e-01 -1.66584236e-01
 -1.56826592e-01 -1.56217407e-01 -1.55876603e-01 -1.52934798e-01
 -1.51963289e-01 -1.46571994e-01 -1.45205155e-01 -1.41899162e-01
 -1.41847993e-01 -1.39853445e-01 -1.37316232e-01 -1.31121341e-01
 -1.29730290e-01 -1.20230085e-01 -1.15678783e-01 -1.13660136e-01
 -1.13033062e-01 -1.12592933e-01 -1.12455437e-01 -1.10900665e-01
 -1.08768466e-01 -1.07098327e-01 -1.01694152e-01 -9.99686774e-02
 -9.87984150e-02 -8.92036218e-02 -8.75731363e-02 -8.36172528e-02
 -8.30140811e-02 -8.28384984e-02 -7.90239696e-02 -7.89662692e-02
 -7.68124351e-02 -7.39890625e-02 -6.91163244e-02 -6.90960205e-02
 -6.86932094e-02 -6.72872350e-02 -6.70432133e-02 -6.26655549e-02
 -6.14347832e-02 -5.73570841e-02 -5.71597563e-02 -5.47097121e-02
 -5.40614787e-02 -5.38700836e-02 -5.34244810e-02 -5.17543657e-02
 -5.01603963e-02 -5.00167354e-02 -4.89627298e-02 -4.64412911e-02
 -4.46221093e-02 -4.38525230e-02 -3.88408103e-02 -3.60634172e-02
 -3.59994526e-02 -3.46058956e-02 -3.46058859e-02 -3.37888428e-02
 -2.96735919e-02 -2.65347828e-02 -2.56689402e-02 -2.51792832e-02
 -2.04482053e-02 -2.02706434e-02 -1.95126505e-02 -1.71257192e-02
 -1.68023666e-02 -1.57314662e-02 -1.30056260e-02 -9.06408792e-03
 -8.94646640e-03 -2.75343007e-03 -6.93828218e-04  1.36604216e-03
  3.22893340e-03  6.12087213e-03  1.19374446e-02  1.30495832e-02
  1.67494925e-02  1.70354969e-02  1.70920131e-02  1.99313464e-02
  2.07395526e-02  2.25458929e-02  2.33911119e-02  2.37115302e-02
  2.47417688e-02  2.64425647e-02  2.77353504e-02  2.93728069e-02
  3.18457972e-02  3.62225939e-02  4.12268463e-02  4.18742329e-02
  4.41573548e-02  4.60710438e-02  4.71159068e-02  5.05072017e-02
  5.28930772e-02  5.43327201e-02  5.76007455e-02  5.90376841e-02
  6.06460185e-02  6.07623918e-02  6.08574931e-02  6.54989150e-02
  6.61207433e-02  6.76968865e-02  6.86420017e-02  6.98807307e-02
  7.07045877e-02  7.32843318e-02  7.42170204e-02  8.11908840e-02
  8.15635578e-02  8.38794887e-02  8.62141540e-02  8.63201574e-02
  8.70616420e-02  9.37834885e-02  9.45950940e-02  9.67021391e-02
  9.70676998e-02  9.75126033e-02  9.79638725e-02  9.88283083e-02
  1.00603926e-01  1.01446462e-01  1.02436707e-01  1.04065271e-01
  1.05689060e-01  1.06723107e-01  1.06897783e-01  1.08147703e-01
  1.08562510e-01  1.14196919e-01  1.16594905e-01  1.17406382e-01
  1.18491278e-01  1.18692519e-01  1.20284985e-01  1.23395955e-01
  1.24651338e-01  1.29015310e-01  1.32492059e-01  1.33452360e-01
  1.33693306e-01  1.34290926e-01  1.35724445e-01  1.36182124e-01
  1.37142267e-01  1.40390440e-01  1.50705207e-01  1.51245738e-01
  1.52026457e-01  1.52185564e-01  1.54018187e-01  1.55080293e-01
  1.56792597e-01  1.57138412e-01  1.57491595e-01  1.59279335e-01
  1.59396339e-01  1.60138400e-01  1.60694916e-01  1.61398690e-01
  1.62596920e-01  1.67357835e-01  1.68079167e-01  1.69592294e-01
  1.73028908e-01  1.77110367e-01  1.80993683e-01  1.81645760e-01
  1.82131693e-01  1.82168155e-01  1.85924973e-01  1.88311658e-01
  1.89073557e-01  1.92497530e-01  1.94468158e-01  1.96396275e-01
  1.96614459e-01  1.96801680e-01  1.96893851e-01  1.97816212e-01
  1.97985006e-01  1.98377214e-01  1.99272270e-01  2.00485919e-01
  2.00722553e-01  2.02398961e-01  2.03445952e-01  2.04346876e-01
  2.04388787e-01  2.05515396e-01  2.08376013e-01  2.08537551e-01
  2.10660961e-01  2.12307223e-01  2.14359194e-01  2.14920025e-01
  2.14953145e-01  2.17757788e-01  2.18856261e-01  2.18918807e-01
  2.20104747e-01  2.22883884e-01  2.23567185e-01  2.24470805e-01
  2.24549313e-01  2.25869762e-01  2.26340022e-01  2.28998484e-01
  2.31771076e-01  2.32469493e-01  2.34568186e-01  2.35990067e-01
  2.37455693e-01  2.41362410e-01  2.41990089e-01  2.43657190e-01
  2.43792187e-01  2.46830659e-01  2.49180788e-01  2.49369473e-01
  2.49608563e-01  2.50679671e-01  2.50952715e-01  2.52060897e-01
  2.52126098e-01  2.52419769e-01  2.52456871e-01  2.53782561e-01
  2.57176440e-01  2.57233486e-01  2.58752620e-01  2.61732175e-01
  2.64302729e-01  2.64311135e-01  2.65505674e-01  2.70994499e-01
  2.72122658e-01  2.74419130e-01  2.74457866e-01  2.75006524e-01
  2.75189442e-01  2.75417202e-01  2.75579346e-01  2.77672222e-01
  2.79016221e-01  2.79066348e-01  2.80138157e-01  2.83673712e-01
  2.84347825e-01  2.85029363e-01  2.86068518e-01  2.87554188e-01
  2.87827098e-01  2.88488675e-01  2.89657080e-01  2.92245360e-01
  2.93285671e-01  2.95225372e-01  2.97638567e-01  3.05034607e-01
  3.05963258e-01  3.07847679e-01  3.08543670e-01  3.08750182e-01
  3.09569745e-01  3.09570723e-01  3.11536848e-01  3.14123993e-01
  3.14788793e-01  3.14892790e-01  3.17775771e-01  3.18252905e-01
  3.21305938e-01  3.21548168e-01  3.22254444e-01  3.23108474e-01
  3.24373721e-01  3.24724983e-01  3.25216115e-01  3.28784616e-01
  3.28917875e-01  3.30291204e-01  3.30499154e-01  3.31333807e-01
  3.31434564e-01  3.32736115e-01  3.33202053e-01  3.33367075e-01
  3.33453689e-01  3.38344400e-01  3.39407006e-01  3.40124463e-01
  3.40311955e-01  3.43510281e-01  3.43515722e-01  3.50635875e-01
  3.55187550e-01  3.56476819e-01  3.56696528e-01  3.57671352e-01
  3.58667243e-01  3.59110876e-01  3.59196548e-01  3.59802228e-01
  3.60789693e-01  3.61789855e-01  3.64422749e-01  3.66954907e-01
  3.69747228e-01  3.73798153e-01  3.75852998e-01  3.76409877e-01
  3.77650783e-01  3.78737380e-01  3.80623099e-01  3.81785139e-01
  3.82561460e-01  3.83663580e-01  3.83902029e-01  3.85016393e-01
  3.86561822e-01  3.88074208e-01  3.91426363e-01  3.93391825e-01
  3.95107313e-01  3.97963492e-01  3.98144683e-01  3.99400903e-01
  4.02604585e-01  4.03077309e-01  4.03348958e-01  4.04918293e-01
  4.08798284e-01  4.13243833e-01  4.14139571e-01  4.14421433e-01
  4.18595838e-01  4.19651462e-01  4.19906395e-01  4.20564815e-01
  4.21506400e-01  4.23106775e-01  4.24137016e-01  4.25539280e-01
  4.25750219e-01  4.26073182e-01  4.27895311e-01  4.28106523e-01
  4.30010384e-01  4.30039646e-01  4.30084279e-01  4.33542608e-01
  4.34414115e-01  4.36177416e-01  4.40121384e-01  4.40547852e-01
  4.41658409e-01  4.44657557e-01  4.47189636e-01  4.52371518e-01
  4.52816313e-01  4.53517148e-01  4.57698121e-01  4.62357324e-01
  4.63329650e-01  4.65536676e-01  4.65821691e-01  4.68770035e-01
  4.70322927e-01  4.73654197e-01  4.75352666e-01  4.77081028e-01
  4.78727203e-01  4.79138844e-01  4.79713203e-01  4.79797353e-01
  4.82804997e-01  4.84619947e-01  4.87866237e-01  4.88511338e-01
  4.89433772e-01  4.90557590e-01  4.91275146e-01  4.93809405e-01
  4.94635448e-01  4.95599727e-01  4.98847385e-01  5.01009619e-01
  5.02665686e-01  5.02968629e-01  5.03413046e-01  5.05367496e-01
  5.06103297e-01  5.06729841e-01  5.10627749e-01  5.12097042e-01
  5.14302042e-01  5.15577480e-01  5.20881596e-01  5.23938882e-01
  5.27072192e-01  5.29042077e-01  5.30707138e-01  5.31782258e-01
  5.33492087e-01  5.36115680e-01  5.36757074e-01  5.37372334e-01
  5.42604631e-01  5.43209941e-01  5.43898351e-01  5.45029704e-01
  5.45438227e-01  5.46583081e-01  5.52988576e-01  5.53315215e-01
  5.54505661e-01  5.61414667e-01  5.64854592e-01  5.65769151e-01
  5.66881318e-01  5.66893313e-01  5.67463993e-01  5.68009230e-01
  5.68262574e-01  5.69194358e-01  5.69342453e-01  5.69565265e-01
  5.70196025e-01  5.70240938e-01  5.81000403e-01  5.85084778e-01
  5.88333724e-01  5.88874445e-01  5.89189015e-01  5.89502723e-01
  5.89958809e-01  6.03562125e-01  6.08637371e-01  6.11769918e-01
  6.13078314e-01  6.14653305e-01  6.17227267e-01  6.17636718e-01
  6.17701963e-01  6.18609822e-01  6.19023986e-01  6.20347845e-01
  6.27528557e-01  6.32127247e-01  6.36340738e-01  6.38989744e-01
  6.42602617e-01  6.42646395e-01  6.48488186e-01  6.48737707e-01
  6.49893463e-01  6.50924639e-01  6.52910705e-01  6.52957146e-01
  6.57468969e-01  6.57648893e-01  6.58431528e-01  6.59867698e-01
  6.60277044e-01  6.62947884e-01  6.64034986e-01  6.64686119e-01
  6.69421276e-01  6.88274823e-01  6.91433055e-01  7.11662984e-01
  7.29053282e-01  7.37488723e-01  7.51572699e-01  7.57682448e-01
  7.58084910e-01]

  warnings.warn(

2022-11-03 10:50:47,692:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16924712 -0.16008648 -0.15949763 -0.15915973 -0.1489059  -0.14500532
 -0.13418585 -0.13156108 -0.12914246 -0.1250813  -0.12264856 -0.12233405
 -0.12100034 -0.11917802 -0.11814222 -0.11613573 -0.1134715  -0.11258835
 -0.11184957 -0.10782349 -0.10516944 -0.10327508 -0.10268867 -0.09849763
 -0.09378371 -0.09255714 -0.09006023 -0.08983682 -0.08897983 -0.08617682
 -0.08613843 -0.08332439 -0.08281758 -0.08127969 -0.07966177 -0.07502592
 -0.07354819 -0.07320407 -0.07147239 -0.06777924 -0.06663731 -0.06591017
 -0.06257666 -0.06037159 -0.05992959 -0.05956709 -0.05465403 -0.05363659
 -0.05034938 -0.04619382 -0.04536974 -0.04508079 -0.04284867 -0.03676453
 -0.03260773 -0.02978373 -0.0271581  -0.02414384 -0.02328284 -0.02109009
 -0.01942268 -0.01881226 -0.0187299  -0.01553252 -0.01489999 -0.01211869
 -0.01207045 -0.01162248 -0.01032621 -0.00586942 -0.00560136 -0.00449574
 -0.00441305 -0.00354891 -0.00333197  0.00512224  0.00640759  0.00667876
  0.00755986  0.00903346  0.01028035  0.01276159  0.01289596  0.01407873
  0.01728032  0.01850777  0.01986862  0.02088584  0.02124162  0.02262054
  0.02672332  0.0268824   0.02794507  0.03167994  0.03295545  0.03678944
  0.03848869  0.04021185  0.04640968  0.0464269   0.0505129   0.05254236
  0.05298988  0.05483402  0.05528166  0.05554554  0.05947412  0.06107428
  0.0611862   0.06209306  0.06255061  0.06476804  0.06586781  0.07136423
  0.07343468  0.07415811  0.07438805  0.07454147  0.07537583  0.07613098
  0.07800917  0.07871046  0.08025222  0.08067114  0.08107186  0.08726343
  0.08795039  0.08861144  0.08903665  0.08964088  0.09143051  0.09190208
  0.09365927  0.09463922  0.09680194  0.09873994  0.10291904  0.10313989
  0.10389948  0.10415545  0.10446859  0.10735606  0.10932236  0.10970304
  0.1107027   0.11267555  0.11386078  0.120165    0.12026276  0.12144796
  0.12187531  0.12394303  0.12791422  0.13402984  0.13530417  0.13590259
  0.13622637  0.13688695  0.13720882  0.13736941  0.13985407  0.14087387
  0.14167407  0.14273793  0.14510228  0.14516497  0.14639408  0.15142456
  0.15183725  0.15268569  0.1529656   0.15708416  0.15821603  0.159286
  0.16047524  0.16204107  0.16372662  0.16432382  0.16590806  0.16908068
  0.16979022  0.17459796  0.18174282  0.18246568  0.18359177  0.18531637
  0.18718789  0.18757581  0.18804656  0.19265386  0.1930483   0.19308926
  0.19350132  0.19359994  0.19444912  0.19636302  0.19662016  0.19756132
  0.19785089  0.1986346   0.19876285  0.19935965  0.20029076  0.20138315
  0.20372488  0.20402742  0.20408385  0.20428809  0.20467797  0.2064574
  0.20708303  0.20773814  0.2106895   0.21115489  0.21155392  0.21178168
  0.21202802  0.21270687  0.21308967  0.21427419  0.21455035  0.21575597
  0.21613677  0.216828    0.22022436  0.22033226  0.22090014  0.22115339
  0.22305055  0.22316544  0.22326956  0.2233842   0.22604018  0.22756694
  0.23352284  0.23372223  0.23692239  0.23763976  0.23785996  0.24026271
  0.24182704  0.24376938  0.24399478  0.24539088  0.24619696  0.24967608
  0.25197209  0.25253458  0.25260804  0.25325566  0.25404522  0.25636003
  0.25731151  0.25784496  0.25867133  0.25942185  0.26520028  0.26558049
  0.26709175  0.26709236  0.26768813  0.27077012  0.27342705  0.27421782
  0.27426865  0.27673157  0.27751464  0.27835135  0.28028759  0.28151495
  0.28191278  0.28371653  0.28679245  0.28698952  0.28812105  0.28961923
  0.28996185  0.29611052  0.29619403  0.29686995  0.29809907  0.29865989
  0.30613622  0.31087378  0.31311589  0.31371601  0.31410193  0.31460561
  0.31481048  0.31481695  0.31682038  0.31963933  0.32004744  0.32037125
  0.32392203  0.32783278  0.32850953  0.33375464  0.33403115  0.33537463
  0.33613106  0.33733109  0.33802342  0.33804172  0.3414046   0.34349015
  0.34371729  0.34508416  0.3475162   0.34826274  0.34879216  0.3499346
  0.3501539   0.35020318  0.35391089  0.35580419  0.35703567  0.35813904
  0.36396818  0.36641887  0.36738915  0.3689475   0.37112672  0.37637888
  0.37639389  0.37699856  0.37729604  0.37944258  0.38259519  0.3827798
  0.38367893  0.38569148  0.38675769  0.38910102  0.39479448  0.39530038
  0.39940917  0.40003611  0.40078674  0.40353184  0.40507369  0.40534523
  0.40706904  0.40899199  0.41158244  0.41300711  0.41431221  0.41472875
  0.41486556  0.41542146  0.41563331  0.42071298  0.42087225  0.42174862
  0.42204722  0.42350662  0.42397335  0.42702001  0.42927758  0.43137426
  0.43447419  0.43592342  0.43948043  0.44085354  0.44098108  0.44166298
  0.45374287  0.45520196  0.45743051  0.45825879  0.45918385  0.45942276
  0.45991227  0.46048682  0.46135743  0.46576385  0.46688572  0.46881856
  0.46932521  0.47244609  0.47647559  0.47769237  0.48076325  0.48138893
  0.48212085  0.48318839  0.48521396  0.48877702  0.4904427   0.49083207
  0.49416954  0.49498683  0.49597642  0.49806293  0.50055128  0.50131918
  0.50265889  0.50405322  0.50411704  0.50414654  0.50510539  0.50704753
  0.51362171  0.5137022   0.51586601  0.52135725  0.52783611  0.52793151
  0.52920774  0.53005173  0.53158375  0.53240029  0.53401317  0.53502194
  0.53582996  0.5358955   0.53753959  0.53957144  0.5473091   0.54882382
  0.55087921  0.55211712  0.55325488  0.55337434  0.55753908  0.55754178
  0.55964959  0.56077197  0.56168506  0.56194962  0.5628155   0.56531134
  0.566038    0.57404188  0.57506679  0.57621922  0.57734005  0.57848991
  0.57900756  0.57923093  0.58051355  0.58194284  0.58346793  0.58444589
  0.58485126  0.58519952  0.58755892  0.588555    0.59064934  0.59090092
  0.59458843  0.59695899  0.59748321  0.599776    0.6016395   0.60238154
  0.60317285  0.60343432  0.60799795  0.60898677  0.61049205  0.61106237
  0.61858561  0.62187661  0.62209404  0.62448974  0.62787933  0.62831496
  0.62947784  0.64142146  0.64478937  0.64583233  0.64835114  0.64935084
  0.65252741  0.65274045  0.655339    0.6562755   0.66337496  0.66365826
  0.66380924  0.66698358  0.67336514  0.67378226  0.67664203  0.68061152
  0.68228094  0.69037238  0.69305641  0.69736421  0.70124146  0.72811842
  0.72869143]

  warnings.warn(

2022-11-03 10:50:47,692:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.10543296e-01 -2.01935483e-01 -1.98896468e-01 -1.82192516e-01
 -1.73851126e-01 -1.71123268e-01 -1.68305528e-01 -1.57034090e-01
 -1.54540910e-01 -1.53723075e-01 -1.51879609e-01 -1.44400185e-01
 -1.42810470e-01 -1.32679743e-01 -1.31035641e-01 -1.30857881e-01
 -1.30465318e-01 -1.30065170e-01 -1.27468064e-01 -1.27192494e-01
 -1.22978050e-01 -1.21903594e-01 -1.15347293e-01 -1.13742131e-01
 -1.13115662e-01 -1.12795168e-01 -1.06850631e-01 -1.02859513e-01
 -1.00390915e-01 -9.72216435e-02 -9.62939852e-02 -9.50173054e-02
 -9.39814184e-02 -9.38017289e-02 -9.28989192e-02 -9.12564686e-02
 -9.10857617e-02 -9.03789547e-02 -8.42722883e-02 -7.45423594e-02
 -7.43211477e-02 -6.91155874e-02 -6.68398042e-02 -6.42783931e-02
 -6.36983480e-02 -6.35361107e-02 -6.01300730e-02 -5.93990909e-02
 -5.75052612e-02 -5.65422796e-02 -5.42468424e-02 -5.32018823e-02
 -5.27143399e-02 -5.24162659e-02 -5.07793236e-02 -4.90425091e-02
 -4.61965599e-02 -4.39323973e-02 -4.17722641e-02 -4.17507141e-02
 -4.14895030e-02 -4.08591795e-02 -3.95173009e-02 -3.60642131e-02
 -3.48726097e-02 -2.92466453e-02 -2.53530149e-02 -2.25316981e-02
 -2.19552336e-02 -2.00970847e-02 -1.51671744e-02 -1.36024586e-02
 -1.29073904e-02 -1.21982021e-02 -9.82013256e-03 -9.05445097e-03
 -8.22978001e-03 -7.80191446e-03 -6.84657495e-03 -6.14852744e-03
 -5.92002400e-03 -5.45201165e-03 -4.60770116e-03 -2.77438308e-03
 -2.16432263e-03  5.10539709e-04  1.61485942e-03  2.15114094e-03
  3.62315278e-03  5.71149623e-03  6.78988794e-03  8.63620066e-03
  1.04567037e-02  1.24331518e-02  1.33431231e-02  1.84060843e-02
  1.95449191e-02  2.31298689e-02  2.34030769e-02  2.57893911e-02
  3.09201701e-02  3.19874762e-02  3.25998818e-02  3.64377238e-02
  3.97079032e-02  4.21854987e-02  4.25418973e-02  4.34763563e-02
  4.45038855e-02  4.53767233e-02  4.78964086e-02  5.22686448e-02
  5.72448488e-02  5.76371202e-02  5.90489929e-02  6.10330416e-02
  6.12214725e-02  6.22150945e-02  6.35350459e-02  6.65618605e-02
  6.74225907e-02  6.87478870e-02  7.17351224e-02  7.20853620e-02
  7.31601424e-02  7.47180813e-02  7.48151850e-02  7.55536131e-02
  7.58749463e-02  7.63294024e-02  7.76652791e-02  7.85525929e-02
  8.02277885e-02  8.05053268e-02  8.21549526e-02  8.29038577e-02
  8.31933560e-02  8.44074111e-02  8.53164117e-02  8.60093106e-02
  8.80552822e-02  8.82613154e-02  9.35360832e-02  9.37496503e-02
  9.44383084e-02  9.60789084e-02  9.71624975e-02  1.02213343e-01
  1.02677489e-01  1.05053169e-01  1.06042130e-01  1.06485428e-01
  1.07794311e-01  1.07848825e-01  1.07878464e-01  1.08053349e-01
  1.11488576e-01  1.13125311e-01  1.13729851e-01  1.13789310e-01
  1.19211681e-01  1.19709563e-01  1.21312393e-01  1.24806998e-01
  1.25548751e-01  1.26866172e-01  1.27284980e-01  1.28096980e-01
  1.28292224e-01  1.29910879e-01  1.30481991e-01  1.30779081e-01
  1.32314618e-01  1.33280890e-01  1.35397595e-01  1.37173473e-01
  1.41050100e-01  1.41171082e-01  1.41623263e-01  1.43829535e-01
  1.44048686e-01  1.44994700e-01  1.45397856e-01  1.46343383e-01
  1.46659929e-01  1.47855622e-01  1.51676885e-01  1.51698716e-01
  1.52198321e-01  1.52807687e-01  1.54917552e-01  1.57741237e-01
  1.60963482e-01  1.61163268e-01  1.63084028e-01  1.63535419e-01
  1.64207538e-01  1.64555484e-01  1.65252673e-01  1.65315326e-01
  1.65396433e-01  1.70194466e-01  1.70914042e-01  1.73988322e-01
  1.74106328e-01  1.75060439e-01  1.78163837e-01  1.80172855e-01
  1.80717394e-01  1.83389053e-01  1.85640411e-01  1.89734815e-01
  1.90657278e-01  1.91240623e-01  1.92457000e-01  1.97815843e-01
  1.99369440e-01  1.99418350e-01  1.99484401e-01  2.00814798e-01
  2.00919030e-01  2.01404853e-01  2.02866848e-01  2.05472475e-01
  2.05653561e-01  2.06357762e-01  2.07162561e-01  2.08837151e-01
  2.09317961e-01  2.10968172e-01  2.15123364e-01  2.20620276e-01
  2.20779894e-01  2.23100026e-01  2.25689143e-01  2.26912147e-01
  2.28066641e-01  2.28567319e-01  2.31326742e-01  2.32424255e-01
  2.32995217e-01  2.33722913e-01  2.35588474e-01  2.35629707e-01
  2.36233047e-01  2.36500507e-01  2.40326530e-01  2.40775900e-01
  2.41917606e-01  2.43031359e-01  2.45255941e-01  2.46642106e-01
  2.47011311e-01  2.52413270e-01  2.52736123e-01  2.52945920e-01
  2.55555972e-01  2.60635575e-01  2.65172105e-01  2.65630885e-01
  2.69179880e-01  2.69824151e-01  2.70116603e-01  2.70828856e-01
  2.79465972e-01  2.79822798e-01  2.81042548e-01  2.81955007e-01
  2.84547265e-01  2.86173681e-01  2.87087577e-01  2.88213924e-01
  2.89571879e-01  2.90148882e-01  2.92577573e-01  2.93398082e-01
  2.94921501e-01  2.96547946e-01  2.97146219e-01  2.97590274e-01
  2.97920422e-01  2.98394068e-01  2.99756374e-01  3.00619037e-01
  3.01762401e-01  3.02225308e-01  3.08781574e-01  3.11008139e-01
  3.12420884e-01  3.12699039e-01  3.15966209e-01  3.16106596e-01
  3.17086104e-01  3.17484537e-01  3.20624086e-01  3.22253557e-01
  3.25623708e-01  3.28234314e-01  3.30922248e-01  3.32833250e-01
  3.34837010e-01  3.35188899e-01  3.35247088e-01  3.38037817e-01
  3.41111327e-01  3.41249132e-01  3.41980290e-01  3.45570006e-01
  3.45768210e-01  3.46988986e-01  3.47543152e-01  3.51106812e-01
  3.51203456e-01  3.51734795e-01  3.54767418e-01  3.55169651e-01
  3.57288685e-01  3.61184435e-01  3.61434675e-01  3.62750813e-01
  3.62819256e-01  3.63013548e-01  3.65036590e-01  3.67807678e-01
  3.69434137e-01  3.69755737e-01  3.71385778e-01  3.71536136e-01
  3.79154302e-01  3.79576769e-01  3.79828437e-01  3.81160472e-01
  3.82834532e-01  3.84723289e-01  3.86080381e-01  3.87637979e-01
  3.88443868e-01  3.92653407e-01  3.92692709e-01  3.94490171e-01
  3.99201912e-01  4.01024673e-01  4.01522950e-01  4.02462479e-01
  4.03254097e-01  4.03992095e-01  4.04557752e-01  4.05059431e-01
  4.08307721e-01  4.09894646e-01  4.12660564e-01  4.13435727e-01
  4.14047908e-01  4.18120157e-01  4.20921078e-01  4.21109270e-01
  4.26782470e-01  4.27996566e-01  4.28825540e-01  4.29007606e-01
  4.29724955e-01  4.31468241e-01  4.33031096e-01  4.33179885e-01
  4.36460727e-01  4.36869181e-01  4.37639073e-01  4.38154318e-01
  4.39870517e-01  4.42541366e-01  4.44811545e-01  4.45511726e-01
  4.47510583e-01  4.48454128e-01  4.51995607e-01  4.52241201e-01
  4.53615552e-01  4.54909317e-01  4.61961844e-01  4.66501035e-01
  4.69724847e-01  4.70587375e-01  4.71099942e-01  4.73908125e-01
  4.73957279e-01  4.74621655e-01  4.78513609e-01  4.79501194e-01
  4.80803978e-01  4.81534266e-01  4.82936389e-01  4.83552554e-01
  4.84906556e-01  4.86681984e-01  4.87067103e-01  4.91579209e-01
  4.91912851e-01  4.94282470e-01  4.97010325e-01  4.98249649e-01
  4.98609025e-01  4.98625179e-01  5.06348762e-01  5.06719001e-01
  5.06949122e-01  5.10482391e-01  5.10992740e-01  5.13064590e-01
  5.13170860e-01  5.13532720e-01  5.15853576e-01  5.17959595e-01
  5.18199374e-01  5.19149401e-01  5.20675438e-01  5.21398000e-01
  5.22193455e-01  5.23577933e-01  5.25516878e-01  5.27635730e-01
  5.28195450e-01  5.30100270e-01  5.30983604e-01  5.34919526e-01
  5.36421102e-01  5.38148757e-01  5.39659077e-01  5.40764131e-01
  5.44305369e-01  5.46263749e-01  5.46557054e-01  5.50489683e-01
  5.53905560e-01  5.53949083e-01  5.54873895e-01  5.61107783e-01
  5.64053304e-01  5.67813059e-01  5.69669614e-01  5.69960192e-01
  5.71223946e-01  5.75790365e-01  5.77110671e-01  5.79738720e-01
  5.85723659e-01  5.85982530e-01  5.90177780e-01  5.90852291e-01
  5.91308698e-01  5.93629547e-01  5.94242053e-01  5.96918357e-01
  5.97613470e-01  5.98250686e-01  6.01093506e-01  6.12152899e-01
  6.16194300e-01  6.17263383e-01  6.17591759e-01  6.17690784e-01
  6.17817782e-01  6.23083125e-01  6.23679279e-01  6.25826361e-01
  6.27143568e-01  6.27435967e-01  6.28542559e-01  6.34561066e-01
  6.38889305e-01  6.39497775e-01  6.39585526e-01  6.42807580e-01
  6.43131476e-01  6.43187090e-01  6.47938742e-01  6.50510308e-01
  6.53413724e-01  6.60506628e-01  6.61613704e-01  6.78900392e-01
  6.79263232e-01  6.85076006e-01  6.93153994e-01  6.94042489e-01
  7.02530751e-01  7.03088261e-01  7.09209791e-01  7.09854645e-01
  7.11279411e-01  7.12057177e-01  7.13032596e-01  7.13335218e-01
  7.14428738e-01]

  warnings.warn(

2022-11-03 10:50:47,692:INFO:Calculating mean and std
2022-11-03 10:50:47,700:INFO:Creating metrics dataframe
2022-11-03 10:50:47,708:INFO:Uploading results into container
2022-11-03 10:50:47,708:INFO:Uploading model into container now
2022-11-03 10:50:47,708:INFO:master_model_container: 15
2022-11-03 10:50:47,708:INFO:display_container: 2
2022-11-03 10:50:47,708:INFO:BayesianRidge()
2022-11-03 10:50:47,708:INFO:create_model() successfully completed......................................
2022-11-03 10:50:47,962:WARNING:create_model() for BayesianRidge() raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:50:47,962:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:50:47,962:INFO:Initializing create_model()
2022-11-03 10:50:47,962:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:50:47,962:INFO:Checking exceptions
2022-11-03 10:50:47,977:INFO:Importing libraries
2022-11-03 10:50:47,977:INFO:Copying training dataset
2022-11-03 10:50:47,993:INFO:Defining folds
2022-11-03 10:50:47,993:INFO:Declaring metric variables
2022-11-03 10:50:47,993:INFO:Importing untrained model
2022-11-03 10:50:47,993:INFO:Bayesian Ridge Imported successfully
2022-11-03 10:50:47,993:INFO:Starting cross validation
2022-11-03 10:50:47,993:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:50:52,100:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.79233620e-01 -1.68329052e-01 -1.54274179e-01 -1.45957929e-01
 -1.41834897e-01 -1.32772610e-01 -1.32711678e-01 -1.28974592e-01
 -1.25313361e-01 -1.22675210e-01 -1.22113020e-01 -1.16012272e-01
 -1.11064926e-01 -1.09458701e-01 -1.09428103e-01 -1.08376003e-01
 -1.07115103e-01 -1.05626526e-01 -1.02276763e-01 -1.01569563e-01
 -1.00185287e-01 -9.72084671e-02 -9.65286399e-02 -9.22939490e-02
 -8.80735878e-02 -8.17552274e-02 -7.79674279e-02 -7.41384639e-02
 -7.10039912e-02 -7.08647545e-02 -6.57540004e-02 -5.76474888e-02
 -5.63198394e-02 -5.53994459e-02 -5.25800450e-02 -5.20759258e-02
 -5.09082925e-02 -5.05572045e-02 -4.92485390e-02 -4.69210306e-02
 -4.51347833e-02 -4.38562935e-02 -4.31120705e-02 -4.20006692e-02
 -4.12551731e-02 -3.77891101e-02 -3.58042157e-02 -3.57870899e-02
 -3.30971546e-02 -3.19525714e-02 -3.10890001e-02 -2.72577646e-02
 -2.61387256e-02 -2.58566434e-02 -2.56697198e-02 -2.45050663e-02
 -2.36075365e-02 -2.33109485e-02 -1.69199157e-02 -1.02265643e-02
 -9.66789127e-03 -9.53531642e-03 -9.31751657e-03 -9.23878554e-03
 -9.02536177e-03 -7.19359361e-03 -2.91507177e-03 -1.82272351e-04
  3.53969686e-04  8.07507755e-04  8.65501875e-04  2.34937507e-03
  3.07885836e-03  3.34011447e-03  3.87878545e-03  4.12142180e-03
  6.61946548e-03  6.88693971e-03  1.30949882e-02  1.40684891e-02
  1.58868848e-02  1.92879410e-02  2.19907022e-02  2.26062662e-02
  2.53501598e-02  2.74698884e-02  2.83611609e-02  2.89317944e-02
  2.90077450e-02  3.00912398e-02  3.09446413e-02  3.15501684e-02
  3.18362960e-02  3.22806780e-02  3.30880142e-02  3.43096979e-02
  3.50700054e-02  3.79923819e-02  3.83993829e-02  3.90831961e-02
  4.07144962e-02  4.12725739e-02  4.26442821e-02  4.41777820e-02
  4.51483133e-02  4.62491237e-02  4.67418685e-02  5.69645398e-02
  5.77095540e-02  6.31560374e-02  6.32972679e-02  6.36754474e-02
  6.52919392e-02  6.79070205e-02  7.11195628e-02  7.34203305e-02
  7.58984538e-02  7.89706905e-02  7.91055053e-02  8.10904495e-02
  8.28640694e-02  8.37821128e-02  8.87088990e-02  9.04638615e-02
  9.24896029e-02  9.29328193e-02  9.38980768e-02  9.43751463e-02
  9.64198678e-02  9.78557086e-02  9.84871040e-02  9.95650000e-02
  1.00353932e-01  1.00839598e-01  1.01140121e-01  1.01882689e-01
  1.02567425e-01  1.03148895e-01  1.03213329e-01  1.05449780e-01
  1.06704233e-01  1.06892758e-01  1.07117391e-01  1.08960070e-01
  1.09932242e-01  1.10678065e-01  1.10945733e-01  1.13870888e-01
  1.14120856e-01  1.14657588e-01  1.17293764e-01  1.17702753e-01
  1.17748210e-01  1.18541374e-01  1.18769406e-01  1.21743775e-01
  1.25266920e-01  1.25711872e-01  1.25910432e-01  1.27318433e-01
  1.28354793e-01  1.29262811e-01  1.30146482e-01  1.33967904e-01
  1.34337424e-01  1.35172827e-01  1.37096376e-01  1.37952907e-01
  1.41051114e-01  1.41833530e-01  1.41986758e-01  1.42346611e-01
  1.45731764e-01  1.47182310e-01  1.47461404e-01  1.51214607e-01
  1.53150951e-01  1.54459924e-01  1.56829821e-01  1.57407722e-01
  1.64098519e-01  1.68195529e-01  1.68405562e-01  1.68480189e-01
  1.68987789e-01  1.69506623e-01  1.69954956e-01  1.74768427e-01
  1.79091698e-01  1.79879260e-01  1.82053746e-01  1.84376510e-01
  1.84802909e-01  1.85414593e-01  1.85445574e-01  1.86114882e-01
  1.87300866e-01  1.90303443e-01  1.91353366e-01  1.91845656e-01
  1.94297246e-01  1.94948169e-01  1.95679089e-01  1.96848847e-01
  1.98238144e-01  2.00205536e-01  2.03384025e-01  2.08550931e-01
  2.08688114e-01  2.09203644e-01  2.12834616e-01  2.13192795e-01
  2.13537225e-01  2.14090186e-01  2.15388585e-01  2.15691316e-01
  2.15792603e-01  2.17556950e-01  2.20552600e-01  2.20843682e-01
  2.20896206e-01  2.26526138e-01  2.26935005e-01  2.26939866e-01
  2.27495279e-01  2.27812255e-01  2.28359815e-01  2.32865710e-01
  2.33349617e-01  2.35839724e-01  2.35951354e-01  2.36771236e-01
  2.37870282e-01  2.38927259e-01  2.39468533e-01  2.42203227e-01
  2.42597497e-01  2.42746710e-01  2.42758645e-01  2.46603267e-01
  2.46834948e-01  2.48513425e-01  2.48529489e-01  2.49670987e-01
  2.49695743e-01  2.50427088e-01  2.51037328e-01  2.51122570e-01
  2.52353235e-01  2.53239097e-01  2.54012309e-01  2.55689346e-01
  2.55770341e-01  2.56169275e-01  2.57572719e-01  2.57583069e-01
  2.60487115e-01  2.60564075e-01  2.61697003e-01  2.67379860e-01
  2.70247332e-01  2.71955553e-01  2.73494216e-01  2.73537267e-01
  2.73558388e-01  2.74429543e-01  2.75011764e-01  2.75864462e-01
  2.76194627e-01  2.76635589e-01  2.77333342e-01  2.78373310e-01
  2.78554387e-01  2.80029010e-01  2.80162390e-01  2.81041280e-01
  2.81321336e-01  2.81325898e-01  2.81650143e-01  2.84617432e-01
  2.86859108e-01  2.91063329e-01  2.95778491e-01  3.02385239e-01
  3.03132947e-01  3.04011661e-01  3.09793139e-01  3.10939388e-01
  3.15618569e-01  3.18477814e-01  3.18617285e-01  3.18688995e-01
  3.19735021e-01  3.21408654e-01  3.22566924e-01  3.22922671e-01
  3.23351697e-01  3.24949585e-01  3.25058880e-01  3.25886587e-01
  3.26086690e-01  3.28878529e-01  3.30455430e-01  3.31122676e-01
  3.31289105e-01  3.31481053e-01  3.33556348e-01  3.34664779e-01
  3.36356002e-01  3.38787436e-01  3.39022535e-01  3.39295380e-01
  3.40217988e-01  3.48657687e-01  3.49709897e-01  3.57784464e-01
  3.58149622e-01  3.61824304e-01  3.63453892e-01  3.64437916e-01
  3.66664433e-01  3.67484524e-01  3.69610081e-01  3.70095642e-01
  3.72824837e-01  3.73192637e-01  3.74968134e-01  3.78395091e-01
  3.79462786e-01  3.80339660e-01  3.80490836e-01  3.81200719e-01
  3.82761653e-01  3.85223907e-01  3.85859998e-01  3.88960239e-01
  3.89015726e-01  3.89929377e-01  3.96656373e-01  3.99354343e-01
  4.01591965e-01  4.01778280e-01  4.03479221e-01  4.06005007e-01
  4.06495488e-01  4.06941655e-01  4.08558109e-01  4.11200246e-01
  4.13515650e-01  4.14634802e-01  4.27224206e-01  4.27679299e-01
  4.29169719e-01  4.29217666e-01  4.30357033e-01  4.32925917e-01
  4.33205549e-01  4.34999128e-01  4.35297041e-01  4.36803393e-01
  4.40956826e-01  4.41200914e-01  4.41528511e-01  4.42413833e-01
  4.45356606e-01  4.48124279e-01  4.49433600e-01  4.52408297e-01
  4.53436648e-01  4.55615255e-01  4.55945874e-01  4.56962058e-01
  4.57167681e-01  4.60715753e-01  4.61916486e-01  4.62227012e-01
  4.62389394e-01  4.62446891e-01  4.62578772e-01  4.63269194e-01
  4.64840070e-01  4.65397378e-01  4.72382991e-01  4.73839355e-01
  4.74080221e-01  4.74515190e-01  4.74627659e-01  4.75118088e-01
  4.75842551e-01  4.75999408e-01  4.76377466e-01  4.77265471e-01
  4.81254545e-01  4.81634325e-01  4.83175769e-01  4.84467999e-01
  4.85346916e-01  4.88120421e-01  4.88228966e-01  4.92638055e-01
  4.93284084e-01  4.93613031e-01  4.93998237e-01  4.94911000e-01
  4.95008319e-01  4.99550996e-01  5.00390876e-01  5.01053866e-01
  5.01632576e-01  5.05455369e-01  5.07944315e-01  5.11482782e-01
  5.11990662e-01  5.14923208e-01  5.18686451e-01  5.19017994e-01
  5.19037398e-01  5.19470228e-01  5.24268788e-01  5.25572158e-01
  5.26746900e-01  5.33796013e-01  5.34678456e-01  5.40593803e-01
  5.42169473e-01  5.42231130e-01  5.46435927e-01  5.48097891e-01
  5.49940677e-01  5.51145805e-01  5.54210776e-01  5.55771301e-01
  5.58873859e-01  5.59696308e-01  5.62022668e-01  5.64533499e-01
  5.65253305e-01  5.68619584e-01  5.69860354e-01  5.71482825e-01
  5.71959159e-01  5.76037025e-01  5.79603581e-01  5.80707385e-01
  5.81720708e-01  5.81901599e-01  5.85153673e-01  5.85376233e-01
  5.90833850e-01  5.95153402e-01  5.97229616e-01  6.01590805e-01
  6.03254278e-01  6.03888247e-01  6.03899632e-01  6.06716167e-01
  6.11884871e-01  6.15049692e-01  6.21463516e-01  6.21699639e-01
  6.23669024e-01  6.25224540e-01  6.25407842e-01  6.26182434e-01
  6.27792002e-01  6.28878364e-01  6.29666411e-01  6.31897656e-01
  6.35570001e-01  6.36603522e-01  6.37210482e-01  6.39226681e-01
  6.41710709e-01  6.48064784e-01  6.48995189e-01  6.49289914e-01
  6.55736918e-01  6.59022266e-01  6.59270163e-01  6.61604942e-01
  6.64936937e-01  6.66875825e-01  6.78041392e-01  6.88857191e-01
  6.89091652e-01  6.90332520e-01  6.92332535e-01  7.00516966e-01
  7.03533247e-01  7.07922954e-01  7.08363818e-01  7.18854641e-01
  7.23286973e-01]

  warnings.warn(

2022-11-03 10:50:52,166:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.73664340e-01 -1.70814324e-01 -1.64020826e-01 -1.59568552e-01
 -1.56759485e-01 -1.56572998e-01 -1.47049558e-01 -1.46140434e-01
 -1.42938694e-01 -1.42757081e-01 -1.42585535e-01 -1.40252852e-01
 -1.39719816e-01 -1.33749383e-01 -1.31846706e-01 -1.31097741e-01
 -1.29082928e-01 -1.27627764e-01 -1.18358645e-01 -1.16094971e-01
 -1.15913370e-01 -1.07697129e-01 -1.03855689e-01 -9.54559122e-02
 -9.46609175e-02 -9.04481547e-02 -8.97997858e-02 -8.80244018e-02
 -8.59615299e-02 -8.25919885e-02 -7.96822202e-02 -7.96802980e-02
 -7.67308565e-02 -7.52825226e-02 -7.30284558e-02 -6.93137885e-02
 -6.88601055e-02 -6.68759931e-02 -6.61213514e-02 -6.11377306e-02
 -6.03764605e-02 -5.90491011e-02 -5.74285069e-02 -5.49990001e-02
 -5.49952877e-02 -5.27649771e-02 -5.01681689e-02 -4.92270592e-02
 -4.51885557e-02 -4.31539107e-02 -3.49390051e-02 -3.30461694e-02
 -2.76527110e-02 -2.72452254e-02 -2.56473823e-02 -2.54819364e-02
 -2.51270713e-02 -2.40759493e-02 -2.28232803e-02 -2.24889732e-02
 -1.93119378e-02 -1.83636635e-02 -1.66590051e-02 -1.59320623e-02
 -1.54651989e-02 -1.24381326e-02 -9.56092298e-03 -7.58825968e-03
 -6.86525024e-03 -1.44451808e-03  7.12783208e-04  2.00566164e-03
  2.97567738e-03  3.35389953e-03  5.89305645e-03  8.16187350e-03
  8.43528645e-03  9.72709864e-03  1.13950541e-02  1.15813759e-02
  1.28305232e-02  1.59379270e-02  2.23512793e-02  2.50469032e-02
  2.79394314e-02  3.04146143e-02  3.12690544e-02  3.22761134e-02
  3.46237698e-02  4.02981601e-02  4.08030714e-02  4.12989732e-02
  4.26661612e-02  4.26952350e-02  4.27179821e-02  4.38874147e-02
  4.85875044e-02  4.86571118e-02  4.92956227e-02  6.04244893e-02
  6.37634180e-02  6.45494855e-02  6.66332900e-02  6.94312375e-02
  6.95010857e-02  7.01901692e-02  7.02003670e-02  7.02091033e-02
  7.62026555e-02  7.81522911e-02  8.24135138e-02  8.42242464e-02
  8.49920619e-02  8.54027009e-02  8.63269035e-02  8.63440386e-02
  8.63875718e-02  8.67068806e-02  8.93274056e-02  9.02433464e-02
  9.04727073e-02  9.13807467e-02  9.25175889e-02  9.32394510e-02
  9.39121285e-02  9.44242883e-02  9.70278546e-02  9.71646602e-02
  9.89138626e-02  1.01614350e-01  1.02337100e-01  1.04625680e-01
  1.06636233e-01  1.07414176e-01  1.09422165e-01  1.11438254e-01
  1.12853650e-01  1.15166407e-01  1.16008269e-01  1.17284138e-01
  1.19893063e-01  1.20116687e-01  1.22217773e-01  1.23280518e-01
  1.24887338e-01  1.25038144e-01  1.26711473e-01  1.27604340e-01
  1.29218519e-01  1.31150451e-01  1.31669622e-01  1.31879320e-01
  1.32132083e-01  1.36153407e-01  1.38277800e-01  1.39081316e-01
  1.39254159e-01  1.39422949e-01  1.40648417e-01  1.41725232e-01
  1.42666521e-01  1.42969416e-01  1.46241767e-01  1.47185136e-01
  1.48946999e-01  1.49613594e-01  1.49689292e-01  1.49836484e-01
  1.50095474e-01  1.57620776e-01  1.58647016e-01  1.60715940e-01
  1.61019400e-01  1.62508017e-01  1.65696432e-01  1.69040988e-01
  1.69054588e-01  1.69184008e-01  1.69730305e-01  1.70159999e-01
  1.71993608e-01  1.72169218e-01  1.72251082e-01  1.72569404e-01
  1.73480676e-01  1.73866977e-01  1.74206081e-01  1.74529911e-01
  1.76022340e-01  1.78308432e-01  1.78399059e-01  1.78403133e-01
  1.78454186e-01  1.79815124e-01  1.80684780e-01  1.80881324e-01
  1.81898720e-01  1.81978623e-01  1.82153297e-01  1.83483411e-01
  1.87163822e-01  1.87253913e-01  1.89018899e-01  1.89761977e-01
  1.91673814e-01  1.93428665e-01  1.97741652e-01  1.99335688e-01
  1.99799623e-01  2.00200437e-01  2.00931069e-01  2.01070696e-01
  2.03223901e-01  2.03506860e-01  2.03601179e-01  2.05512409e-01
  2.05958243e-01  2.06413689e-01  2.08590385e-01  2.10706166e-01
  2.11472660e-01  2.11509192e-01  2.12120966e-01  2.14424954e-01
  2.14937532e-01  2.16160286e-01  2.16535790e-01  2.17926320e-01
  2.18793366e-01  2.20242337e-01  2.21781876e-01  2.26152259e-01
  2.26450205e-01  2.26573621e-01  2.27672212e-01  2.28283999e-01
  2.30910168e-01  2.31628948e-01  2.32452302e-01  2.33750344e-01
  2.36506226e-01  2.40471673e-01  2.40815148e-01  2.41228668e-01
  2.43096265e-01  2.43115528e-01  2.44190218e-01  2.44364651e-01
  2.48835018e-01  2.50998917e-01  2.51693541e-01  2.54980363e-01
  2.55461847e-01  2.57261341e-01  2.59759196e-01  2.60939638e-01
  2.63320793e-01  2.64500570e-01  2.64923153e-01  2.66443381e-01
  2.71127899e-01  2.71412954e-01  2.71507005e-01  2.78891134e-01
  2.82904186e-01  2.82972540e-01  2.83579225e-01  2.86360307e-01
  2.87622010e-01  2.89048011e-01  2.90011278e-01  2.90083582e-01
  2.90129236e-01  2.90291005e-01  2.92312943e-01  2.92970766e-01
  2.95717346e-01  2.96713829e-01  2.97909565e-01  3.03623371e-01
  3.06269859e-01  3.08033476e-01  3.08414389e-01  3.09678594e-01
  3.11147198e-01  3.11911543e-01  3.11978779e-01  3.14439619e-01
  3.15789894e-01  3.16962333e-01  3.17437982e-01  3.18344364e-01
  3.19797806e-01  3.20262619e-01  3.23179165e-01  3.23597402e-01
  3.23604841e-01  3.24595690e-01  3.26166655e-01  3.27703576e-01
  3.27844042e-01  3.28312553e-01  3.28538634e-01  3.36840292e-01
  3.40879322e-01  3.41471057e-01  3.41712914e-01  3.44557111e-01
  3.44854788e-01  3.45894123e-01  3.47768317e-01  3.50961851e-01
  3.53015833e-01  3.53270621e-01  3.54228133e-01  3.54497888e-01
  3.55586085e-01  3.56683778e-01  3.61235928e-01  3.64737063e-01
  3.68653879e-01  3.70375216e-01  3.71027969e-01  3.72025231e-01
  3.73412362e-01  3.77953542e-01  3.77976188e-01  3.79042023e-01
  3.79251730e-01  3.80580394e-01  3.82860784e-01  3.86085218e-01
  3.86578398e-01  3.90502521e-01  3.92715963e-01  3.93769557e-01
  3.94425710e-01  3.95402423e-01  3.97439536e-01  3.98523129e-01
  3.99368323e-01  4.06660130e-01  4.07075187e-01  4.07218316e-01
  4.10882697e-01  4.11674588e-01  4.12997254e-01  4.16209248e-01
  4.19683425e-01  4.20529721e-01  4.24225152e-01  4.25360128e-01
  4.27188909e-01  4.28191740e-01  4.28830327e-01  4.29675973e-01
  4.33646170e-01  4.35925129e-01  4.42518125e-01  4.47068303e-01
  4.47759284e-01  4.48971355e-01  4.51832767e-01  4.53350163e-01
  4.53919635e-01  4.54193477e-01  4.54316284e-01  4.54694917e-01
  4.54747044e-01  4.56774334e-01  4.61276057e-01  4.61767933e-01
  4.62710475e-01  4.62769360e-01  4.64372754e-01  4.65271044e-01
  4.67811229e-01  4.68413908e-01  4.68847018e-01  4.70804184e-01
  4.71687890e-01  4.75996372e-01  4.76981520e-01  4.79693878e-01
  4.81212287e-01  4.83405009e-01  4.83553060e-01  4.92084656e-01
  4.92382027e-01  4.94307109e-01  4.95095438e-01  4.96445832e-01
  4.97117625e-01  4.98663668e-01  4.99299358e-01  5.02243974e-01
  5.05060329e-01  5.07697288e-01  5.08021960e-01  5.08828665e-01
  5.10455391e-01  5.14816295e-01  5.18129191e-01  5.18502293e-01
  5.18598148e-01  5.18630966e-01  5.24662218e-01  5.26072281e-01
  5.29597816e-01  5.33008599e-01  5.34063547e-01  5.34660287e-01
  5.35280998e-01  5.38345078e-01  5.40659644e-01  5.41566907e-01
  5.42146354e-01  5.42512548e-01  5.42669563e-01  5.47751097e-01
  5.47843433e-01  5.47843655e-01  5.49228796e-01  5.50553394e-01
  5.51726552e-01  5.51888538e-01  5.54087474e-01  5.54761183e-01
  5.54986847e-01  5.56343235e-01  5.56453971e-01  5.56511045e-01
  5.56524507e-01  5.63802173e-01  5.64028442e-01  5.66338069e-01
  5.68109125e-01  5.70919882e-01  5.79981358e-01  5.80880682e-01
  5.82176785e-01  5.86822278e-01  5.89466211e-01  5.99225196e-01
  6.00752809e-01  6.03120296e-01  6.05171867e-01  6.06132333e-01
  6.06907697e-01  6.08126415e-01  6.09400639e-01  6.11355450e-01
  6.12567356e-01  6.17257506e-01  6.18719097e-01  6.22064595e-01
  6.26608232e-01  6.27199248e-01  6.30656050e-01  6.31989246e-01
  6.33124324e-01  6.37990714e-01  6.39894833e-01  6.42047097e-01
  6.43866281e-01  6.44417223e-01  6.47033056e-01  6.49105745e-01
  6.54532021e-01  6.55644469e-01  6.57614511e-01  6.61456105e-01
  6.61907096e-01  6.69360983e-01  6.74256889e-01  6.74448058e-01
  6.74541296e-01  6.76865112e-01  6.77968106e-01  6.79128799e-01
  6.86025224e-01  6.89161844e-01  6.89633444e-01  7.00518518e-01
  7.13533633e-01  7.14766633e-01  7.19807607e-01  7.22392720e-01
  7.53981484e-01  7.55879948e-01  7.59369222e-01  7.60767498e-01
  7.70543243e-01]

  warnings.warn(

2022-11-03 10:50:52,202:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.21227549 -0.16979526 -0.16248269 -0.15814182 -0.148695   -0.14604559
 -0.1354599  -0.13495242 -0.13269035 -0.12969945 -0.12648436 -0.12565762
 -0.12504077 -0.11881392 -0.1138547  -0.11259228 -0.10968298 -0.10940607
 -0.10903423 -0.10761337 -0.10424113 -0.10103071 -0.10076058 -0.09936239
 -0.09883419 -0.09246691 -0.08635161 -0.07957852 -0.07738912 -0.07113382
 -0.06989331 -0.0648136  -0.06219308 -0.06019977 -0.05859338 -0.05752304
 -0.05441505 -0.05098917 -0.05092668 -0.04957223 -0.04911774 -0.04892642
 -0.04791445 -0.04715452 -0.04535142 -0.04439641 -0.04244158 -0.0418049
 -0.03945525 -0.03569171 -0.0355503  -0.03515793 -0.03215923 -0.03127173
 -0.02871115 -0.02866525 -0.02810685 -0.02388449 -0.0195057  -0.01917767
 -0.01813805 -0.01480004 -0.01362827 -0.01270768 -0.00971886 -0.00604656
  0.00384204  0.00578375  0.00668997  0.00768284  0.00884903  0.00895651
  0.00908771  0.01079894  0.01357178  0.01428505  0.01915865  0.01962805
  0.02200355  0.02511208  0.03007116  0.03155088  0.03158432  0.03222175
  0.03288548  0.0354411   0.03675283  0.03680052  0.03731022  0.03939888
  0.04100079  0.04182097  0.04494379  0.04638717  0.04671887  0.0490721
  0.05043847  0.05153484  0.05225532  0.05278701  0.05426552  0.05430581
  0.0572586   0.05909782  0.05987035  0.06031321  0.06521979  0.06589879
  0.06880847  0.06937248  0.07147313  0.07412608  0.07434124  0.07640248
  0.07711312  0.07744809  0.07758923  0.08120435  0.08261978  0.08300438
  0.08354662  0.08457828  0.0860562   0.08734639  0.08735955  0.08843292
  0.09155853  0.09316794  0.09333239  0.09601867  0.09625417  0.10003828
  0.10198587  0.10204447  0.10213357  0.10273507  0.10607644  0.10711661
  0.10733271  0.11044111  0.11051617  0.11101661  0.11302553  0.113189
  0.11457264  0.11648155  0.11727939  0.11803736  0.11863654  0.1209784
  0.12163569  0.12227456  0.12332219  0.12423448  0.12479538  0.12504823
  0.12607588  0.12805689  0.12842838  0.13003677  0.13390383  0.1371035
  0.13741414  0.13937935  0.14137834  0.14273202  0.14515222  0.14666545
  0.1470479   0.14707757  0.14946365  0.15115219  0.1531069   0.16060792
  0.16156291  0.16161266  0.16219954  0.16399858  0.16615832  0.17019499
  0.17120681  0.17349182  0.17413455  0.17433969  0.17605175  0.17605773
  0.1761313   0.17863581  0.17943517  0.18003697  0.18206248  0.18361307
  0.18601351  0.18808998  0.18830953  0.19089839  0.19091415  0.19122052
  0.19127922  0.19256462  0.19508117  0.19526381  0.19531284  0.19565106
  0.19835076  0.19928476  0.20035601  0.20131256  0.20192973  0.20390034
  0.20540372  0.20586786  0.20701077  0.20870813  0.20971621  0.21058104
  0.21085682  0.21162333  0.21273819  0.21555208  0.21557347  0.21579827
  0.21754291  0.21852374  0.22105868  0.22117604  0.22265782  0.22288097
  0.22583205  0.22879933  0.22890848  0.22935483  0.22968502  0.22980367
  0.2301059   0.23110363  0.23512584  0.23545152  0.23575327  0.23578831
  0.23579905  0.23632061  0.23686058  0.23950006  0.24184539  0.24444117
  0.24468332  0.2448084   0.24889804  0.25049182  0.25055685  0.25199083
  0.25364804  0.25385666  0.25448129  0.25538101  0.25912945  0.26045476
  0.26135476  0.26174372  0.26308825  0.26627056  0.26755318  0.26848043
  0.26874107  0.27086697  0.27295107  0.27684645  0.28011716  0.2855781
  0.28579244  0.28654433  0.28946583  0.29068927  0.29248845  0.29305778
  0.29519955  0.29658275  0.29662423  0.29687053  0.29738207  0.29887859
  0.30023371  0.30156307  0.30185497  0.3026742   0.3032752   0.3039306
  0.30439509  0.30466558  0.3049635   0.3050262   0.30558109  0.30602684
  0.30642785  0.30722117  0.30857096  0.30913474  0.30973441  0.3102648
  0.31163725  0.31202904  0.3140027   0.31452345  0.31591837  0.31696479
  0.31715321  0.31758768  0.31890605  0.32103467  0.32380583  0.32496005
  0.32555169  0.32713488  0.32789116  0.32793856  0.32879383  0.32882821
  0.32932467  0.33087285  0.33459198  0.33493423  0.33856507  0.34303474
  0.34309424  0.34360619  0.34457122  0.34983562  0.35053284  0.3525288
  0.35360078  0.35779929  0.35902956  0.36023525  0.36129358  0.36135767
  0.36248011  0.36390292  0.36607118  0.36907766  0.36982366  0.37197622
  0.37303057  0.37414033  0.37656402  0.37830399  0.37939631  0.38058976
  0.38231489  0.38262851  0.38311279  0.38405768  0.38496046  0.38558245
  0.38836869  0.39280362  0.39558421  0.39586417  0.40039495  0.40142032
  0.41276225  0.41356116  0.41627792  0.4174557   0.41866115  0.41984949
  0.42149484  0.42293609  0.42434459  0.4250872   0.42514113  0.42576175
  0.43053102  0.43269689  0.43395389  0.43596281  0.43727266  0.43966142
  0.44099411  0.4413308   0.44461153  0.44561589  0.44649677  0.44722692
  0.45279064  0.45364476  0.45516736  0.45988353  0.46307708  0.46556885
  0.46738412  0.46794694  0.47096677  0.47102897  0.47137947  0.47715607
  0.4775179   0.48259746  0.48417377  0.485767    0.4858794   0.48806939
  0.49111502  0.49249743  0.49373593  0.49622478  0.49685666  0.49939075
  0.50058323  0.50101597  0.50327041  0.5098589   0.51393917  0.5145728
  0.51569245  0.51599532  0.51969675  0.52146914  0.52407424  0.52607035
  0.52639091  0.52685654  0.5302436   0.53084244  0.53210153  0.53230131
  0.5346022   0.53616177  0.53678108  0.53690728  0.54265959  0.54413429
  0.54466358  0.5479149   0.54856617  0.55137984  0.55552336  0.56035264
  0.5625363   0.5627953   0.56459123  0.57474261  0.57617284  0.57655665
  0.57965737  0.58034524  0.58779389  0.59068084  0.59120077  0.59120527
  0.59835441  0.60013328  0.60146908  0.60155715  0.6034981   0.60471034
  0.60575079  0.60697887  0.6074538   0.60835725  0.60993852  0.61248278
  0.61302397  0.61814374  0.62091729  0.6230532   0.6280799   0.63018289
  0.633361    0.63507376  0.64259726  0.65226757  0.6526865   0.65484616
  0.66232078  0.66382788  0.66559962  0.66733172  0.66886297  0.67039306
  0.67224147  0.68126469  0.68133896  0.6830735   0.68403825  0.68801664
  0.69440353  0.69713229  0.69733077  0.70753831  0.72335201  0.72363208
  0.72742759]

  warnings.warn(

2022-11-03 10:50:52,255:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.88495778e-01 -1.66805387e-01 -1.61900028e-01 -1.53058841e-01
 -1.47654750e-01 -1.46804812e-01 -1.40839819e-01 -1.40593167e-01
 -1.36269524e-01 -1.32319327e-01 -1.30119507e-01 -1.29588137e-01
 -1.28062164e-01 -1.26991294e-01 -1.23053514e-01 -1.22652936e-01
 -1.22631866e-01 -1.22611980e-01 -1.19581866e-01 -1.17957475e-01
 -1.16525025e-01 -1.09054156e-01 -1.06846124e-01 -1.03743465e-01
 -1.02427926e-01 -1.02046317e-01 -1.01573370e-01 -9.74013606e-02
 -9.61498525e-02 -9.16701525e-02 -8.93057131e-02 -8.77472950e-02
 -8.36351213e-02 -8.31841863e-02 -8.31058779e-02 -7.84478357e-02
 -7.76074990e-02 -7.31073731e-02 -7.02994763e-02 -6.56931530e-02
 -6.55008599e-02 -6.35502420e-02 -6.24709695e-02 -6.13923177e-02
 -6.09276834e-02 -6.03511939e-02 -5.44062199e-02 -5.26429508e-02
 -4.91882564e-02 -4.69756875e-02 -4.22441715e-02 -4.16740410e-02
 -3.96175094e-02 -3.46551047e-02 -2.86699278e-02 -2.82325280e-02
 -2.70246552e-02 -2.69906461e-02 -2.50836095e-02 -2.31652189e-02
 -2.31639746e-02 -2.29808774e-02 -1.98781773e-02 -1.70759493e-02
 -1.66011666e-02 -1.64957891e-02 -1.58299597e-02 -1.54588243e-02
 -1.47080043e-02 -1.41677345e-02 -1.29732611e-02 -1.25733848e-02
 -1.04311880e-02 -7.16517922e-03 -4.56934350e-03 -4.38412224e-03
 -4.06656498e-03 -2.71446964e-03  7.28875968e-04  7.74424108e-04
  6.24842361e-03  8.78651037e-03  1.06989580e-02  1.21910175e-02
  1.42756649e-02  1.63382250e-02  1.63813263e-02  1.72761111e-02
  1.93387746e-02  2.04036604e-02  2.04916963e-02  2.06029078e-02
  2.09091007e-02  2.21639717e-02  2.32413973e-02  2.41758887e-02
  2.55169625e-02  2.55491735e-02  2.94057260e-02  3.25167021e-02
  3.25642904e-02  3.49252228e-02  4.05844732e-02  4.13316163e-02
  4.15996888e-02  4.47156887e-02  4.52388187e-02  4.55989950e-02
  4.57531106e-02  4.57647607e-02  4.81357138e-02  5.20609806e-02
  5.33029279e-02  5.36792198e-02  5.45853697e-02  5.66465683e-02
  5.72973037e-02  5.79601281e-02  5.96867428e-02  6.21958098e-02
  6.30843259e-02  6.37866069e-02  6.82121944e-02  6.99073856e-02
  7.39348219e-02  7.45558724e-02  7.63020563e-02  7.64360208e-02
  7.75427362e-02  8.13282223e-02  8.21181466e-02  8.44756833e-02
  8.45145498e-02  8.59005307e-02  8.71213827e-02  8.77824447e-02
  8.78021343e-02  8.78051084e-02  8.80490426e-02  8.89803484e-02
  9.29849253e-02  9.64772879e-02  9.80229443e-02  1.02919550e-01
  1.06547988e-01  1.07923117e-01  1.09833815e-01  1.11413952e-01
  1.15327534e-01  1.16865848e-01  1.17792478e-01  1.19794892e-01
  1.20389225e-01  1.22594101e-01  1.25508539e-01  1.27351498e-01
  1.28352698e-01  1.29893617e-01  1.30017880e-01  1.30880932e-01
  1.31300751e-01  1.32600492e-01  1.34848803e-01  1.37290761e-01
  1.37667266e-01  1.37795980e-01  1.37870597e-01  1.38309178e-01
  1.39467221e-01  1.40798951e-01  1.40955182e-01  1.42920466e-01
  1.44596732e-01  1.45659974e-01  1.49127250e-01  1.51262980e-01
  1.53160029e-01  1.56444997e-01  1.56450469e-01  1.56975965e-01
  1.59458053e-01  1.60835801e-01  1.61399748e-01  1.64892501e-01
  1.65688178e-01  1.69213025e-01  1.69301379e-01  1.70749349e-01
  1.73325326e-01  1.73425516e-01  1.77274451e-01  1.78966451e-01
  1.83614639e-01  1.86219430e-01  1.86450042e-01  1.88616662e-01
  1.91200063e-01  1.95563118e-01  1.96504196e-01  1.96713929e-01
  1.99148977e-01  2.09154421e-01  2.09504689e-01  2.11112516e-01
  2.12123869e-01  2.13065854e-01  2.13291263e-01  2.16793507e-01
  2.18440508e-01  2.18441714e-01  2.19839062e-01  2.20302820e-01
  2.20425501e-01  2.21782852e-01  2.22096237e-01  2.22205387e-01
  2.23463538e-01  2.23754109e-01  2.24768080e-01  2.25725629e-01
  2.28497457e-01  2.30756431e-01  2.31734622e-01  2.36213818e-01
  2.37635207e-01  2.38259751e-01  2.40422211e-01  2.42199396e-01
  2.42711571e-01  2.42903759e-01  2.43659717e-01  2.44269709e-01
  2.44552077e-01  2.46594965e-01  2.50283956e-01  2.51194141e-01
  2.54794760e-01  2.58289662e-01  2.61197219e-01  2.62309791e-01
  2.62339188e-01  2.64909772e-01  2.65645985e-01  2.65918776e-01
  2.65949818e-01  2.67067412e-01  2.67634687e-01  2.72638128e-01
  2.74901066e-01  2.76184443e-01  2.76233020e-01  2.80032694e-01
  2.81155376e-01  2.82560096e-01  2.83412732e-01  2.83951187e-01
  2.86530220e-01  2.88470908e-01  2.89736477e-01  2.92537621e-01
  2.93069822e-01  2.94956504e-01  2.95983727e-01  2.97023882e-01
  2.97411658e-01  2.97646956e-01  2.98737691e-01  3.00785443e-01
  3.00875691e-01  3.04513074e-01  3.09387423e-01  3.10627185e-01
  3.10740817e-01  3.11967554e-01  3.12968688e-01  3.13741911e-01
  3.18513746e-01  3.19074135e-01  3.20430442e-01  3.21098849e-01
  3.22750774e-01  3.22914027e-01  3.24301157e-01  3.27783185e-01
  3.29319850e-01  3.30482169e-01  3.33933411e-01  3.34372500e-01
  3.34555183e-01  3.34934464e-01  3.35282743e-01  3.40770646e-01
  3.40965455e-01  3.41202225e-01  3.45959545e-01  3.46450738e-01
  3.48185007e-01  3.48335659e-01  3.49126313e-01  3.51311496e-01
  3.53820930e-01  3.56222514e-01  3.56651129e-01  3.59856563e-01
  3.60542213e-01  3.61764166e-01  3.63581607e-01  3.65463755e-01
  3.66804488e-01  3.67830966e-01  3.71790068e-01  3.71903765e-01
  3.72906516e-01  3.73352539e-01  3.73994395e-01  3.75811600e-01
  3.76414900e-01  3.78919284e-01  3.79898587e-01  3.80330461e-01
  3.81738700e-01  3.82778747e-01  3.84278199e-01  3.86292191e-01
  3.87131309e-01  3.90082437e-01  3.91253270e-01  3.91483996e-01
  3.91534452e-01  3.93353806e-01  3.93582926e-01  3.94597378e-01
  3.96089107e-01  4.02757605e-01  4.05141001e-01  4.06788520e-01
  4.08162148e-01  4.08390411e-01  4.09992452e-01  4.10685298e-01
  4.11032788e-01  4.12747049e-01  4.12907310e-01  4.13302521e-01
  4.15400625e-01  4.16413668e-01  4.20938765e-01  4.21034672e-01
  4.21902803e-01  4.26272625e-01  4.27643005e-01  4.28215802e-01
  4.28314174e-01  4.32075922e-01  4.32835615e-01  4.41658871e-01
  4.42279181e-01  4.42281658e-01  4.43672809e-01  4.47236604e-01
  4.48352645e-01  4.48792954e-01  4.49909887e-01  4.49972645e-01
  4.51715483e-01  4.53469052e-01  4.53960016e-01  4.54606580e-01
  4.59086792e-01  4.59259738e-01  4.60862820e-01  4.63460342e-01
  4.67661784e-01  4.68761267e-01  4.69727712e-01  4.72157316e-01
  4.72865710e-01  4.73432370e-01  4.73645745e-01  4.73963072e-01
  4.75771794e-01  4.77066979e-01  4.79068255e-01  4.79300190e-01
  4.80747803e-01  4.82337324e-01  4.83708916e-01  4.93374941e-01
  4.93599977e-01  4.93624438e-01  4.94577986e-01  4.94759716e-01
  4.94793079e-01  4.98202015e-01  4.99330120e-01  5.03714280e-01
  5.07793631e-01  5.09470259e-01  5.09832894e-01  5.10769960e-01
  5.12850551e-01  5.16122071e-01  5.17158314e-01  5.17275371e-01
  5.20258448e-01  5.20415462e-01  5.21162258e-01  5.22114483e-01
  5.22587739e-01  5.26063820e-01  5.26627026e-01  5.26852482e-01
  5.30334940e-01  5.34127675e-01  5.34827051e-01  5.35311819e-01
  5.35530486e-01  5.36184152e-01  5.36378670e-01  5.36396146e-01
  5.36537118e-01  5.36966493e-01  5.39970126e-01  5.43221276e-01
  5.44169953e-01  5.44818715e-01  5.49032696e-01  5.52353731e-01
  5.57584122e-01  5.59537174e-01  5.60107509e-01  5.62863049e-01
  5.63577156e-01  5.66089214e-01  5.68375039e-01  5.72757534e-01
  5.73742888e-01  5.79044786e-01  5.79994561e-01  5.82778411e-01
  5.82843485e-01  5.89090454e-01  5.90929878e-01  5.93647268e-01
  5.94794635e-01  5.96815198e-01  5.98197896e-01  6.02976139e-01
  6.05393997e-01  6.06623601e-01  6.08427648e-01  6.08454350e-01
  6.09536389e-01  6.10774002e-01  6.13240473e-01  6.13398987e-01
  6.16101557e-01  6.16437932e-01  6.16882449e-01  6.17405889e-01
  6.18167871e-01  6.21101745e-01  6.23073774e-01  6.26498996e-01
  6.27189984e-01  6.27282229e-01  6.28053144e-01  6.29613823e-01
  6.30394683e-01  6.32515819e-01  6.33690078e-01  6.36395658e-01
  6.36640190e-01  6.44390285e-01  6.46414850e-01  6.54867178e-01
  6.54877980e-01  6.59219556e-01  6.65929644e-01  6.69964522e-01
  6.70409470e-01  6.71810851e-01  6.74661176e-01  6.78749214e-01
  6.79034912e-01  6.81431603e-01  6.85593641e-01  7.02015623e-01
  7.08000439e-01  7.12136130e-01  7.22105155e-01  7.35327976e-01
  7.50577087e-01]

  warnings.warn(

2022-11-03 10:50:52,374:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.1858178  -0.18523791 -0.14937594 -0.14914649 -0.14573196 -0.1404447
 -0.13721105 -0.13453853 -0.13436871 -0.13354472 -0.13127526 -0.13119366
 -0.13092752 -0.12969674 -0.12910107 -0.12776109 -0.12684199 -0.12548519
 -0.12522214 -0.12282995 -0.12207035 -0.12143129 -0.11759893 -0.11664442
 -0.11486781 -0.1145079  -0.11110178 -0.10076064 -0.09968907 -0.09941117
 -0.09692984 -0.09563822 -0.09393579 -0.09089733 -0.09025844 -0.08929462
 -0.08852075 -0.08538528 -0.0830378  -0.08065005 -0.07972137 -0.07933722
 -0.07760436 -0.07552354 -0.07509455 -0.07482642 -0.07262352 -0.07261618
 -0.07057725 -0.06823385 -0.06271807 -0.06265511 -0.05528378 -0.05447057
 -0.05349532 -0.05319307 -0.05242617 -0.05065778 -0.05026382 -0.04936213
 -0.04843971 -0.04544454 -0.04462167 -0.04402641 -0.039013   -0.03891393
 -0.03839533 -0.03804097 -0.03738363 -0.03726992 -0.03452361 -0.03412122
 -0.03265722 -0.03216237 -0.03207732 -0.03148624 -0.02605154 -0.02464608
 -0.0244059  -0.02228761 -0.02124923 -0.02084609 -0.01678861 -0.0159481
 -0.01584593 -0.01114945 -0.01106202 -0.0097737  -0.00926966 -0.00607298
 -0.0032489  -0.00311556  0.0011966   0.00347783  0.00481911  0.00622837
  0.00715118  0.0088141   0.01042052  0.01683472  0.02110244  0.02343226
  0.02361128  0.02582905  0.02632247  0.02895879  0.03231646  0.03418443
  0.03718327  0.03914202  0.03931088  0.04008805  0.04092982  0.04102043
  0.04128496  0.041493    0.04230755  0.04284057  0.04645434  0.0486542
  0.04980712  0.05096086  0.05532804  0.05604611  0.05633605  0.05672889
  0.06047408  0.06110628  0.06275225  0.0631849   0.06329674  0.06343779
  0.06426895  0.06624191  0.06730016  0.07051501  0.07376407  0.07524547
  0.07744494  0.07858493  0.07996451  0.08045754  0.08049691  0.08104448
  0.08248635  0.08316045  0.08656741  0.08929494  0.09055454  0.09249305
  0.0980747   0.10032469  0.10084151  0.10569561  0.10585757  0.10938076
  0.10992067  0.11262224  0.11270431  0.11423128  0.11550078  0.11590901
  0.1162997   0.11840178  0.1187062   0.12052569  0.12122989  0.12323168
  0.12546425  0.12818721  0.1297677   0.13030672  0.13262424  0.13425498
  0.13430294  0.13573982  0.13641558  0.14036364  0.14105778  0.14257581
  0.14443208  0.1451438   0.14554668  0.14751808  0.1478098   0.14947895
  0.14968408  0.14980309  0.15063599  0.1518148   0.15257096  0.15426288
  0.15463609  0.15716176  0.15784578  0.16001129  0.16393243  0.16571392
  0.16722751  0.16838325  0.16998471  0.1703517   0.17117556  0.17217903
  0.17240748  0.17625492  0.17684216  0.17690839  0.18030446  0.18113094
  0.18331327  0.18589158  0.18614192  0.18642136  0.18691874  0.18774717
  0.18811042  0.18836771  0.18953461  0.19010601  0.19041402  0.19430857
  0.19543482  0.19719302  0.19785586  0.19908377  0.20207388  0.20313803
  0.20514579  0.20527324  0.20800751  0.20897829  0.21019296  0.21204339
  0.21243681  0.21633156  0.21832407  0.21940134  0.2202374   0.22042219
  0.22048616  0.2231556   0.22338566  0.22366325  0.22466017  0.22509349
  0.22668439  0.22741206  0.22800616  0.22878716  0.2298672   0.23143313
  0.23915472  0.23937037  0.24034758  0.24161179  0.2418309   0.24354022
  0.24462789  0.24514249  0.24666569  0.24999759  0.25087147  0.25140641
  0.25789084  0.25891233  0.26135589  0.26138788  0.26239196  0.26350975
  0.26474987  0.26603216  0.26892679  0.27016483  0.27213607  0.27400293
  0.27688352  0.27945097  0.28012725  0.2812499   0.28377739  0.28456471
  0.28550041  0.286015    0.2889251   0.30098539  0.30146453  0.30343566
  0.30499397  0.30767389  0.30790123  0.30800252  0.30980109  0.31651589
  0.31668931  0.32741008  0.32823004  0.33054109  0.33612764  0.33616774
  0.338628    0.33949338  0.34603906  0.34660968  0.34819335  0.35031108
  0.35054113  0.35170571  0.35222331  0.35237258  0.3532793   0.35415048
  0.35460197  0.35561029  0.35767238  0.35774991  0.35855347  0.35988266
  0.36611719  0.36856364  0.37045724  0.37256177  0.3726342   0.37574512
  0.37963047  0.38161112  0.38183544  0.3821105   0.38326428  0.38425593
  0.38680349  0.38858591  0.39306669  0.39646322  0.39771011  0.39860081
  0.39959852  0.4021048   0.40227521  0.40405581  0.40507262  0.40515814
  0.40603919  0.40764424  0.40884633  0.41361352  0.41446585  0.4163235
  0.42071407  0.42512721  0.42574813  0.42826042  0.4311314   0.43225811
  0.43276196  0.43432739  0.43744571  0.43966707  0.44217951  0.44235384
  0.44273951  0.44390442  0.44576281  0.4462287   0.44632612  0.44926386
  0.45131508  0.45199419  0.45758674  0.46072136  0.46130056  0.46214463
  0.46323438  0.46340749  0.46382518  0.46425572  0.46509822  0.4659724
  0.46827697  0.46834908  0.46853706  0.46962183  0.47206802  0.47255736
  0.47379159  0.47402877  0.47435098  0.47438967  0.47517591  0.47745281
  0.48000922  0.48104341  0.48415926  0.48609407  0.48738378  0.49336065
  0.49558294  0.4971712   0.5001828   0.50048054  0.50084028  0.50265703
  0.50504821  0.50884653  0.50938956  0.51257332  0.51413014  0.51881469
  0.5208183   0.52187063  0.52283678  0.52429197  0.52672069  0.52837517
  0.53076148  0.53234528  0.53418693  0.53720535  0.54115434  0.54283856
  0.54318081  0.54873985  0.55020396  0.55067402  0.5510734   0.55119183
  0.55238949  0.55350371  0.55777865  0.56033847  0.56129932  0.56280916
  0.56301779  0.563764    0.5665566   0.56681553  0.5670815   0.56716693
  0.56843611  0.56944455  0.56984676  0.57163116  0.57246815  0.57264103
  0.57367222  0.57590034  0.57641405  0.58037232  0.58061255  0.58347663
  0.58579257  0.58998954  0.59138033  0.59633475  0.6058484   0.6064259
  0.6067866   0.60890367  0.61384315  0.61526394  0.61554502  0.61569841
  0.6181153   0.6198661   0.62346744  0.62476079  0.62496768  0.64207761
  0.64245791  0.64394416  0.64445997  0.6467781   0.64915811  0.65026686
  0.65882938  0.66207575  0.66597326  0.66694228  0.66745187  0.66798766
  0.6714852   0.67192078  0.6759338   0.68018889  0.68349563  0.68527653
  0.68757428  0.69048485  0.6984434   0.70033552  0.70173735  0.70884698
  0.71720177]

  warnings.warn(

2022-11-03 10:50:52,484:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.17673864 -0.17048987 -0.15979402 -0.14818382 -0.14114884 -0.13395027
 -0.13146779 -0.13069872 -0.12550519 -0.12074727 -0.11960249 -0.11510166
 -0.11378273 -0.10821942 -0.107764   -0.10750714 -0.10667065 -0.10333323
 -0.10248055 -0.10042046 -0.10022143 -0.09855319 -0.0947004  -0.09346623
 -0.08509704 -0.08466409 -0.08269339 -0.08225847 -0.07959583 -0.07870752
 -0.07573521 -0.07465144 -0.07119171 -0.06377678 -0.06008879 -0.05932318
 -0.05880952 -0.05740793 -0.05565138 -0.05132321 -0.04933676 -0.04908029
 -0.04830076 -0.04815382 -0.04631891 -0.04528172 -0.04466669 -0.04343109
 -0.04295808 -0.04281641 -0.04261954 -0.04047501 -0.0400243  -0.0375289
 -0.0356903  -0.03401872 -0.03318291 -0.03279815 -0.03191314 -0.03176281
 -0.03167602 -0.03071845 -0.02670288 -0.02668581 -0.02422929 -0.02277454
 -0.02146864 -0.02065124 -0.01764508 -0.01537311 -0.01194772 -0.01173088
 -0.01091289 -0.00462435 -0.0043224   0.00085299  0.00125697  0.0012765
  0.00374117  0.00630151  0.00668372  0.01130411  0.01323219  0.01331636
  0.0142217   0.01482961  0.01768813  0.0206917   0.02357945  0.02372196
  0.02528396  0.0275215   0.02855484  0.03142226  0.03474338  0.03488561
  0.03528377  0.03676884  0.03790629  0.03796427  0.03877159  0.03884832
  0.0396781   0.0411765   0.0413205   0.04265428  0.04491629  0.04502302
  0.04576918  0.05053583  0.05082958  0.05180741  0.05317945  0.05331836
  0.05776487  0.05946863  0.05970295  0.05974101  0.06009296  0.06046698
  0.06190057  0.0640654   0.06476261  0.0657423   0.06576706  0.066214
  0.07018646  0.07170608  0.07362751  0.07640989  0.07684782  0.07825005
  0.08188373  0.08198395  0.08205556  0.08415505  0.08671672  0.08679543
  0.08891593  0.08928032  0.09032495  0.092826    0.09303742  0.09374227
  0.09384968  0.0958171   0.09650753  0.09705736  0.09736851  0.09920963
  0.0997628   0.10225252  0.10234481  0.10354589  0.10432811  0.10441412
  0.10555475  0.10705929  0.10754077  0.11034501  0.11146433  0.11204439
  0.11205803  0.11504288  0.11760678  0.11939862  0.12247297  0.12597549
  0.12778198  0.1294546   0.13023853  0.13270507  0.13387242  0.13533148
  0.14035033  0.14295321  0.1491367   0.14965372  0.15018005  0.15080086
  0.15081726  0.1559557   0.15680042  0.15815967  0.15952175  0.16292546
  0.16324038  0.16527766  0.16662175  0.16824801  0.17056607  0.17315002
  0.17366391  0.1737516   0.17550195  0.17631458  0.17911883  0.18031237
  0.18049773  0.18238479  0.18272284  0.18604101  0.18764568  0.18799956
  0.18811974  0.18953683  0.19098649  0.19212812  0.19444593  0.19722912
  0.19955403  0.20435073  0.20473535  0.20677439  0.20758481  0.20879928
  0.21052873  0.21202095  0.21451216  0.21502424  0.21727802  0.2178271
  0.21792036  0.22053641  0.22135087  0.22155306  0.22429026  0.22510362
  0.22598149  0.22620516  0.22716548  0.2274459   0.22758391  0.23126792
  0.23134577  0.23322296  0.23540193  0.23575423  0.23626687  0.23654402
  0.23833924  0.23887358  0.23925945  0.24068727  0.24070252  0.24101557
  0.24792496  0.24840681  0.24921926  0.24964179  0.25069618  0.25236852
  0.25334362  0.2540027   0.25822897  0.25961938  0.26355655  0.26394538
  0.26578524  0.26594001  0.26902092  0.26924172  0.26967597  0.27208989
  0.27520465  0.27546944  0.2773874   0.27768585  0.27774989  0.27838714
  0.28408154  0.28749907  0.28797705  0.28932275  0.29367966  0.29385556
  0.29399141  0.30004505  0.30121362  0.30135131  0.30309898  0.30335515
  0.3037312   0.30488083  0.30652383  0.30707589  0.30714959  0.30715846
  0.30813035  0.30864128  0.30949616  0.31017159  0.31092885  0.31122608
  0.31245411  0.31276688  0.31407169  0.31474349  0.315524    0.31683357
  0.31858021  0.32147788  0.32267583  0.32603576  0.32647189  0.32669164
  0.3271636   0.32737605  0.3280575   0.33250438  0.33264428  0.33466383
  0.33504552  0.34032129  0.34497357  0.34512568  0.34665641  0.34742946
  0.35116424  0.35424921  0.35460347  0.35563995  0.35671905  0.35679426
  0.35822992  0.35937929  0.36110086  0.36199554  0.36317348  0.364781
  0.3684344   0.36960356  0.36998044  0.37131105  0.37438389  0.37539415
  0.37745983  0.37874135  0.38004769  0.38092131  0.38143833  0.38299865
  0.38756152  0.38816498  0.38844925  0.39726331  0.39927256  0.40054198
  0.41002524  0.41130947  0.41131104  0.41215558  0.41254449  0.41443596
  0.41501904  0.41521829  0.41734495  0.41779583  0.42113133  0.42298968
  0.42855862  0.4301076   0.43109988  0.43392404  0.43429456  0.43508867
  0.43600278  0.43858651  0.43964053  0.44208689  0.44265201  0.44539012
  0.44571865  0.4487335   0.44893671  0.45363935  0.4564092   0.45655088
  0.45808413  0.46150486  0.46210155  0.46260002  0.4631179   0.46507746
  0.46573712  0.46881886  0.47133271  0.47222066  0.4736494   0.47395454
  0.47507529  0.48079908  0.48719231  0.48758816  0.48983572  0.49250933
  0.49276883  0.49321688  0.4959082   0.49763666  0.49815368  0.5000148
  0.5035305   0.50378698  0.50531122  0.5065384   0.50757457  0.51353238
  0.51854303  0.51965416  0.5214473   0.52576862  0.52771022  0.52801351
  0.52881894  0.52925872  0.53154334  0.53414487  0.53731443  0.54057488
  0.54321129  0.54537587  0.54998832  0.55181748  0.55210803  0.55486806
  0.55517333  0.55811348  0.56529834  0.56873361  0.57139518  0.57625014
  0.57720566  0.57801386  0.58004371  0.58653295  0.58655271  0.59418016
  0.59446167  0.59620959  0.59734587  0.5977213   0.59798903  0.59829372
  0.59863986  0.60014084  0.60356568  0.60494488  0.60516143  0.60743673
  0.60827295  0.60983745  0.61203596  0.61312995  0.61364221  0.61503698
  0.61509186  0.61584937  0.61955444  0.62120696  0.62285977  0.62792964
  0.63001315  0.63132215  0.63183862  0.64609191  0.64949575  0.65132336
  0.65194794  0.65286652  0.65346175  0.65406206  0.65750559  0.6607066
  0.6625536   0.66375611  0.6646906   0.66690442  0.66793696  0.67307952
  0.67425893  0.67628405  0.69402936  0.69958648  0.7010376   0.70964483
  0.70982924  0.71159708  0.71427134  0.7196269   0.72709859  0.75332583]

  warnings.warn(

2022-11-03 10:50:52,492:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.24341827 -0.17857274 -0.17691416 -0.17296837 -0.17041277 -0.16946066
 -0.16230322 -0.16206649 -0.15652817 -0.15579386 -0.15540803 -0.15415235
 -0.14943975 -0.14843484 -0.14017239 -0.13828496 -0.13267954 -0.13085668
 -0.12639932 -0.12604202 -0.12534788 -0.12351528 -0.12272938 -0.11795286
 -0.11779562 -0.11059087 -0.10808982 -0.10743168 -0.10345447 -0.1027219
 -0.09406842 -0.09327054 -0.09157913 -0.08962934 -0.08664557 -0.08489692
 -0.08380001 -0.08202565 -0.07454646 -0.07216751 -0.07041507 -0.0677622
 -0.06678023 -0.06544485 -0.06351672 -0.06235327 -0.06053988 -0.05745078
 -0.05555982 -0.05532408 -0.05244157 -0.05241235 -0.05127813 -0.05116598
 -0.05019253 -0.04825883 -0.04806802 -0.04804899 -0.04473671 -0.04151826
 -0.04125333 -0.03972189 -0.03930853 -0.03619789 -0.03565441 -0.03415583
 -0.03379147 -0.03346172 -0.03023254 -0.02871629 -0.02709492 -0.02278221
 -0.01838077 -0.01786413 -0.01352395 -0.01327602 -0.01258126 -0.0077164
 -0.00704583 -0.00587445 -0.00326077 -0.00323934  0.00316148  0.00354514
  0.00656073  0.01016739  0.01034605  0.01224721  0.01366668  0.01955242
  0.02218427  0.02672927  0.02782716  0.02859259  0.02938517  0.02992787
  0.03145424  0.03371752  0.03595197  0.03789856  0.04081397  0.04339737
  0.04584697  0.04717551  0.04742938  0.04744578  0.04889587  0.05197236
  0.05204182  0.05292942  0.05330371  0.05357465  0.05369261  0.05419398
  0.05513246  0.05666909  0.05680072  0.05738952  0.05839861  0.05851547
  0.05902295  0.06239672  0.06659095  0.06888096  0.06917746  0.06926927
  0.06955376  0.06956457  0.0702645   0.07055472  0.07089214  0.07116453
  0.07176201  0.07218687  0.07274392  0.07320609  0.07474528  0.0765455
  0.07675567  0.07675624  0.07985074  0.08024774  0.08295455  0.08353611
  0.08630196  0.08984459  0.09029219  0.09054224  0.09171683  0.09211266
  0.09527122  0.10130786  0.10340408  0.10540832  0.11043713  0.11101627
  0.11166852  0.11539553  0.11697138  0.11809251  0.1221655   0.12279067
  0.12896673  0.12990009  0.13263173  0.13321074  0.13332362  0.13408737
  0.13689454  0.13708158  0.13834912  0.13841725  0.13979776  0.13996184
  0.14016191  0.14049813  0.14457071  0.1472537   0.14897582  0.15188122
  0.15195076  0.15214638  0.1524689   0.15261798  0.15293119  0.15802265
  0.1600626   0.16268301  0.1629951   0.16309666  0.16528761  0.16613374
  0.17166689  0.1727096   0.17464145  0.17683044  0.17731236  0.17736216
  0.17779424  0.1778632   0.17819033  0.17928408  0.18221498  0.18508222
  0.18588063  0.18709483  0.18798859  0.19526883  0.19628527  0.19636986
  0.19847567  0.19953521  0.1997558   0.20268487  0.20394079  0.20414217
  0.20473609  0.20540704  0.20592035  0.20603405  0.20624673  0.20635347
  0.20768989  0.20956902  0.21080574  0.2108063   0.21105623  0.21258711
  0.21380959  0.21619389  0.21635982  0.21835221  0.21911812  0.21928013
  0.22122099  0.22155096  0.22256921  0.22311224  0.22426822  0.22674128
  0.22684224  0.23450131  0.23641747  0.23849723  0.23965201  0.2404463
  0.24083858  0.24115209  0.24310162  0.24359744  0.24469098  0.24715571
  0.24824945  0.24843104  0.24966106  0.25039933  0.25144884  0.25269432
  0.25435177  0.25506322  0.25614344  0.26008616  0.26126226  0.26517352
  0.2655254   0.26557802  0.2661027   0.26673275  0.26678856  0.26810129
  0.26842268  0.2690573   0.26930937  0.26937425  0.27158676  0.27291417
  0.27638116  0.27758467  0.27839363  0.27841963  0.27999801  0.28012718
  0.28061606  0.28192455  0.28460257  0.28468273  0.28506099  0.28562105
  0.28694334  0.28982725  0.29106683  0.29138247  0.29173124  0.29244465
  0.29322011  0.2954334   0.30095961  0.30243457  0.30302068  0.30503722
  0.30737711  0.30956401  0.30986667  0.31008035  0.31033098  0.31457813
  0.3148277   0.31495756  0.31501574  0.31922058  0.32022169  0.32413846
  0.32612237  0.32738928  0.32745323  0.33057253  0.33427674  0.33692715
  0.34077875  0.3441087   0.34478076  0.34479921  0.34596927  0.34711692
  0.34940075  0.34994697  0.35120043  0.3550579   0.35532722  0.35592073
  0.35673576  0.35836227  0.36054504  0.36227272  0.36288888  0.36429238
  0.36447991  0.36615836  0.37156229  0.37256825  0.37350013  0.37858519
  0.38006703  0.38142006  0.38220097  0.38599077  0.38702664  0.3874502
  0.38783333  0.38914722  0.39689771  0.40143571  0.40567552  0.40653986
  0.41007382  0.41068931  0.41193087  0.41377615  0.41478424  0.41831145
  0.41989406  0.42196793  0.42296325  0.42955869  0.43039047  0.43165277
  0.43166389  0.43258145  0.43418799  0.43473818  0.43647483  0.44274012
  0.443638    0.44635952  0.44803714  0.448427    0.44922935  0.45069648
  0.45513792  0.46329084  0.46388692  0.46547578  0.46917014  0.47060979
  0.4726968   0.47521323  0.47531699  0.4798648   0.48363239  0.48493778
  0.48825399  0.48864638  0.49432752  0.49591075  0.49881925  0.5011637
  0.50144266  0.50633985  0.50747354  0.509804    0.51137555  0.51208025
  0.51381117  0.51436067  0.51730338  0.51973923  0.52093371  0.52448721
  0.52480561  0.52634389  0.52885265  0.52904535  0.53045638  0.53287799
  0.53361561  0.53398836  0.53772133  0.53879401  0.53889749  0.53953279
  0.53983079  0.54220275  0.54353963  0.54419764  0.54445086  0.5444928
  0.54461154  0.54510733  0.54656711  0.5483301   0.54930363  0.55099118
  0.55106648  0.55419236  0.56041537  0.56095522  0.56335283  0.56396084
  0.56565586  0.56854818  0.57284392  0.57540136  0.57759762  0.57860446
  0.58050241  0.58053298  0.58281587  0.58505469  0.58527595  0.58692457
  0.58766867  0.59428736  0.59904795  0.59946096  0.60005483  0.60564731
  0.61259266  0.61393944  0.61604566  0.62325665  0.62353888  0.62473852
  0.62497758  0.62599269  0.62649274  0.62762447  0.62943826  0.63101225
  0.63659309  0.6404849   0.64347814  0.64775558  0.6509795   0.6522847
  0.65230974  0.65761434  0.66642852  0.66992967  0.67248999  0.67362881
  0.67399656  0.67516973  0.67660994  0.67780165  0.67883188  0.68053812
  0.68135968  0.68215385  0.6828771   0.69423607  0.70693165  0.7071685
  0.71985015]

  warnings.warn(

2022-11-03 10:50:52,548:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.04564258e-01 -1.89722738e-01 -1.76508404e-01 -1.75014545e-01
 -1.68699999e-01 -1.68251831e-01 -1.67374944e-01 -1.66584236e-01
 -1.56826592e-01 -1.56217407e-01 -1.55876603e-01 -1.52934798e-01
 -1.51963289e-01 -1.46571994e-01 -1.45205155e-01 -1.41899162e-01
 -1.41847993e-01 -1.39853445e-01 -1.37316232e-01 -1.31121341e-01
 -1.29730290e-01 -1.20230085e-01 -1.15678783e-01 -1.13660136e-01
 -1.13033062e-01 -1.12592933e-01 -1.12455437e-01 -1.10900665e-01
 -1.08768466e-01 -1.07098327e-01 -1.01694152e-01 -9.99686774e-02
 -9.87984150e-02 -8.92036218e-02 -8.75731363e-02 -8.36172528e-02
 -8.30140811e-02 -8.28384984e-02 -7.90239696e-02 -7.89662692e-02
 -7.68124351e-02 -7.39890625e-02 -6.91163244e-02 -6.90960205e-02
 -6.86932094e-02 -6.72872350e-02 -6.70432133e-02 -6.26655549e-02
 -6.14347832e-02 -5.73570841e-02 -5.71597563e-02 -5.47097121e-02
 -5.40614787e-02 -5.38700836e-02 -5.34244810e-02 -5.17543657e-02
 -5.01603963e-02 -5.00167354e-02 -4.89627298e-02 -4.64412911e-02
 -4.46221093e-02 -4.38525230e-02 -3.88408103e-02 -3.60634172e-02
 -3.59994526e-02 -3.46058956e-02 -3.46058859e-02 -3.37888428e-02
 -2.96735919e-02 -2.65347828e-02 -2.56689402e-02 -2.51792832e-02
 -2.04482053e-02 -2.02706434e-02 -1.95126505e-02 -1.71257192e-02
 -1.68023666e-02 -1.57314662e-02 -1.30056260e-02 -9.06408792e-03
 -8.94646640e-03 -2.75343007e-03 -6.93828218e-04  1.36604216e-03
  3.22893340e-03  6.12087213e-03  1.19374446e-02  1.30495832e-02
  1.67494925e-02  1.70354969e-02  1.70920131e-02  1.99313464e-02
  2.07395526e-02  2.25458929e-02  2.33911119e-02  2.37115302e-02
  2.47417688e-02  2.64425647e-02  2.77353504e-02  2.93728069e-02
  3.18457972e-02  3.62225939e-02  4.12268463e-02  4.18742329e-02
  4.41573548e-02  4.60710438e-02  4.71159068e-02  5.05072017e-02
  5.28930772e-02  5.43327201e-02  5.76007455e-02  5.90376841e-02
  6.06460185e-02  6.07623918e-02  6.08574931e-02  6.54989150e-02
  6.61207433e-02  6.76968865e-02  6.86420017e-02  6.98807307e-02
  7.07045877e-02  7.32843318e-02  7.42170204e-02  8.11908840e-02
  8.15635578e-02  8.38794887e-02  8.62141540e-02  8.63201574e-02
  8.70616420e-02  9.37834885e-02  9.45950940e-02  9.67021391e-02
  9.70676998e-02  9.75126033e-02  9.79638725e-02  9.88283083e-02
  1.00603926e-01  1.01446462e-01  1.02436707e-01  1.04065271e-01
  1.05689060e-01  1.06723107e-01  1.06897783e-01  1.08147703e-01
  1.08562510e-01  1.14196919e-01  1.16594905e-01  1.17406382e-01
  1.18491278e-01  1.18692519e-01  1.20284985e-01  1.23395955e-01
  1.24651338e-01  1.29015310e-01  1.32492059e-01  1.33452360e-01
  1.33693306e-01  1.34290926e-01  1.35724445e-01  1.36182124e-01
  1.37142267e-01  1.40390440e-01  1.50705207e-01  1.51245738e-01
  1.52026457e-01  1.52185564e-01  1.54018187e-01  1.55080293e-01
  1.56792597e-01  1.57138412e-01  1.57491595e-01  1.59279335e-01
  1.59396339e-01  1.60138400e-01  1.60694916e-01  1.61398690e-01
  1.62596920e-01  1.67357835e-01  1.68079167e-01  1.69592294e-01
  1.73028908e-01  1.77110367e-01  1.80993683e-01  1.81645760e-01
  1.82131693e-01  1.82168155e-01  1.85924973e-01  1.88311658e-01
  1.89073557e-01  1.92497530e-01  1.94468158e-01  1.96396275e-01
  1.96614459e-01  1.96801680e-01  1.96893851e-01  1.97816212e-01
  1.97985006e-01  1.98377214e-01  1.99272270e-01  2.00485919e-01
  2.00722553e-01  2.02398961e-01  2.03445952e-01  2.04346876e-01
  2.04388787e-01  2.05515396e-01  2.08376013e-01  2.08537551e-01
  2.10660961e-01  2.12307223e-01  2.14359194e-01  2.14920025e-01
  2.14953145e-01  2.17757788e-01  2.18856261e-01  2.18918807e-01
  2.20104747e-01  2.22883884e-01  2.23567185e-01  2.24470805e-01
  2.24549313e-01  2.25869762e-01  2.26340022e-01  2.28998484e-01
  2.31771076e-01  2.32469493e-01  2.34568186e-01  2.35990067e-01
  2.37455693e-01  2.41362410e-01  2.41990089e-01  2.43657190e-01
  2.43792187e-01  2.46830659e-01  2.49180788e-01  2.49369473e-01
  2.49608563e-01  2.50679671e-01  2.50952715e-01  2.52060897e-01
  2.52126098e-01  2.52419769e-01  2.52456871e-01  2.53782561e-01
  2.57176440e-01  2.57233486e-01  2.58752620e-01  2.61732175e-01
  2.64302729e-01  2.64311135e-01  2.65505674e-01  2.70994499e-01
  2.72122658e-01  2.74419130e-01  2.74457866e-01  2.75006524e-01
  2.75189442e-01  2.75417202e-01  2.75579346e-01  2.77672222e-01
  2.79016221e-01  2.79066348e-01  2.80138157e-01  2.83673712e-01
  2.84347825e-01  2.85029363e-01  2.86068518e-01  2.87554188e-01
  2.87827098e-01  2.88488675e-01  2.89657080e-01  2.92245360e-01
  2.93285671e-01  2.95225372e-01  2.97638567e-01  3.05034607e-01
  3.05963258e-01  3.07847679e-01  3.08543670e-01  3.08750182e-01
  3.09569745e-01  3.09570723e-01  3.11536848e-01  3.14123993e-01
  3.14788793e-01  3.14892790e-01  3.17775771e-01  3.18252905e-01
  3.21305938e-01  3.21548168e-01  3.22254444e-01  3.23108474e-01
  3.24373721e-01  3.24724983e-01  3.25216115e-01  3.28784616e-01
  3.28917875e-01  3.30291204e-01  3.30499154e-01  3.31333807e-01
  3.31434564e-01  3.32736115e-01  3.33202053e-01  3.33367075e-01
  3.33453689e-01  3.38344400e-01  3.39407006e-01  3.40124463e-01
  3.40311955e-01  3.43510281e-01  3.43515722e-01  3.50635875e-01
  3.55187550e-01  3.56476819e-01  3.56696528e-01  3.57671352e-01
  3.58667243e-01  3.59110876e-01  3.59196548e-01  3.59802228e-01
  3.60789693e-01  3.61789855e-01  3.64422749e-01  3.66954907e-01
  3.69747228e-01  3.73798153e-01  3.75852998e-01  3.76409877e-01
  3.77650783e-01  3.78737380e-01  3.80623099e-01  3.81785139e-01
  3.82561460e-01  3.83663580e-01  3.83902029e-01  3.85016393e-01
  3.86561822e-01  3.88074208e-01  3.91426363e-01  3.93391825e-01
  3.95107313e-01  3.97963492e-01  3.98144683e-01  3.99400903e-01
  4.02604585e-01  4.03077309e-01  4.03348958e-01  4.04918293e-01
  4.08798284e-01  4.13243833e-01  4.14139571e-01  4.14421433e-01
  4.18595838e-01  4.19651462e-01  4.19906395e-01  4.20564815e-01
  4.21506400e-01  4.23106775e-01  4.24137016e-01  4.25539280e-01
  4.25750219e-01  4.26073182e-01  4.27895311e-01  4.28106523e-01
  4.30010384e-01  4.30039646e-01  4.30084279e-01  4.33542608e-01
  4.34414115e-01  4.36177416e-01  4.40121384e-01  4.40547852e-01
  4.41658409e-01  4.44657557e-01  4.47189636e-01  4.52371518e-01
  4.52816313e-01  4.53517148e-01  4.57698121e-01  4.62357324e-01
  4.63329650e-01  4.65536676e-01  4.65821691e-01  4.68770035e-01
  4.70322927e-01  4.73654197e-01  4.75352666e-01  4.77081028e-01
  4.78727203e-01  4.79138844e-01  4.79713203e-01  4.79797353e-01
  4.82804997e-01  4.84619947e-01  4.87866237e-01  4.88511338e-01
  4.89433772e-01  4.90557590e-01  4.91275146e-01  4.93809405e-01
  4.94635448e-01  4.95599727e-01  4.98847385e-01  5.01009619e-01
  5.02665686e-01  5.02968629e-01  5.03413046e-01  5.05367496e-01
  5.06103297e-01  5.06729841e-01  5.10627749e-01  5.12097042e-01
  5.14302042e-01  5.15577480e-01  5.20881596e-01  5.23938882e-01
  5.27072192e-01  5.29042077e-01  5.30707138e-01  5.31782258e-01
  5.33492087e-01  5.36115680e-01  5.36757074e-01  5.37372334e-01
  5.42604631e-01  5.43209941e-01  5.43898351e-01  5.45029704e-01
  5.45438227e-01  5.46583081e-01  5.52988576e-01  5.53315215e-01
  5.54505661e-01  5.61414667e-01  5.64854592e-01  5.65769151e-01
  5.66881318e-01  5.66893313e-01  5.67463993e-01  5.68009230e-01
  5.68262574e-01  5.69194358e-01  5.69342453e-01  5.69565265e-01
  5.70196025e-01  5.70240938e-01  5.81000403e-01  5.85084778e-01
  5.88333724e-01  5.88874445e-01  5.89189015e-01  5.89502723e-01
  5.89958809e-01  6.03562125e-01  6.08637371e-01  6.11769918e-01
  6.13078314e-01  6.14653305e-01  6.17227267e-01  6.17636718e-01
  6.17701963e-01  6.18609822e-01  6.19023986e-01  6.20347845e-01
  6.27528557e-01  6.32127247e-01  6.36340738e-01  6.38989744e-01
  6.42602617e-01  6.42646395e-01  6.48488186e-01  6.48737707e-01
  6.49893463e-01  6.50924639e-01  6.52910705e-01  6.52957146e-01
  6.57468969e-01  6.57648893e-01  6.58431528e-01  6.59867698e-01
  6.60277044e-01  6.62947884e-01  6.64034986e-01  6.64686119e-01
  6.69421276e-01  6.88274823e-01  6.91433055e-01  7.11662984e-01
  7.29053282e-01  7.37488723e-01  7.51572699e-01  7.57682448e-01
  7.58084910e-01]

  warnings.warn(

2022-11-03 10:50:54,748:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.10543296e-01 -2.01935483e-01 -1.98896468e-01 -1.82192516e-01
 -1.73851126e-01 -1.71123268e-01 -1.68305528e-01 -1.57034090e-01
 -1.54540910e-01 -1.53723075e-01 -1.51879609e-01 -1.44400185e-01
 -1.42810470e-01 -1.32679743e-01 -1.31035641e-01 -1.30857881e-01
 -1.30465318e-01 -1.30065170e-01 -1.27468064e-01 -1.27192494e-01
 -1.22978050e-01 -1.21903594e-01 -1.15347293e-01 -1.13742131e-01
 -1.13115662e-01 -1.12795168e-01 -1.06850631e-01 -1.02859513e-01
 -1.00390915e-01 -9.72216435e-02 -9.62939852e-02 -9.50173054e-02
 -9.39814184e-02 -9.38017289e-02 -9.28989192e-02 -9.12564686e-02
 -9.10857617e-02 -9.03789547e-02 -8.42722883e-02 -7.45423594e-02
 -7.43211477e-02 -6.91155874e-02 -6.68398042e-02 -6.42783931e-02
 -6.36983480e-02 -6.35361107e-02 -6.01300730e-02 -5.93990909e-02
 -5.75052612e-02 -5.65422796e-02 -5.42468424e-02 -5.32018823e-02
 -5.27143399e-02 -5.24162659e-02 -5.07793236e-02 -4.90425091e-02
 -4.61965599e-02 -4.39323973e-02 -4.17722641e-02 -4.17507141e-02
 -4.14895030e-02 -4.08591795e-02 -3.95173009e-02 -3.60642131e-02
 -3.48726097e-02 -2.92466453e-02 -2.53530149e-02 -2.25316981e-02
 -2.19552336e-02 -2.00970847e-02 -1.51671744e-02 -1.36024586e-02
 -1.29073904e-02 -1.21982021e-02 -9.82013256e-03 -9.05445097e-03
 -8.22978001e-03 -7.80191446e-03 -6.84657495e-03 -6.14852744e-03
 -5.92002400e-03 -5.45201165e-03 -4.60770116e-03 -2.77438308e-03
 -2.16432263e-03  5.10539709e-04  1.61485942e-03  2.15114094e-03
  3.62315278e-03  5.71149623e-03  6.78988794e-03  8.63620066e-03
  1.04567037e-02  1.24331518e-02  1.33431231e-02  1.84060843e-02
  1.95449191e-02  2.31298689e-02  2.34030769e-02  2.57893911e-02
  3.09201701e-02  3.19874762e-02  3.25998818e-02  3.64377238e-02
  3.97079032e-02  4.21854987e-02  4.25418973e-02  4.34763563e-02
  4.45038855e-02  4.53767233e-02  4.78964086e-02  5.22686448e-02
  5.72448488e-02  5.76371202e-02  5.90489929e-02  6.10330416e-02
  6.12214725e-02  6.22150945e-02  6.35350459e-02  6.65618605e-02
  6.74225907e-02  6.87478870e-02  7.17351224e-02  7.20853620e-02
  7.31601424e-02  7.47180813e-02  7.48151850e-02  7.55536131e-02
  7.58749463e-02  7.63294024e-02  7.76652791e-02  7.85525929e-02
  8.02277885e-02  8.05053268e-02  8.21549526e-02  8.29038577e-02
  8.31933560e-02  8.44074111e-02  8.53164117e-02  8.60093106e-02
  8.80552822e-02  8.82613154e-02  9.35360832e-02  9.37496503e-02
  9.44383084e-02  9.60789084e-02  9.71624975e-02  1.02213343e-01
  1.02677489e-01  1.05053169e-01  1.06042130e-01  1.06485428e-01
  1.07794311e-01  1.07848825e-01  1.07878464e-01  1.08053349e-01
  1.11488576e-01  1.13125311e-01  1.13729851e-01  1.13789310e-01
  1.19211681e-01  1.19709563e-01  1.21312393e-01  1.24806998e-01
  1.25548751e-01  1.26866172e-01  1.27284980e-01  1.28096980e-01
  1.28292224e-01  1.29910879e-01  1.30481991e-01  1.30779081e-01
  1.32314618e-01  1.33280890e-01  1.35397595e-01  1.37173473e-01
  1.41050100e-01  1.41171082e-01  1.41623263e-01  1.43829535e-01
  1.44048686e-01  1.44994700e-01  1.45397856e-01  1.46343383e-01
  1.46659929e-01  1.47855622e-01  1.51676885e-01  1.51698716e-01
  1.52198321e-01  1.52807687e-01  1.54917552e-01  1.57741237e-01
  1.60963482e-01  1.61163268e-01  1.63084028e-01  1.63535419e-01
  1.64207538e-01  1.64555484e-01  1.65252673e-01  1.65315326e-01
  1.65396433e-01  1.70194466e-01  1.70914042e-01  1.73988322e-01
  1.74106328e-01  1.75060439e-01  1.78163837e-01  1.80172855e-01
  1.80717394e-01  1.83389053e-01  1.85640411e-01  1.89734815e-01
  1.90657278e-01  1.91240623e-01  1.92457000e-01  1.97815843e-01
  1.99369440e-01  1.99418350e-01  1.99484401e-01  2.00814798e-01
  2.00919030e-01  2.01404853e-01  2.02866848e-01  2.05472475e-01
  2.05653561e-01  2.06357762e-01  2.07162561e-01  2.08837151e-01
  2.09317961e-01  2.10968172e-01  2.15123364e-01  2.20620276e-01
  2.20779894e-01  2.23100026e-01  2.25689143e-01  2.26912147e-01
  2.28066641e-01  2.28567319e-01  2.31326742e-01  2.32424255e-01
  2.32995217e-01  2.33722913e-01  2.35588474e-01  2.35629707e-01
  2.36233047e-01  2.36500507e-01  2.40326530e-01  2.40775900e-01
  2.41917606e-01  2.43031359e-01  2.45255941e-01  2.46642106e-01
  2.47011311e-01  2.52413270e-01  2.52736123e-01  2.52945920e-01
  2.55555972e-01  2.60635575e-01  2.65172105e-01  2.65630885e-01
  2.69179880e-01  2.69824151e-01  2.70116603e-01  2.70828856e-01
  2.79465972e-01  2.79822798e-01  2.81042548e-01  2.81955007e-01
  2.84547265e-01  2.86173681e-01  2.87087577e-01  2.88213924e-01
  2.89571879e-01  2.90148882e-01  2.92577573e-01  2.93398082e-01
  2.94921501e-01  2.96547946e-01  2.97146219e-01  2.97590274e-01
  2.97920422e-01  2.98394068e-01  2.99756374e-01  3.00619037e-01
  3.01762401e-01  3.02225308e-01  3.08781574e-01  3.11008139e-01
  3.12420884e-01  3.12699039e-01  3.15966209e-01  3.16106596e-01
  3.17086104e-01  3.17484537e-01  3.20624086e-01  3.22253557e-01
  3.25623708e-01  3.28234314e-01  3.30922248e-01  3.32833250e-01
  3.34837010e-01  3.35188899e-01  3.35247088e-01  3.38037817e-01
  3.41111327e-01  3.41249132e-01  3.41980290e-01  3.45570006e-01
  3.45768210e-01  3.46988986e-01  3.47543152e-01  3.51106812e-01
  3.51203456e-01  3.51734795e-01  3.54767418e-01  3.55169651e-01
  3.57288685e-01  3.61184435e-01  3.61434675e-01  3.62750813e-01
  3.62819256e-01  3.63013548e-01  3.65036590e-01  3.67807678e-01
  3.69434137e-01  3.69755737e-01  3.71385778e-01  3.71536136e-01
  3.79154302e-01  3.79576769e-01  3.79828437e-01  3.81160472e-01
  3.82834532e-01  3.84723289e-01  3.86080381e-01  3.87637979e-01
  3.88443868e-01  3.92653407e-01  3.92692709e-01  3.94490171e-01
  3.99201912e-01  4.01024673e-01  4.01522950e-01  4.02462479e-01
  4.03254097e-01  4.03992095e-01  4.04557752e-01  4.05059431e-01
  4.08307721e-01  4.09894646e-01  4.12660564e-01  4.13435727e-01
  4.14047908e-01  4.18120157e-01  4.20921078e-01  4.21109270e-01
  4.26782470e-01  4.27996566e-01  4.28825540e-01  4.29007606e-01
  4.29724955e-01  4.31468241e-01  4.33031096e-01  4.33179885e-01
  4.36460727e-01  4.36869181e-01  4.37639073e-01  4.38154318e-01
  4.39870517e-01  4.42541366e-01  4.44811545e-01  4.45511726e-01
  4.47510583e-01  4.48454128e-01  4.51995607e-01  4.52241201e-01
  4.53615552e-01  4.54909317e-01  4.61961844e-01  4.66501035e-01
  4.69724847e-01  4.70587375e-01  4.71099942e-01  4.73908125e-01
  4.73957279e-01  4.74621655e-01  4.78513609e-01  4.79501194e-01
  4.80803978e-01  4.81534266e-01  4.82936389e-01  4.83552554e-01
  4.84906556e-01  4.86681984e-01  4.87067103e-01  4.91579209e-01
  4.91912851e-01  4.94282470e-01  4.97010325e-01  4.98249649e-01
  4.98609025e-01  4.98625179e-01  5.06348762e-01  5.06719001e-01
  5.06949122e-01  5.10482391e-01  5.10992740e-01  5.13064590e-01
  5.13170860e-01  5.13532720e-01  5.15853576e-01  5.17959595e-01
  5.18199374e-01  5.19149401e-01  5.20675438e-01  5.21398000e-01
  5.22193455e-01  5.23577933e-01  5.25516878e-01  5.27635730e-01
  5.28195450e-01  5.30100270e-01  5.30983604e-01  5.34919526e-01
  5.36421102e-01  5.38148757e-01  5.39659077e-01  5.40764131e-01
  5.44305369e-01  5.46263749e-01  5.46557054e-01  5.50489683e-01
  5.53905560e-01  5.53949083e-01  5.54873895e-01  5.61107783e-01
  5.64053304e-01  5.67813059e-01  5.69669614e-01  5.69960192e-01
  5.71223946e-01  5.75790365e-01  5.77110671e-01  5.79738720e-01
  5.85723659e-01  5.85982530e-01  5.90177780e-01  5.90852291e-01
  5.91308698e-01  5.93629547e-01  5.94242053e-01  5.96918357e-01
  5.97613470e-01  5.98250686e-01  6.01093506e-01  6.12152899e-01
  6.16194300e-01  6.17263383e-01  6.17591759e-01  6.17690784e-01
  6.17817782e-01  6.23083125e-01  6.23679279e-01  6.25826361e-01
  6.27143568e-01  6.27435967e-01  6.28542559e-01  6.34561066e-01
  6.38889305e-01  6.39497775e-01  6.39585526e-01  6.42807580e-01
  6.43131476e-01  6.43187090e-01  6.47938742e-01  6.50510308e-01
  6.53413724e-01  6.60506628e-01  6.61613704e-01  6.78900392e-01
  6.79263232e-01  6.85076006e-01  6.93153994e-01  6.94042489e-01
  7.02530751e-01  7.03088261e-01  7.09209791e-01  7.09854645e-01
  7.11279411e-01  7.12057177e-01  7.13032596e-01  7.13335218e-01
  7.14428738e-01]

  warnings.warn(

2022-11-03 10:50:55,012:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.16924712 -0.16008648 -0.15949763 -0.15915973 -0.1489059  -0.14500532
 -0.13418585 -0.13156108 -0.12914246 -0.1250813  -0.12264856 -0.12233405
 -0.12100034 -0.11917802 -0.11814222 -0.11613573 -0.1134715  -0.11258835
 -0.11184957 -0.10782349 -0.10516944 -0.10327508 -0.10268867 -0.09849763
 -0.09378371 -0.09255714 -0.09006023 -0.08983682 -0.08897983 -0.08617682
 -0.08613843 -0.08332439 -0.08281758 -0.08127969 -0.07966177 -0.07502592
 -0.07354819 -0.07320407 -0.07147239 -0.06777924 -0.06663731 -0.06591017
 -0.06257666 -0.06037159 -0.05992959 -0.05956709 -0.05465403 -0.05363659
 -0.05034938 -0.04619382 -0.04536974 -0.04508079 -0.04284867 -0.03676453
 -0.03260773 -0.02978373 -0.0271581  -0.02414384 -0.02328284 -0.02109009
 -0.01942268 -0.01881226 -0.0187299  -0.01553252 -0.01489999 -0.01211869
 -0.01207045 -0.01162248 -0.01032621 -0.00586942 -0.00560136 -0.00449574
 -0.00441305 -0.00354891 -0.00333197  0.00512224  0.00640759  0.00667876
  0.00755986  0.00903346  0.01028035  0.01276159  0.01289596  0.01407873
  0.01728032  0.01850777  0.01986862  0.02088584  0.02124162  0.02262054
  0.02672332  0.0268824   0.02794507  0.03167994  0.03295545  0.03678944
  0.03848869  0.04021185  0.04640968  0.0464269   0.0505129   0.05254236
  0.05298988  0.05483402  0.05528166  0.05554554  0.05947412  0.06107428
  0.0611862   0.06209306  0.06255061  0.06476804  0.06586781  0.07136423
  0.07343468  0.07415811  0.07438805  0.07454147  0.07537583  0.07613098
  0.07800917  0.07871046  0.08025222  0.08067114  0.08107186  0.08726343
  0.08795039  0.08861144  0.08903665  0.08964088  0.09143051  0.09190208
  0.09365927  0.09463922  0.09680194  0.09873994  0.10291904  0.10313989
  0.10389948  0.10415545  0.10446859  0.10735606  0.10932236  0.10970304
  0.1107027   0.11267555  0.11386078  0.120165    0.12026276  0.12144796
  0.12187531  0.12394303  0.12791422  0.13402984  0.13530417  0.13590259
  0.13622637  0.13688695  0.13720882  0.13736941  0.13985407  0.14087387
  0.14167407  0.14273793  0.14510228  0.14516497  0.14639408  0.15142456
  0.15183725  0.15268569  0.1529656   0.15708416  0.15821603  0.159286
  0.16047524  0.16204107  0.16372662  0.16432382  0.16590806  0.16908068
  0.16979022  0.17459796  0.18174282  0.18246568  0.18359177  0.18531637
  0.18718789  0.18757581  0.18804656  0.19265386  0.1930483   0.19308926
  0.19350132  0.19359994  0.19444912  0.19636302  0.19662016  0.19756132
  0.19785089  0.1986346   0.19876285  0.19935965  0.20029076  0.20138315
  0.20372488  0.20402742  0.20408385  0.20428809  0.20467797  0.2064574
  0.20708303  0.20773814  0.2106895   0.21115489  0.21155392  0.21178168
  0.21202802  0.21270687  0.21308967  0.21427419  0.21455035  0.21575597
  0.21613677  0.216828    0.22022436  0.22033226  0.22090014  0.22115339
  0.22305055  0.22316544  0.22326956  0.2233842   0.22604018  0.22756694
  0.23352284  0.23372223  0.23692239  0.23763976  0.23785996  0.24026271
  0.24182704  0.24376938  0.24399478  0.24539088  0.24619696  0.24967608
  0.25197209  0.25253458  0.25260804  0.25325566  0.25404522  0.25636003
  0.25731151  0.25784496  0.25867133  0.25942185  0.26520028  0.26558049
  0.26709175  0.26709236  0.26768813  0.27077012  0.27342705  0.27421782
  0.27426865  0.27673157  0.27751464  0.27835135  0.28028759  0.28151495
  0.28191278  0.28371653  0.28679245  0.28698952  0.28812105  0.28961923
  0.28996185  0.29611052  0.29619403  0.29686995  0.29809907  0.29865989
  0.30613622  0.31087378  0.31311589  0.31371601  0.31410193  0.31460561
  0.31481048  0.31481695  0.31682038  0.31963933  0.32004744  0.32037125
  0.32392203  0.32783278  0.32850953  0.33375464  0.33403115  0.33537463
  0.33613106  0.33733109  0.33802342  0.33804172  0.3414046   0.34349015
  0.34371729  0.34508416  0.3475162   0.34826274  0.34879216  0.3499346
  0.3501539   0.35020318  0.35391089  0.35580419  0.35703567  0.35813904
  0.36396818  0.36641887  0.36738915  0.3689475   0.37112672  0.37637888
  0.37639389  0.37699856  0.37729604  0.37944258  0.38259519  0.3827798
  0.38367893  0.38569148  0.38675769  0.38910102  0.39479448  0.39530038
  0.39940917  0.40003611  0.40078674  0.40353184  0.40507369  0.40534523
  0.40706904  0.40899199  0.41158244  0.41300711  0.41431221  0.41472875
  0.41486556  0.41542146  0.41563331  0.42071298  0.42087225  0.42174862
  0.42204722  0.42350662  0.42397335  0.42702001  0.42927758  0.43137426
  0.43447419  0.43592342  0.43948043  0.44085354  0.44098108  0.44166298
  0.45374287  0.45520196  0.45743051  0.45825879  0.45918385  0.45942276
  0.45991227  0.46048682  0.46135743  0.46576385  0.46688572  0.46881856
  0.46932521  0.47244609  0.47647559  0.47769237  0.48076325  0.48138893
  0.48212085  0.48318839  0.48521396  0.48877702  0.4904427   0.49083207
  0.49416954  0.49498683  0.49597642  0.49806293  0.50055128  0.50131918
  0.50265889  0.50405322  0.50411704  0.50414654  0.50510539  0.50704753
  0.51362171  0.5137022   0.51586601  0.52135725  0.52783611  0.52793151
  0.52920774  0.53005173  0.53158375  0.53240029  0.53401317  0.53502194
  0.53582996  0.5358955   0.53753959  0.53957144  0.5473091   0.54882382
  0.55087921  0.55211712  0.55325488  0.55337434  0.55753908  0.55754178
  0.55964959  0.56077197  0.56168506  0.56194962  0.5628155   0.56531134
  0.566038    0.57404188  0.57506679  0.57621922  0.57734005  0.57848991
  0.57900756  0.57923093  0.58051355  0.58194284  0.58346793  0.58444589
  0.58485126  0.58519952  0.58755892  0.588555    0.59064934  0.59090092
  0.59458843  0.59695899  0.59748321  0.599776    0.6016395   0.60238154
  0.60317285  0.60343432  0.60799795  0.60898677  0.61049205  0.61106237
  0.61858561  0.62187661  0.62209404  0.62448974  0.62787933  0.62831496
  0.62947784  0.64142146  0.64478937  0.64583233  0.64835114  0.64935084
  0.65252741  0.65274045  0.655339    0.6562755   0.66337496  0.66365826
  0.66380924  0.66698358  0.67336514  0.67378226  0.67664203  0.68061152
  0.68228094  0.69037238  0.69305641  0.69736421  0.70124146  0.72811842
  0.72869143]

  warnings.warn(

2022-11-03 10:50:55,012:INFO:Calculating mean and std
2022-11-03 10:50:55,012:INFO:Creating metrics dataframe
2022-11-03 10:50:55,012:INFO:Uploading results into container
2022-11-03 10:50:55,029:INFO:Uploading model into container now
2022-11-03 10:50:55,029:INFO:master_model_container: 16
2022-11-03 10:50:55,029:INFO:display_container: 2
2022-11-03 10:50:55,029:INFO:BayesianRidge()
2022-11-03 10:50:55,029:INFO:create_model() successfully completed......................................
2022-11-03 10:50:55,333:ERROR:create_model() for BayesianRidge() raised an exception or returned all 0.0:
2022-11-03 10:50:55,333:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:50:55,333:INFO:Initializing Passive Aggressive Regressor
2022-11-03 10:50:55,333:INFO:Total runtime is 2.323728779951731 minutes
2022-11-03 10:50:55,333:INFO:SubProcess create_model() called ==================================
2022-11-03 10:50:55,333:INFO:Initializing create_model()
2022-11-03 10:50:55,333:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:50:55,333:INFO:Checking exceptions
2022-11-03 10:50:55,349:INFO:Importing libraries
2022-11-03 10:50:55,349:INFO:Copying training dataset
2022-11-03 10:50:55,349:INFO:Defining folds
2022-11-03 10:50:55,349:INFO:Declaring metric variables
2022-11-03 10:50:55,349:INFO:Importing untrained model
2022-11-03 10:50:55,349:INFO:Passive Aggressive Regressor Imported successfully
2022-11-03 10:50:55,349:INFO:Starting cross validation
2022-11-03 10:50:55,364:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:50:59,424:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.96564193e-01 -4.87326750e-01 -4.67216905e-01 -4.65987679e-01
 -4.61416224e-01 -4.56167051e-01 -4.53070517e-01 -4.47952414e-01
 -4.46353290e-01 -4.43848193e-01 -4.42209122e-01 -4.36200171e-01
 -4.34214853e-01 -4.30207021e-01 -4.28268870e-01 -4.22497591e-01
 -4.22484473e-01 -4.19471067e-01 -4.14008357e-01 -4.04502050e-01
 -4.02594793e-01 -4.02363254e-01 -3.97605362e-01 -3.95840537e-01
 -3.95608177e-01 -3.95590080e-01 -3.95096231e-01 -3.93550519e-01
 -3.90732116e-01 -3.90187371e-01 -3.86584880e-01 -3.80477528e-01
 -3.78169072e-01 -3.73547988e-01 -3.71098116e-01 -3.69053213e-01
 -3.67206832e-01 -3.65813952e-01 -3.53179563e-01 -3.51127101e-01
 -3.48955169e-01 -3.47301004e-01 -3.46809549e-01 -3.41778283e-01
 -3.32157965e-01 -3.22101554e-01 -3.18934385e-01 -3.11917703e-01
 -3.08356333e-01 -3.06032817e-01 -3.02269899e-01 -3.00635035e-01
 -2.99522067e-01 -2.96302058e-01 -2.95321473e-01 -2.93255700e-01
 -2.89278220e-01 -2.76844244e-01 -2.73445912e-01 -2.69138801e-01
 -2.68014621e-01 -2.63943405e-01 -2.59714244e-01 -2.53547834e-01
 -2.51731787e-01 -2.49799100e-01 -2.46834531e-01 -2.41360275e-01
 -2.40359733e-01 -2.39409039e-01 -2.29482274e-01 -2.22896934e-01
 -2.21739993e-01 -2.21629116e-01 -2.21359237e-01 -2.15795035e-01
 -2.10629826e-01 -2.10015110e-01 -2.09617409e-01 -2.06335751e-01
 -2.06009794e-01 -2.03172423e-01 -1.99726736e-01 -1.97737928e-01
 -1.97727055e-01 -1.95836201e-01 -1.93745878e-01 -1.89262612e-01
 -1.87545277e-01 -1.84706793e-01 -1.81559688e-01 -1.80068492e-01
 -1.76872246e-01 -1.76161521e-01 -1.75660139e-01 -1.74823018e-01
 -1.73509295e-01 -1.73283699e-01 -1.71122861e-01 -1.65992343e-01
 -1.64495918e-01 -1.61454494e-01 -1.60944675e-01 -1.51894020e-01
 -1.51019103e-01 -1.50767436e-01 -1.47696529e-01 -1.47352508e-01
 -1.42866639e-01 -1.38383666e-01 -1.37074967e-01 -1.36884562e-01
 -1.34541898e-01 -1.33566750e-01 -1.26675527e-01 -1.25280372e-01
 -1.24480948e-01 -1.24272137e-01 -1.22388996e-01 -1.19010702e-01
 -1.15730012e-01 -1.15261049e-01 -1.14714775e-01 -1.11126029e-01
 -1.06136771e-01 -1.05894615e-01 -1.02221632e-01 -1.00636255e-01
 -1.00315130e-01 -9.53494795e-02 -9.44548696e-02 -9.41121678e-02
 -8.25475930e-02 -7.65895177e-02 -7.45372043e-02 -7.39423236e-02
 -7.38807231e-02 -7.30271530e-02 -7.13232876e-02 -6.90154937e-02
 -6.78697121e-02 -6.76330779e-02 -6.29997717e-02 -5.11596373e-02
 -5.03941117e-02 -4.94041501e-02 -4.91698762e-02 -4.69817698e-02
 -4.62364996e-02 -4.50529993e-02 -4.42676662e-02 -4.25884721e-02
 -3.65807113e-02 -3.62003007e-02 -3.44166971e-02 -3.17094367e-02
 -2.97422531e-02 -2.92844330e-02 -2.86454517e-02 -2.81988254e-02
 -2.75586539e-02 -2.73840578e-02 -2.67179154e-02 -2.66217111e-02
 -2.62238437e-02 -2.54021085e-02 -1.85163963e-02 -1.71321138e-02
 -1.43052767e-02 -1.21429936e-02 -4.29403869e-03 -3.02532316e-03
 -5.50729737e-04  5.44022506e-03  6.11818227e-03  7.24127436e-03
  7.64401882e-03  1.05987730e-02  1.34753178e-02  1.35711801e-02
  2.46803678e-02  3.04471955e-02  3.15791878e-02  3.37114361e-02
  3.44897919e-02  3.51201703e-02  3.57614574e-02  4.17771775e-02
  4.26313856e-02  4.53407169e-02  4.56778982e-02  4.58588336e-02
  4.88956691e-02  5.13751377e-02  5.23307002e-02  5.31973683e-02
  5.39494072e-02  5.46362239e-02  5.51901079e-02  5.67889400e-02
  6.03048589e-02  6.05265845e-02  6.12630919e-02  6.18334958e-02
  6.18379999e-02  6.38900400e-02  7.02138363e-02  7.42980677e-02
  7.63564569e-02  7.73384413e-02  8.13982006e-02  8.53144437e-02
  8.85427769e-02  8.99488025e-02  9.02098446e-02  9.03063432e-02
  9.12392895e-02  9.23464941e-02  9.51238957e-02  9.52606402e-02
  9.58694610e-02  9.83929054e-02  9.91489388e-02  1.02474863e-01
  1.06237093e-01  1.07633852e-01  1.10973353e-01  1.16637692e-01
  1.17529861e-01  1.22589712e-01  1.24395346e-01  1.27510078e-01
  1.30761541e-01  1.31979835e-01  1.32157443e-01  1.33183301e-01
  1.36644848e-01  1.41679148e-01  1.43571543e-01  1.47194282e-01
  1.48400927e-01  1.49404216e-01  1.50475087e-01  1.50741818e-01
  1.56045371e-01  1.66097755e-01  1.69572264e-01  1.69883348e-01
  1.73204107e-01  1.76497377e-01  1.78501922e-01  1.80477923e-01
  1.82096326e-01  1.83293525e-01  1.84232315e-01  1.84895243e-01
  1.85832245e-01  1.86032147e-01  1.86077806e-01  1.87703161e-01
  1.92148942e-01  1.93042098e-01  1.96150231e-01  1.97616788e-01
  2.00248374e-01  2.01784352e-01  2.02773996e-01  2.03616681e-01
  2.04745377e-01  2.08442330e-01  2.09715904e-01  2.10978699e-01
  2.13381839e-01  2.15902645e-01  2.16257455e-01  2.17176724e-01
  2.19899351e-01  2.20317288e-01  2.21171184e-01  2.24595098e-01
  2.25113119e-01  2.25285790e-01  2.27441464e-01  2.30379183e-01
  2.33186434e-01  2.34918502e-01  2.34958911e-01  2.35245060e-01
  2.35354954e-01  2.37454186e-01  2.38101731e-01  2.39335062e-01
  2.40691843e-01  2.46424041e-01  2.57841836e-01  2.58981194e-01
  2.59578410e-01  2.59674883e-01  2.59964625e-01  2.67636833e-01
  2.70234562e-01  2.70448881e-01  2.70808518e-01  2.74541442e-01
  2.75175613e-01  2.78837165e-01  2.82385628e-01  2.84049352e-01
  2.84135758e-01  2.85218024e-01  2.86312720e-01  2.87404010e-01
  2.88914114e-01  2.90924540e-01  2.92323369e-01  2.94075046e-01
  2.95430551e-01  2.98891533e-01  3.00444630e-01  3.00968900e-01
  3.02430640e-01  3.03796326e-01  3.07347264e-01  3.08100262e-01
  3.09269371e-01  3.10015690e-01  3.12581613e-01  3.17120741e-01
  3.17915217e-01  3.18007898e-01  3.18066455e-01  3.18881727e-01
  3.22863843e-01  3.30758769e-01  3.31067913e-01  3.41518101e-01
  3.42697664e-01  3.44228441e-01  3.54245028e-01  3.54287121e-01
  3.56551239e-01  3.59660863e-01  3.66945897e-01  3.67596874e-01
  3.69841870e-01  3.71140973e-01  3.72602528e-01  3.74009843e-01
  3.74059410e-01  3.75138914e-01  3.78111922e-01  3.78267163e-01
  3.78989020e-01  3.85027775e-01  3.86370054e-01  3.89563877e-01
  3.89706861e-01  3.93127505e-01  3.99230489e-01  4.00242990e-01
  4.04712973e-01  4.05517872e-01  4.05963200e-01  4.06342108e-01
  4.06892279e-01  4.09031582e-01  4.09140940e-01  4.10377887e-01
  4.10645458e-01  4.11523366e-01  4.15951886e-01  4.18792487e-01
  4.20492324e-01  4.23431289e-01  4.29067470e-01  4.38413556e-01
  4.40961296e-01  4.41599245e-01  4.43167623e-01  4.45966443e-01
  4.52419653e-01  4.53866841e-01  4.58116228e-01  4.59118005e-01
  4.61023936e-01  4.62848129e-01  4.68024317e-01  4.68108166e-01
  4.69418596e-01  4.72477304e-01  4.75069155e-01  4.75232496e-01
  4.76143326e-01  4.78485984e-01  4.79297425e-01  4.80776257e-01
  4.81826089e-01  4.84249364e-01  4.86203532e-01  4.86301732e-01
  4.86887491e-01  4.87593147e-01  4.88832160e-01  4.89057995e-01
  4.90654992e-01  4.92148721e-01  4.93807002e-01  5.00243714e-01
  5.01065793e-01  5.03639328e-01  5.08026557e-01  5.08315382e-01
  5.09817155e-01  5.12006073e-01  5.12698610e-01  5.13446454e-01
  5.17845912e-01  5.19060004e-01  5.20345579e-01  5.25832048e-01
  5.28566029e-01  5.34492812e-01  5.35825722e-01  5.38298341e-01
  5.39868927e-01  5.40422911e-01  5.42535255e-01  5.49002167e-01
  5.49815045e-01  5.50120943e-01  5.50214848e-01  5.54458872e-01
  5.54744221e-01  5.62850171e-01  5.65117680e-01  5.70751069e-01
  5.74644868e-01  5.75567643e-01  5.75672880e-01  5.76034307e-01
  5.80575355e-01  5.81612333e-01  5.82141528e-01  5.86341930e-01
  5.86985314e-01  5.87135127e-01  5.97313337e-01  5.99986433e-01
  6.03737836e-01  6.05696345e-01  6.07906550e-01  6.08104988e-01
  6.09278927e-01  6.11447718e-01  6.20615313e-01  6.21148134e-01
  6.21260134e-01  6.21340175e-01  6.22258038e-01  6.22559360e-01
  6.22792405e-01  6.27398830e-01  6.29735862e-01  6.34607541e-01
  6.36535898e-01  6.39074312e-01  6.39745371e-01  6.46856598e-01
  6.46974714e-01  6.53115847e-01  6.58911573e-01  6.62796233e-01
  6.63369725e-01  6.75990793e-01  6.76854164e-01  6.86423626e-01
  6.91270571e-01  6.97613846e-01  7.12417428e-01  7.16531056e-01
  7.16560606e-01  7.20704801e-01  7.20802527e-01  7.23024738e-01
  7.28718123e-01  7.31761543e-01  7.36147185e-01  7.37432874e-01
  7.88480113e-01  7.91980393e-01  7.96951436e-01  8.00872618e-01
  8.05872116e-01]

  warnings.warn(

2022-11-03 10:50:59,484:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.73020183e+00 -1.72803201e+00 -1.72455623e+00 -1.71732890e+00
 -1.71034979e+00 -1.69946384e+00 -1.66269869e+00 -1.63071159e+00
 -1.62500335e+00 -1.62129470e+00 -1.61551585e+00 -1.61258424e+00
 -1.60985801e+00 -1.58099016e+00 -1.56849608e+00 -1.54975460e+00
 -1.51632019e+00 -1.51292807e+00 -1.51143621e+00 -1.51103955e+00
 -1.50837929e+00 -1.50706628e+00 -1.50680274e+00 -1.48912126e+00
 -1.47536355e+00 -1.47087940e+00 -1.47039873e+00 -1.46787865e+00
 -1.46307164e+00 -1.46050732e+00 -1.45528910e+00 -1.44904728e+00
 -1.44675860e+00 -1.43363307e+00 -1.42640842e+00 -1.42304439e+00
 -1.42084841e+00 -1.39261131e+00 -1.39257105e+00 -1.39175981e+00
 -1.39165872e+00 -1.39155533e+00 -1.38421604e+00 -1.36577193e+00
 -1.36129310e+00 -1.35501382e+00 -1.33806838e+00 -1.33613483e+00
 -1.33457471e+00 -1.33230579e+00 -1.33044746e+00 -1.32761043e+00
 -1.32241121e+00 -1.30961135e+00 -1.30626929e+00 -1.30621342e+00
 -1.30431012e+00 -1.30125870e+00 -1.29303574e+00 -1.28207461e+00
 -1.27778650e+00 -1.27651478e+00 -1.26243188e+00 -1.25350043e+00
 -1.24987266e+00 -1.24699367e+00 -1.24306592e+00 -1.23772156e+00
 -1.23404629e+00 -1.23243264e+00 -1.22962063e+00 -1.22934515e+00
 -1.22830479e+00 -1.22213827e+00 -1.21366507e+00 -1.21152448e+00
 -1.20994997e+00 -1.20580691e+00 -1.20235764e+00 -1.19294975e+00
 -1.19212513e+00 -1.18377772e+00 -1.18326634e+00 -1.17994235e+00
 -1.17794777e+00 -1.17524392e+00 -1.17240137e+00 -1.15676860e+00
 -1.15466544e+00 -1.15248086e+00 -1.15115471e+00 -1.13713723e+00
 -1.13443805e+00 -1.13288378e+00 -1.12894422e+00 -1.12107428e+00
 -1.12038508e+00 -1.11518498e+00 -1.11265955e+00 -1.11132908e+00
 -1.10371181e+00 -1.10082615e+00 -1.10022889e+00 -1.09602863e+00
 -1.09348544e+00 -1.09136555e+00 -1.08079550e+00 -1.07978609e+00
 -1.07643393e+00 -1.07180310e+00 -1.06307192e+00 -1.04954523e+00
 -1.04553178e+00 -1.04484731e+00 -1.04473854e+00 -1.04466488e+00
 -1.04134674e+00 -1.03284337e+00 -1.02974285e+00 -1.01556436e+00
 -1.01238532e+00 -1.00403326e+00 -1.00262834e+00 -1.00191900e+00
 -1.00162939e+00 -9.96607421e-01 -9.87692943e-01 -9.86770061e-01
 -9.83354184e-01 -9.71662436e-01 -9.70246900e-01 -9.57899469e-01
 -9.50883210e-01 -9.49855479e-01 -9.47214008e-01 -9.42806048e-01
 -9.42106587e-01 -9.36358068e-01 -9.33170289e-01 -9.31118614e-01
 -9.22237617e-01 -9.17805711e-01 -9.13842876e-01 -8.94881832e-01
 -8.93527923e-01 -8.93040538e-01 -8.91145331e-01 -8.90308252e-01
 -8.85657535e-01 -8.85336666e-01 -8.83627088e-01 -8.82125022e-01
 -8.81490511e-01 -8.79487894e-01 -8.78134920e-01 -8.78053428e-01
 -8.75575254e-01 -8.68853077e-01 -8.61255749e-01 -8.59747364e-01
 -8.57472697e-01 -8.41708845e-01 -8.36910508e-01 -8.35791990e-01
 -8.32023378e-01 -8.25764418e-01 -8.25577613e-01 -8.08620059e-01
 -8.08284602e-01 -8.08243235e-01 -8.08189352e-01 -8.06237545e-01
 -8.05719615e-01 -7.93373153e-01 -7.92356612e-01 -7.88824569e-01
 -7.87436486e-01 -7.85386880e-01 -7.85064750e-01 -7.84187074e-01
 -7.81237348e-01 -7.75279627e-01 -7.75109456e-01 -7.72912579e-01
 -7.69548714e-01 -7.69333372e-01 -7.60650079e-01 -7.56214874e-01
 -7.53005583e-01 -7.50933862e-01 -7.43813337e-01 -7.42722382e-01
 -7.40067955e-01 -7.37405105e-01 -7.36755714e-01 -7.36658979e-01
 -7.35917588e-01 -7.35573616e-01 -7.32340857e-01 -7.31578155e-01
 -7.27916662e-01 -7.27347586e-01 -7.24146136e-01 -7.18378370e-01
 -7.16785908e-01 -7.04363286e-01 -7.02708510e-01 -6.97461384e-01
 -6.94873834e-01 -6.94240981e-01 -6.89725795e-01 -6.86226750e-01
 -6.69228859e-01 -6.65666467e-01 -6.60334303e-01 -6.52399956e-01
 -6.47801400e-01 -6.44909001e-01 -6.44462186e-01 -6.38155133e-01
 -6.37693189e-01 -6.35641836e-01 -6.29564028e-01 -6.26799104e-01
 -6.15475067e-01 -6.09467937e-01 -6.09374103e-01 -6.03684234e-01
 -6.01158332e-01 -6.00851821e-01 -6.00734775e-01 -5.97620096e-01
 -5.97120725e-01 -5.85473387e-01 -5.83525786e-01 -5.81206323e-01
 -5.77164167e-01 -5.75381262e-01 -5.74047999e-01 -5.73872249e-01
 -5.70021013e-01 -5.67924887e-01 -5.67225349e-01 -5.61942992e-01
 -5.57548943e-01 -5.53381924e-01 -5.53264154e-01 -5.45647315e-01
 -5.39311073e-01 -5.30412949e-01 -5.30388503e-01 -5.30189153e-01
 -5.27832068e-01 -5.27687491e-01 -5.20313648e-01 -5.20281095e-01
 -5.16043244e-01 -5.14109149e-01 -5.13860855e-01 -5.13528548e-01
 -5.13363324e-01 -4.95222458e-01 -4.92152876e-01 -4.91665148e-01
 -4.88840095e-01 -4.85666656e-01 -4.80655317e-01 -4.80424353e-01
 -4.74666309e-01 -4.71878910e-01 -4.71063155e-01 -4.70827825e-01
 -4.61567596e-01 -4.58490098e-01 -4.56173329e-01 -4.55701118e-01
 -4.53556582e-01 -4.53451953e-01 -4.52676685e-01 -4.52161456e-01
 -4.51990023e-01 -4.51239413e-01 -4.51157911e-01 -4.45637583e-01
 -4.41408213e-01 -4.27571123e-01 -4.27357953e-01 -4.21248492e-01
 -4.16994244e-01 -4.15534748e-01 -4.14019335e-01 -4.13561737e-01
 -4.13455768e-01 -4.10193839e-01 -4.08654752e-01 -4.07612224e-01
 -4.05520747e-01 -3.96450042e-01 -3.93643086e-01 -3.92825731e-01
 -3.90621528e-01 -3.90603607e-01 -3.90243560e-01 -3.90055276e-01
 -3.89183940e-01 -3.88888301e-01 -3.86491917e-01 -3.85847882e-01
 -3.82880797e-01 -3.81937097e-01 -3.78777029e-01 -3.76265855e-01
 -3.71625520e-01 -3.71501009e-01 -3.69441580e-01 -3.67676451e-01
 -3.66254325e-01 -3.64320855e-01 -3.62991073e-01 -3.58996537e-01
 -3.54837951e-01 -3.49149944e-01 -3.42650871e-01 -3.38161758e-01
 -3.37008104e-01 -3.35477505e-01 -3.31779183e-01 -3.31413616e-01
 -3.28545778e-01 -3.27501315e-01 -3.25298796e-01 -3.24313329e-01
 -3.19515145e-01 -3.15226490e-01 -3.15176625e-01 -3.14102446e-01
 -3.14050757e-01 -3.12702394e-01 -3.10741172e-01 -3.10368793e-01
 -3.08960994e-01 -3.08634781e-01 -3.06527129e-01 -3.01552924e-01
 -3.01015981e-01 -2.99607111e-01 -2.96720283e-01 -2.90934220e-01
 -2.86988013e-01 -2.86122324e-01 -2.85820347e-01 -2.84147472e-01
 -2.81912723e-01 -2.81636550e-01 -2.80918876e-01 -2.79335014e-01
 -2.78118699e-01 -2.71960299e-01 -2.71138356e-01 -2.69790214e-01
 -2.66878200e-01 -2.64120087e-01 -2.61316895e-01 -2.57768465e-01
 -2.55708936e-01 -2.55639910e-01 -2.55537940e-01 -2.54893873e-01
 -2.54718565e-01 -2.52465481e-01 -2.50990084e-01 -2.48143813e-01
 -2.46966130e-01 -2.44753978e-01 -2.38627001e-01 -2.38269537e-01
 -2.38236425e-01 -2.31863472e-01 -2.31407554e-01 -2.30046985e-01
 -2.26112000e-01 -2.24204091e-01 -2.23156315e-01 -2.23112961e-01
 -2.19179926e-01 -2.16312749e-01 -2.14382747e-01 -2.07987803e-01
 -2.07292758e-01 -2.06295329e-01 -2.02511108e-01 -2.01217200e-01
 -1.99216534e-01 -1.99001718e-01 -1.98025151e-01 -1.94465277e-01
 -1.90158190e-01 -1.90050449e-01 -1.86501699e-01 -1.84434634e-01
 -1.82306037e-01 -1.81864504e-01 -1.79679312e-01 -1.77868012e-01
 -1.77261478e-01 -1.76158670e-01 -1.74965974e-01 -1.73764482e-01
 -1.67869581e-01 -1.57786945e-01 -1.54293465e-01 -1.49638546e-01
 -1.48450593e-01 -1.44319435e-01 -1.43601520e-01 -1.39948283e-01
 -1.36889717e-01 -1.36673334e-01 -1.34916185e-01 -1.34217319e-01
 -1.30148096e-01 -1.27587130e-01 -1.25411641e-01 -1.24558286e-01
 -1.22787615e-01 -1.20155367e-01 -1.19252021e-01 -1.18167557e-01
 -1.17361246e-01 -1.16548397e-01 -1.15274310e-01 -1.10522735e-01
 -1.07975763e-01 -1.03104093e-01 -1.02522008e-01 -1.02151043e-01
 -1.00276371e-01 -9.92807473e-02 -9.78575165e-02 -9.39363924e-02
 -9.32724802e-02 -9.29545388e-02 -9.26043582e-02 -7.51810469e-02
 -7.09625994e-02 -6.98952791e-02 -6.78522848e-02 -6.67880300e-02
 -6.28829281e-02 -6.13452781e-02 -5.69516389e-02 -5.32762618e-02
 -5.18782874e-02 -5.18249631e-02 -4.90425258e-02 -4.72351509e-02
 -4.56559464e-02 -4.51010416e-02 -4.35641269e-02 -4.16194690e-02
 -3.84160790e-02 -3.38375473e-02 -2.74330603e-02 -2.05518827e-02
 -1.96364510e-02 -1.91215985e-02 -1.86200718e-02 -1.71893839e-02
 -1.49938445e-02 -1.14938777e-02 -1.49277191e-03 -3.23314580e-04
 -2.37875592e-04  7.64084329e-04  2.64976640e-03  1.78139973e-02
  1.83460118e-02  1.86029213e-02  2.83623178e-02  3.06959396e-02
  3.29184157e-02  3.71491295e-02  3.83693090e-02  4.09935749e-02
  4.31375623e-02  4.76807763e-02  5.72881691e-02  5.76763781e-02
  6.26701403e-02  7.33197190e-02  7.57499931e-02  1.06825047e-01
  2.78189154e-01]

  warnings.warn(

2022-11-03 10:50:59,668:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.07031558e-01 -4.03669812e-01 -3.56853027e-01 -3.38862626e-01
 -3.31154423e-01 -3.30493365e-01 -3.28681826e-01 -3.22627184e-01
 -3.15716443e-01 -3.07418914e-01 -3.06121842e-01 -3.02287534e-01
 -2.98984894e-01 -2.96258054e-01 -2.94360304e-01 -2.79462396e-01
 -2.79046397e-01 -2.77587605e-01 -2.76616059e-01 -2.72020155e-01
 -2.62020337e-01 -2.60122959e-01 -2.56902515e-01 -2.55924224e-01
 -2.55552664e-01 -2.49487240e-01 -2.49426600e-01 -2.45014838e-01
 -2.41403656e-01 -2.31204109e-01 -2.25303213e-01 -2.21575700e-01
 -2.19822683e-01 -2.18269499e-01 -2.17840511e-01 -2.13773164e-01
 -2.08720782e-01 -2.02381384e-01 -1.96117911e-01 -1.95198645e-01
 -1.93928843e-01 -1.93474487e-01 -1.91491139e-01 -1.90825598e-01
 -1.90067447e-01 -1.86271435e-01 -1.80637599e-01 -1.74923695e-01
 -1.74522954e-01 -1.73631678e-01 -1.72526876e-01 -1.71866977e-01
 -1.71175870e-01 -1.68137748e-01 -1.65453865e-01 -1.65365190e-01
 -1.65298172e-01 -1.63966959e-01 -1.60895009e-01 -1.60206541e-01
 -1.54540444e-01 -1.52778159e-01 -1.49406618e-01 -1.47965127e-01
 -1.45106003e-01 -1.35787604e-01 -1.31871838e-01 -1.30434766e-01
 -1.28960643e-01 -1.27207626e-01 -1.26536575e-01 -1.25537785e-01
 -1.25395913e-01 -1.20702859e-01 -1.19760937e-01 -1.18555094e-01
 -1.17400082e-01 -1.16025142e-01 -1.15222102e-01 -1.11648633e-01
 -1.10067265e-01 -1.08488112e-01 -1.07209449e-01 -1.05029801e-01
 -1.04860628e-01 -9.99821617e-02 -9.72103385e-02 -9.55400637e-02
 -9.48041535e-02 -9.03129103e-02 -8.80466469e-02 -8.64976921e-02
 -8.54716933e-02 -8.54303281e-02 -8.47941464e-02 -8.40546219e-02
 -8.09807556e-02 -7.89179354e-02 -7.82732646e-02 -7.80258269e-02
 -7.57830212e-02 -7.57066127e-02 -7.39347968e-02 -7.28143664e-02
 -7.18756959e-02 -7.14391371e-02 -7.09241302e-02 -6.90250144e-02
 -6.87084479e-02 -6.77638429e-02 -6.63556404e-02 -6.62977616e-02
 -6.52116547e-02 -6.51681899e-02 -6.37161586e-02 -6.31111151e-02
 -6.15647861e-02 -6.11543102e-02 -5.56047474e-02 -5.50917446e-02
 -5.46506721e-02 -5.30032258e-02 -5.29058171e-02 -5.04292159e-02
 -4.83923351e-02 -4.55684827e-02 -4.55452489e-02 -4.41221848e-02
 -4.30385177e-02 -4.18632551e-02 -3.99271100e-02 -3.95787344e-02
 -3.88389397e-02 -3.86612348e-02 -3.84909853e-02 -3.84680348e-02
 -3.84230543e-02 -3.83775013e-02 -3.72223155e-02 -3.22192701e-02
 -3.21027520e-02 -2.88686028e-02 -2.78165075e-02 -2.56297175e-02
 -2.56157652e-02 -2.31846356e-02 -9.39940455e-03 -8.70376383e-03
 -7.58721585e-03 -6.49584084e-03 -5.55633139e-03 -5.32458983e-03
 -4.69213181e-03 -2.94005585e-03 -2.90649010e-03 -2.88755931e-03
 -2.19185678e-03 -5.63975975e-04 -5.09641604e-04 -4.83757838e-04
  4.87590100e-04  1.27845240e-03  1.59905887e-03  2.56032749e-03
  3.13748461e-03  3.20404930e-03  3.32936581e-03  6.20365162e-03
  6.25446112e-03  6.86067998e-03  7.15637920e-03  7.57847133e-03
  9.23657501e-03  1.09409894e-02  1.28328030e-02  1.29346302e-02
  1.34365176e-02  1.41625635e-02  1.46479953e-02  1.49113011e-02
  1.49735602e-02  1.58034867e-02  1.84946695e-02  1.88990338e-02
  1.92664699e-02  1.94826131e-02  2.04004974e-02  2.16299570e-02
  2.25685063e-02  2.27457190e-02  2.28777726e-02  2.30939160e-02
  2.41457462e-02  2.47153908e-02  2.48702007e-02  2.55053755e-02
  2.59464923e-02  2.62756669e-02  2.64817723e-02  2.67445258e-02
  2.73488656e-02  2.81098290e-02  2.86514088e-02  2.88203463e-02
  3.17854217e-02  3.23873053e-02  3.26117955e-02  3.35378986e-02
  3.38361392e-02  3.40450363e-02  3.84776311e-02  3.86226975e-02
  3.95352581e-02  4.03049692e-02  4.05560525e-02  4.16059892e-02
  4.23892264e-02  4.27279533e-02  4.33988759e-02  4.36596513e-02
  4.38939391e-02  4.63455472e-02  4.76145455e-02  5.01641084e-02
  5.02062389e-02  5.05503192e-02  5.37205325e-02  5.39706675e-02
  5.43557226e-02  5.48140355e-02  6.00943388e-02  6.07635043e-02
  6.19504281e-02  6.22673409e-02  6.25474115e-02  6.29142028e-02
  6.29267587e-02  6.33276038e-02  6.37526626e-02  6.53794246e-02
  6.61251887e-02  6.67459387e-02  6.85836706e-02  6.91366344e-02
  7.00002429e-02  7.02526662e-02  7.08342796e-02  7.16004467e-02
  7.16782463e-02  7.20158337e-02  7.23590004e-02  7.31295345e-02
  7.47393034e-02  7.64721420e-02  7.67899499e-02  7.67946752e-02
  8.02260882e-02  8.02932790e-02  8.27147956e-02  8.34539459e-02
  8.42595063e-02  8.47123737e-02  8.51453376e-02  8.72792409e-02
  8.74441743e-02  8.82204979e-02  9.12313671e-02  9.26961257e-02
  9.37631768e-02  9.38150527e-02  9.42401877e-02  9.42868274e-02
  9.43008216e-02  9.46602544e-02  9.51332554e-02  9.52558973e-02
  9.55321580e-02  9.55420011e-02  9.58860421e-02  9.60643919e-02
  9.61148257e-02  9.63612728e-02  9.76503956e-02  9.80679342e-02
  9.80884478e-02  9.92926149e-02  9.97283892e-02  9.97762806e-02
  9.98971981e-02  1.00389869e-01  1.00753393e-01  1.01380495e-01
  1.02694170e-01  1.02987464e-01  1.03580217e-01  1.03702144e-01
  1.04281384e-01  1.04876572e-01  1.05041049e-01  1.06261972e-01
  1.06400874e-01  1.07058301e-01  1.08106498e-01  1.08245352e-01
  1.08293153e-01  1.08754138e-01  1.11817792e-01  1.12133831e-01
  1.12603561e-01  1.14572887e-01  1.15698747e-01  1.16788977e-01
  1.16897225e-01  1.17785901e-01  1.18454349e-01  1.19491991e-01
  1.20492918e-01  1.21452449e-01  1.21927670e-01  1.22377200e-01
  1.22779884e-01  1.23482359e-01  1.26720351e-01  1.26994410e-01
  1.27071380e-01  1.27662494e-01  1.28008122e-01  1.29342301e-01
  1.29715039e-01  1.29878154e-01  1.33090600e-01  1.34340275e-01
  1.34721171e-01  1.34983962e-01  1.35259453e-01  1.35703251e-01
  1.35795092e-01  1.36161404e-01  1.36205689e-01  1.36707786e-01
  1.37713025e-01  1.39017712e-01  1.39159170e-01  1.40097029e-01
  1.41865148e-01  1.43664933e-01  1.44170242e-01  1.44365919e-01
  1.45844031e-01  1.49066986e-01  1.50500199e-01  1.50677843e-01
  1.50799342e-01  1.53287726e-01  1.53786671e-01  1.54528611e-01
  1.54840758e-01  1.55104981e-01  1.55293081e-01  1.55379894e-01
  1.55430398e-01  1.56569316e-01  1.56996541e-01  1.57089723e-01
  1.57113029e-01  1.59297018e-01  1.60671695e-01  1.63391201e-01
  1.64028619e-01  1.64311984e-01  1.65011976e-01  1.66608527e-01
  1.68632404e-01  1.69344808e-01  1.69595674e-01  1.70892612e-01
  1.72278730e-01  1.72360854e-01  1.72879574e-01  1.74420326e-01
  1.74621880e-01  1.76931843e-01  1.77820188e-01  1.78716244e-01
  1.79148942e-01  1.79248254e-01  1.80532184e-01  1.81404349e-01
  1.82204387e-01  1.83634200e-01  1.83953477e-01  1.84354966e-01
  1.84990783e-01  1.85689884e-01  1.86580131e-01  1.87324272e-01
  1.87433258e-01  1.88510003e-01  1.90779444e-01  1.91263813e-01
  1.91364014e-01  1.91806025e-01  1.91972828e-01  1.92773288e-01
  1.96203940e-01  1.96546348e-01  1.98572916e-01  1.99999163e-01
  2.02076481e-01  2.04576755e-01  2.05456038e-01  2.09370610e-01
  2.10722491e-01  2.13728521e-01  2.15964819e-01  2.17605985e-01
  2.21742431e-01  2.23321106e-01  2.24005816e-01  2.24098021e-01
  2.24509162e-01  2.25279234e-01  2.25631286e-01  2.26001557e-01
  2.26090766e-01  2.26150183e-01  2.29475374e-01  2.30230996e-01
  2.31053782e-01  2.32921362e-01  2.33032921e-01  2.33396102e-01
  2.34630346e-01  2.36114323e-01  2.40293558e-01  2.42176841e-01
  2.42419441e-01  2.44044640e-01  2.45568194e-01  2.47787745e-01
  2.47861942e-01  2.50223710e-01  2.50335427e-01  2.51086827e-01
  2.51235695e-01  2.51895840e-01  2.53537858e-01  2.54390105e-01
  2.54581362e-01  2.57041032e-01  2.58828921e-01  2.59733618e-01
  2.63447946e-01  2.66839552e-01  2.67385856e-01  2.68017066e-01
  2.68956866e-01  2.71456318e-01  2.72152946e-01  2.74898045e-01
  2.79666831e-01  2.79992888e-01  2.82941651e-01  2.83327670e-01
  2.84173034e-01  2.87744231e-01  2.92586079e-01  2.99680887e-01
  3.01013726e-01  3.05723654e-01  3.05800819e-01  3.06876764e-01
  3.09642001e-01  3.15442557e-01  3.19724345e-01  3.20051803e-01
  3.26506361e-01  3.29438413e-01  3.29988024e-01  3.30994888e-01
  3.36524558e-01  3.48744184e-01  3.56738263e-01  3.63812799e-01
  3.69015279e-01  3.73635012e-01  3.74080103e-01  3.84161469e-01
  4.13743612e-01  4.16355092e-01  4.68432447e-01  4.76566814e-01
  5.28670543e-01]

  warnings.warn(

2022-11-03 10:50:59,693:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.75892182e+00 -1.68835217e+00 -1.67911116e+00 -1.67879239e+00
 -1.62064654e+00 -1.61821658e+00 -1.58926046e+00 -1.58125666e+00
 -1.58074627e+00 -1.57929639e+00 -1.57893390e+00 -1.57835358e+00
 -1.57489034e+00 -1.55320442e+00 -1.54779098e+00 -1.54544530e+00
 -1.53735486e+00 -1.53266576e+00 -1.52795791e+00 -1.52114561e+00
 -1.51854533e+00 -1.51788836e+00 -1.51489650e+00 -1.51303456e+00
 -1.50872777e+00 -1.50134505e+00 -1.49697565e+00 -1.49057247e+00
 -1.48421916e+00 -1.47979405e+00 -1.47860463e+00 -1.47430696e+00
 -1.47392704e+00 -1.46974200e+00 -1.46637597e+00 -1.46270074e+00
 -1.45478771e+00 -1.45459376e+00 -1.43054784e+00 -1.42909579e+00
 -1.42427789e+00 -1.41802209e+00 -1.41631563e+00 -1.41517240e+00
 -1.41395829e+00 -1.41105104e+00 -1.41039981e+00 -1.40977822e+00
 -1.40758392e+00 -1.40586254e+00 -1.39906147e+00 -1.39435561e+00
 -1.38438678e+00 -1.35399685e+00 -1.35397123e+00 -1.31851950e+00
 -1.31541009e+00 -1.31379567e+00 -1.30913199e+00 -1.29212524e+00
 -1.27972217e+00 -1.27746296e+00 -1.26937317e+00 -1.25898262e+00
 -1.25825573e+00 -1.25399660e+00 -1.25058954e+00 -1.24372040e+00
 -1.24313703e+00 -1.24062464e+00 -1.24042572e+00 -1.23772154e+00
 -1.23400458e+00 -1.22331559e+00 -1.22128568e+00 -1.21289315e+00
 -1.20930801e+00 -1.20680441e+00 -1.19950760e+00 -1.18590563e+00
 -1.18093012e+00 -1.17403449e+00 -1.16822995e+00 -1.16787339e+00
 -1.16686238e+00 -1.16634410e+00 -1.15711506e+00 -1.14849825e+00
 -1.14637774e+00 -1.14626266e+00 -1.14572027e+00 -1.13713630e+00
 -1.13259862e+00 -1.12740849e+00 -1.11860305e+00 -1.11654292e+00
 -1.11238325e+00 -1.10446844e+00 -1.09909778e+00 -1.09681119e+00
 -1.08817448e+00 -1.08666407e+00 -1.08451561e+00 -1.07738480e+00
 -1.06446590e+00 -1.06129967e+00 -1.06002167e+00 -1.05865121e+00
 -1.05404532e+00 -1.04887786e+00 -1.04781956e+00 -1.04627939e+00
 -1.04483607e+00 -1.03224699e+00 -1.03097437e+00 -1.02309260e+00
 -1.02301542e+00 -1.02100450e+00 -1.01877587e+00 -1.01861255e+00
 -1.01678897e+00 -1.01571235e+00 -1.01032708e+00 -9.99882734e-01
 -9.96223318e-01 -9.94069291e-01 -9.92951638e-01 -9.83639569e-01
 -9.67306314e-01 -9.64213328e-01 -9.59951790e-01 -9.58362296e-01
 -9.57469549e-01 -9.56649877e-01 -9.55205814e-01 -9.54469139e-01
 -9.49921179e-01 -9.46472640e-01 -9.42602014e-01 -9.41343569e-01
 -9.40561700e-01 -9.39845941e-01 -9.37481266e-01 -9.36573318e-01
 -9.36538991e-01 -9.35915431e-01 -9.26387103e-01 -9.22610550e-01
 -9.22375133e-01 -9.22317185e-01 -9.20131522e-01 -9.19378018e-01
 -9.16127263e-01 -9.12580579e-01 -9.05312640e-01 -8.93633170e-01
 -8.92833133e-01 -8.92233154e-01 -8.75808975e-01 -8.72274803e-01
 -8.69504274e-01 -8.69468347e-01 -8.63450679e-01 -8.63150663e-01
 -8.53000435e-01 -8.51872802e-01 -8.45897101e-01 -8.43875779e-01
 -8.41270965e-01 -8.39152357e-01 -8.30491212e-01 -8.29534176e-01
 -8.26118841e-01 -8.23341832e-01 -8.20432590e-01 -8.18175240e-01
 -8.17645640e-01 -8.17523933e-01 -8.02114767e-01 -7.98564515e-01
 -7.97679093e-01 -7.93241778e-01 -7.90847715e-01 -7.84355866e-01
 -7.71921361e-01 -7.71410009e-01 -7.67303205e-01 -7.64643516e-01
 -7.64451898e-01 -7.60817005e-01 -7.55894959e-01 -7.55337729e-01
 -7.53644432e-01 -7.47368718e-01 -7.45471779e-01 -7.34191938e-01
 -7.31818335e-01 -7.25755124e-01 -7.19109408e-01 -7.13287261e-01
 -7.13219558e-01 -7.05963941e-01 -6.94782017e-01 -6.85903520e-01
 -6.75724594e-01 -6.74940463e-01 -6.74059021e-01 -6.73555982e-01
 -6.71253747e-01 -6.69509177e-01 -6.67770268e-01 -6.62950296e-01
 -6.62600587e-01 -6.62066630e-01 -6.60308283e-01 -6.59956144e-01
 -6.59597962e-01 -6.54694580e-01 -6.50611374e-01 -6.42826940e-01
 -6.41936345e-01 -6.29154512e-01 -6.26659263e-01 -6.26345843e-01
 -6.24398469e-01 -6.17484150e-01 -6.17245345e-01 -6.16821135e-01
 -6.15989518e-01 -6.15648014e-01 -6.15554370e-01 -6.09947615e-01
 -6.03679574e-01 -5.94499470e-01 -5.88284363e-01 -5.87775507e-01
 -5.85096897e-01 -5.81647451e-01 -5.71061349e-01 -5.70477985e-01
 -5.62060289e-01 -5.60528319e-01 -5.59605368e-01 -5.58518259e-01
 -5.54482699e-01 -5.51299916e-01 -5.42596597e-01 -5.39921183e-01
 -5.38495458e-01 -5.38154096e-01 -5.34538658e-01 -5.22649485e-01
 -5.21450623e-01 -5.14228330e-01 -5.14170752e-01 -5.12710742e-01
 -4.95559758e-01 -4.87874663e-01 -4.86829832e-01 -4.85049211e-01
 -4.82204519e-01 -4.77207574e-01 -4.76576691e-01 -4.73930216e-01
 -4.73906595e-01 -4.71195242e-01 -4.70828127e-01 -4.68115111e-01
 -4.66961941e-01 -4.57597228e-01 -4.56549575e-01 -4.53718988e-01
 -4.52530148e-01 -4.43986529e-01 -4.40390880e-01 -4.39374415e-01
 -4.38730794e-01 -4.33345556e-01 -4.32963983e-01 -4.27425586e-01
 -4.26938514e-01 -4.20510278e-01 -4.03925835e-01 -4.02348824e-01
 -3.98557480e-01 -3.96964906e-01 -3.96915021e-01 -3.91625548e-01
 -3.89601760e-01 -3.88546224e-01 -3.88423240e-01 -3.82141472e-01
 -3.73944249e-01 -3.65518390e-01 -3.65372904e-01 -3.63344509e-01
 -3.61666114e-01 -3.54886932e-01 -3.54190183e-01 -3.50333858e-01
 -3.48345963e-01 -3.46205187e-01 -3.44822105e-01 -3.40496531e-01
 -3.39804539e-01 -3.35802877e-01 -3.31678576e-01 -3.24630945e-01
 -3.23369847e-01 -3.21271891e-01 -3.19216290e-01 -3.18519104e-01
 -3.15539745e-01 -3.10413217e-01 -3.06294517e-01 -3.03051470e-01
 -2.99847899e-01 -2.94536526e-01 -2.91498756e-01 -2.90774705e-01
 -2.89937055e-01 -2.85565321e-01 -2.84159218e-01 -2.76428760e-01
 -2.76298940e-01 -2.75121747e-01 -2.74451290e-01 -2.74117360e-01
 -2.68716429e-01 -2.62977372e-01 -2.59609594e-01 -2.58399432e-01
 -2.55229089e-01 -2.48712729e-01 -2.39129492e-01 -2.37432189e-01
 -2.37156956e-01 -2.33569209e-01 -2.32113608e-01 -2.31007894e-01
 -2.28072555e-01 -2.22167377e-01 -2.21538468e-01 -2.19289979e-01
 -2.18232879e-01 -2.18028525e-01 -2.15248195e-01 -2.13241551e-01
 -2.08780813e-01 -2.08270753e-01 -2.06309746e-01 -2.05452584e-01
 -2.03310595e-01 -2.02834540e-01 -1.99711669e-01 -1.99650974e-01
 -1.96620761e-01 -1.95558077e-01 -1.91690374e-01 -1.91217665e-01
 -1.85467992e-01 -1.84638887e-01 -1.83152053e-01 -1.82815775e-01
 -1.82346917e-01 -1.79924464e-01 -1.76666643e-01 -1.75918818e-01
 -1.75651190e-01 -1.74279577e-01 -1.73653698e-01 -1.71548804e-01
 -1.70314777e-01 -1.69752638e-01 -1.65560165e-01 -1.62717661e-01
 -1.61462726e-01 -1.60386048e-01 -1.60340060e-01 -1.58692749e-01
 -1.38600853e-01 -1.38188747e-01 -1.33138309e-01 -1.31335481e-01
 -1.30785970e-01 -1.26626219e-01 -1.23713754e-01 -1.22411591e-01
 -1.20650014e-01 -1.19795274e-01 -1.18354988e-01 -1.11543483e-01
 -1.10076592e-01 -1.08949490e-01 -1.08818105e-01 -1.02483305e-01
 -9.92391780e-02 -9.79133036e-02 -9.12503838e-02 -8.84504278e-02
 -8.81788312e-02 -8.47624841e-02 -7.97740785e-02 -7.90020983e-02
 -7.55754008e-02 -7.33185107e-02 -6.98362772e-02 -6.95905957e-02
 -6.89143532e-02 -6.76796489e-02 -6.62447126e-02 -6.61471264e-02
 -5.97336042e-02 -5.57874018e-02 -5.55923921e-02 -5.47423956e-02
 -5.45811523e-02 -4.94386139e-02 -4.69885524e-02 -4.56435306e-02
 -4.13703643e-02 -4.00821246e-02 -3.98839151e-02 -3.94966673e-02
 -3.68758348e-02 -3.29698630e-02 -2.83344412e-02 -2.67453106e-02
 -2.67035250e-02 -2.31736934e-02 -1.92190331e-02 -1.90722575e-02
 -1.65737373e-02 -1.35438990e-02 -1.29797405e-02 -8.25850619e-03
 -1.31782071e-03 -1.11191117e-04  7.70577538e-04  8.62181969e-04
  5.19214958e-03  7.93587070e-03  8.95608620e-03  1.40928438e-02
  1.73764589e-02  1.98646529e-02  2.00220296e-02  2.24216645e-02
  2.42866996e-02  2.48184985e-02  2.78193696e-02  2.78364541e-02
  3.62478464e-02  3.65224184e-02  3.73253974e-02  3.78258672e-02
  3.94422507e-02  3.96744213e-02  5.01174805e-02  5.13009747e-02
  5.59024354e-02  5.73945858e-02  6.48879898e-02  6.78535447e-02
  6.80798698e-02  7.41553822e-02  8.20573389e-02  8.62009870e-02
  8.79640320e-02  8.81911450e-02  9.13764579e-02  1.02698303e-01
  1.04095558e-01  1.05314280e-01  1.06578552e-01  1.08625609e-01
  1.10880630e-01  1.13237418e-01  1.14658620e-01  1.16421207e-01
  1.18336849e-01  1.20829134e-01  1.26432652e-01  1.31640652e-01
  1.35967636e-01  1.36979342e-01  1.40960945e-01  1.46439446e-01
  1.63083661e-01  1.68368677e-01  1.99357304e-01  2.60257483e-01
  3.34408926e-01]

  warnings.warn(

2022-11-03 10:50:59,838:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.79896668e+00 -1.76746238e+00 -1.76745801e+00 -1.73264382e+00
 -1.72683685e+00 -1.72615717e+00 -1.72487347e+00 -1.70868525e+00
 -1.69667704e+00 -1.66912058e+00 -1.66602124e+00 -1.66295771e+00
 -1.65378566e+00 -1.65155127e+00 -1.63206417e+00 -1.63099115e+00
 -1.62806881e+00 -1.62324325e+00 -1.61793807e+00 -1.61788218e+00
 -1.61632121e+00 -1.61341940e+00 -1.59605836e+00 -1.59153107e+00
 -1.58575670e+00 -1.56746398e+00 -1.56073072e+00 -1.55686353e+00
 -1.55071369e+00 -1.54891065e+00 -1.54777918e+00 -1.54159793e+00
 -1.53525048e+00 -1.52199569e+00 -1.51793308e+00 -1.50818713e+00
 -1.49569029e+00 -1.49311527e+00 -1.48322644e+00 -1.48229399e+00
 -1.48169135e+00 -1.46514568e+00 -1.46185402e+00 -1.45611528e+00
 -1.45336729e+00 -1.45146715e+00 -1.45043135e+00 -1.44771558e+00
 -1.44015242e+00 -1.43898338e+00 -1.43764753e+00 -1.43512019e+00
 -1.43450678e+00 -1.43157997e+00 -1.41809815e+00 -1.41706328e+00
 -1.41698410e+00 -1.41662760e+00 -1.40439216e+00 -1.39688059e+00
 -1.39235228e+00 -1.39211221e+00 -1.39152528e+00 -1.38965986e+00
 -1.38939113e+00 -1.38913069e+00 -1.38471104e+00 -1.37872324e+00
 -1.37484121e+00 -1.37302920e+00 -1.37105330e+00 -1.36545630e+00
 -1.36268685e+00 -1.35442212e+00 -1.34863841e+00 -1.34255699e+00
 -1.33961417e+00 -1.33368652e+00 -1.32802036e+00 -1.32214743e+00
 -1.31696915e+00 -1.31030546e+00 -1.28987273e+00 -1.28629334e+00
 -1.27862714e+00 -1.27602480e+00 -1.25704409e+00 -1.24987742e+00
 -1.24110023e+00 -1.24013825e+00 -1.22075729e+00 -1.21313623e+00
 -1.21156787e+00 -1.20541132e+00 -1.19891161e+00 -1.19834170e+00
 -1.19204393e+00 -1.19176373e+00 -1.18792608e+00 -1.18733526e+00
 -1.17692491e+00 -1.17241767e+00 -1.17161630e+00 -1.16953230e+00
 -1.16335958e+00 -1.15936269e+00 -1.15657058e+00 -1.14819890e+00
 -1.14806203e+00 -1.13226545e+00 -1.13225123e+00 -1.13151654e+00
 -1.12554276e+00 -1.11102802e+00 -1.10970793e+00 -1.10221968e+00
 -1.10066891e+00 -1.09582720e+00 -1.09371730e+00 -1.09029972e+00
 -1.08642135e+00 -1.08545868e+00 -1.07606429e+00 -1.07497434e+00
 -1.06873786e+00 -1.06782242e+00 -1.06730185e+00 -1.06583665e+00
 -1.06539139e+00 -1.06355134e+00 -1.06196161e+00 -1.06170060e+00
 -1.06041504e+00 -1.05788269e+00 -1.05055121e+00 -1.04455467e+00
 -1.04449825e+00 -1.02947247e+00 -1.02102507e+00 -1.01964049e+00
 -1.01770777e+00 -1.01681538e+00 -1.01646764e+00 -1.01542219e+00
 -1.01426229e+00 -1.01227118e+00 -1.01184848e+00 -1.00844079e+00
 -1.00157057e+00 -9.99590784e-01 -9.94566512e-01 -9.90417803e-01
 -9.88836907e-01 -9.88796706e-01 -9.83893583e-01 -9.81098408e-01
 -9.73915175e-01 -9.72017767e-01 -9.64908260e-01 -9.61633427e-01
 -9.59927239e-01 -9.55879539e-01 -9.49291510e-01 -9.39749714e-01
 -9.36067361e-01 -9.25845133e-01 -9.25256292e-01 -9.22981038e-01
 -9.22956247e-01 -9.18645364e-01 -9.15837779e-01 -9.08821991e-01
 -9.03520036e-01 -9.00955750e-01 -8.97568536e-01 -8.94795500e-01
 -8.94587590e-01 -8.90316144e-01 -8.89763229e-01 -8.88174549e-01
 -8.79452453e-01 -8.71877675e-01 -8.58826175e-01 -8.57467595e-01
 -8.50776300e-01 -8.31984514e-01 -8.24167290e-01 -8.23635475e-01
 -8.21628525e-01 -8.19224245e-01 -8.17841032e-01 -8.10983812e-01
 -8.09879187e-01 -8.08382161e-01 -8.03990979e-01 -8.01460843e-01
 -7.99454257e-01 -7.95723724e-01 -7.93773846e-01 -7.82708217e-01
 -7.80508092e-01 -7.79201473e-01 -7.73616309e-01 -7.73053416e-01
 -7.58723335e-01 -7.58644107e-01 -7.53186917e-01 -7.47488530e-01
 -7.46460467e-01 -7.44855679e-01 -7.43895635e-01 -7.41456920e-01
 -7.39022045e-01 -7.31905160e-01 -7.23962052e-01 -7.20558354e-01
 -7.05489897e-01 -7.01085217e-01 -6.98478566e-01 -6.98189280e-01
 -6.96283809e-01 -6.86614408e-01 -6.84320656e-01 -6.80826747e-01
 -6.75780236e-01 -6.73228455e-01 -6.66140576e-01 -6.61094054e-01
 -6.55748474e-01 -6.48003293e-01 -6.47442714e-01 -6.46564873e-01
 -6.37047789e-01 -6.28595959e-01 -6.27420256e-01 -6.24540922e-01
 -6.19780298e-01 -6.19194067e-01 -6.15132421e-01 -6.06097913e-01
 -6.04492545e-01 -5.99585655e-01 -5.98386362e-01 -5.97740010e-01
 -5.91729941e-01 -5.88456311e-01 -5.87710624e-01 -5.83178652e-01
 -5.81585958e-01 -5.78318790e-01 -5.74331647e-01 -5.73461129e-01
 -5.68679470e-01 -5.66149638e-01 -5.65442468e-01 -5.62532979e-01
 -5.58465808e-01 -5.55931496e-01 -5.45274817e-01 -5.44544691e-01
 -5.42513010e-01 -5.41131329e-01 -5.37063200e-01 -5.33952264e-01
 -5.33670832e-01 -5.29439622e-01 -5.28697940e-01 -5.16787418e-01
 -5.13759800e-01 -5.12245077e-01 -5.11542575e-01 -5.10405353e-01
 -5.09165192e-01 -5.00773803e-01 -4.96632918e-01 -4.93573415e-01
 -4.93460133e-01 -4.86154371e-01 -4.85312488e-01 -4.84008988e-01
 -4.83153953e-01 -4.82947950e-01 -4.68381192e-01 -4.68207674e-01
 -4.66080413e-01 -4.59471954e-01 -4.58988935e-01 -4.55631785e-01
 -4.52778809e-01 -4.51922097e-01 -4.50261456e-01 -4.49867235e-01
 -4.42741259e-01 -4.41548293e-01 -4.41540261e-01 -4.40892076e-01
 -4.40564623e-01 -4.36019117e-01 -4.33511962e-01 -4.29376560e-01
 -4.27858254e-01 -4.23867921e-01 -4.20418508e-01 -4.19318285e-01
 -4.18993672e-01 -4.14698275e-01 -4.13004356e-01 -4.10192285e-01
 -4.08051648e-01 -4.02633647e-01 -4.02629803e-01 -4.01685348e-01
 -4.01267296e-01 -3.97292505e-01 -3.97029288e-01 -3.96113300e-01
 -3.92090672e-01 -3.91140751e-01 -3.86873385e-01 -3.85968321e-01
 -3.83726055e-01 -3.81741910e-01 -3.71807784e-01 -3.69735982e-01
 -3.67211057e-01 -3.63160091e-01 -3.59113515e-01 -3.58401479e-01
 -3.56654773e-01 -3.45548605e-01 -3.45445603e-01 -3.42975131e-01
 -3.34386403e-01 -3.33288457e-01 -3.30160741e-01 -3.26537538e-01
 -3.24720569e-01 -3.21201169e-01 -3.19819809e-01 -3.19203475e-01
 -3.19146361e-01 -3.19106955e-01 -3.16674411e-01 -3.14411650e-01
 -3.11414028e-01 -3.10345717e-01 -3.03889562e-01 -2.97988819e-01
 -2.95878540e-01 -2.94323338e-01 -2.91321987e-01 -2.90982115e-01
 -2.87455235e-01 -2.86463404e-01 -2.82831777e-01 -2.67007946e-01
 -2.65985384e-01 -2.64808248e-01 -2.61260192e-01 -2.60582701e-01
 -2.60334694e-01 -2.59213406e-01 -2.56222632e-01 -2.54935756e-01
 -2.53999228e-01 -2.47085124e-01 -2.46165083e-01 -2.38419762e-01
 -2.36159658e-01 -2.35999554e-01 -2.33826115e-01 -2.33331584e-01
 -2.30356268e-01 -2.27268540e-01 -2.24275531e-01 -2.23466278e-01
 -2.22780076e-01 -2.14334083e-01 -2.08225166e-01 -2.08119400e-01
 -2.00777948e-01 -1.95855252e-01 -1.90482022e-01 -1.85898460e-01
 -1.74865278e-01 -1.69885112e-01 -1.67594168e-01 -1.67350924e-01
 -1.67205361e-01 -1.66513368e-01 -1.66451115e-01 -1.62896087e-01
 -1.62016529e-01 -1.60139547e-01 -1.57596807e-01 -1.57181141e-01
 -1.56153157e-01 -1.52592565e-01 -1.51418016e-01 -1.48332784e-01
 -1.45995946e-01 -1.42942520e-01 -1.42075896e-01 -1.41503778e-01
 -1.40763853e-01 -1.39918751e-01 -1.37645860e-01 -1.36375161e-01
 -1.36133481e-01 -1.34103184e-01 -1.32544093e-01 -1.31664358e-01
 -1.30602609e-01 -1.29515594e-01 -1.29481933e-01 -1.27200070e-01
 -1.19829017e-01 -1.17876382e-01 -1.17801916e-01 -1.14743299e-01
 -1.14478441e-01 -1.13230800e-01 -1.12100693e-01 -1.10927726e-01
 -1.02571743e-01 -1.02067985e-01 -9.94514881e-02 -9.94153177e-02
 -9.88682139e-02 -9.66682440e-02 -8.69793597e-02 -8.57945571e-02
 -7.97833163e-02 -7.44376612e-02 -7.07439239e-02 -6.90828608e-02
 -6.86930387e-02 -6.21003584e-02 -6.11929219e-02 -5.94710568e-02
 -5.48588343e-02 -5.00248984e-02 -4.68603611e-02 -2.79547278e-02
 -2.78054263e-02 -2.44122848e-02 -2.39734986e-02 -1.75660541e-02
 -1.14452675e-02 -9.73592735e-03 -2.57242016e-03  1.04028726e-03
  2.61475749e-03  5.44122971e-03  6.32042187e-03  6.47468517e-03
  7.65039047e-03  1.26723922e-02  1.27430430e-02  1.97862935e-02
  1.98040917e-02  2.37741268e-02  2.67226866e-02  2.90494002e-02
  3.01865448e-02  3.96379639e-02  4.25656627e-02  5.08894679e-02
  5.14532207e-02  5.25535219e-02  5.51078025e-02  5.77988962e-02
  5.80569740e-02  5.86335988e-02  6.14099790e-02  7.01079754e-02
  7.73193190e-02  8.11228883e-02  8.91658775e-02  9.01399800e-02
  1.06455799e-01  1.30644549e-01  1.38273763e-01  1.46513848e-01
  1.52166197e-01  1.66453208e-01  2.37758956e-01  2.42985463e-01
  2.56770747e-01  2.67993187e-01  2.83139691e-01  2.85878591e-01
  2.99335383e-01]

  warnings.warn(

2022-11-03 10:50:59,838:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.61239494e-01 -4.58656194e-01 -4.19557369e-01 -4.17774124e-01
 -3.96496142e-01 -3.93337678e-01 -3.90881197e-01 -3.87961861e-01
 -3.79110304e-01 -3.78796280e-01 -3.71115601e-01 -3.70639638e-01
 -3.70016758e-01 -3.69837005e-01 -3.66323277e-01 -3.56381787e-01
 -3.54307121e-01 -3.50713405e-01 -3.48173551e-01 -3.33543965e-01
 -3.27826850e-01 -3.27815688e-01 -3.26272282e-01 -3.25876546e-01
 -3.21153595e-01 -3.20544844e-01 -3.12674708e-01 -3.11225979e-01
 -3.11200888e-01 -3.09659312e-01 -3.07921453e-01 -3.07412524e-01
 -3.07258742e-01 -3.07084309e-01 -3.05728349e-01 -3.00166351e-01
 -2.96616822e-01 -2.95283872e-01 -2.92281349e-01 -2.89593629e-01
 -2.89201521e-01 -2.89076987e-01 -2.88402735e-01 -2.83960509e-01
 -2.83708911e-01 -2.74939188e-01 -2.74757211e-01 -2.72482002e-01
 -2.70555363e-01 -2.68881946e-01 -2.68649921e-01 -2.68334282e-01
 -2.66932287e-01 -2.63192315e-01 -2.61642640e-01 -2.60297808e-01
 -2.59767226e-01 -2.58453286e-01 -2.54097032e-01 -2.51176025e-01
 -2.41819316e-01 -2.37015558e-01 -2.34637187e-01 -2.33916891e-01
 -2.31383643e-01 -2.31213754e-01 -2.31021632e-01 -2.30966231e-01
 -2.29639899e-01 -2.23088821e-01 -2.21192111e-01 -2.15628417e-01
 -2.14198779e-01 -2.12267941e-01 -2.08835111e-01 -2.08354995e-01
 -2.08266400e-01 -2.06824014e-01 -2.06042760e-01 -2.05922632e-01
 -2.05826829e-01 -2.04667334e-01 -1.95488033e-01 -1.94522358e-01
 -1.86023630e-01 -1.85632780e-01 -1.81276355e-01 -1.79602274e-01
 -1.72970039e-01 -1.68318197e-01 -1.62665457e-01 -1.62325035e-01
 -1.60680395e-01 -1.56012135e-01 -1.55277922e-01 -1.54053550e-01
 -1.53128469e-01 -1.51974894e-01 -1.48178868e-01 -1.46503403e-01
 -1.46457332e-01 -1.46057500e-01 -1.40496429e-01 -1.40074493e-01
 -1.39662044e-01 -1.33567092e-01 -1.30211828e-01 -1.29119183e-01
 -1.28297053e-01 -1.27316258e-01 -1.26433057e-01 -1.25565214e-01
 -1.25368402e-01 -1.25261065e-01 -1.22883714e-01 -1.21234599e-01
 -1.16782459e-01 -1.15697570e-01 -1.15151193e-01 -1.12058170e-01
 -1.11741193e-01 -1.08743586e-01 -1.05357561e-01 -1.02492913e-01
 -9.53862817e-02 -9.52971159e-02 -9.21301720e-02 -9.01584580e-02
 -8.83722040e-02 -8.70355190e-02 -8.43818463e-02 -8.42560575e-02
 -8.22682615e-02 -8.21690806e-02 -8.08180736e-02 -8.03692194e-02
 -7.90761884e-02 -7.84829829e-02 -7.81513352e-02 -7.71932837e-02
 -7.70948311e-02 -7.56589544e-02 -7.52237332e-02 -7.21132554e-02
 -7.17043882e-02 -7.05908546e-02 -6.87838095e-02 -6.86422537e-02
 -6.86030330e-02 -6.75657545e-02 -6.73255772e-02 -6.62286549e-02
 -6.57403962e-02 -6.45435587e-02 -6.37031315e-02 -6.13077882e-02
 -6.06687050e-02 -5.98832048e-02 -5.97055032e-02 -5.29380382e-02
 -5.23456827e-02 -5.12379050e-02 -5.01711740e-02 -4.96994990e-02
 -4.92743587e-02 -4.80688806e-02 -4.76596215e-02 -4.71778880e-02
 -4.63780527e-02 -4.61766283e-02 -4.58866383e-02 -4.51277744e-02
 -4.41606254e-02 -4.34637007e-02 -4.22602012e-02 -4.16668315e-02
 -4.03553906e-02 -4.01129924e-02 -4.00225161e-02 -3.98433476e-02
 -3.90911417e-02 -3.82614548e-02 -3.46107912e-02 -3.39392300e-02
 -3.24835986e-02 -3.20132791e-02 -3.18163980e-02 -3.17978060e-02
 -3.15089005e-02 -3.08535626e-02 -3.05695626e-02 -3.05437261e-02
 -3.02479969e-02 -3.01537099e-02 -2.99906864e-02 -2.97552034e-02
 -2.95485618e-02 -2.93682714e-02 -2.92034354e-02 -2.86559154e-02
 -2.81761142e-02 -2.33580983e-02 -2.33253395e-02 -2.29292790e-02
 -2.23986168e-02 -2.18318286e-02 -1.98436007e-02 -1.96345462e-02
 -1.89146866e-02 -1.86269520e-02 -1.80844741e-02 -1.79155582e-02
 -1.63709112e-02 -1.57261021e-02 -1.50099541e-02 -1.42764825e-02
 -1.39532508e-02 -1.33781486e-02 -1.19658847e-02 -1.16492855e-02
 -1.07765212e-02 -1.06862085e-02 -9.97372298e-03 -8.40354073e-03
 -8.20441213e-03 -6.75576733e-03 -6.50403690e-03 -4.45431180e-03
 -2.99457910e-03 -2.79605816e-03 -2.36035471e-03 -2.22749128e-03
 -1.70381313e-03  3.21549664e-04  9.81541066e-04  2.06038188e-03
  2.50904742e-03  5.26061114e-03  6.10701157e-03  7.22334794e-03
  7.99289573e-03  8.40463315e-03  8.90746092e-03  1.06780311e-02
  1.07884852e-02  1.12444981e-02  1.25598071e-02  1.30094684e-02
  1.33979887e-02  1.49770123e-02  1.60633403e-02  1.78322858e-02
  1.84549454e-02  1.92153349e-02  2.06855669e-02  2.09424287e-02
  2.21312340e-02  2.26876755e-02  2.34369583e-02  2.40378864e-02
  2.48023432e-02  2.51914617e-02  2.58554878e-02  2.61798265e-02
  2.62813657e-02  2.65036266e-02  2.70754666e-02  2.77200349e-02
  2.83443372e-02  3.07194748e-02  3.11817882e-02  3.12610326e-02
  3.26314197e-02  3.34542024e-02  3.45603860e-02  3.49773311e-02
  3.52082293e-02  3.54937852e-02  3.59045561e-02  3.61927963e-02
  3.63908331e-02  3.69971337e-02  3.93271218e-02  4.11007631e-02
  4.23794011e-02  4.30977600e-02  4.46116849e-02  4.46147712e-02
  4.52600849e-02  4.61153190e-02  4.68205864e-02  4.74909931e-02
  4.91381793e-02  4.99284732e-02  5.22527023e-02  5.26131635e-02
  5.27083741e-02  5.36156900e-02  5.37066987e-02  5.40531121e-02
  5.52827171e-02  5.77095296e-02  5.80514277e-02  5.83897972e-02
  6.15523095e-02  6.16737674e-02  6.29475882e-02  6.47667965e-02
  6.54002147e-02  6.65208747e-02  6.72185338e-02  7.04138126e-02
  7.07979900e-02  7.15546767e-02  7.46179328e-02  7.55680690e-02
  7.62381533e-02  8.12186450e-02  8.16700341e-02  8.29281802e-02
  8.32937111e-02  8.45721561e-02  8.86278451e-02  8.96310958e-02
  8.98367403e-02  9.37866766e-02  9.49212325e-02  9.52111165e-02
  9.62218282e-02  9.74262137e-02  9.99996065e-02  1.00395994e-01
  1.01150926e-01  1.01892711e-01  1.02030204e-01  1.02624348e-01
  1.03142577e-01  1.03166959e-01  1.03524347e-01  1.03784067e-01
  1.04635954e-01  1.05220197e-01  1.05604949e-01  1.05784541e-01
  1.06321893e-01  1.06454758e-01  1.07466895e-01  1.07681861e-01
  1.09592677e-01  1.12028732e-01  1.12099782e-01  1.12332498e-01
  1.12884630e-01  1.14381340e-01  1.16464968e-01  1.18189397e-01
  1.18777213e-01  1.19631287e-01  1.19850711e-01  1.20098110e-01
  1.20523263e-01  1.21030989e-01  1.22174745e-01  1.24764293e-01
  1.24801082e-01  1.26175390e-01  1.26605368e-01  1.26635586e-01
  1.26738481e-01  1.26744601e-01  1.28075074e-01  1.29948277e-01
  1.30197229e-01  1.30365110e-01  1.31196787e-01  1.31436833e-01
  1.31720363e-01  1.32686031e-01  1.33048344e-01  1.34720414e-01
  1.34754868e-01  1.37046179e-01  1.39785749e-01  1.39924176e-01
  1.40597960e-01  1.41791685e-01  1.41902943e-01  1.41955559e-01
  1.42087484e-01  1.42936161e-01  1.44774193e-01  1.45997882e-01
  1.46734692e-01  1.47196046e-01  1.47785017e-01  1.48275388e-01
  1.49237754e-01  1.51869046e-01  1.53605939e-01  1.55863069e-01
  1.56171712e-01  1.56173037e-01  1.57962954e-01  1.58871370e-01
  1.59627113e-01  1.59677848e-01  1.61405703e-01  1.61651476e-01
  1.65193115e-01  1.68471346e-01  1.69111430e-01  1.72776836e-01
  1.74096496e-01  1.74325349e-01  1.76275315e-01  1.76395882e-01
  1.77223755e-01  1.77592954e-01  1.77773288e-01  1.77808911e-01
  1.79627549e-01  1.80166958e-01  1.81033444e-01  1.81249218e-01
  1.83776904e-01  1.86919487e-01  1.88955749e-01  1.89365601e-01
  1.89609342e-01  1.90033395e-01  1.93422926e-01  2.03260405e-01
  2.04011669e-01  2.04764918e-01  2.07823441e-01  2.08811078e-01
  2.09217623e-01  2.09337192e-01  2.10431379e-01  2.13720047e-01
  2.13820248e-01  2.16650462e-01  2.17083142e-01  2.18367290e-01
  2.19513473e-01  2.23262737e-01  2.23928867e-01  2.24738002e-01
  2.25806497e-01  2.25952276e-01  2.30402367e-01  2.30937344e-01
  2.31098037e-01  2.33082334e-01  2.33767569e-01  2.36373854e-01
  2.40560662e-01  2.40813386e-01  2.43311953e-01  2.44220426e-01
  2.44808079e-01  2.47393966e-01  2.56829701e-01  2.56900129e-01
  2.57282431e-01  2.59011916e-01  2.59640716e-01  2.59720856e-01
  2.70423678e-01  2.75185671e-01  2.77580398e-01  2.77924828e-01
  2.86306895e-01  2.88625216e-01  2.95974744e-01  2.96029590e-01
  3.05119554e-01  3.06309804e-01  3.06685394e-01  3.08641182e-01
  3.08863226e-01  3.10709746e-01  3.10819508e-01  3.11612301e-01
  3.15587867e-01  3.17225807e-01  3.17332806e-01  3.25458892e-01
  3.30344736e-01  3.65505292e-01  4.04465997e-01  4.62433893e-01]

  warnings.warn(

2022-11-03 10:50:59,871:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.11430245 -0.08376289 -0.08073702  0.00426243  0.01545966  0.01592205
  0.02243004  0.03339882  0.04098552  0.04339201  0.04622376  0.0476431
  0.05072223  0.06310452  0.06521058  0.07487437  0.07496497  0.0839122
  0.08748541  0.08796663  0.09009781  0.09568949  0.09725874  0.09824632
  0.10161709  0.10320753  0.10524877  0.10875188  0.11124141  0.11223418
  0.11548138  0.12068095  0.12130676  0.1239269   0.1249544   0.12542389
  0.12619479  0.12824394  0.13061877  0.13269334  0.13329459  0.1360657
  0.1388242   0.14360332  0.14364489  0.14383532  0.14518529  0.14770919
  0.14793936  0.14795969  0.14805214  0.15027198  0.15191618  0.15786142
  0.15902446  0.1595346   0.16000422  0.1624764   0.16434214  0.16681774
  0.16863602  0.17042343  0.1718224   0.17240962  0.17271642  0.17324314
  0.17334222  0.17346561  0.17496536  0.17691094  0.17859584  0.17995315
  0.18174064  0.18334288  0.18391455  0.18477971  0.1851092   0.18644304
  0.18661793  0.18677538  0.18703739  0.18760011  0.18764968  0.18786143
  0.1883241   0.1894549   0.19206519  0.19257589  0.19286924  0.19341957
  0.19667798  0.19678324  0.19712473  0.19870015  0.20125528  0.2022187
  0.20260304  0.20464631  0.20574648  0.20981009  0.21162267  0.21235002
  0.21241446  0.21324603  0.2152298   0.21587328  0.21652676  0.21686683
  0.21785719  0.21940896  0.22253734  0.2238043   0.22527037  0.22600478
  0.2262284   0.22671385  0.22780919  0.22812263  0.22821799  0.22848618
  0.23008411  0.23202387  0.23266828  0.2352961   0.23547045  0.23608622
  0.23661612  0.23687109  0.23733149  0.23893126  0.23912657  0.23979173
  0.24076803  0.24094683  0.24259166  0.24321529  0.24420582  0.24510305
  0.2471556   0.24723975  0.24743253  0.24814297  0.25218335  0.25224439
  0.25233545  0.25269961  0.25296426  0.25380719  0.25397274  0.25438139
  0.25636752  0.2568052   0.25681718  0.25827118  0.25850519  0.2591305
  0.2595466   0.25987524  0.25993823  0.26008344  0.26014504  0.26126364
  0.26188614  0.26308916  0.26313826  0.26321045  0.26473059  0.26515647
  0.26554056  0.26647159  0.26778522  0.27143114  0.27345243  0.27437524
  0.27496477  0.27513092  0.27550121  0.27645831  0.27677929  0.2778093
  0.28045353  0.28244503  0.28377522  0.28675979  0.28847871  0.29010992
  0.29284372  0.29421084  0.29478399  0.29535475  0.29670878  0.29672289
  0.29707352  0.29736118  0.29748387  0.29772893  0.29775665  0.30055862
  0.30104308  0.30108998  0.30139595  0.30165883  0.30192963  0.3031357
  0.30383801  0.30494776  0.30501697  0.30553665  0.30898514  0.30933105
  0.30975644  0.31022099  0.31107209  0.31141987  0.31155331  0.3130695
  0.31317487  0.31320386  0.31665518  0.3172065   0.31890399  0.32116012
  0.32121187  0.32162993  0.32306364  0.32328483  0.32335389  0.3236701
  0.32627856  0.32681691  0.32701028  0.32734233  0.32829321  0.32877261
  0.32885727  0.32952426  0.32976787  0.3300279   0.33097508  0.33101174
  0.33128834  0.33224228  0.33245941  0.33304351  0.33310234  0.33480187
  0.33761666  0.33833128  0.33862709  0.33936404  0.34004718  0.34008731
  0.34043452  0.3412973   0.34452185  0.34556479  0.34580686  0.3481591
  0.34830123  0.34917052  0.34974662  0.35048009  0.35400056  0.35465514
  0.35491149  0.35502487  0.35602112  0.35639381  0.35696786  0.35890359
  0.36010704  0.3626382   0.36419764  0.36501316  0.36512984  0.36513179
  0.36539061  0.36607331  0.36638419  0.36643222  0.3665264   0.36744501
  0.36831649  0.36966677  0.3698866   0.37018503  0.37028039  0.37044682
  0.37048732  0.37171503  0.37184728  0.37213992  0.37232805  0.37426984
  0.37598042  0.37760849  0.37793546  0.37877521  0.37922372  0.37940249
  0.37950256  0.38142575  0.3817058   0.38187554  0.38190002  0.38305999
  0.38310883  0.38320085  0.38468878  0.38485367  0.38501147  0.38591536
  0.38633959  0.3865852   0.38807516  0.38819226  0.38858761  0.38877741
  0.39043457  0.39046274  0.39463579  0.39511746  0.39633672  0.39734407
  0.39790585  0.39897437  0.39956654  0.40010038  0.40168104  0.40229955
  0.4048175   0.40504106  0.40690454  0.40778724  0.4078491   0.40855745
  0.41160163  0.41175806  0.41303058  0.41531231  0.41720262  0.41888302
  0.41950757  0.42075524  0.4223397   0.42310906  0.42311062  0.42332013
  0.42366986  0.42588692  0.42606332  0.42657292  0.42673814  0.42930714
  0.43035013  0.4313004   0.43403267  0.43517764  0.43667139  0.43709392
  0.43816492  0.43910918  0.44058014  0.44086293  0.44493842  0.44520797
  0.44547104  0.44571662  0.44612245  0.44623889  0.44626238  0.44683385
  0.44703669  0.44743189  0.44830874  0.45010931  0.45050773  0.45202918
  0.45207687  0.45343689  0.45669374  0.4575377   0.4575749   0.45783584
  0.45857449  0.45882572  0.45926431  0.46026517  0.46037429  0.46076933
  0.4615678   0.46388724  0.4641076   0.46443625  0.46449057  0.46453033
  0.46572425  0.46575281  0.4674524   0.46813178  0.46843954  0.46907505
  0.47105063  0.47135345  0.47563517  0.47576355  0.47664898  0.47794048
  0.47958759  0.48110056  0.48145317  0.48410542  0.48461302  0.48539952
  0.48568871  0.48690871  0.48696131  0.48804748  0.49075295  0.490781
  0.49167597  0.49340529  0.49447839  0.49468378  0.49498098  0.49620643
  0.49670313  0.49978267  0.50124323  0.50305967  0.50410338  0.50609911
  0.50749063  0.50797152  0.50836186  0.50866604  0.51072899  0.51368766
  0.51413374  0.51512647  0.51535935  0.51671886  0.51808888  0.51858928
  0.51966827  0.52264987  0.52316618  0.52644257  0.52736986  0.53022092
  0.53471987  0.53563412  0.53608561  0.53859191  0.53940994  0.54892387
  0.54937248  0.55016736  0.55073153  0.55412344  0.55644577  0.55761066
  0.56420129  0.56487493  0.56518989  0.56767163  0.56842807  0.57027706
  0.57145812  0.57244907  0.58029166  0.581831    0.58257167  0.59138325
  0.59357867  0.59604671  0.60207406  0.60615238  0.60966489  0.61290163
  0.61909909  0.64398624  0.6539714   0.65720322  0.66134522  0.66419215
  0.66795401  0.67351027  0.67886143  0.68451892  0.70667967  0.71094901
  0.7757262 ]

  warnings.warn(

2022-11-03 10:50:59,887:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.88609782e-01 -6.62311032e-01 -6.59146454e-01 -6.46811222e-01
 -6.44481515e-01 -6.43191277e-01 -6.39406497e-01 -6.37208651e-01
 -6.35616353e-01 -6.24273740e-01 -6.24121910e-01 -6.21368724e-01
 -6.18234337e-01 -6.12896354e-01 -6.10503490e-01 -6.04767721e-01
 -5.95415170e-01 -5.90270871e-01 -5.86266978e-01 -5.70869273e-01
 -5.70624147e-01 -5.66289233e-01 -5.51232481e-01 -5.42478628e-01
 -5.41583319e-01 -5.39934412e-01 -5.39435107e-01 -5.38516942e-01
 -5.34154858e-01 -5.28082122e-01 -5.22281808e-01 -5.15480950e-01
 -5.10590858e-01 -5.05802034e-01 -4.90286724e-01 -4.80067683e-01
 -4.76266474e-01 -4.76206642e-01 -4.73390924e-01 -4.73386001e-01
 -4.72413474e-01 -4.70539500e-01 -4.69742973e-01 -4.69083738e-01
 -4.65381878e-01 -4.63016630e-01 -4.61420479e-01 -4.58505321e-01
 -4.57010419e-01 -4.56700046e-01 -4.51839933e-01 -4.51563061e-01
 -4.48174502e-01 -4.47126416e-01 -4.42521310e-01 -4.41252254e-01
 -4.39863984e-01 -4.38977784e-01 -4.36573679e-01 -4.35102974e-01
 -4.30838866e-01 -4.24983529e-01 -4.23253536e-01 -4.21335849e-01
 -4.18185972e-01 -4.11428111e-01 -4.05665834e-01 -4.04498754e-01
 -4.04421714e-01 -4.01329623e-01 -3.98057564e-01 -3.95395728e-01
 -3.94941881e-01 -3.94896238e-01 -3.93823976e-01 -3.92325566e-01
 -3.91667901e-01 -3.91588483e-01 -3.88418408e-01 -3.87637990e-01
 -3.86078051e-01 -3.85645539e-01 -3.85526751e-01 -3.84965439e-01
 -3.83829763e-01 -3.80870645e-01 -3.78393005e-01 -3.77949294e-01
 -3.71053735e-01 -3.68873176e-01 -3.65716368e-01 -3.62761479e-01
 -3.60247301e-01 -3.56684653e-01 -3.55524127e-01 -3.50184653e-01
 -3.49872421e-01 -3.48166660e-01 -3.47860317e-01 -3.47471599e-01
 -3.46209979e-01 -3.46136783e-01 -3.43211495e-01 -3.42468490e-01
 -3.37803842e-01 -3.36264704e-01 -3.35796971e-01 -3.35210709e-01
 -3.31301097e-01 -3.30078781e-01 -3.23161200e-01 -3.18731805e-01
 -3.18716434e-01 -3.14676388e-01 -3.14152341e-01 -3.14098219e-01
 -3.12699317e-01 -3.09948781e-01 -3.03264281e-01 -3.01897613e-01
 -2.99820666e-01 -2.98190972e-01 -2.96238841e-01 -2.95988005e-01
 -2.89694342e-01 -2.89017681e-01 -2.88759769e-01 -2.84487431e-01
 -2.84135971e-01 -2.72067723e-01 -2.70043242e-01 -2.66460962e-01
 -2.61252643e-01 -2.61216842e-01 -2.58964027e-01 -2.57303419e-01
 -2.56524869e-01 -2.56473755e-01 -2.53506169e-01 -2.52389410e-01
 -2.47604695e-01 -2.46676442e-01 -2.46344437e-01 -2.45499725e-01
 -2.44939171e-01 -2.44014079e-01 -2.42634563e-01 -2.40015172e-01
 -2.38911820e-01 -2.37531869e-01 -2.37403716e-01 -2.35821530e-01
 -2.30584366e-01 -2.27981861e-01 -2.27636504e-01 -2.18439325e-01
 -2.14294039e-01 -2.13749194e-01 -2.11319799e-01 -2.11226540e-01
 -2.11029630e-01 -2.09451522e-01 -2.08063637e-01 -2.06482384e-01
 -2.03692124e-01 -2.00315086e-01 -1.99864986e-01 -1.99554855e-01
 -1.99042758e-01 -1.97435212e-01 -1.96756939e-01 -1.95594733e-01
 -1.95392353e-01 -1.95006990e-01 -1.94573415e-01 -1.93287909e-01
 -1.91892098e-01 -1.82095979e-01 -1.76633313e-01 -1.76135649e-01
 -1.74006542e-01 -1.63458624e-01 -1.59447619e-01 -1.57967130e-01
 -1.54177576e-01 -1.53462636e-01 -1.53261226e-01 -1.52513116e-01
 -1.49037220e-01 -1.46332931e-01 -1.45212699e-01 -1.43807965e-01
 -1.41543847e-01 -1.41454264e-01 -1.40665086e-01 -1.40558701e-01
 -1.36311378e-01 -1.35769795e-01 -1.35740193e-01 -1.33743939e-01
 -1.31819295e-01 -1.28550439e-01 -1.27550032e-01 -1.25123436e-01
 -1.22253543e-01 -1.20244466e-01 -1.19133986e-01 -1.17317406e-01
 -1.16584691e-01 -1.16514712e-01 -1.15788423e-01 -1.13216205e-01
 -1.11550919e-01 -1.08297843e-01 -1.05728728e-01 -1.04241917e-01
 -1.00793182e-01 -1.00755832e-01 -1.00108760e-01 -9.95454564e-02
 -9.77606449e-02 -9.48476473e-02 -9.21776064e-02 -9.09343507e-02
 -8.82234390e-02 -8.54495798e-02 -8.25004398e-02 -8.17635100e-02
 -8.13334128e-02 -7.64865990e-02 -7.31079448e-02 -7.27902077e-02
 -7.15736702e-02 -6.90365566e-02 -6.82541266e-02 -6.65192578e-02
 -5.68922544e-02 -5.66216258e-02 -5.30747241e-02 -5.04835576e-02
 -4.94613789e-02 -4.73145800e-02 -4.53392399e-02 -4.38475691e-02
 -4.24572175e-02 -4.04946732e-02 -3.83298248e-02 -3.45252891e-02
 -3.21162442e-02 -2.68914041e-02 -2.36737882e-02 -2.09274830e-02
 -1.70736045e-02 -1.55974054e-02 -1.45287616e-02 -1.26755512e-02
 -1.07174698e-02 -9.88647689e-03 -7.93243007e-03 -7.37154482e-03
 -6.24660806e-03 -4.82856634e-03 -4.48360786e-03 -4.00367317e-03
 -3.71389318e-03 -3.17511689e-03 -1.65494480e-03 -3.22672717e-04
  6.98915628e-03  7.89422977e-03  1.63078710e-02  1.72980727e-02
  1.92286731e-02  2.06909628e-02  2.09504027e-02  2.32564526e-02
  2.39908533e-02  2.53707360e-02  2.83314242e-02  2.91032737e-02
  3.24062374e-02  3.36869183e-02  3.48090329e-02  3.67037647e-02
  3.67875450e-02  4.09827822e-02  4.28706530e-02  4.36503281e-02
  4.39637497e-02  4.63007570e-02  4.75680120e-02  5.14540894e-02
  5.42777548e-02  5.48882979e-02  5.52635739e-02  5.57271850e-02
  5.90861573e-02  6.08159382e-02  6.33078195e-02  6.54819743e-02
  6.60026720e-02  6.63956877e-02  6.86753283e-02  7.07103886e-02
  7.15943877e-02  7.18098808e-02  7.89665773e-02  7.90877158e-02
  8.08539186e-02  8.08868112e-02  8.22196925e-02  8.37069020e-02
  8.51696032e-02  8.68394869e-02  8.72617661e-02  8.81255434e-02
  8.94301253e-02  9.04106062e-02  9.08490875e-02  9.46726215e-02
  9.61606619e-02  9.67654829e-02  1.03053819e-01  1.05085449e-01
  1.05103866e-01  1.05745943e-01  1.06610579e-01  1.06823996e-01
  1.06953996e-01  1.07559642e-01  1.08361999e-01  1.09827113e-01
  1.13912176e-01  1.14435142e-01  1.15981701e-01  1.16343543e-01
  1.16532024e-01  1.17251530e-01  1.17724524e-01  1.18068199e-01
  1.18791011e-01  1.19039898e-01  1.20130910e-01  1.21149790e-01
  1.22255048e-01  1.23521349e-01  1.24660811e-01  1.25147541e-01
  1.25618759e-01  1.26022904e-01  1.31236209e-01  1.31716158e-01
  1.33729019e-01  1.34015204e-01  1.34038709e-01  1.35666065e-01
  1.36505611e-01  1.36916780e-01  1.38937049e-01  1.39656118e-01
  1.39913986e-01  1.42622173e-01  1.42801611e-01  1.42903867e-01
  1.43210768e-01  1.44599277e-01  1.44872365e-01  1.45348781e-01
  1.46042981e-01  1.48387404e-01  1.52167082e-01  1.57781170e-01
  1.58451473e-01  1.60476837e-01  1.60811302e-01  1.61042167e-01
  1.62906700e-01  1.63879907e-01  1.67022877e-01  1.68835839e-01
  1.70821718e-01  1.72954594e-01  1.74308284e-01  1.77481401e-01
  1.77809002e-01  1.79557891e-01  1.80525171e-01  1.80648653e-01
  1.81099815e-01  1.82286645e-01  1.83911712e-01  1.84398737e-01
  1.86294614e-01  1.88769833e-01  1.91939218e-01  1.93573013e-01
  1.95553744e-01  1.98491234e-01  2.02869412e-01  2.04602037e-01
  2.06100612e-01  2.07228117e-01  2.08016855e-01  2.09926712e-01
  2.17827257e-01  2.18788941e-01  2.21022883e-01  2.23989935e-01
  2.24606697e-01  2.26253808e-01  2.30455246e-01  2.31144782e-01
  2.31680843e-01  2.33138468e-01  2.34286180e-01  2.37115194e-01
  2.37506766e-01  2.38826860e-01  2.40975724e-01  2.42693754e-01
  2.44957565e-01  2.48149064e-01  2.49662694e-01  2.50157408e-01
  2.51256259e-01  2.55297670e-01  2.55855172e-01  2.57367827e-01
  2.61131795e-01  2.63243219e-01  2.65183739e-01  2.66125762e-01
  2.68406722e-01  2.71368678e-01  2.75862599e-01  2.76010481e-01
  2.76830020e-01  2.78435673e-01  2.87314510e-01  2.88439853e-01
  2.91535902e-01  2.94287012e-01  2.95176731e-01  3.06226513e-01
  3.06327890e-01  3.08599958e-01  3.09575097e-01  3.10581885e-01
  3.11547517e-01  3.15100582e-01  3.15179066e-01  3.16647662e-01
  3.27772842e-01  3.29990002e-01  3.31561756e-01  3.33911345e-01
  3.37123195e-01  3.37472828e-01  3.41788290e-01  3.44472360e-01
  3.45206058e-01  3.45687116e-01  3.46518692e-01  3.46696436e-01
  3.47152143e-01  3.51952342e-01  3.52077173e-01  3.56085779e-01
  3.58236633e-01  3.58569719e-01  3.59808023e-01  3.60414315e-01
  3.65207076e-01  3.65912554e-01  3.66008418e-01  3.67573178e-01
  3.67699540e-01  3.72057529e-01  3.72307858e-01  3.77608099e-01
  3.78632693e-01  3.78851842e-01  3.80668828e-01  3.82595816e-01
  3.91744825e-01  3.93539347e-01  3.98568684e-01  4.01665183e-01
  4.13653845e-01  4.14549989e-01  4.16832101e-01  4.17559139e-01
  4.51231975e-01]

  warnings.warn(

2022-11-03 10:51:02,287:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.45944083 -0.45185488 -0.43259267 -0.43216121 -0.42229774 -0.42101904
 -0.40750149 -0.40502784 -0.40461161 -0.3789278  -0.37435327 -0.36344998
 -0.34960477 -0.34371865 -0.34195783 -0.33406362 -0.33097874 -0.31793856
 -0.31749107 -0.31395354 -0.31233694 -0.29453186 -0.28455225 -0.27665861
 -0.27336499 -0.25388816 -0.25355558 -0.24787495 -0.22920357 -0.22739059
 -0.21873699 -0.21631023 -0.20998967 -0.20402743 -0.20212818 -0.20177081
 -0.20072098 -0.19004305 -0.18975383 -0.18913556 -0.18831538 -0.18585144
 -0.17812138 -0.16946586 -0.1662443  -0.1650406  -0.16448003 -0.16248878
 -0.16214433 -0.15663517 -0.15519291 -0.15211449 -0.14188355 -0.13551834
 -0.13424218 -0.12760895 -0.12375683 -0.1223593  -0.12109338 -0.118282
 -0.11574546 -0.10893304 -0.10529482 -0.10276827 -0.09855281 -0.09839996
 -0.09713575 -0.09110314 -0.0893985  -0.08510844 -0.08456994 -0.08402823
 -0.08338984 -0.0807451  -0.07974649 -0.07932607 -0.07795019 -0.07706767
 -0.07694441 -0.07378053 -0.07170347 -0.07091209 -0.07071828 -0.06981117
 -0.06693403 -0.06559866 -0.06398036 -0.06144787 -0.05752126 -0.05675573
 -0.05625813 -0.05370571 -0.05044816 -0.04638288 -0.04395187 -0.03468395
 -0.03204715 -0.0285286  -0.02620191 -0.02249475 -0.01569237 -0.01492308
 -0.01012174  0.00342309  0.01231569  0.0230919   0.0282157   0.03012954
  0.03038664  0.03264022  0.03611118  0.0390308   0.04455426  0.04901065
  0.05043683  0.05195198  0.05439954  0.0567912   0.05815206  0.05938515
  0.06046894  0.07249563  0.07569157  0.07584271  0.07596517  0.07684826
  0.0793116   0.08301695  0.08539403  0.09027509  0.09093168  0.09278339
  0.09348584  0.09638408  0.09925026  0.09999208  0.10373095  0.11045114
  0.11056845  0.11127267  0.12067628  0.12175364  0.12415035  0.12663705
  0.12925477  0.13124609  0.13514142  0.13533874  0.13572965  0.14211639
  0.14458972  0.14690384  0.1548601   0.15544823  0.15793407  0.15841895
  0.15912338  0.16069501  0.16502523  0.1655216   0.16695484  0.17218898
  0.17511394  0.17659253  0.1797987   0.18230813  0.18533571  0.18822908
  0.18916013  0.19054366  0.1921073   0.19346839  0.19575028  0.19915206
  0.21243675  0.2133009   0.21365235  0.21391166  0.21928556  0.22272264
  0.22428756  0.22498021  0.22986071  0.23139916  0.23203815  0.2341495
  0.23554484  0.23831959  0.23878049  0.23955914  0.24068814  0.24154307
  0.2446944   0.2483683   0.24936183  0.25216731  0.25406908  0.25435162
  0.25693385  0.25753062  0.25782211  0.25818892  0.25942736  0.25985172
  0.26055107  0.26065196  0.26579482  0.26640628  0.26699592  0.26779219
  0.26821164  0.26884032  0.27080544  0.2712138   0.27181899  0.27350432
  0.27653716  0.27704724  0.2776127   0.28372286  0.28425484  0.2875854
  0.28882935  0.2903373   0.29107661  0.29143942  0.29522925  0.2961259
  0.29877431  0.30025592  0.30125435  0.30235882  0.30349766  0.30782449
  0.30837291  0.31061075  0.31261946  0.31563182  0.31627923  0.31694003
  0.31738872  0.31993453  0.32202452  0.32242784  0.32863628  0.3292411
  0.3322005   0.33386991  0.33482998  0.33497093  0.33626139  0.34075182
  0.34148661  0.3419314   0.34391248  0.34489958  0.34761597  0.34949732
  0.35083396  0.35285856  0.35461822  0.35485982  0.35523843  0.35650292
  0.35802216  0.35888512  0.36275146  0.3652619   0.36758464  0.37134021
  0.373906    0.37489561  0.37782908  0.3793185   0.38314495  0.38583819
  0.38664716  0.38716417  0.38982616  0.39045063  0.39122835  0.39141729
  0.39205024  0.39308118  0.39428297  0.39719723  0.39778473  0.40231928
  0.40235041  0.40595998  0.40882898  0.40915987  0.4141352   0.41714482
  0.42518737  0.42844089  0.42995528  0.43805269  0.44076292  0.4411346
  0.44355291  0.44674448  0.45251693  0.45563844  0.45700769  0.45735336
  0.4579131   0.45809868  0.4695271   0.4719714   0.4760049   0.47747883
  0.47846018  0.48365296  0.48393575  0.48422003  0.49068002  0.49406963
  0.50300509  0.50709002  0.50944081  0.51108162  0.51306279  0.51675908
  0.51948843  0.52140603  0.52303993  0.5231012   0.52323815  0.52524515
  0.52816745  0.52957964  0.52979845  0.53217071  0.53258043  0.53403685
  0.53533777  0.5395018   0.54111933  0.54276025  0.54601003  0.55028797
  0.55577175  0.55689839  0.55765677  0.559144    0.56305371  0.56318498
  0.56569446  0.56638278  0.57373424  0.58126755  0.58312307  0.58691619
  0.59098006  0.59325148  0.59446426  0.59447786  0.59749189  0.59956188
  0.60001072  0.6023663   0.60419442  0.60514238  0.60803795  0.60992155
  0.61016574  0.62038651  0.62070511  0.62840997  0.63653993  0.63952029
  0.6401554   0.65082231  0.65108628  0.65200047  0.65449099  0.65590929
  0.65706301  0.65746081  0.6605604   0.66173565  0.66197668  0.66723747
  0.67226724  0.67982884  0.69042541  0.69214379  0.69719617  0.69757094
  0.69767051  0.70391642  0.70444914  0.71198012  0.7141733   0.72424958
  0.7251911   0.7298636   0.73266983  0.73345872  0.73371189  0.73520001
  0.73810981  0.74028902  0.74197161  0.74701035  0.74903886  0.75148372
  0.75247683  0.75457743  0.75691604  0.75736807  0.76140762  0.76205851
  0.76833906  0.76985978  0.77533102  0.77578714  0.77802687  0.77851893
  0.77891765  0.78053949  0.78103531  0.78145554  0.78408899  0.78523556
  0.78801599  0.7903911   0.79562216  0.7957048   0.80013439  0.80404082
  0.80513587  0.81188661  0.81757873  0.81887155  0.82096774  0.82389912
  0.83137465  0.83241587  0.83405277  0.83776502  0.8389375   0.83999229
  0.84078855  0.84299525  0.8444157   0.85060674  0.85710642  0.86153832
  0.86481098  0.865269    0.86592991  0.87586742  0.87849022  0.88020183
  0.88168078  0.88416517  0.88593504  0.88749049  0.89305141  0.8932844
  0.89398415  0.89462849  0.8983316   0.8984619   0.90334482  0.90377443
  0.90641708  0.90664677  0.90990051  0.91295255  0.92340568  0.93234583
  0.93664085  0.9477697   0.94814329  0.95065239  0.95166353  0.95929858
  0.96470913  0.96798307  0.97547947  0.97785907  0.98515334  0.98600078
  0.988451    0.99861368  1.00793675  1.00897745  1.03381256  1.07281716
  1.09515511]

  warnings.warn(

2022-11-03 10:51:02,352:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.44044307e-01 -2.18825527e-01 -2.13646021e-01 -2.10802635e-01
 -2.06264470e-01 -2.05150023e-01 -1.96777606e-01 -1.95939798e-01
 -1.93619847e-01 -1.91045706e-01 -1.80872476e-01 -1.79777519e-01
 -1.77557238e-01 -1.73651809e-01 -1.64835025e-01 -1.63175932e-01
 -1.61015113e-01 -1.59321179e-01 -1.55627595e-01 -1.47616508e-01
 -1.45418252e-01 -1.43880390e-01 -1.42783666e-01 -1.40742266e-01
 -1.36309838e-01 -1.27464975e-01 -1.25768909e-01 -1.25281613e-01
 -1.22703884e-01 -1.22145159e-01 -1.21617442e-01 -9.94266629e-02
 -9.58101265e-02 -9.47027146e-02 -9.10507505e-02 -9.07514394e-02
 -9.01632406e-02 -8.95203857e-02 -8.78684637e-02 -8.59484760e-02
 -7.80981498e-02 -7.17225967e-02 -7.07318850e-02 -6.99222446e-02
 -6.64414137e-02 -6.60848160e-02 -6.36242679e-02 -6.23439378e-02
 -6.17738202e-02 -5.63842853e-02 -5.53339791e-02 -5.42865392e-02
 -5.22952324e-02 -5.22676370e-02 -4.75493425e-02 -4.55636267e-02
 -4.49053304e-02 -4.48678649e-02 -4.41395843e-02 -4.33886337e-02
 -4.03036677e-02 -3.90742235e-02 -3.53611950e-02 -3.28748914e-02
 -2.23314043e-02 -2.17669765e-02 -1.95589696e-02 -1.68283829e-02
 -1.29860922e-02 -1.18092328e-02 -6.35357326e-03 -4.12082819e-03
 -2.72187903e-03  6.56669207e-04  7.66177002e-03  9.90519386e-03
  9.96963392e-03  1.35868431e-02  1.58806964e-02  1.61641114e-02
  2.09133084e-02  2.55249267e-02  2.71743517e-02  2.83882844e-02
  2.85876778e-02  2.86872238e-02  3.03263373e-02  3.34244746e-02
  3.43386170e-02  3.59332217e-02  3.65455835e-02  3.78099080e-02
  4.46613041e-02  4.61048667e-02  4.82474814e-02  4.90403328e-02
  4.93298957e-02  5.48215473e-02  5.85648834e-02  6.11212969e-02
  6.32748075e-02  6.62833740e-02  6.96489214e-02  7.11264914e-02
  7.36469700e-02  7.62631853e-02  7.93276890e-02  8.37561896e-02
  8.38947211e-02  8.81307198e-02  9.63100510e-02  9.97897810e-02
  1.00632923e-01  1.02610803e-01  1.05651576e-01  1.06169603e-01
  1.08199462e-01  1.08470535e-01  1.09376113e-01  1.10308705e-01
  1.12213785e-01  1.13419488e-01  1.14465582e-01  1.14848704e-01
  1.15301595e-01  1.15468320e-01  1.15633112e-01  1.17650094e-01
  1.18966805e-01  1.19246536e-01  1.21115167e-01  1.23064421e-01
  1.24280116e-01  1.25536276e-01  1.27047682e-01  1.29120869e-01
  1.30399158e-01  1.32649370e-01  1.33008418e-01  1.37480311e-01
  1.41232673e-01  1.41479587e-01  1.44339627e-01  1.45852199e-01
  1.47184211e-01  1.47784643e-01  1.48008309e-01  1.49593172e-01
  1.49943334e-01  1.51215115e-01  1.52473609e-01  1.53017480e-01
  1.54128192e-01  1.55091013e-01  1.55569271e-01  1.57187443e-01
  1.57269528e-01  1.57746210e-01  1.62004729e-01  1.63125332e-01
  1.67542635e-01  1.69782957e-01  1.70468028e-01  1.73739676e-01
  1.77211723e-01  1.78684648e-01  1.79010627e-01  1.80075902e-01
  1.81253737e-01  1.83740398e-01  1.85175541e-01  1.85754571e-01
  1.88679906e-01  1.89834144e-01  1.92426816e-01  1.93900295e-01
  1.94365635e-01  1.97460811e-01  1.97602274e-01  2.04289084e-01
  2.06251860e-01  2.07673422e-01  2.08527788e-01  2.12295275e-01
  2.14771812e-01  2.17876029e-01  2.20957820e-01  2.23191089e-01
  2.27541419e-01  2.27641741e-01  2.28620324e-01  2.29338339e-01
  2.29924156e-01  2.30340635e-01  2.30592611e-01  2.32097462e-01
  2.33106360e-01  2.36798764e-01  2.39340058e-01  2.39493475e-01
  2.40367173e-01  2.40846326e-01  2.41346741e-01  2.45221414e-01
  2.45987759e-01  2.46426944e-01  2.46898054e-01  2.47093868e-01
  2.47656625e-01  2.48069000e-01  2.53260274e-01  2.53284731e-01
  2.54245723e-01  2.54662370e-01  2.55920395e-01  2.56062173e-01
  2.56434110e-01  2.57834302e-01  2.58033719e-01  2.62609074e-01
  2.64995567e-01  2.66159489e-01  2.67180331e-01  2.69690590e-01
  2.72183902e-01  2.79601510e-01  2.81863387e-01  2.82292476e-01
  2.83346954e-01  2.92231139e-01  2.93820717e-01  2.95133925e-01
  2.96491525e-01  2.96582132e-01  3.03205795e-01  3.07711560e-01
  3.09386953e-01  3.10403175e-01  3.16273991e-01  3.17498546e-01
  3.17702758e-01  3.20438074e-01  3.20977392e-01  3.21206242e-01
  3.21373388e-01  3.22195840e-01  3.22521442e-01  3.25654165e-01
  3.28618459e-01  3.29373714e-01  3.32437989e-01  3.34055071e-01
  3.35742284e-01  3.38623558e-01  3.40314851e-01  3.43019769e-01
  3.50351941e-01  3.51029588e-01  3.52648638e-01  3.54418923e-01
  3.56279971e-01  3.59706456e-01  3.60722370e-01  3.64529754e-01
  3.67160451e-01  3.67730113e-01  3.74586177e-01  3.76722402e-01
  3.77592634e-01  3.78841890e-01  3.81584635e-01  3.82081751e-01
  3.84757556e-01  3.88867403e-01  3.90244841e-01  3.92105223e-01
  3.94251034e-01  3.94849163e-01  3.95126039e-01  3.96959123e-01
  4.02291161e-01  4.06578070e-01  4.06923355e-01  4.09587700e-01
  4.09678913e-01  4.11772905e-01  4.11839094e-01  4.12879224e-01
  4.13066693e-01  4.20259463e-01  4.22520803e-01  4.24096074e-01
  4.24107441e-01  4.25474638e-01  4.25997238e-01  4.28055729e-01
  4.29490050e-01  4.33845836e-01  4.34773349e-01  4.37161135e-01
  4.38764147e-01  4.40431385e-01  4.42282667e-01  4.43708218e-01
  4.43743788e-01  4.47984321e-01  4.49089566e-01  4.49112742e-01
  4.50156513e-01  4.55652640e-01  4.56263678e-01  4.57930478e-01
  4.58398667e-01  4.61079043e-01  4.62710976e-01  4.64363475e-01
  4.70560726e-01  4.74041288e-01  4.76900684e-01  4.77708032e-01
  4.78469268e-01  4.81649612e-01  4.81669730e-01  4.81726238e-01
  4.82671725e-01  4.86156664e-01  4.88764343e-01  4.90456293e-01
  4.91522983e-01  4.91702612e-01  4.91897330e-01  4.93239706e-01
  4.93546340e-01  4.96884905e-01  5.05610170e-01  5.07887433e-01
  5.08001883e-01  5.09323514e-01  5.12525992e-01  5.16827872e-01
  5.18809378e-01  5.20176177e-01  5.20418018e-01  5.22423650e-01
  5.24982234e-01  5.28003134e-01  5.35195119e-01  5.35659445e-01
  5.38236257e-01  5.39305572e-01  5.39942956e-01  5.41843165e-01
  5.45090111e-01  5.47257830e-01  5.47875787e-01  5.47941303e-01
  5.49759990e-01  5.50182154e-01  5.52180538e-01  5.52918424e-01
  5.53487389e-01  5.53791749e-01  5.53941566e-01  5.54730556e-01
  5.58049764e-01  5.59068596e-01  5.61620821e-01  5.62017922e-01
  5.63657277e-01  5.65910964e-01  5.66522619e-01  5.71038128e-01
  5.73430627e-01  5.73625018e-01  5.73846719e-01  5.82453607e-01
  5.83799630e-01  5.84025325e-01  5.87487588e-01  5.88563572e-01
  5.94109995e-01  5.95892023e-01  5.97477281e-01  6.01649488e-01
  6.04460774e-01  6.06767574e-01  6.09991920e-01  6.10390535e-01
  6.13637256e-01  6.15640558e-01  6.19864705e-01  6.22596895e-01
  6.25043246e-01  6.25764199e-01  6.28616010e-01  6.30202636e-01
  6.31774966e-01  6.32710218e-01  6.34123675e-01  6.34585696e-01
  6.36935338e-01  6.43009571e-01  6.43576373e-01  6.44286581e-01
  6.47428968e-01  6.51071035e-01  6.54092676e-01  6.54110454e-01
  6.55483691e-01  6.57240714e-01  6.58782895e-01  6.59040732e-01
  6.59262636e-01  6.63161890e-01  6.64080773e-01  6.66114393e-01
  6.67816645e-01  6.68435509e-01  6.69106676e-01  6.73869586e-01
  6.74421918e-01  6.75095679e-01  6.75597742e-01  6.79763769e-01
  6.85531677e-01  6.87263455e-01  6.87932762e-01  6.90652007e-01
  6.94614394e-01  6.95147299e-01  6.96030574e-01  6.96691294e-01
  7.00170176e-01  7.00296190e-01  7.02937708e-01  7.03613538e-01
  7.05133547e-01  7.15471605e-01  7.19036399e-01  7.20611046e-01
  7.20805146e-01  7.21206598e-01  7.26157156e-01  7.31979627e-01
  7.33857290e-01  7.35935787e-01  7.38668391e-01  7.41145804e-01
  7.42774015e-01  7.44584781e-01  7.47473397e-01  7.49598213e-01
  7.57380025e-01  7.57530871e-01  7.57974840e-01  7.58100145e-01
  7.59637669e-01  7.61704923e-01  7.63367503e-01  7.66791785e-01
  7.69426718e-01  7.70531616e-01  7.72921809e-01  7.73829347e-01
  7.76958577e-01  7.80901380e-01  7.81296526e-01  7.82452121e-01
  7.85124358e-01  7.87755960e-01  7.89367707e-01  7.99710687e-01
  8.12814106e-01  8.17705104e-01  8.21288801e-01  8.21911463e-01
  8.23579715e-01  8.24367889e-01  8.24395730e-01  8.26693425e-01
  8.27335055e-01  8.29200552e-01  8.45753338e-01  8.46313013e-01
  8.49871371e-01  8.50249686e-01  8.59805359e-01  8.62422847e-01
  8.71754793e-01  9.00565830e-01  9.09402450e-01  9.09656904e-01
  9.23760176e-01]

  warnings.warn(

2022-11-03 10:51:02,352:INFO:Calculating mean and std
2022-11-03 10:51:02,352:INFO:Creating metrics dataframe
2022-11-03 10:51:02,367:INFO:Uploading results into container
2022-11-03 10:51:02,367:INFO:Uploading model into container now
2022-11-03 10:51:02,367:INFO:master_model_container: 17
2022-11-03 10:51:02,367:INFO:display_container: 2
2022-11-03 10:51:02,383:INFO:PassiveAggressiveRegressor(random_state=4411)
2022-11-03 10:51:02,383:INFO:create_model() successfully completed......................................
2022-11-03 10:51:02,663:WARNING:create_model() for PassiveAggressiveRegressor(random_state=4411) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:51:02,671:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:51:02,671:INFO:Initializing create_model()
2022-11-03 10:51:02,671:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:51:02,671:INFO:Checking exceptions
2022-11-03 10:51:02,679:INFO:Importing libraries
2022-11-03 10:51:02,679:INFO:Copying training dataset
2022-11-03 10:51:02,687:INFO:Defining folds
2022-11-03 10:51:02,687:INFO:Declaring metric variables
2022-11-03 10:51:02,687:INFO:Importing untrained model
2022-11-03 10:51:02,687:INFO:Passive Aggressive Regressor Imported successfully
2022-11-03 10:51:02,687:INFO:Starting cross validation
2022-11-03 10:51:02,703:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:51:06,836:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.79896668e+00 -1.76746238e+00 -1.76745801e+00 -1.73264382e+00
 -1.72683685e+00 -1.72615717e+00 -1.72487347e+00 -1.70868525e+00
 -1.69667704e+00 -1.66912058e+00 -1.66602124e+00 -1.66295771e+00
 -1.65378566e+00 -1.65155127e+00 -1.63206417e+00 -1.63099115e+00
 -1.62806881e+00 -1.62324325e+00 -1.61793807e+00 -1.61788218e+00
 -1.61632121e+00 -1.61341940e+00 -1.59605836e+00 -1.59153107e+00
 -1.58575670e+00 -1.56746398e+00 -1.56073072e+00 -1.55686353e+00
 -1.55071369e+00 -1.54891065e+00 -1.54777918e+00 -1.54159793e+00
 -1.53525048e+00 -1.52199569e+00 -1.51793308e+00 -1.50818713e+00
 -1.49569029e+00 -1.49311527e+00 -1.48322644e+00 -1.48229399e+00
 -1.48169135e+00 -1.46514568e+00 -1.46185402e+00 -1.45611528e+00
 -1.45336729e+00 -1.45146715e+00 -1.45043135e+00 -1.44771558e+00
 -1.44015242e+00 -1.43898338e+00 -1.43764753e+00 -1.43512019e+00
 -1.43450678e+00 -1.43157997e+00 -1.41809815e+00 -1.41706328e+00
 -1.41698410e+00 -1.41662760e+00 -1.40439216e+00 -1.39688059e+00
 -1.39235228e+00 -1.39211221e+00 -1.39152528e+00 -1.38965986e+00
 -1.38939113e+00 -1.38913069e+00 -1.38471104e+00 -1.37872324e+00
 -1.37484121e+00 -1.37302920e+00 -1.37105330e+00 -1.36545630e+00
 -1.36268685e+00 -1.35442212e+00 -1.34863841e+00 -1.34255699e+00
 -1.33961417e+00 -1.33368652e+00 -1.32802036e+00 -1.32214743e+00
 -1.31696915e+00 -1.31030546e+00 -1.28987273e+00 -1.28629334e+00
 -1.27862714e+00 -1.27602480e+00 -1.25704409e+00 -1.24987742e+00
 -1.24110023e+00 -1.24013825e+00 -1.22075729e+00 -1.21313623e+00
 -1.21156787e+00 -1.20541132e+00 -1.19891161e+00 -1.19834170e+00
 -1.19204393e+00 -1.19176373e+00 -1.18792608e+00 -1.18733526e+00
 -1.17692491e+00 -1.17241767e+00 -1.17161630e+00 -1.16953230e+00
 -1.16335958e+00 -1.15936269e+00 -1.15657058e+00 -1.14819890e+00
 -1.14806203e+00 -1.13226545e+00 -1.13225123e+00 -1.13151654e+00
 -1.12554276e+00 -1.11102802e+00 -1.10970793e+00 -1.10221968e+00
 -1.10066891e+00 -1.09582720e+00 -1.09371730e+00 -1.09029972e+00
 -1.08642135e+00 -1.08545868e+00 -1.07606429e+00 -1.07497434e+00
 -1.06873786e+00 -1.06782242e+00 -1.06730185e+00 -1.06583665e+00
 -1.06539139e+00 -1.06355134e+00 -1.06196161e+00 -1.06170060e+00
 -1.06041504e+00 -1.05788269e+00 -1.05055121e+00 -1.04455467e+00
 -1.04449825e+00 -1.02947247e+00 -1.02102507e+00 -1.01964049e+00
 -1.01770777e+00 -1.01681538e+00 -1.01646764e+00 -1.01542219e+00
 -1.01426229e+00 -1.01227118e+00 -1.01184848e+00 -1.00844079e+00
 -1.00157057e+00 -9.99590784e-01 -9.94566512e-01 -9.90417803e-01
 -9.88836907e-01 -9.88796706e-01 -9.83893583e-01 -9.81098408e-01
 -9.73915175e-01 -9.72017767e-01 -9.64908260e-01 -9.61633427e-01
 -9.59927239e-01 -9.55879539e-01 -9.49291510e-01 -9.39749714e-01
 -9.36067361e-01 -9.25845133e-01 -9.25256292e-01 -9.22981038e-01
 -9.22956247e-01 -9.18645364e-01 -9.15837779e-01 -9.08821991e-01
 -9.03520036e-01 -9.00955750e-01 -8.97568536e-01 -8.94795500e-01
 -8.94587590e-01 -8.90316144e-01 -8.89763229e-01 -8.88174549e-01
 -8.79452453e-01 -8.71877675e-01 -8.58826175e-01 -8.57467595e-01
 -8.50776300e-01 -8.31984514e-01 -8.24167290e-01 -8.23635475e-01
 -8.21628525e-01 -8.19224245e-01 -8.17841032e-01 -8.10983812e-01
 -8.09879187e-01 -8.08382161e-01 -8.03990979e-01 -8.01460843e-01
 -7.99454257e-01 -7.95723724e-01 -7.93773846e-01 -7.82708217e-01
 -7.80508092e-01 -7.79201473e-01 -7.73616309e-01 -7.73053416e-01
 -7.58723335e-01 -7.58644107e-01 -7.53186917e-01 -7.47488530e-01
 -7.46460467e-01 -7.44855679e-01 -7.43895635e-01 -7.41456920e-01
 -7.39022045e-01 -7.31905160e-01 -7.23962052e-01 -7.20558354e-01
 -7.05489897e-01 -7.01085217e-01 -6.98478566e-01 -6.98189280e-01
 -6.96283809e-01 -6.86614408e-01 -6.84320656e-01 -6.80826747e-01
 -6.75780236e-01 -6.73228455e-01 -6.66140576e-01 -6.61094054e-01
 -6.55748474e-01 -6.48003293e-01 -6.47442714e-01 -6.46564873e-01
 -6.37047789e-01 -6.28595959e-01 -6.27420256e-01 -6.24540922e-01
 -6.19780298e-01 -6.19194067e-01 -6.15132421e-01 -6.06097913e-01
 -6.04492545e-01 -5.99585655e-01 -5.98386362e-01 -5.97740010e-01
 -5.91729941e-01 -5.88456311e-01 -5.87710624e-01 -5.83178652e-01
 -5.81585958e-01 -5.78318790e-01 -5.74331647e-01 -5.73461129e-01
 -5.68679470e-01 -5.66149638e-01 -5.65442468e-01 -5.62532979e-01
 -5.58465808e-01 -5.55931496e-01 -5.45274817e-01 -5.44544691e-01
 -5.42513010e-01 -5.41131329e-01 -5.37063200e-01 -5.33952264e-01
 -5.33670832e-01 -5.29439622e-01 -5.28697940e-01 -5.16787418e-01
 -5.13759800e-01 -5.12245077e-01 -5.11542575e-01 -5.10405353e-01
 -5.09165192e-01 -5.00773803e-01 -4.96632918e-01 -4.93573415e-01
 -4.93460133e-01 -4.86154371e-01 -4.85312488e-01 -4.84008988e-01
 -4.83153953e-01 -4.82947950e-01 -4.68381192e-01 -4.68207674e-01
 -4.66080413e-01 -4.59471954e-01 -4.58988935e-01 -4.55631785e-01
 -4.52778809e-01 -4.51922097e-01 -4.50261456e-01 -4.49867235e-01
 -4.42741259e-01 -4.41548293e-01 -4.41540261e-01 -4.40892076e-01
 -4.40564623e-01 -4.36019117e-01 -4.33511962e-01 -4.29376560e-01
 -4.27858254e-01 -4.23867921e-01 -4.20418508e-01 -4.19318285e-01
 -4.18993672e-01 -4.14698275e-01 -4.13004356e-01 -4.10192285e-01
 -4.08051648e-01 -4.02633647e-01 -4.02629803e-01 -4.01685348e-01
 -4.01267296e-01 -3.97292505e-01 -3.97029288e-01 -3.96113300e-01
 -3.92090672e-01 -3.91140751e-01 -3.86873385e-01 -3.85968321e-01
 -3.83726055e-01 -3.81741910e-01 -3.71807784e-01 -3.69735982e-01
 -3.67211057e-01 -3.63160091e-01 -3.59113515e-01 -3.58401479e-01
 -3.56654773e-01 -3.45548605e-01 -3.45445603e-01 -3.42975131e-01
 -3.34386403e-01 -3.33288457e-01 -3.30160741e-01 -3.26537538e-01
 -3.24720569e-01 -3.21201169e-01 -3.19819809e-01 -3.19203475e-01
 -3.19146361e-01 -3.19106955e-01 -3.16674411e-01 -3.14411650e-01
 -3.11414028e-01 -3.10345717e-01 -3.03889562e-01 -2.97988819e-01
 -2.95878540e-01 -2.94323338e-01 -2.91321987e-01 -2.90982115e-01
 -2.87455235e-01 -2.86463404e-01 -2.82831777e-01 -2.67007946e-01
 -2.65985384e-01 -2.64808248e-01 -2.61260192e-01 -2.60582701e-01
 -2.60334694e-01 -2.59213406e-01 -2.56222632e-01 -2.54935756e-01
 -2.53999228e-01 -2.47085124e-01 -2.46165083e-01 -2.38419762e-01
 -2.36159658e-01 -2.35999554e-01 -2.33826115e-01 -2.33331584e-01
 -2.30356268e-01 -2.27268540e-01 -2.24275531e-01 -2.23466278e-01
 -2.22780076e-01 -2.14334083e-01 -2.08225166e-01 -2.08119400e-01
 -2.00777948e-01 -1.95855252e-01 -1.90482022e-01 -1.85898460e-01
 -1.74865278e-01 -1.69885112e-01 -1.67594168e-01 -1.67350924e-01
 -1.67205361e-01 -1.66513368e-01 -1.66451115e-01 -1.62896087e-01
 -1.62016529e-01 -1.60139547e-01 -1.57596807e-01 -1.57181141e-01
 -1.56153157e-01 -1.52592565e-01 -1.51418016e-01 -1.48332784e-01
 -1.45995946e-01 -1.42942520e-01 -1.42075896e-01 -1.41503778e-01
 -1.40763853e-01 -1.39918751e-01 -1.37645860e-01 -1.36375161e-01
 -1.36133481e-01 -1.34103184e-01 -1.32544093e-01 -1.31664358e-01
 -1.30602609e-01 -1.29515594e-01 -1.29481933e-01 -1.27200070e-01
 -1.19829017e-01 -1.17876382e-01 -1.17801916e-01 -1.14743299e-01
 -1.14478441e-01 -1.13230800e-01 -1.12100693e-01 -1.10927726e-01
 -1.02571743e-01 -1.02067985e-01 -9.94514881e-02 -9.94153177e-02
 -9.88682139e-02 -9.66682440e-02 -8.69793597e-02 -8.57945571e-02
 -7.97833163e-02 -7.44376612e-02 -7.07439239e-02 -6.90828608e-02
 -6.86930387e-02 -6.21003584e-02 -6.11929219e-02 -5.94710568e-02
 -5.48588343e-02 -5.00248984e-02 -4.68603611e-02 -2.79547278e-02
 -2.78054263e-02 -2.44122848e-02 -2.39734986e-02 -1.75660541e-02
 -1.14452675e-02 -9.73592735e-03 -2.57242016e-03  1.04028726e-03
  2.61475749e-03  5.44122971e-03  6.32042187e-03  6.47468517e-03
  7.65039047e-03  1.26723922e-02  1.27430430e-02  1.97862935e-02
  1.98040917e-02  2.37741268e-02  2.67226866e-02  2.90494002e-02
  3.01865448e-02  3.96379639e-02  4.25656627e-02  5.08894679e-02
  5.14532207e-02  5.25535219e-02  5.51078025e-02  5.77988962e-02
  5.80569740e-02  5.86335988e-02  6.14099790e-02  7.01079754e-02
  7.73193190e-02  8.11228883e-02  8.91658775e-02  9.01399800e-02
  1.06455799e-01  1.30644549e-01  1.38273763e-01  1.46513848e-01
  1.52166197e-01  1.66453208e-01  2.37758956e-01  2.42985463e-01
  2.56770747e-01  2.67993187e-01  2.83139691e-01  2.85878591e-01
  2.99335383e-01]

  warnings.warn(

2022-11-03 10:51:06,902:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.75892182e+00 -1.68835217e+00 -1.67911116e+00 -1.67879239e+00
 -1.62064654e+00 -1.61821658e+00 -1.58926046e+00 -1.58125666e+00
 -1.58074627e+00 -1.57929639e+00 -1.57893390e+00 -1.57835358e+00
 -1.57489034e+00 -1.55320442e+00 -1.54779098e+00 -1.54544530e+00
 -1.53735486e+00 -1.53266576e+00 -1.52795791e+00 -1.52114561e+00
 -1.51854533e+00 -1.51788836e+00 -1.51489650e+00 -1.51303456e+00
 -1.50872777e+00 -1.50134505e+00 -1.49697565e+00 -1.49057247e+00
 -1.48421916e+00 -1.47979405e+00 -1.47860463e+00 -1.47430696e+00
 -1.47392704e+00 -1.46974200e+00 -1.46637597e+00 -1.46270074e+00
 -1.45478771e+00 -1.45459376e+00 -1.43054784e+00 -1.42909579e+00
 -1.42427789e+00 -1.41802209e+00 -1.41631563e+00 -1.41517240e+00
 -1.41395829e+00 -1.41105104e+00 -1.41039981e+00 -1.40977822e+00
 -1.40758392e+00 -1.40586254e+00 -1.39906147e+00 -1.39435561e+00
 -1.38438678e+00 -1.35399685e+00 -1.35397123e+00 -1.31851950e+00
 -1.31541009e+00 -1.31379567e+00 -1.30913199e+00 -1.29212524e+00
 -1.27972217e+00 -1.27746296e+00 -1.26937317e+00 -1.25898262e+00
 -1.25825573e+00 -1.25399660e+00 -1.25058954e+00 -1.24372040e+00
 -1.24313703e+00 -1.24062464e+00 -1.24042572e+00 -1.23772154e+00
 -1.23400458e+00 -1.22331559e+00 -1.22128568e+00 -1.21289315e+00
 -1.20930801e+00 -1.20680441e+00 -1.19950760e+00 -1.18590563e+00
 -1.18093012e+00 -1.17403449e+00 -1.16822995e+00 -1.16787339e+00
 -1.16686238e+00 -1.16634410e+00 -1.15711506e+00 -1.14849825e+00
 -1.14637774e+00 -1.14626266e+00 -1.14572027e+00 -1.13713630e+00
 -1.13259862e+00 -1.12740849e+00 -1.11860305e+00 -1.11654292e+00
 -1.11238325e+00 -1.10446844e+00 -1.09909778e+00 -1.09681119e+00
 -1.08817448e+00 -1.08666407e+00 -1.08451561e+00 -1.07738480e+00
 -1.06446590e+00 -1.06129967e+00 -1.06002167e+00 -1.05865121e+00
 -1.05404532e+00 -1.04887786e+00 -1.04781956e+00 -1.04627939e+00
 -1.04483607e+00 -1.03224699e+00 -1.03097437e+00 -1.02309260e+00
 -1.02301542e+00 -1.02100450e+00 -1.01877587e+00 -1.01861255e+00
 -1.01678897e+00 -1.01571235e+00 -1.01032708e+00 -9.99882734e-01
 -9.96223318e-01 -9.94069291e-01 -9.92951638e-01 -9.83639569e-01
 -9.67306314e-01 -9.64213328e-01 -9.59951790e-01 -9.58362296e-01
 -9.57469549e-01 -9.56649877e-01 -9.55205814e-01 -9.54469139e-01
 -9.49921179e-01 -9.46472640e-01 -9.42602014e-01 -9.41343569e-01
 -9.40561700e-01 -9.39845941e-01 -9.37481266e-01 -9.36573318e-01
 -9.36538991e-01 -9.35915431e-01 -9.26387103e-01 -9.22610550e-01
 -9.22375133e-01 -9.22317185e-01 -9.20131522e-01 -9.19378018e-01
 -9.16127263e-01 -9.12580579e-01 -9.05312640e-01 -8.93633170e-01
 -8.92833133e-01 -8.92233154e-01 -8.75808975e-01 -8.72274803e-01
 -8.69504274e-01 -8.69468347e-01 -8.63450679e-01 -8.63150663e-01
 -8.53000435e-01 -8.51872802e-01 -8.45897101e-01 -8.43875779e-01
 -8.41270965e-01 -8.39152357e-01 -8.30491212e-01 -8.29534176e-01
 -8.26118841e-01 -8.23341832e-01 -8.20432590e-01 -8.18175240e-01
 -8.17645640e-01 -8.17523933e-01 -8.02114767e-01 -7.98564515e-01
 -7.97679093e-01 -7.93241778e-01 -7.90847715e-01 -7.84355866e-01
 -7.71921361e-01 -7.71410009e-01 -7.67303205e-01 -7.64643516e-01
 -7.64451898e-01 -7.60817005e-01 -7.55894959e-01 -7.55337729e-01
 -7.53644432e-01 -7.47368718e-01 -7.45471779e-01 -7.34191938e-01
 -7.31818335e-01 -7.25755124e-01 -7.19109408e-01 -7.13287261e-01
 -7.13219558e-01 -7.05963941e-01 -6.94782017e-01 -6.85903520e-01
 -6.75724594e-01 -6.74940463e-01 -6.74059021e-01 -6.73555982e-01
 -6.71253747e-01 -6.69509177e-01 -6.67770268e-01 -6.62950296e-01
 -6.62600587e-01 -6.62066630e-01 -6.60308283e-01 -6.59956144e-01
 -6.59597962e-01 -6.54694580e-01 -6.50611374e-01 -6.42826940e-01
 -6.41936345e-01 -6.29154512e-01 -6.26659263e-01 -6.26345843e-01
 -6.24398469e-01 -6.17484150e-01 -6.17245345e-01 -6.16821135e-01
 -6.15989518e-01 -6.15648014e-01 -6.15554370e-01 -6.09947615e-01
 -6.03679574e-01 -5.94499470e-01 -5.88284363e-01 -5.87775507e-01
 -5.85096897e-01 -5.81647451e-01 -5.71061349e-01 -5.70477985e-01
 -5.62060289e-01 -5.60528319e-01 -5.59605368e-01 -5.58518259e-01
 -5.54482699e-01 -5.51299916e-01 -5.42596597e-01 -5.39921183e-01
 -5.38495458e-01 -5.38154096e-01 -5.34538658e-01 -5.22649485e-01
 -5.21450623e-01 -5.14228330e-01 -5.14170752e-01 -5.12710742e-01
 -4.95559758e-01 -4.87874663e-01 -4.86829832e-01 -4.85049211e-01
 -4.82204519e-01 -4.77207574e-01 -4.76576691e-01 -4.73930216e-01
 -4.73906595e-01 -4.71195242e-01 -4.70828127e-01 -4.68115111e-01
 -4.66961941e-01 -4.57597228e-01 -4.56549575e-01 -4.53718988e-01
 -4.52530148e-01 -4.43986529e-01 -4.40390880e-01 -4.39374415e-01
 -4.38730794e-01 -4.33345556e-01 -4.32963983e-01 -4.27425586e-01
 -4.26938514e-01 -4.20510278e-01 -4.03925835e-01 -4.02348824e-01
 -3.98557480e-01 -3.96964906e-01 -3.96915021e-01 -3.91625548e-01
 -3.89601760e-01 -3.88546224e-01 -3.88423240e-01 -3.82141472e-01
 -3.73944249e-01 -3.65518390e-01 -3.65372904e-01 -3.63344509e-01
 -3.61666114e-01 -3.54886932e-01 -3.54190183e-01 -3.50333858e-01
 -3.48345963e-01 -3.46205187e-01 -3.44822105e-01 -3.40496531e-01
 -3.39804539e-01 -3.35802877e-01 -3.31678576e-01 -3.24630945e-01
 -3.23369847e-01 -3.21271891e-01 -3.19216290e-01 -3.18519104e-01
 -3.15539745e-01 -3.10413217e-01 -3.06294517e-01 -3.03051470e-01
 -2.99847899e-01 -2.94536526e-01 -2.91498756e-01 -2.90774705e-01
 -2.89937055e-01 -2.85565321e-01 -2.84159218e-01 -2.76428760e-01
 -2.76298940e-01 -2.75121747e-01 -2.74451290e-01 -2.74117360e-01
 -2.68716429e-01 -2.62977372e-01 -2.59609594e-01 -2.58399432e-01
 -2.55229089e-01 -2.48712729e-01 -2.39129492e-01 -2.37432189e-01
 -2.37156956e-01 -2.33569209e-01 -2.32113608e-01 -2.31007894e-01
 -2.28072555e-01 -2.22167377e-01 -2.21538468e-01 -2.19289979e-01
 -2.18232879e-01 -2.18028525e-01 -2.15248195e-01 -2.13241551e-01
 -2.08780813e-01 -2.08270753e-01 -2.06309746e-01 -2.05452584e-01
 -2.03310595e-01 -2.02834540e-01 -1.99711669e-01 -1.99650974e-01
 -1.96620761e-01 -1.95558077e-01 -1.91690374e-01 -1.91217665e-01
 -1.85467992e-01 -1.84638887e-01 -1.83152053e-01 -1.82815775e-01
 -1.82346917e-01 -1.79924464e-01 -1.76666643e-01 -1.75918818e-01
 -1.75651190e-01 -1.74279577e-01 -1.73653698e-01 -1.71548804e-01
 -1.70314777e-01 -1.69752638e-01 -1.65560165e-01 -1.62717661e-01
 -1.61462726e-01 -1.60386048e-01 -1.60340060e-01 -1.58692749e-01
 -1.38600853e-01 -1.38188747e-01 -1.33138309e-01 -1.31335481e-01
 -1.30785970e-01 -1.26626219e-01 -1.23713754e-01 -1.22411591e-01
 -1.20650014e-01 -1.19795274e-01 -1.18354988e-01 -1.11543483e-01
 -1.10076592e-01 -1.08949490e-01 -1.08818105e-01 -1.02483305e-01
 -9.92391780e-02 -9.79133036e-02 -9.12503838e-02 -8.84504278e-02
 -8.81788312e-02 -8.47624841e-02 -7.97740785e-02 -7.90020983e-02
 -7.55754008e-02 -7.33185107e-02 -6.98362772e-02 -6.95905957e-02
 -6.89143532e-02 -6.76796489e-02 -6.62447126e-02 -6.61471264e-02
 -5.97336042e-02 -5.57874018e-02 -5.55923921e-02 -5.47423956e-02
 -5.45811523e-02 -4.94386139e-02 -4.69885524e-02 -4.56435306e-02
 -4.13703643e-02 -4.00821246e-02 -3.98839151e-02 -3.94966673e-02
 -3.68758348e-02 -3.29698630e-02 -2.83344412e-02 -2.67453106e-02
 -2.67035250e-02 -2.31736934e-02 -1.92190331e-02 -1.90722575e-02
 -1.65737373e-02 -1.35438990e-02 -1.29797405e-02 -8.25850619e-03
 -1.31782071e-03 -1.11191117e-04  7.70577538e-04  8.62181969e-04
  5.19214958e-03  7.93587070e-03  8.95608620e-03  1.40928438e-02
  1.73764589e-02  1.98646529e-02  2.00220296e-02  2.24216645e-02
  2.42866996e-02  2.48184985e-02  2.78193696e-02  2.78364541e-02
  3.62478464e-02  3.65224184e-02  3.73253974e-02  3.78258672e-02
  3.94422507e-02  3.96744213e-02  5.01174805e-02  5.13009747e-02
  5.59024354e-02  5.73945858e-02  6.48879898e-02  6.78535447e-02
  6.80798698e-02  7.41553822e-02  8.20573389e-02  8.62009870e-02
  8.79640320e-02  8.81911450e-02  9.13764579e-02  1.02698303e-01
  1.04095558e-01  1.05314280e-01  1.06578552e-01  1.08625609e-01
  1.10880630e-01  1.13237418e-01  1.14658620e-01  1.16421207e-01
  1.18336849e-01  1.20829134e-01  1.26432652e-01  1.31640652e-01
  1.35967636e-01  1.36979342e-01  1.40960945e-01  1.46439446e-01
  1.63083661e-01  1.68368677e-01  1.99357304e-01  2.60257483e-01
  3.34408926e-01]

  warnings.warn(

2022-11-03 10:51:06,918:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.73020183e+00 -1.72803201e+00 -1.72455623e+00 -1.71732890e+00
 -1.71034979e+00 -1.69946384e+00 -1.66269869e+00 -1.63071159e+00
 -1.62500335e+00 -1.62129470e+00 -1.61551585e+00 -1.61258424e+00
 -1.60985801e+00 -1.58099016e+00 -1.56849608e+00 -1.54975460e+00
 -1.51632019e+00 -1.51292807e+00 -1.51143621e+00 -1.51103955e+00
 -1.50837929e+00 -1.50706628e+00 -1.50680274e+00 -1.48912126e+00
 -1.47536355e+00 -1.47087940e+00 -1.47039873e+00 -1.46787865e+00
 -1.46307164e+00 -1.46050732e+00 -1.45528910e+00 -1.44904728e+00
 -1.44675860e+00 -1.43363307e+00 -1.42640842e+00 -1.42304439e+00
 -1.42084841e+00 -1.39261131e+00 -1.39257105e+00 -1.39175981e+00
 -1.39165872e+00 -1.39155533e+00 -1.38421604e+00 -1.36577193e+00
 -1.36129310e+00 -1.35501382e+00 -1.33806838e+00 -1.33613483e+00
 -1.33457471e+00 -1.33230579e+00 -1.33044746e+00 -1.32761043e+00
 -1.32241121e+00 -1.30961135e+00 -1.30626929e+00 -1.30621342e+00
 -1.30431012e+00 -1.30125870e+00 -1.29303574e+00 -1.28207461e+00
 -1.27778650e+00 -1.27651478e+00 -1.26243188e+00 -1.25350043e+00
 -1.24987266e+00 -1.24699367e+00 -1.24306592e+00 -1.23772156e+00
 -1.23404629e+00 -1.23243264e+00 -1.22962063e+00 -1.22934515e+00
 -1.22830479e+00 -1.22213827e+00 -1.21366507e+00 -1.21152448e+00
 -1.20994997e+00 -1.20580691e+00 -1.20235764e+00 -1.19294975e+00
 -1.19212513e+00 -1.18377772e+00 -1.18326634e+00 -1.17994235e+00
 -1.17794777e+00 -1.17524392e+00 -1.17240137e+00 -1.15676860e+00
 -1.15466544e+00 -1.15248086e+00 -1.15115471e+00 -1.13713723e+00
 -1.13443805e+00 -1.13288378e+00 -1.12894422e+00 -1.12107428e+00
 -1.12038508e+00 -1.11518498e+00 -1.11265955e+00 -1.11132908e+00
 -1.10371181e+00 -1.10082615e+00 -1.10022889e+00 -1.09602863e+00
 -1.09348544e+00 -1.09136555e+00 -1.08079550e+00 -1.07978609e+00
 -1.07643393e+00 -1.07180310e+00 -1.06307192e+00 -1.04954523e+00
 -1.04553178e+00 -1.04484731e+00 -1.04473854e+00 -1.04466488e+00
 -1.04134674e+00 -1.03284337e+00 -1.02974285e+00 -1.01556436e+00
 -1.01238532e+00 -1.00403326e+00 -1.00262834e+00 -1.00191900e+00
 -1.00162939e+00 -9.96607421e-01 -9.87692943e-01 -9.86770061e-01
 -9.83354184e-01 -9.71662436e-01 -9.70246900e-01 -9.57899469e-01
 -9.50883210e-01 -9.49855479e-01 -9.47214008e-01 -9.42806048e-01
 -9.42106587e-01 -9.36358068e-01 -9.33170289e-01 -9.31118614e-01
 -9.22237617e-01 -9.17805711e-01 -9.13842876e-01 -8.94881832e-01
 -8.93527923e-01 -8.93040538e-01 -8.91145331e-01 -8.90308252e-01
 -8.85657535e-01 -8.85336666e-01 -8.83627088e-01 -8.82125022e-01
 -8.81490511e-01 -8.79487894e-01 -8.78134920e-01 -8.78053428e-01
 -8.75575254e-01 -8.68853077e-01 -8.61255749e-01 -8.59747364e-01
 -8.57472697e-01 -8.41708845e-01 -8.36910508e-01 -8.35791990e-01
 -8.32023378e-01 -8.25764418e-01 -8.25577613e-01 -8.08620059e-01
 -8.08284602e-01 -8.08243235e-01 -8.08189352e-01 -8.06237545e-01
 -8.05719615e-01 -7.93373153e-01 -7.92356612e-01 -7.88824569e-01
 -7.87436486e-01 -7.85386880e-01 -7.85064750e-01 -7.84187074e-01
 -7.81237348e-01 -7.75279627e-01 -7.75109456e-01 -7.72912579e-01
 -7.69548714e-01 -7.69333372e-01 -7.60650079e-01 -7.56214874e-01
 -7.53005583e-01 -7.50933862e-01 -7.43813337e-01 -7.42722382e-01
 -7.40067955e-01 -7.37405105e-01 -7.36755714e-01 -7.36658979e-01
 -7.35917588e-01 -7.35573616e-01 -7.32340857e-01 -7.31578155e-01
 -7.27916662e-01 -7.27347586e-01 -7.24146136e-01 -7.18378370e-01
 -7.16785908e-01 -7.04363286e-01 -7.02708510e-01 -6.97461384e-01
 -6.94873834e-01 -6.94240981e-01 -6.89725795e-01 -6.86226750e-01
 -6.69228859e-01 -6.65666467e-01 -6.60334303e-01 -6.52399956e-01
 -6.47801400e-01 -6.44909001e-01 -6.44462186e-01 -6.38155133e-01
 -6.37693189e-01 -6.35641836e-01 -6.29564028e-01 -6.26799104e-01
 -6.15475067e-01 -6.09467937e-01 -6.09374103e-01 -6.03684234e-01
 -6.01158332e-01 -6.00851821e-01 -6.00734775e-01 -5.97620096e-01
 -5.97120725e-01 -5.85473387e-01 -5.83525786e-01 -5.81206323e-01
 -5.77164167e-01 -5.75381262e-01 -5.74047999e-01 -5.73872249e-01
 -5.70021013e-01 -5.67924887e-01 -5.67225349e-01 -5.61942992e-01
 -5.57548943e-01 -5.53381924e-01 -5.53264154e-01 -5.45647315e-01
 -5.39311073e-01 -5.30412949e-01 -5.30388503e-01 -5.30189153e-01
 -5.27832068e-01 -5.27687491e-01 -5.20313648e-01 -5.20281095e-01
 -5.16043244e-01 -5.14109149e-01 -5.13860855e-01 -5.13528548e-01
 -5.13363324e-01 -4.95222458e-01 -4.92152876e-01 -4.91665148e-01
 -4.88840095e-01 -4.85666656e-01 -4.80655317e-01 -4.80424353e-01
 -4.74666309e-01 -4.71878910e-01 -4.71063155e-01 -4.70827825e-01
 -4.61567596e-01 -4.58490098e-01 -4.56173329e-01 -4.55701118e-01
 -4.53556582e-01 -4.53451953e-01 -4.52676685e-01 -4.52161456e-01
 -4.51990023e-01 -4.51239413e-01 -4.51157911e-01 -4.45637583e-01
 -4.41408213e-01 -4.27571123e-01 -4.27357953e-01 -4.21248492e-01
 -4.16994244e-01 -4.15534748e-01 -4.14019335e-01 -4.13561737e-01
 -4.13455768e-01 -4.10193839e-01 -4.08654752e-01 -4.07612224e-01
 -4.05520747e-01 -3.96450042e-01 -3.93643086e-01 -3.92825731e-01
 -3.90621528e-01 -3.90603607e-01 -3.90243560e-01 -3.90055276e-01
 -3.89183940e-01 -3.88888301e-01 -3.86491917e-01 -3.85847882e-01
 -3.82880797e-01 -3.81937097e-01 -3.78777029e-01 -3.76265855e-01
 -3.71625520e-01 -3.71501009e-01 -3.69441580e-01 -3.67676451e-01
 -3.66254325e-01 -3.64320855e-01 -3.62991073e-01 -3.58996537e-01
 -3.54837951e-01 -3.49149944e-01 -3.42650871e-01 -3.38161758e-01
 -3.37008104e-01 -3.35477505e-01 -3.31779183e-01 -3.31413616e-01
 -3.28545778e-01 -3.27501315e-01 -3.25298796e-01 -3.24313329e-01
 -3.19515145e-01 -3.15226490e-01 -3.15176625e-01 -3.14102446e-01
 -3.14050757e-01 -3.12702394e-01 -3.10741172e-01 -3.10368793e-01
 -3.08960994e-01 -3.08634781e-01 -3.06527129e-01 -3.01552924e-01
 -3.01015981e-01 -2.99607111e-01 -2.96720283e-01 -2.90934220e-01
 -2.86988013e-01 -2.86122324e-01 -2.85820347e-01 -2.84147472e-01
 -2.81912723e-01 -2.81636550e-01 -2.80918876e-01 -2.79335014e-01
 -2.78118699e-01 -2.71960299e-01 -2.71138356e-01 -2.69790214e-01
 -2.66878200e-01 -2.64120087e-01 -2.61316895e-01 -2.57768465e-01
 -2.55708936e-01 -2.55639910e-01 -2.55537940e-01 -2.54893873e-01
 -2.54718565e-01 -2.52465481e-01 -2.50990084e-01 -2.48143813e-01
 -2.46966130e-01 -2.44753978e-01 -2.38627001e-01 -2.38269537e-01
 -2.38236425e-01 -2.31863472e-01 -2.31407554e-01 -2.30046985e-01
 -2.26112000e-01 -2.24204091e-01 -2.23156315e-01 -2.23112961e-01
 -2.19179926e-01 -2.16312749e-01 -2.14382747e-01 -2.07987803e-01
 -2.07292758e-01 -2.06295329e-01 -2.02511108e-01 -2.01217200e-01
 -1.99216534e-01 -1.99001718e-01 -1.98025151e-01 -1.94465277e-01
 -1.90158190e-01 -1.90050449e-01 -1.86501699e-01 -1.84434634e-01
 -1.82306037e-01 -1.81864504e-01 -1.79679312e-01 -1.77868012e-01
 -1.77261478e-01 -1.76158670e-01 -1.74965974e-01 -1.73764482e-01
 -1.67869581e-01 -1.57786945e-01 -1.54293465e-01 -1.49638546e-01
 -1.48450593e-01 -1.44319435e-01 -1.43601520e-01 -1.39948283e-01
 -1.36889717e-01 -1.36673334e-01 -1.34916185e-01 -1.34217319e-01
 -1.30148096e-01 -1.27587130e-01 -1.25411641e-01 -1.24558286e-01
 -1.22787615e-01 -1.20155367e-01 -1.19252021e-01 -1.18167557e-01
 -1.17361246e-01 -1.16548397e-01 -1.15274310e-01 -1.10522735e-01
 -1.07975763e-01 -1.03104093e-01 -1.02522008e-01 -1.02151043e-01
 -1.00276371e-01 -9.92807473e-02 -9.78575165e-02 -9.39363924e-02
 -9.32724802e-02 -9.29545388e-02 -9.26043582e-02 -7.51810469e-02
 -7.09625994e-02 -6.98952791e-02 -6.78522848e-02 -6.67880300e-02
 -6.28829281e-02 -6.13452781e-02 -5.69516389e-02 -5.32762618e-02
 -5.18782874e-02 -5.18249631e-02 -4.90425258e-02 -4.72351509e-02
 -4.56559464e-02 -4.51010416e-02 -4.35641269e-02 -4.16194690e-02
 -3.84160790e-02 -3.38375473e-02 -2.74330603e-02 -2.05518827e-02
 -1.96364510e-02 -1.91215985e-02 -1.86200718e-02 -1.71893839e-02
 -1.49938445e-02 -1.14938777e-02 -1.49277191e-03 -3.23314580e-04
 -2.37875592e-04  7.64084329e-04  2.64976640e-03  1.78139973e-02
  1.83460118e-02  1.86029213e-02  2.83623178e-02  3.06959396e-02
  3.29184157e-02  3.71491295e-02  3.83693090e-02  4.09935749e-02
  4.31375623e-02  4.76807763e-02  5.72881691e-02  5.76763781e-02
  6.26701403e-02  7.33197190e-02  7.57499931e-02  1.06825047e-01
  2.78189154e-01]

  warnings.warn(

2022-11-03 10:51:07,191:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.07031558e-01 -4.03669812e-01 -3.56853027e-01 -3.38862626e-01
 -3.31154423e-01 -3.30493365e-01 -3.28681826e-01 -3.22627184e-01
 -3.15716443e-01 -3.07418914e-01 -3.06121842e-01 -3.02287534e-01
 -2.98984894e-01 -2.96258054e-01 -2.94360304e-01 -2.79462396e-01
 -2.79046397e-01 -2.77587605e-01 -2.76616059e-01 -2.72020155e-01
 -2.62020337e-01 -2.60122959e-01 -2.56902515e-01 -2.55924224e-01
 -2.55552664e-01 -2.49487240e-01 -2.49426600e-01 -2.45014838e-01
 -2.41403656e-01 -2.31204109e-01 -2.25303213e-01 -2.21575700e-01
 -2.19822683e-01 -2.18269499e-01 -2.17840511e-01 -2.13773164e-01
 -2.08720782e-01 -2.02381384e-01 -1.96117911e-01 -1.95198645e-01
 -1.93928843e-01 -1.93474487e-01 -1.91491139e-01 -1.90825598e-01
 -1.90067447e-01 -1.86271435e-01 -1.80637599e-01 -1.74923695e-01
 -1.74522954e-01 -1.73631678e-01 -1.72526876e-01 -1.71866977e-01
 -1.71175870e-01 -1.68137748e-01 -1.65453865e-01 -1.65365190e-01
 -1.65298172e-01 -1.63966959e-01 -1.60895009e-01 -1.60206541e-01
 -1.54540444e-01 -1.52778159e-01 -1.49406618e-01 -1.47965127e-01
 -1.45106003e-01 -1.35787604e-01 -1.31871838e-01 -1.30434766e-01
 -1.28960643e-01 -1.27207626e-01 -1.26536575e-01 -1.25537785e-01
 -1.25395913e-01 -1.20702859e-01 -1.19760937e-01 -1.18555094e-01
 -1.17400082e-01 -1.16025142e-01 -1.15222102e-01 -1.11648633e-01
 -1.10067265e-01 -1.08488112e-01 -1.07209449e-01 -1.05029801e-01
 -1.04860628e-01 -9.99821617e-02 -9.72103385e-02 -9.55400637e-02
 -9.48041535e-02 -9.03129103e-02 -8.80466469e-02 -8.64976921e-02
 -8.54716933e-02 -8.54303281e-02 -8.47941464e-02 -8.40546219e-02
 -8.09807556e-02 -7.89179354e-02 -7.82732646e-02 -7.80258269e-02
 -7.57830212e-02 -7.57066127e-02 -7.39347968e-02 -7.28143664e-02
 -7.18756959e-02 -7.14391371e-02 -7.09241302e-02 -6.90250144e-02
 -6.87084479e-02 -6.77638429e-02 -6.63556404e-02 -6.62977616e-02
 -6.52116547e-02 -6.51681899e-02 -6.37161586e-02 -6.31111151e-02
 -6.15647861e-02 -6.11543102e-02 -5.56047474e-02 -5.50917446e-02
 -5.46506721e-02 -5.30032258e-02 -5.29058171e-02 -5.04292159e-02
 -4.83923351e-02 -4.55684827e-02 -4.55452489e-02 -4.41221848e-02
 -4.30385177e-02 -4.18632551e-02 -3.99271100e-02 -3.95787344e-02
 -3.88389397e-02 -3.86612348e-02 -3.84909853e-02 -3.84680348e-02
 -3.84230543e-02 -3.83775013e-02 -3.72223155e-02 -3.22192701e-02
 -3.21027520e-02 -2.88686028e-02 -2.78165075e-02 -2.56297175e-02
 -2.56157652e-02 -2.31846356e-02 -9.39940455e-03 -8.70376383e-03
 -7.58721585e-03 -6.49584084e-03 -5.55633139e-03 -5.32458983e-03
 -4.69213181e-03 -2.94005585e-03 -2.90649010e-03 -2.88755931e-03
 -2.19185678e-03 -5.63975975e-04 -5.09641604e-04 -4.83757838e-04
  4.87590100e-04  1.27845240e-03  1.59905887e-03  2.56032749e-03
  3.13748461e-03  3.20404930e-03  3.32936581e-03  6.20365162e-03
  6.25446112e-03  6.86067998e-03  7.15637920e-03  7.57847133e-03
  9.23657501e-03  1.09409894e-02  1.28328030e-02  1.29346302e-02
  1.34365176e-02  1.41625635e-02  1.46479953e-02  1.49113011e-02
  1.49735602e-02  1.58034867e-02  1.84946695e-02  1.88990338e-02
  1.92664699e-02  1.94826131e-02  2.04004974e-02  2.16299570e-02
  2.25685063e-02  2.27457190e-02  2.28777726e-02  2.30939160e-02
  2.41457462e-02  2.47153908e-02  2.48702007e-02  2.55053755e-02
  2.59464923e-02  2.62756669e-02  2.64817723e-02  2.67445258e-02
  2.73488656e-02  2.81098290e-02  2.86514088e-02  2.88203463e-02
  3.17854217e-02  3.23873053e-02  3.26117955e-02  3.35378986e-02
  3.38361392e-02  3.40450363e-02  3.84776311e-02  3.86226975e-02
  3.95352581e-02  4.03049692e-02  4.05560525e-02  4.16059892e-02
  4.23892264e-02  4.27279533e-02  4.33988759e-02  4.36596513e-02
  4.38939391e-02  4.63455472e-02  4.76145455e-02  5.01641084e-02
  5.02062389e-02  5.05503192e-02  5.37205325e-02  5.39706675e-02
  5.43557226e-02  5.48140355e-02  6.00943388e-02  6.07635043e-02
  6.19504281e-02  6.22673409e-02  6.25474115e-02  6.29142028e-02
  6.29267587e-02  6.33276038e-02  6.37526626e-02  6.53794246e-02
  6.61251887e-02  6.67459387e-02  6.85836706e-02  6.91366344e-02
  7.00002429e-02  7.02526662e-02  7.08342796e-02  7.16004467e-02
  7.16782463e-02  7.20158337e-02  7.23590004e-02  7.31295345e-02
  7.47393034e-02  7.64721420e-02  7.67899499e-02  7.67946752e-02
  8.02260882e-02  8.02932790e-02  8.27147956e-02  8.34539459e-02
  8.42595063e-02  8.47123737e-02  8.51453376e-02  8.72792409e-02
  8.74441743e-02  8.82204979e-02  9.12313671e-02  9.26961257e-02
  9.37631768e-02  9.38150527e-02  9.42401877e-02  9.42868274e-02
  9.43008216e-02  9.46602544e-02  9.51332554e-02  9.52558973e-02
  9.55321580e-02  9.55420011e-02  9.58860421e-02  9.60643919e-02
  9.61148257e-02  9.63612728e-02  9.76503956e-02  9.80679342e-02
  9.80884478e-02  9.92926149e-02  9.97283892e-02  9.97762806e-02
  9.98971981e-02  1.00389869e-01  1.00753393e-01  1.01380495e-01
  1.02694170e-01  1.02987464e-01  1.03580217e-01  1.03702144e-01
  1.04281384e-01  1.04876572e-01  1.05041049e-01  1.06261972e-01
  1.06400874e-01  1.07058301e-01  1.08106498e-01  1.08245352e-01
  1.08293153e-01  1.08754138e-01  1.11817792e-01  1.12133831e-01
  1.12603561e-01  1.14572887e-01  1.15698747e-01  1.16788977e-01
  1.16897225e-01  1.17785901e-01  1.18454349e-01  1.19491991e-01
  1.20492918e-01  1.21452449e-01  1.21927670e-01  1.22377200e-01
  1.22779884e-01  1.23482359e-01  1.26720351e-01  1.26994410e-01
  1.27071380e-01  1.27662494e-01  1.28008122e-01  1.29342301e-01
  1.29715039e-01  1.29878154e-01  1.33090600e-01  1.34340275e-01
  1.34721171e-01  1.34983962e-01  1.35259453e-01  1.35703251e-01
  1.35795092e-01  1.36161404e-01  1.36205689e-01  1.36707786e-01
  1.37713025e-01  1.39017712e-01  1.39159170e-01  1.40097029e-01
  1.41865148e-01  1.43664933e-01  1.44170242e-01  1.44365919e-01
  1.45844031e-01  1.49066986e-01  1.50500199e-01  1.50677843e-01
  1.50799342e-01  1.53287726e-01  1.53786671e-01  1.54528611e-01
  1.54840758e-01  1.55104981e-01  1.55293081e-01  1.55379894e-01
  1.55430398e-01  1.56569316e-01  1.56996541e-01  1.57089723e-01
  1.57113029e-01  1.59297018e-01  1.60671695e-01  1.63391201e-01
  1.64028619e-01  1.64311984e-01  1.65011976e-01  1.66608527e-01
  1.68632404e-01  1.69344808e-01  1.69595674e-01  1.70892612e-01
  1.72278730e-01  1.72360854e-01  1.72879574e-01  1.74420326e-01
  1.74621880e-01  1.76931843e-01  1.77820188e-01  1.78716244e-01
  1.79148942e-01  1.79248254e-01  1.80532184e-01  1.81404349e-01
  1.82204387e-01  1.83634200e-01  1.83953477e-01  1.84354966e-01
  1.84990783e-01  1.85689884e-01  1.86580131e-01  1.87324272e-01
  1.87433258e-01  1.88510003e-01  1.90779444e-01  1.91263813e-01
  1.91364014e-01  1.91806025e-01  1.91972828e-01  1.92773288e-01
  1.96203940e-01  1.96546348e-01  1.98572916e-01  1.99999163e-01
  2.02076481e-01  2.04576755e-01  2.05456038e-01  2.09370610e-01
  2.10722491e-01  2.13728521e-01  2.15964819e-01  2.17605985e-01
  2.21742431e-01  2.23321106e-01  2.24005816e-01  2.24098021e-01
  2.24509162e-01  2.25279234e-01  2.25631286e-01  2.26001557e-01
  2.26090766e-01  2.26150183e-01  2.29475374e-01  2.30230996e-01
  2.31053782e-01  2.32921362e-01  2.33032921e-01  2.33396102e-01
  2.34630346e-01  2.36114323e-01  2.40293558e-01  2.42176841e-01
  2.42419441e-01  2.44044640e-01  2.45568194e-01  2.47787745e-01
  2.47861942e-01  2.50223710e-01  2.50335427e-01  2.51086827e-01
  2.51235695e-01  2.51895840e-01  2.53537858e-01  2.54390105e-01
  2.54581362e-01  2.57041032e-01  2.58828921e-01  2.59733618e-01
  2.63447946e-01  2.66839552e-01  2.67385856e-01  2.68017066e-01
  2.68956866e-01  2.71456318e-01  2.72152946e-01  2.74898045e-01
  2.79666831e-01  2.79992888e-01  2.82941651e-01  2.83327670e-01
  2.84173034e-01  2.87744231e-01  2.92586079e-01  2.99680887e-01
  3.01013726e-01  3.05723654e-01  3.05800819e-01  3.06876764e-01
  3.09642001e-01  3.15442557e-01  3.19724345e-01  3.20051803e-01
  3.26506361e-01  3.29438413e-01  3.29988024e-01  3.30994888e-01
  3.36524558e-01  3.48744184e-01  3.56738263e-01  3.63812799e-01
  3.69015279e-01  3.73635012e-01  3.74080103e-01  3.84161469e-01
  4.13743612e-01  4.16355092e-01  4.68432447e-01  4.76566814e-01
  5.28670543e-01]

  warnings.warn(

2022-11-03 10:51:07,207:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.96564193e-01 -4.87326750e-01 -4.67216905e-01 -4.65987679e-01
 -4.61416224e-01 -4.56167051e-01 -4.53070517e-01 -4.47952414e-01
 -4.46353290e-01 -4.43848193e-01 -4.42209122e-01 -4.36200171e-01
 -4.34214853e-01 -4.30207021e-01 -4.28268870e-01 -4.22497591e-01
 -4.22484473e-01 -4.19471067e-01 -4.14008357e-01 -4.04502050e-01
 -4.02594793e-01 -4.02363254e-01 -3.97605362e-01 -3.95840537e-01
 -3.95608177e-01 -3.95590080e-01 -3.95096231e-01 -3.93550519e-01
 -3.90732116e-01 -3.90187371e-01 -3.86584880e-01 -3.80477528e-01
 -3.78169072e-01 -3.73547988e-01 -3.71098116e-01 -3.69053213e-01
 -3.67206832e-01 -3.65813952e-01 -3.53179563e-01 -3.51127101e-01
 -3.48955169e-01 -3.47301004e-01 -3.46809549e-01 -3.41778283e-01
 -3.32157965e-01 -3.22101554e-01 -3.18934385e-01 -3.11917703e-01
 -3.08356333e-01 -3.06032817e-01 -3.02269899e-01 -3.00635035e-01
 -2.99522067e-01 -2.96302058e-01 -2.95321473e-01 -2.93255700e-01
 -2.89278220e-01 -2.76844244e-01 -2.73445912e-01 -2.69138801e-01
 -2.68014621e-01 -2.63943405e-01 -2.59714244e-01 -2.53547834e-01
 -2.51731787e-01 -2.49799100e-01 -2.46834531e-01 -2.41360275e-01
 -2.40359733e-01 -2.39409039e-01 -2.29482274e-01 -2.22896934e-01
 -2.21739993e-01 -2.21629116e-01 -2.21359237e-01 -2.15795035e-01
 -2.10629826e-01 -2.10015110e-01 -2.09617409e-01 -2.06335751e-01
 -2.06009794e-01 -2.03172423e-01 -1.99726736e-01 -1.97737928e-01
 -1.97727055e-01 -1.95836201e-01 -1.93745878e-01 -1.89262612e-01
 -1.87545277e-01 -1.84706793e-01 -1.81559688e-01 -1.80068492e-01
 -1.76872246e-01 -1.76161521e-01 -1.75660139e-01 -1.74823018e-01
 -1.73509295e-01 -1.73283699e-01 -1.71122861e-01 -1.65992343e-01
 -1.64495918e-01 -1.61454494e-01 -1.60944675e-01 -1.51894020e-01
 -1.51019103e-01 -1.50767436e-01 -1.47696529e-01 -1.47352508e-01
 -1.42866639e-01 -1.38383666e-01 -1.37074967e-01 -1.36884562e-01
 -1.34541898e-01 -1.33566750e-01 -1.26675527e-01 -1.25280372e-01
 -1.24480948e-01 -1.24272137e-01 -1.22388996e-01 -1.19010702e-01
 -1.15730012e-01 -1.15261049e-01 -1.14714775e-01 -1.11126029e-01
 -1.06136771e-01 -1.05894615e-01 -1.02221632e-01 -1.00636255e-01
 -1.00315130e-01 -9.53494795e-02 -9.44548696e-02 -9.41121678e-02
 -8.25475930e-02 -7.65895177e-02 -7.45372043e-02 -7.39423236e-02
 -7.38807231e-02 -7.30271530e-02 -7.13232876e-02 -6.90154937e-02
 -6.78697121e-02 -6.76330779e-02 -6.29997717e-02 -5.11596373e-02
 -5.03941117e-02 -4.94041501e-02 -4.91698762e-02 -4.69817698e-02
 -4.62364996e-02 -4.50529993e-02 -4.42676662e-02 -4.25884721e-02
 -3.65807113e-02 -3.62003007e-02 -3.44166971e-02 -3.17094367e-02
 -2.97422531e-02 -2.92844330e-02 -2.86454517e-02 -2.81988254e-02
 -2.75586539e-02 -2.73840578e-02 -2.67179154e-02 -2.66217111e-02
 -2.62238437e-02 -2.54021085e-02 -1.85163963e-02 -1.71321138e-02
 -1.43052767e-02 -1.21429936e-02 -4.29403869e-03 -3.02532316e-03
 -5.50729737e-04  5.44022506e-03  6.11818227e-03  7.24127436e-03
  7.64401882e-03  1.05987730e-02  1.34753178e-02  1.35711801e-02
  2.46803678e-02  3.04471955e-02  3.15791878e-02  3.37114361e-02
  3.44897919e-02  3.51201703e-02  3.57614574e-02  4.17771775e-02
  4.26313856e-02  4.53407169e-02  4.56778982e-02  4.58588336e-02
  4.88956691e-02  5.13751377e-02  5.23307002e-02  5.31973683e-02
  5.39494072e-02  5.46362239e-02  5.51901079e-02  5.67889400e-02
  6.03048589e-02  6.05265845e-02  6.12630919e-02  6.18334958e-02
  6.18379999e-02  6.38900400e-02  7.02138363e-02  7.42980677e-02
  7.63564569e-02  7.73384413e-02  8.13982006e-02  8.53144437e-02
  8.85427769e-02  8.99488025e-02  9.02098446e-02  9.03063432e-02
  9.12392895e-02  9.23464941e-02  9.51238957e-02  9.52606402e-02
  9.58694610e-02  9.83929054e-02  9.91489388e-02  1.02474863e-01
  1.06237093e-01  1.07633852e-01  1.10973353e-01  1.16637692e-01
  1.17529861e-01  1.22589712e-01  1.24395346e-01  1.27510078e-01
  1.30761541e-01  1.31979835e-01  1.32157443e-01  1.33183301e-01
  1.36644848e-01  1.41679148e-01  1.43571543e-01  1.47194282e-01
  1.48400927e-01  1.49404216e-01  1.50475087e-01  1.50741818e-01
  1.56045371e-01  1.66097755e-01  1.69572264e-01  1.69883348e-01
  1.73204107e-01  1.76497377e-01  1.78501922e-01  1.80477923e-01
  1.82096326e-01  1.83293525e-01  1.84232315e-01  1.84895243e-01
  1.85832245e-01  1.86032147e-01  1.86077806e-01  1.87703161e-01
  1.92148942e-01  1.93042098e-01  1.96150231e-01  1.97616788e-01
  2.00248374e-01  2.01784352e-01  2.02773996e-01  2.03616681e-01
  2.04745377e-01  2.08442330e-01  2.09715904e-01  2.10978699e-01
  2.13381839e-01  2.15902645e-01  2.16257455e-01  2.17176724e-01
  2.19899351e-01  2.20317288e-01  2.21171184e-01  2.24595098e-01
  2.25113119e-01  2.25285790e-01  2.27441464e-01  2.30379183e-01
  2.33186434e-01  2.34918502e-01  2.34958911e-01  2.35245060e-01
  2.35354954e-01  2.37454186e-01  2.38101731e-01  2.39335062e-01
  2.40691843e-01  2.46424041e-01  2.57841836e-01  2.58981194e-01
  2.59578410e-01  2.59674883e-01  2.59964625e-01  2.67636833e-01
  2.70234562e-01  2.70448881e-01  2.70808518e-01  2.74541442e-01
  2.75175613e-01  2.78837165e-01  2.82385628e-01  2.84049352e-01
  2.84135758e-01  2.85218024e-01  2.86312720e-01  2.87404010e-01
  2.88914114e-01  2.90924540e-01  2.92323369e-01  2.94075046e-01
  2.95430551e-01  2.98891533e-01  3.00444630e-01  3.00968900e-01
  3.02430640e-01  3.03796326e-01  3.07347264e-01  3.08100262e-01
  3.09269371e-01  3.10015690e-01  3.12581613e-01  3.17120741e-01
  3.17915217e-01  3.18007898e-01  3.18066455e-01  3.18881727e-01
  3.22863843e-01  3.30758769e-01  3.31067913e-01  3.41518101e-01
  3.42697664e-01  3.44228441e-01  3.54245028e-01  3.54287121e-01
  3.56551239e-01  3.59660863e-01  3.66945897e-01  3.67596874e-01
  3.69841870e-01  3.71140973e-01  3.72602528e-01  3.74009843e-01
  3.74059410e-01  3.75138914e-01  3.78111922e-01  3.78267163e-01
  3.78989020e-01  3.85027775e-01  3.86370054e-01  3.89563877e-01
  3.89706861e-01  3.93127505e-01  3.99230489e-01  4.00242990e-01
  4.04712973e-01  4.05517872e-01  4.05963200e-01  4.06342108e-01
  4.06892279e-01  4.09031582e-01  4.09140940e-01  4.10377887e-01
  4.10645458e-01  4.11523366e-01  4.15951886e-01  4.18792487e-01
  4.20492324e-01  4.23431289e-01  4.29067470e-01  4.38413556e-01
  4.40961296e-01  4.41599245e-01  4.43167623e-01  4.45966443e-01
  4.52419653e-01  4.53866841e-01  4.58116228e-01  4.59118005e-01
  4.61023936e-01  4.62848129e-01  4.68024317e-01  4.68108166e-01
  4.69418596e-01  4.72477304e-01  4.75069155e-01  4.75232496e-01
  4.76143326e-01  4.78485984e-01  4.79297425e-01  4.80776257e-01
  4.81826089e-01  4.84249364e-01  4.86203532e-01  4.86301732e-01
  4.86887491e-01  4.87593147e-01  4.88832160e-01  4.89057995e-01
  4.90654992e-01  4.92148721e-01  4.93807002e-01  5.00243714e-01
  5.01065793e-01  5.03639328e-01  5.08026557e-01  5.08315382e-01
  5.09817155e-01  5.12006073e-01  5.12698610e-01  5.13446454e-01
  5.17845912e-01  5.19060004e-01  5.20345579e-01  5.25832048e-01
  5.28566029e-01  5.34492812e-01  5.35825722e-01  5.38298341e-01
  5.39868927e-01  5.40422911e-01  5.42535255e-01  5.49002167e-01
  5.49815045e-01  5.50120943e-01  5.50214848e-01  5.54458872e-01
  5.54744221e-01  5.62850171e-01  5.65117680e-01  5.70751069e-01
  5.74644868e-01  5.75567643e-01  5.75672880e-01  5.76034307e-01
  5.80575355e-01  5.81612333e-01  5.82141528e-01  5.86341930e-01
  5.86985314e-01  5.87135127e-01  5.97313337e-01  5.99986433e-01
  6.03737836e-01  6.05696345e-01  6.07906550e-01  6.08104988e-01
  6.09278927e-01  6.11447718e-01  6.20615313e-01  6.21148134e-01
  6.21260134e-01  6.21340175e-01  6.22258038e-01  6.22559360e-01
  6.22792405e-01  6.27398830e-01  6.29735862e-01  6.34607541e-01
  6.36535898e-01  6.39074312e-01  6.39745371e-01  6.46856598e-01
  6.46974714e-01  6.53115847e-01  6.58911573e-01  6.62796233e-01
  6.63369725e-01  6.75990793e-01  6.76854164e-01  6.86423626e-01
  6.91270571e-01  6.97613846e-01  7.12417428e-01  7.16531056e-01
  7.16560606e-01  7.20704801e-01  7.20802527e-01  7.23024738e-01
  7.28718123e-01  7.31761543e-01  7.36147185e-01  7.37432874e-01
  7.88480113e-01  7.91980393e-01  7.96951436e-01  8.00872618e-01
  8.05872116e-01]

  warnings.warn(

2022-11-03 10:51:07,337:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.11430245 -0.08376289 -0.08073702  0.00426243  0.01545966  0.01592205
  0.02243004  0.03339882  0.04098552  0.04339201  0.04622376  0.0476431
  0.05072223  0.06310452  0.06521058  0.07487437  0.07496497  0.0839122
  0.08748541  0.08796663  0.09009781  0.09568949  0.09725874  0.09824632
  0.10161709  0.10320753  0.10524877  0.10875188  0.11124141  0.11223418
  0.11548138  0.12068095  0.12130676  0.1239269   0.1249544   0.12542389
  0.12619479  0.12824394  0.13061877  0.13269334  0.13329459  0.1360657
  0.1388242   0.14360332  0.14364489  0.14383532  0.14518529  0.14770919
  0.14793936  0.14795969  0.14805214  0.15027198  0.15191618  0.15786142
  0.15902446  0.1595346   0.16000422  0.1624764   0.16434214  0.16681774
  0.16863602  0.17042343  0.1718224   0.17240962  0.17271642  0.17324314
  0.17334222  0.17346561  0.17496536  0.17691094  0.17859584  0.17995315
  0.18174064  0.18334288  0.18391455  0.18477971  0.1851092   0.18644304
  0.18661793  0.18677538  0.18703739  0.18760011  0.18764968  0.18786143
  0.1883241   0.1894549   0.19206519  0.19257589  0.19286924  0.19341957
  0.19667798  0.19678324  0.19712473  0.19870015  0.20125528  0.2022187
  0.20260304  0.20464631  0.20574648  0.20981009  0.21162267  0.21235002
  0.21241446  0.21324603  0.2152298   0.21587328  0.21652676  0.21686683
  0.21785719  0.21940896  0.22253734  0.2238043   0.22527037  0.22600478
  0.2262284   0.22671385  0.22780919  0.22812263  0.22821799  0.22848618
  0.23008411  0.23202387  0.23266828  0.2352961   0.23547045  0.23608622
  0.23661612  0.23687109  0.23733149  0.23893126  0.23912657  0.23979173
  0.24076803  0.24094683  0.24259166  0.24321529  0.24420582  0.24510305
  0.2471556   0.24723975  0.24743253  0.24814297  0.25218335  0.25224439
  0.25233545  0.25269961  0.25296426  0.25380719  0.25397274  0.25438139
  0.25636752  0.2568052   0.25681718  0.25827118  0.25850519  0.2591305
  0.2595466   0.25987524  0.25993823  0.26008344  0.26014504  0.26126364
  0.26188614  0.26308916  0.26313826  0.26321045  0.26473059  0.26515647
  0.26554056  0.26647159  0.26778522  0.27143114  0.27345243  0.27437524
  0.27496477  0.27513092  0.27550121  0.27645831  0.27677929  0.2778093
  0.28045353  0.28244503  0.28377522  0.28675979  0.28847871  0.29010992
  0.29284372  0.29421084  0.29478399  0.29535475  0.29670878  0.29672289
  0.29707352  0.29736118  0.29748387  0.29772893  0.29775665  0.30055862
  0.30104308  0.30108998  0.30139595  0.30165883  0.30192963  0.3031357
  0.30383801  0.30494776  0.30501697  0.30553665  0.30898514  0.30933105
  0.30975644  0.31022099  0.31107209  0.31141987  0.31155331  0.3130695
  0.31317487  0.31320386  0.31665518  0.3172065   0.31890399  0.32116012
  0.32121187  0.32162993  0.32306364  0.32328483  0.32335389  0.3236701
  0.32627856  0.32681691  0.32701028  0.32734233  0.32829321  0.32877261
  0.32885727  0.32952426  0.32976787  0.3300279   0.33097508  0.33101174
  0.33128834  0.33224228  0.33245941  0.33304351  0.33310234  0.33480187
  0.33761666  0.33833128  0.33862709  0.33936404  0.34004718  0.34008731
  0.34043452  0.3412973   0.34452185  0.34556479  0.34580686  0.3481591
  0.34830123  0.34917052  0.34974662  0.35048009  0.35400056  0.35465514
  0.35491149  0.35502487  0.35602112  0.35639381  0.35696786  0.35890359
  0.36010704  0.3626382   0.36419764  0.36501316  0.36512984  0.36513179
  0.36539061  0.36607331  0.36638419  0.36643222  0.3665264   0.36744501
  0.36831649  0.36966677  0.3698866   0.37018503  0.37028039  0.37044682
  0.37048732  0.37171503  0.37184728  0.37213992  0.37232805  0.37426984
  0.37598042  0.37760849  0.37793546  0.37877521  0.37922372  0.37940249
  0.37950256  0.38142575  0.3817058   0.38187554  0.38190002  0.38305999
  0.38310883  0.38320085  0.38468878  0.38485367  0.38501147  0.38591536
  0.38633959  0.3865852   0.38807516  0.38819226  0.38858761  0.38877741
  0.39043457  0.39046274  0.39463579  0.39511746  0.39633672  0.39734407
  0.39790585  0.39897437  0.39956654  0.40010038  0.40168104  0.40229955
  0.4048175   0.40504106  0.40690454  0.40778724  0.4078491   0.40855745
  0.41160163  0.41175806  0.41303058  0.41531231  0.41720262  0.41888302
  0.41950757  0.42075524  0.4223397   0.42310906  0.42311062  0.42332013
  0.42366986  0.42588692  0.42606332  0.42657292  0.42673814  0.42930714
  0.43035013  0.4313004   0.43403267  0.43517764  0.43667139  0.43709392
  0.43816492  0.43910918  0.44058014  0.44086293  0.44493842  0.44520797
  0.44547104  0.44571662  0.44612245  0.44623889  0.44626238  0.44683385
  0.44703669  0.44743189  0.44830874  0.45010931  0.45050773  0.45202918
  0.45207687  0.45343689  0.45669374  0.4575377   0.4575749   0.45783584
  0.45857449  0.45882572  0.45926431  0.46026517  0.46037429  0.46076933
  0.4615678   0.46388724  0.4641076   0.46443625  0.46449057  0.46453033
  0.46572425  0.46575281  0.4674524   0.46813178  0.46843954  0.46907505
  0.47105063  0.47135345  0.47563517  0.47576355  0.47664898  0.47794048
  0.47958759  0.48110056  0.48145317  0.48410542  0.48461302  0.48539952
  0.48568871  0.48690871  0.48696131  0.48804748  0.49075295  0.490781
  0.49167597  0.49340529  0.49447839  0.49468378  0.49498098  0.49620643
  0.49670313  0.49978267  0.50124323  0.50305967  0.50410338  0.50609911
  0.50749063  0.50797152  0.50836186  0.50866604  0.51072899  0.51368766
  0.51413374  0.51512647  0.51535935  0.51671886  0.51808888  0.51858928
  0.51966827  0.52264987  0.52316618  0.52644257  0.52736986  0.53022092
  0.53471987  0.53563412  0.53608561  0.53859191  0.53940994  0.54892387
  0.54937248  0.55016736  0.55073153  0.55412344  0.55644577  0.55761066
  0.56420129  0.56487493  0.56518989  0.56767163  0.56842807  0.57027706
  0.57145812  0.57244907  0.58029166  0.581831    0.58257167  0.59138325
  0.59357867  0.59604671  0.60207406  0.60615238  0.60966489  0.61290163
  0.61909909  0.64398624  0.6539714   0.65720322  0.66134522  0.66419215
  0.66795401  0.67351027  0.67886143  0.68451892  0.70667967  0.71094901
  0.7757262 ]

  warnings.warn(

2022-11-03 10:51:07,372:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.61239494e-01 -4.58656194e-01 -4.19557369e-01 -4.17774124e-01
 -3.96496142e-01 -3.93337678e-01 -3.90881197e-01 -3.87961861e-01
 -3.79110304e-01 -3.78796280e-01 -3.71115601e-01 -3.70639638e-01
 -3.70016758e-01 -3.69837005e-01 -3.66323277e-01 -3.56381787e-01
 -3.54307121e-01 -3.50713405e-01 -3.48173551e-01 -3.33543965e-01
 -3.27826850e-01 -3.27815688e-01 -3.26272282e-01 -3.25876546e-01
 -3.21153595e-01 -3.20544844e-01 -3.12674708e-01 -3.11225979e-01
 -3.11200888e-01 -3.09659312e-01 -3.07921453e-01 -3.07412524e-01
 -3.07258742e-01 -3.07084309e-01 -3.05728349e-01 -3.00166351e-01
 -2.96616822e-01 -2.95283872e-01 -2.92281349e-01 -2.89593629e-01
 -2.89201521e-01 -2.89076987e-01 -2.88402735e-01 -2.83960509e-01
 -2.83708911e-01 -2.74939188e-01 -2.74757211e-01 -2.72482002e-01
 -2.70555363e-01 -2.68881946e-01 -2.68649921e-01 -2.68334282e-01
 -2.66932287e-01 -2.63192315e-01 -2.61642640e-01 -2.60297808e-01
 -2.59767226e-01 -2.58453286e-01 -2.54097032e-01 -2.51176025e-01
 -2.41819316e-01 -2.37015558e-01 -2.34637187e-01 -2.33916891e-01
 -2.31383643e-01 -2.31213754e-01 -2.31021632e-01 -2.30966231e-01
 -2.29639899e-01 -2.23088821e-01 -2.21192111e-01 -2.15628417e-01
 -2.14198779e-01 -2.12267941e-01 -2.08835111e-01 -2.08354995e-01
 -2.08266400e-01 -2.06824014e-01 -2.06042760e-01 -2.05922632e-01
 -2.05826829e-01 -2.04667334e-01 -1.95488033e-01 -1.94522358e-01
 -1.86023630e-01 -1.85632780e-01 -1.81276355e-01 -1.79602274e-01
 -1.72970039e-01 -1.68318197e-01 -1.62665457e-01 -1.62325035e-01
 -1.60680395e-01 -1.56012135e-01 -1.55277922e-01 -1.54053550e-01
 -1.53128469e-01 -1.51974894e-01 -1.48178868e-01 -1.46503403e-01
 -1.46457332e-01 -1.46057500e-01 -1.40496429e-01 -1.40074493e-01
 -1.39662044e-01 -1.33567092e-01 -1.30211828e-01 -1.29119183e-01
 -1.28297053e-01 -1.27316258e-01 -1.26433057e-01 -1.25565214e-01
 -1.25368402e-01 -1.25261065e-01 -1.22883714e-01 -1.21234599e-01
 -1.16782459e-01 -1.15697570e-01 -1.15151193e-01 -1.12058170e-01
 -1.11741193e-01 -1.08743586e-01 -1.05357561e-01 -1.02492913e-01
 -9.53862817e-02 -9.52971159e-02 -9.21301720e-02 -9.01584580e-02
 -8.83722040e-02 -8.70355190e-02 -8.43818463e-02 -8.42560575e-02
 -8.22682615e-02 -8.21690806e-02 -8.08180736e-02 -8.03692194e-02
 -7.90761884e-02 -7.84829829e-02 -7.81513352e-02 -7.71932837e-02
 -7.70948311e-02 -7.56589544e-02 -7.52237332e-02 -7.21132554e-02
 -7.17043882e-02 -7.05908546e-02 -6.87838095e-02 -6.86422537e-02
 -6.86030330e-02 -6.75657545e-02 -6.73255772e-02 -6.62286549e-02
 -6.57403962e-02 -6.45435587e-02 -6.37031315e-02 -6.13077882e-02
 -6.06687050e-02 -5.98832048e-02 -5.97055032e-02 -5.29380382e-02
 -5.23456827e-02 -5.12379050e-02 -5.01711740e-02 -4.96994990e-02
 -4.92743587e-02 -4.80688806e-02 -4.76596215e-02 -4.71778880e-02
 -4.63780527e-02 -4.61766283e-02 -4.58866383e-02 -4.51277744e-02
 -4.41606254e-02 -4.34637007e-02 -4.22602012e-02 -4.16668315e-02
 -4.03553906e-02 -4.01129924e-02 -4.00225161e-02 -3.98433476e-02
 -3.90911417e-02 -3.82614548e-02 -3.46107912e-02 -3.39392300e-02
 -3.24835986e-02 -3.20132791e-02 -3.18163980e-02 -3.17978060e-02
 -3.15089005e-02 -3.08535626e-02 -3.05695626e-02 -3.05437261e-02
 -3.02479969e-02 -3.01537099e-02 -2.99906864e-02 -2.97552034e-02
 -2.95485618e-02 -2.93682714e-02 -2.92034354e-02 -2.86559154e-02
 -2.81761142e-02 -2.33580983e-02 -2.33253395e-02 -2.29292790e-02
 -2.23986168e-02 -2.18318286e-02 -1.98436007e-02 -1.96345462e-02
 -1.89146866e-02 -1.86269520e-02 -1.80844741e-02 -1.79155582e-02
 -1.63709112e-02 -1.57261021e-02 -1.50099541e-02 -1.42764825e-02
 -1.39532508e-02 -1.33781486e-02 -1.19658847e-02 -1.16492855e-02
 -1.07765212e-02 -1.06862085e-02 -9.97372298e-03 -8.40354073e-03
 -8.20441213e-03 -6.75576733e-03 -6.50403690e-03 -4.45431180e-03
 -2.99457910e-03 -2.79605816e-03 -2.36035471e-03 -2.22749128e-03
 -1.70381313e-03  3.21549664e-04  9.81541066e-04  2.06038188e-03
  2.50904742e-03  5.26061114e-03  6.10701157e-03  7.22334794e-03
  7.99289573e-03  8.40463315e-03  8.90746092e-03  1.06780311e-02
  1.07884852e-02  1.12444981e-02  1.25598071e-02  1.30094684e-02
  1.33979887e-02  1.49770123e-02  1.60633403e-02  1.78322858e-02
  1.84549454e-02  1.92153349e-02  2.06855669e-02  2.09424287e-02
  2.21312340e-02  2.26876755e-02  2.34369583e-02  2.40378864e-02
  2.48023432e-02  2.51914617e-02  2.58554878e-02  2.61798265e-02
  2.62813657e-02  2.65036266e-02  2.70754666e-02  2.77200349e-02
  2.83443372e-02  3.07194748e-02  3.11817882e-02  3.12610326e-02
  3.26314197e-02  3.34542024e-02  3.45603860e-02  3.49773311e-02
  3.52082293e-02  3.54937852e-02  3.59045561e-02  3.61927963e-02
  3.63908331e-02  3.69971337e-02  3.93271218e-02  4.11007631e-02
  4.23794011e-02  4.30977600e-02  4.46116849e-02  4.46147712e-02
  4.52600849e-02  4.61153190e-02  4.68205864e-02  4.74909931e-02
  4.91381793e-02  4.99284732e-02  5.22527023e-02  5.26131635e-02
  5.27083741e-02  5.36156900e-02  5.37066987e-02  5.40531121e-02
  5.52827171e-02  5.77095296e-02  5.80514277e-02  5.83897972e-02
  6.15523095e-02  6.16737674e-02  6.29475882e-02  6.47667965e-02
  6.54002147e-02  6.65208747e-02  6.72185338e-02  7.04138126e-02
  7.07979900e-02  7.15546767e-02  7.46179328e-02  7.55680690e-02
  7.62381533e-02  8.12186450e-02  8.16700341e-02  8.29281802e-02
  8.32937111e-02  8.45721561e-02  8.86278451e-02  8.96310958e-02
  8.98367403e-02  9.37866766e-02  9.49212325e-02  9.52111165e-02
  9.62218282e-02  9.74262137e-02  9.99996065e-02  1.00395994e-01
  1.01150926e-01  1.01892711e-01  1.02030204e-01  1.02624348e-01
  1.03142577e-01  1.03166959e-01  1.03524347e-01  1.03784067e-01
  1.04635954e-01  1.05220197e-01  1.05604949e-01  1.05784541e-01
  1.06321893e-01  1.06454758e-01  1.07466895e-01  1.07681861e-01
  1.09592677e-01  1.12028732e-01  1.12099782e-01  1.12332498e-01
  1.12884630e-01  1.14381340e-01  1.16464968e-01  1.18189397e-01
  1.18777213e-01  1.19631287e-01  1.19850711e-01  1.20098110e-01
  1.20523263e-01  1.21030989e-01  1.22174745e-01  1.24764293e-01
  1.24801082e-01  1.26175390e-01  1.26605368e-01  1.26635586e-01
  1.26738481e-01  1.26744601e-01  1.28075074e-01  1.29948277e-01
  1.30197229e-01  1.30365110e-01  1.31196787e-01  1.31436833e-01
  1.31720363e-01  1.32686031e-01  1.33048344e-01  1.34720414e-01
  1.34754868e-01  1.37046179e-01  1.39785749e-01  1.39924176e-01
  1.40597960e-01  1.41791685e-01  1.41902943e-01  1.41955559e-01
  1.42087484e-01  1.42936161e-01  1.44774193e-01  1.45997882e-01
  1.46734692e-01  1.47196046e-01  1.47785017e-01  1.48275388e-01
  1.49237754e-01  1.51869046e-01  1.53605939e-01  1.55863069e-01
  1.56171712e-01  1.56173037e-01  1.57962954e-01  1.58871370e-01
  1.59627113e-01  1.59677848e-01  1.61405703e-01  1.61651476e-01
  1.65193115e-01  1.68471346e-01  1.69111430e-01  1.72776836e-01
  1.74096496e-01  1.74325349e-01  1.76275315e-01  1.76395882e-01
  1.77223755e-01  1.77592954e-01  1.77773288e-01  1.77808911e-01
  1.79627549e-01  1.80166958e-01  1.81033444e-01  1.81249218e-01
  1.83776904e-01  1.86919487e-01  1.88955749e-01  1.89365601e-01
  1.89609342e-01  1.90033395e-01  1.93422926e-01  2.03260405e-01
  2.04011669e-01  2.04764918e-01  2.07823441e-01  2.08811078e-01
  2.09217623e-01  2.09337192e-01  2.10431379e-01  2.13720047e-01
  2.13820248e-01  2.16650462e-01  2.17083142e-01  2.18367290e-01
  2.19513473e-01  2.23262737e-01  2.23928867e-01  2.24738002e-01
  2.25806497e-01  2.25952276e-01  2.30402367e-01  2.30937344e-01
  2.31098037e-01  2.33082334e-01  2.33767569e-01  2.36373854e-01
  2.40560662e-01  2.40813386e-01  2.43311953e-01  2.44220426e-01
  2.44808079e-01  2.47393966e-01  2.56829701e-01  2.56900129e-01
  2.57282431e-01  2.59011916e-01  2.59640716e-01  2.59720856e-01
  2.70423678e-01  2.75185671e-01  2.77580398e-01  2.77924828e-01
  2.86306895e-01  2.88625216e-01  2.95974744e-01  2.96029590e-01
  3.05119554e-01  3.06309804e-01  3.06685394e-01  3.08641182e-01
  3.08863226e-01  3.10709746e-01  3.10819508e-01  3.11612301e-01
  3.15587867e-01  3.17225807e-01  3.17332806e-01  3.25458892e-01
  3.30344736e-01  3.65505292e-01  4.04465997e-01  4.62433893e-01]

  warnings.warn(

2022-11-03 10:51:07,439:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.88609782e-01 -6.62311032e-01 -6.59146454e-01 -6.46811222e-01
 -6.44481515e-01 -6.43191277e-01 -6.39406497e-01 -6.37208651e-01
 -6.35616353e-01 -6.24273740e-01 -6.24121910e-01 -6.21368724e-01
 -6.18234337e-01 -6.12896354e-01 -6.10503490e-01 -6.04767721e-01
 -5.95415170e-01 -5.90270871e-01 -5.86266978e-01 -5.70869273e-01
 -5.70624147e-01 -5.66289233e-01 -5.51232481e-01 -5.42478628e-01
 -5.41583319e-01 -5.39934412e-01 -5.39435107e-01 -5.38516942e-01
 -5.34154858e-01 -5.28082122e-01 -5.22281808e-01 -5.15480950e-01
 -5.10590858e-01 -5.05802034e-01 -4.90286724e-01 -4.80067683e-01
 -4.76266474e-01 -4.76206642e-01 -4.73390924e-01 -4.73386001e-01
 -4.72413474e-01 -4.70539500e-01 -4.69742973e-01 -4.69083738e-01
 -4.65381878e-01 -4.63016630e-01 -4.61420479e-01 -4.58505321e-01
 -4.57010419e-01 -4.56700046e-01 -4.51839933e-01 -4.51563061e-01
 -4.48174502e-01 -4.47126416e-01 -4.42521310e-01 -4.41252254e-01
 -4.39863984e-01 -4.38977784e-01 -4.36573679e-01 -4.35102974e-01
 -4.30838866e-01 -4.24983529e-01 -4.23253536e-01 -4.21335849e-01
 -4.18185972e-01 -4.11428111e-01 -4.05665834e-01 -4.04498754e-01
 -4.04421714e-01 -4.01329623e-01 -3.98057564e-01 -3.95395728e-01
 -3.94941881e-01 -3.94896238e-01 -3.93823976e-01 -3.92325566e-01
 -3.91667901e-01 -3.91588483e-01 -3.88418408e-01 -3.87637990e-01
 -3.86078051e-01 -3.85645539e-01 -3.85526751e-01 -3.84965439e-01
 -3.83829763e-01 -3.80870645e-01 -3.78393005e-01 -3.77949294e-01
 -3.71053735e-01 -3.68873176e-01 -3.65716368e-01 -3.62761479e-01
 -3.60247301e-01 -3.56684653e-01 -3.55524127e-01 -3.50184653e-01
 -3.49872421e-01 -3.48166660e-01 -3.47860317e-01 -3.47471599e-01
 -3.46209979e-01 -3.46136783e-01 -3.43211495e-01 -3.42468490e-01
 -3.37803842e-01 -3.36264704e-01 -3.35796971e-01 -3.35210709e-01
 -3.31301097e-01 -3.30078781e-01 -3.23161200e-01 -3.18731805e-01
 -3.18716434e-01 -3.14676388e-01 -3.14152341e-01 -3.14098219e-01
 -3.12699317e-01 -3.09948781e-01 -3.03264281e-01 -3.01897613e-01
 -2.99820666e-01 -2.98190972e-01 -2.96238841e-01 -2.95988005e-01
 -2.89694342e-01 -2.89017681e-01 -2.88759769e-01 -2.84487431e-01
 -2.84135971e-01 -2.72067723e-01 -2.70043242e-01 -2.66460962e-01
 -2.61252643e-01 -2.61216842e-01 -2.58964027e-01 -2.57303419e-01
 -2.56524869e-01 -2.56473755e-01 -2.53506169e-01 -2.52389410e-01
 -2.47604695e-01 -2.46676442e-01 -2.46344437e-01 -2.45499725e-01
 -2.44939171e-01 -2.44014079e-01 -2.42634563e-01 -2.40015172e-01
 -2.38911820e-01 -2.37531869e-01 -2.37403716e-01 -2.35821530e-01
 -2.30584366e-01 -2.27981861e-01 -2.27636504e-01 -2.18439325e-01
 -2.14294039e-01 -2.13749194e-01 -2.11319799e-01 -2.11226540e-01
 -2.11029630e-01 -2.09451522e-01 -2.08063637e-01 -2.06482384e-01
 -2.03692124e-01 -2.00315086e-01 -1.99864986e-01 -1.99554855e-01
 -1.99042758e-01 -1.97435212e-01 -1.96756939e-01 -1.95594733e-01
 -1.95392353e-01 -1.95006990e-01 -1.94573415e-01 -1.93287909e-01
 -1.91892098e-01 -1.82095979e-01 -1.76633313e-01 -1.76135649e-01
 -1.74006542e-01 -1.63458624e-01 -1.59447619e-01 -1.57967130e-01
 -1.54177576e-01 -1.53462636e-01 -1.53261226e-01 -1.52513116e-01
 -1.49037220e-01 -1.46332931e-01 -1.45212699e-01 -1.43807965e-01
 -1.41543847e-01 -1.41454264e-01 -1.40665086e-01 -1.40558701e-01
 -1.36311378e-01 -1.35769795e-01 -1.35740193e-01 -1.33743939e-01
 -1.31819295e-01 -1.28550439e-01 -1.27550032e-01 -1.25123436e-01
 -1.22253543e-01 -1.20244466e-01 -1.19133986e-01 -1.17317406e-01
 -1.16584691e-01 -1.16514712e-01 -1.15788423e-01 -1.13216205e-01
 -1.11550919e-01 -1.08297843e-01 -1.05728728e-01 -1.04241917e-01
 -1.00793182e-01 -1.00755832e-01 -1.00108760e-01 -9.95454564e-02
 -9.77606449e-02 -9.48476473e-02 -9.21776064e-02 -9.09343507e-02
 -8.82234390e-02 -8.54495798e-02 -8.25004398e-02 -8.17635100e-02
 -8.13334128e-02 -7.64865990e-02 -7.31079448e-02 -7.27902077e-02
 -7.15736702e-02 -6.90365566e-02 -6.82541266e-02 -6.65192578e-02
 -5.68922544e-02 -5.66216258e-02 -5.30747241e-02 -5.04835576e-02
 -4.94613789e-02 -4.73145800e-02 -4.53392399e-02 -4.38475691e-02
 -4.24572175e-02 -4.04946732e-02 -3.83298248e-02 -3.45252891e-02
 -3.21162442e-02 -2.68914041e-02 -2.36737882e-02 -2.09274830e-02
 -1.70736045e-02 -1.55974054e-02 -1.45287616e-02 -1.26755512e-02
 -1.07174698e-02 -9.88647689e-03 -7.93243007e-03 -7.37154482e-03
 -6.24660806e-03 -4.82856634e-03 -4.48360786e-03 -4.00367317e-03
 -3.71389318e-03 -3.17511689e-03 -1.65494480e-03 -3.22672717e-04
  6.98915628e-03  7.89422977e-03  1.63078710e-02  1.72980727e-02
  1.92286731e-02  2.06909628e-02  2.09504027e-02  2.32564526e-02
  2.39908533e-02  2.53707360e-02  2.83314242e-02  2.91032737e-02
  3.24062374e-02  3.36869183e-02  3.48090329e-02  3.67037647e-02
  3.67875450e-02  4.09827822e-02  4.28706530e-02  4.36503281e-02
  4.39637497e-02  4.63007570e-02  4.75680120e-02  5.14540894e-02
  5.42777548e-02  5.48882979e-02  5.52635739e-02  5.57271850e-02
  5.90861573e-02  6.08159382e-02  6.33078195e-02  6.54819743e-02
  6.60026720e-02  6.63956877e-02  6.86753283e-02  7.07103886e-02
  7.15943877e-02  7.18098808e-02  7.89665773e-02  7.90877158e-02
  8.08539186e-02  8.08868112e-02  8.22196925e-02  8.37069020e-02
  8.51696032e-02  8.68394869e-02  8.72617661e-02  8.81255434e-02
  8.94301253e-02  9.04106062e-02  9.08490875e-02  9.46726215e-02
  9.61606619e-02  9.67654829e-02  1.03053819e-01  1.05085449e-01
  1.05103866e-01  1.05745943e-01  1.06610579e-01  1.06823996e-01
  1.06953996e-01  1.07559642e-01  1.08361999e-01  1.09827113e-01
  1.13912176e-01  1.14435142e-01  1.15981701e-01  1.16343543e-01
  1.16532024e-01  1.17251530e-01  1.17724524e-01  1.18068199e-01
  1.18791011e-01  1.19039898e-01  1.20130910e-01  1.21149790e-01
  1.22255048e-01  1.23521349e-01  1.24660811e-01  1.25147541e-01
  1.25618759e-01  1.26022904e-01  1.31236209e-01  1.31716158e-01
  1.33729019e-01  1.34015204e-01  1.34038709e-01  1.35666065e-01
  1.36505611e-01  1.36916780e-01  1.38937049e-01  1.39656118e-01
  1.39913986e-01  1.42622173e-01  1.42801611e-01  1.42903867e-01
  1.43210768e-01  1.44599277e-01  1.44872365e-01  1.45348781e-01
  1.46042981e-01  1.48387404e-01  1.52167082e-01  1.57781170e-01
  1.58451473e-01  1.60476837e-01  1.60811302e-01  1.61042167e-01
  1.62906700e-01  1.63879907e-01  1.67022877e-01  1.68835839e-01
  1.70821718e-01  1.72954594e-01  1.74308284e-01  1.77481401e-01
  1.77809002e-01  1.79557891e-01  1.80525171e-01  1.80648653e-01
  1.81099815e-01  1.82286645e-01  1.83911712e-01  1.84398737e-01
  1.86294614e-01  1.88769833e-01  1.91939218e-01  1.93573013e-01
  1.95553744e-01  1.98491234e-01  2.02869412e-01  2.04602037e-01
  2.06100612e-01  2.07228117e-01  2.08016855e-01  2.09926712e-01
  2.17827257e-01  2.18788941e-01  2.21022883e-01  2.23989935e-01
  2.24606697e-01  2.26253808e-01  2.30455246e-01  2.31144782e-01
  2.31680843e-01  2.33138468e-01  2.34286180e-01  2.37115194e-01
  2.37506766e-01  2.38826860e-01  2.40975724e-01  2.42693754e-01
  2.44957565e-01  2.48149064e-01  2.49662694e-01  2.50157408e-01
  2.51256259e-01  2.55297670e-01  2.55855172e-01  2.57367827e-01
  2.61131795e-01  2.63243219e-01  2.65183739e-01  2.66125762e-01
  2.68406722e-01  2.71368678e-01  2.75862599e-01  2.76010481e-01
  2.76830020e-01  2.78435673e-01  2.87314510e-01  2.88439853e-01
  2.91535902e-01  2.94287012e-01  2.95176731e-01  3.06226513e-01
  3.06327890e-01  3.08599958e-01  3.09575097e-01  3.10581885e-01
  3.11547517e-01  3.15100582e-01  3.15179066e-01  3.16647662e-01
  3.27772842e-01  3.29990002e-01  3.31561756e-01  3.33911345e-01
  3.37123195e-01  3.37472828e-01  3.41788290e-01  3.44472360e-01
  3.45206058e-01  3.45687116e-01  3.46518692e-01  3.46696436e-01
  3.47152143e-01  3.51952342e-01  3.52077173e-01  3.56085779e-01
  3.58236633e-01  3.58569719e-01  3.59808023e-01  3.60414315e-01
  3.65207076e-01  3.65912554e-01  3.66008418e-01  3.67573178e-01
  3.67699540e-01  3.72057529e-01  3.72307858e-01  3.77608099e-01
  3.78632693e-01  3.78851842e-01  3.80668828e-01  3.82595816e-01
  3.91744825e-01  3.93539347e-01  3.98568684e-01  4.01665183e-01
  4.13653845e-01  4.14549989e-01  4.16832101e-01  4.17559139e-01
  4.51231975e-01]

  warnings.warn(

2022-11-03 10:51:09,503:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.44044307e-01 -2.18825527e-01 -2.13646021e-01 -2.10802635e-01
 -2.06264470e-01 -2.05150023e-01 -1.96777606e-01 -1.95939798e-01
 -1.93619847e-01 -1.91045706e-01 -1.80872476e-01 -1.79777519e-01
 -1.77557238e-01 -1.73651809e-01 -1.64835025e-01 -1.63175932e-01
 -1.61015113e-01 -1.59321179e-01 -1.55627595e-01 -1.47616508e-01
 -1.45418252e-01 -1.43880390e-01 -1.42783666e-01 -1.40742266e-01
 -1.36309838e-01 -1.27464975e-01 -1.25768909e-01 -1.25281613e-01
 -1.22703884e-01 -1.22145159e-01 -1.21617442e-01 -9.94266629e-02
 -9.58101265e-02 -9.47027146e-02 -9.10507505e-02 -9.07514394e-02
 -9.01632406e-02 -8.95203857e-02 -8.78684637e-02 -8.59484760e-02
 -7.80981498e-02 -7.17225967e-02 -7.07318850e-02 -6.99222446e-02
 -6.64414137e-02 -6.60848160e-02 -6.36242679e-02 -6.23439378e-02
 -6.17738202e-02 -5.63842853e-02 -5.53339791e-02 -5.42865392e-02
 -5.22952324e-02 -5.22676370e-02 -4.75493425e-02 -4.55636267e-02
 -4.49053304e-02 -4.48678649e-02 -4.41395843e-02 -4.33886337e-02
 -4.03036677e-02 -3.90742235e-02 -3.53611950e-02 -3.28748914e-02
 -2.23314043e-02 -2.17669765e-02 -1.95589696e-02 -1.68283829e-02
 -1.29860922e-02 -1.18092328e-02 -6.35357326e-03 -4.12082819e-03
 -2.72187903e-03  6.56669207e-04  7.66177002e-03  9.90519386e-03
  9.96963392e-03  1.35868431e-02  1.58806964e-02  1.61641114e-02
  2.09133084e-02  2.55249267e-02  2.71743517e-02  2.83882844e-02
  2.85876778e-02  2.86872238e-02  3.03263373e-02  3.34244746e-02
  3.43386170e-02  3.59332217e-02  3.65455835e-02  3.78099080e-02
  4.46613041e-02  4.61048667e-02  4.82474814e-02  4.90403328e-02
  4.93298957e-02  5.48215473e-02  5.85648834e-02  6.11212969e-02
  6.32748075e-02  6.62833740e-02  6.96489214e-02  7.11264914e-02
  7.36469700e-02  7.62631853e-02  7.93276890e-02  8.37561896e-02
  8.38947211e-02  8.81307198e-02  9.63100510e-02  9.97897810e-02
  1.00632923e-01  1.02610803e-01  1.05651576e-01  1.06169603e-01
  1.08199462e-01  1.08470535e-01  1.09376113e-01  1.10308705e-01
  1.12213785e-01  1.13419488e-01  1.14465582e-01  1.14848704e-01
  1.15301595e-01  1.15468320e-01  1.15633112e-01  1.17650094e-01
  1.18966805e-01  1.19246536e-01  1.21115167e-01  1.23064421e-01
  1.24280116e-01  1.25536276e-01  1.27047682e-01  1.29120869e-01
  1.30399158e-01  1.32649370e-01  1.33008418e-01  1.37480311e-01
  1.41232673e-01  1.41479587e-01  1.44339627e-01  1.45852199e-01
  1.47184211e-01  1.47784643e-01  1.48008309e-01  1.49593172e-01
  1.49943334e-01  1.51215115e-01  1.52473609e-01  1.53017480e-01
  1.54128192e-01  1.55091013e-01  1.55569271e-01  1.57187443e-01
  1.57269528e-01  1.57746210e-01  1.62004729e-01  1.63125332e-01
  1.67542635e-01  1.69782957e-01  1.70468028e-01  1.73739676e-01
  1.77211723e-01  1.78684648e-01  1.79010627e-01  1.80075902e-01
  1.81253737e-01  1.83740398e-01  1.85175541e-01  1.85754571e-01
  1.88679906e-01  1.89834144e-01  1.92426816e-01  1.93900295e-01
  1.94365635e-01  1.97460811e-01  1.97602274e-01  2.04289084e-01
  2.06251860e-01  2.07673422e-01  2.08527788e-01  2.12295275e-01
  2.14771812e-01  2.17876029e-01  2.20957820e-01  2.23191089e-01
  2.27541419e-01  2.27641741e-01  2.28620324e-01  2.29338339e-01
  2.29924156e-01  2.30340635e-01  2.30592611e-01  2.32097462e-01
  2.33106360e-01  2.36798764e-01  2.39340058e-01  2.39493475e-01
  2.40367173e-01  2.40846326e-01  2.41346741e-01  2.45221414e-01
  2.45987759e-01  2.46426944e-01  2.46898054e-01  2.47093868e-01
  2.47656625e-01  2.48069000e-01  2.53260274e-01  2.53284731e-01
  2.54245723e-01  2.54662370e-01  2.55920395e-01  2.56062173e-01
  2.56434110e-01  2.57834302e-01  2.58033719e-01  2.62609074e-01
  2.64995567e-01  2.66159489e-01  2.67180331e-01  2.69690590e-01
  2.72183902e-01  2.79601510e-01  2.81863387e-01  2.82292476e-01
  2.83346954e-01  2.92231139e-01  2.93820717e-01  2.95133925e-01
  2.96491525e-01  2.96582132e-01  3.03205795e-01  3.07711560e-01
  3.09386953e-01  3.10403175e-01  3.16273991e-01  3.17498546e-01
  3.17702758e-01  3.20438074e-01  3.20977392e-01  3.21206242e-01
  3.21373388e-01  3.22195840e-01  3.22521442e-01  3.25654165e-01
  3.28618459e-01  3.29373714e-01  3.32437989e-01  3.34055071e-01
  3.35742284e-01  3.38623558e-01  3.40314851e-01  3.43019769e-01
  3.50351941e-01  3.51029588e-01  3.52648638e-01  3.54418923e-01
  3.56279971e-01  3.59706456e-01  3.60722370e-01  3.64529754e-01
  3.67160451e-01  3.67730113e-01  3.74586177e-01  3.76722402e-01
  3.77592634e-01  3.78841890e-01  3.81584635e-01  3.82081751e-01
  3.84757556e-01  3.88867403e-01  3.90244841e-01  3.92105223e-01
  3.94251034e-01  3.94849163e-01  3.95126039e-01  3.96959123e-01
  4.02291161e-01  4.06578070e-01  4.06923355e-01  4.09587700e-01
  4.09678913e-01  4.11772905e-01  4.11839094e-01  4.12879224e-01
  4.13066693e-01  4.20259463e-01  4.22520803e-01  4.24096074e-01
  4.24107441e-01  4.25474638e-01  4.25997238e-01  4.28055729e-01
  4.29490050e-01  4.33845836e-01  4.34773349e-01  4.37161135e-01
  4.38764147e-01  4.40431385e-01  4.42282667e-01  4.43708218e-01
  4.43743788e-01  4.47984321e-01  4.49089566e-01  4.49112742e-01
  4.50156513e-01  4.55652640e-01  4.56263678e-01  4.57930478e-01
  4.58398667e-01  4.61079043e-01  4.62710976e-01  4.64363475e-01
  4.70560726e-01  4.74041288e-01  4.76900684e-01  4.77708032e-01
  4.78469268e-01  4.81649612e-01  4.81669730e-01  4.81726238e-01
  4.82671725e-01  4.86156664e-01  4.88764343e-01  4.90456293e-01
  4.91522983e-01  4.91702612e-01  4.91897330e-01  4.93239706e-01
  4.93546340e-01  4.96884905e-01  5.05610170e-01  5.07887433e-01
  5.08001883e-01  5.09323514e-01  5.12525992e-01  5.16827872e-01
  5.18809378e-01  5.20176177e-01  5.20418018e-01  5.22423650e-01
  5.24982234e-01  5.28003134e-01  5.35195119e-01  5.35659445e-01
  5.38236257e-01  5.39305572e-01  5.39942956e-01  5.41843165e-01
  5.45090111e-01  5.47257830e-01  5.47875787e-01  5.47941303e-01
  5.49759990e-01  5.50182154e-01  5.52180538e-01  5.52918424e-01
  5.53487389e-01  5.53791749e-01  5.53941566e-01  5.54730556e-01
  5.58049764e-01  5.59068596e-01  5.61620821e-01  5.62017922e-01
  5.63657277e-01  5.65910964e-01  5.66522619e-01  5.71038128e-01
  5.73430627e-01  5.73625018e-01  5.73846719e-01  5.82453607e-01
  5.83799630e-01  5.84025325e-01  5.87487588e-01  5.88563572e-01
  5.94109995e-01  5.95892023e-01  5.97477281e-01  6.01649488e-01
  6.04460774e-01  6.06767574e-01  6.09991920e-01  6.10390535e-01
  6.13637256e-01  6.15640558e-01  6.19864705e-01  6.22596895e-01
  6.25043246e-01  6.25764199e-01  6.28616010e-01  6.30202636e-01
  6.31774966e-01  6.32710218e-01  6.34123675e-01  6.34585696e-01
  6.36935338e-01  6.43009571e-01  6.43576373e-01  6.44286581e-01
  6.47428968e-01  6.51071035e-01  6.54092676e-01  6.54110454e-01
  6.55483691e-01  6.57240714e-01  6.58782895e-01  6.59040732e-01
  6.59262636e-01  6.63161890e-01  6.64080773e-01  6.66114393e-01
  6.67816645e-01  6.68435509e-01  6.69106676e-01  6.73869586e-01
  6.74421918e-01  6.75095679e-01  6.75597742e-01  6.79763769e-01
  6.85531677e-01  6.87263455e-01  6.87932762e-01  6.90652007e-01
  6.94614394e-01  6.95147299e-01  6.96030574e-01  6.96691294e-01
  7.00170176e-01  7.00296190e-01  7.02937708e-01  7.03613538e-01
  7.05133547e-01  7.15471605e-01  7.19036399e-01  7.20611046e-01
  7.20805146e-01  7.21206598e-01  7.26157156e-01  7.31979627e-01
  7.33857290e-01  7.35935787e-01  7.38668391e-01  7.41145804e-01
  7.42774015e-01  7.44584781e-01  7.47473397e-01  7.49598213e-01
  7.57380025e-01  7.57530871e-01  7.57974840e-01  7.58100145e-01
  7.59637669e-01  7.61704923e-01  7.63367503e-01  7.66791785e-01
  7.69426718e-01  7.70531616e-01  7.72921809e-01  7.73829347e-01
  7.76958577e-01  7.80901380e-01  7.81296526e-01  7.82452121e-01
  7.85124358e-01  7.87755960e-01  7.89367707e-01  7.99710687e-01
  8.12814106e-01  8.17705104e-01  8.21288801e-01  8.21911463e-01
  8.23579715e-01  8.24367889e-01  8.24395730e-01  8.26693425e-01
  8.27335055e-01  8.29200552e-01  8.45753338e-01  8.46313013e-01
  8.49871371e-01  8.50249686e-01  8.59805359e-01  8.62422847e-01
  8.71754793e-01  9.00565830e-01  9.09402450e-01  9.09656904e-01
  9.23760176e-01]

  warnings.warn(

2022-11-03 10:51:09,669:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.45944083 -0.45185488 -0.43259267 -0.43216121 -0.42229774 -0.42101904
 -0.40750149 -0.40502784 -0.40461161 -0.3789278  -0.37435327 -0.36344998
 -0.34960477 -0.34371865 -0.34195783 -0.33406362 -0.33097874 -0.31793856
 -0.31749107 -0.31395354 -0.31233694 -0.29453186 -0.28455225 -0.27665861
 -0.27336499 -0.25388816 -0.25355558 -0.24787495 -0.22920357 -0.22739059
 -0.21873699 -0.21631023 -0.20998967 -0.20402743 -0.20212818 -0.20177081
 -0.20072098 -0.19004305 -0.18975383 -0.18913556 -0.18831538 -0.18585144
 -0.17812138 -0.16946586 -0.1662443  -0.1650406  -0.16448003 -0.16248878
 -0.16214433 -0.15663517 -0.15519291 -0.15211449 -0.14188355 -0.13551834
 -0.13424218 -0.12760895 -0.12375683 -0.1223593  -0.12109338 -0.118282
 -0.11574546 -0.10893304 -0.10529482 -0.10276827 -0.09855281 -0.09839996
 -0.09713575 -0.09110314 -0.0893985  -0.08510844 -0.08456994 -0.08402823
 -0.08338984 -0.0807451  -0.07974649 -0.07932607 -0.07795019 -0.07706767
 -0.07694441 -0.07378053 -0.07170347 -0.07091209 -0.07071828 -0.06981117
 -0.06693403 -0.06559866 -0.06398036 -0.06144787 -0.05752126 -0.05675573
 -0.05625813 -0.05370571 -0.05044816 -0.04638288 -0.04395187 -0.03468395
 -0.03204715 -0.0285286  -0.02620191 -0.02249475 -0.01569237 -0.01492308
 -0.01012174  0.00342309  0.01231569  0.0230919   0.0282157   0.03012954
  0.03038664  0.03264022  0.03611118  0.0390308   0.04455426  0.04901065
  0.05043683  0.05195198  0.05439954  0.0567912   0.05815206  0.05938515
  0.06046894  0.07249563  0.07569157  0.07584271  0.07596517  0.07684826
  0.0793116   0.08301695  0.08539403  0.09027509  0.09093168  0.09278339
  0.09348584  0.09638408  0.09925026  0.09999208  0.10373095  0.11045114
  0.11056845  0.11127267  0.12067628  0.12175364  0.12415035  0.12663705
  0.12925477  0.13124609  0.13514142  0.13533874  0.13572965  0.14211639
  0.14458972  0.14690384  0.1548601   0.15544823  0.15793407  0.15841895
  0.15912338  0.16069501  0.16502523  0.1655216   0.16695484  0.17218898
  0.17511394  0.17659253  0.1797987   0.18230813  0.18533571  0.18822908
  0.18916013  0.19054366  0.1921073   0.19346839  0.19575028  0.19915206
  0.21243675  0.2133009   0.21365235  0.21391166  0.21928556  0.22272264
  0.22428756  0.22498021  0.22986071  0.23139916  0.23203815  0.2341495
  0.23554484  0.23831959  0.23878049  0.23955914  0.24068814  0.24154307
  0.2446944   0.2483683   0.24936183  0.25216731  0.25406908  0.25435162
  0.25693385  0.25753062  0.25782211  0.25818892  0.25942736  0.25985172
  0.26055107  0.26065196  0.26579482  0.26640628  0.26699592  0.26779219
  0.26821164  0.26884032  0.27080544  0.2712138   0.27181899  0.27350432
  0.27653716  0.27704724  0.2776127   0.28372286  0.28425484  0.2875854
  0.28882935  0.2903373   0.29107661  0.29143942  0.29522925  0.2961259
  0.29877431  0.30025592  0.30125435  0.30235882  0.30349766  0.30782449
  0.30837291  0.31061075  0.31261946  0.31563182  0.31627923  0.31694003
  0.31738872  0.31993453  0.32202452  0.32242784  0.32863628  0.3292411
  0.3322005   0.33386991  0.33482998  0.33497093  0.33626139  0.34075182
  0.34148661  0.3419314   0.34391248  0.34489958  0.34761597  0.34949732
  0.35083396  0.35285856  0.35461822  0.35485982  0.35523843  0.35650292
  0.35802216  0.35888512  0.36275146  0.3652619   0.36758464  0.37134021
  0.373906    0.37489561  0.37782908  0.3793185   0.38314495  0.38583819
  0.38664716  0.38716417  0.38982616  0.39045063  0.39122835  0.39141729
  0.39205024  0.39308118  0.39428297  0.39719723  0.39778473  0.40231928
  0.40235041  0.40595998  0.40882898  0.40915987  0.4141352   0.41714482
  0.42518737  0.42844089  0.42995528  0.43805269  0.44076292  0.4411346
  0.44355291  0.44674448  0.45251693  0.45563844  0.45700769  0.45735336
  0.4579131   0.45809868  0.4695271   0.4719714   0.4760049   0.47747883
  0.47846018  0.48365296  0.48393575  0.48422003  0.49068002  0.49406963
  0.50300509  0.50709002  0.50944081  0.51108162  0.51306279  0.51675908
  0.51948843  0.52140603  0.52303993  0.5231012   0.52323815  0.52524515
  0.52816745  0.52957964  0.52979845  0.53217071  0.53258043  0.53403685
  0.53533777  0.5395018   0.54111933  0.54276025  0.54601003  0.55028797
  0.55577175  0.55689839  0.55765677  0.559144    0.56305371  0.56318498
  0.56569446  0.56638278  0.57373424  0.58126755  0.58312307  0.58691619
  0.59098006  0.59325148  0.59446426  0.59447786  0.59749189  0.59956188
  0.60001072  0.6023663   0.60419442  0.60514238  0.60803795  0.60992155
  0.61016574  0.62038651  0.62070511  0.62840997  0.63653993  0.63952029
  0.6401554   0.65082231  0.65108628  0.65200047  0.65449099  0.65590929
  0.65706301  0.65746081  0.6605604   0.66173565  0.66197668  0.66723747
  0.67226724  0.67982884  0.69042541  0.69214379  0.69719617  0.69757094
  0.69767051  0.70391642  0.70444914  0.71198012  0.7141733   0.72424958
  0.7251911   0.7298636   0.73266983  0.73345872  0.73371189  0.73520001
  0.73810981  0.74028902  0.74197161  0.74701035  0.74903886  0.75148372
  0.75247683  0.75457743  0.75691604  0.75736807  0.76140762  0.76205851
  0.76833906  0.76985978  0.77533102  0.77578714  0.77802687  0.77851893
  0.77891765  0.78053949  0.78103531  0.78145554  0.78408899  0.78523556
  0.78801599  0.7903911   0.79562216  0.7957048   0.80013439  0.80404082
  0.80513587  0.81188661  0.81757873  0.81887155  0.82096774  0.82389912
  0.83137465  0.83241587  0.83405277  0.83776502  0.8389375   0.83999229
  0.84078855  0.84299525  0.8444157   0.85060674  0.85710642  0.86153832
  0.86481098  0.865269    0.86592991  0.87586742  0.87849022  0.88020183
  0.88168078  0.88416517  0.88593504  0.88749049  0.89305141  0.8932844
  0.89398415  0.89462849  0.8983316   0.8984619   0.90334482  0.90377443
  0.90641708  0.90664677  0.90990051  0.91295255  0.92340568  0.93234583
  0.93664085  0.9477697   0.94814329  0.95065239  0.95166353  0.95929858
  0.96470913  0.96798307  0.97547947  0.97785907  0.98515334  0.98600078
  0.988451    0.99861368  1.00793675  1.00897745  1.03381256  1.07281716
  1.09515511]

  warnings.warn(

2022-11-03 10:51:09,669:INFO:Calculating mean and std
2022-11-03 10:51:09,677:INFO:Creating metrics dataframe
2022-11-03 10:51:09,685:INFO:Uploading results into container
2022-11-03 10:51:09,685:INFO:Uploading model into container now
2022-11-03 10:51:09,685:INFO:master_model_container: 18
2022-11-03 10:51:09,685:INFO:display_container: 2
2022-11-03 10:51:09,685:INFO:PassiveAggressiveRegressor(random_state=4411)
2022-11-03 10:51:09,685:INFO:create_model() successfully completed......................................
2022-11-03 10:51:09,940:ERROR:create_model() for PassiveAggressiveRegressor(random_state=4411) raised an exception or returned all 0.0:
2022-11-03 10:51:09,956:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:51:09,956:INFO:Initializing Huber Regressor
2022-11-03 10:51:09,956:INFO:Total runtime is 2.5674511591593423 minutes
2022-11-03 10:51:09,956:INFO:SubProcess create_model() called ==================================
2022-11-03 10:51:09,956:INFO:Initializing create_model()
2022-11-03 10:51:09,956:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:51:09,956:INFO:Checking exceptions
2022-11-03 10:51:09,956:INFO:Importing libraries
2022-11-03 10:51:09,956:INFO:Copying training dataset
2022-11-03 10:51:09,972:INFO:Defining folds
2022-11-03 10:51:09,972:INFO:Declaring metric variables
2022-11-03 10:51:09,972:INFO:Importing untrained model
2022-11-03 10:51:09,972:INFO:Huber Regressor Imported successfully
2022-11-03 10:51:09,972:INFO:Starting cross validation
2022-11-03 10:51:09,972:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:51:13,601:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:13,617:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:13,632:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:13,655:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:13,686:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:13,910:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:14,120:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:14,168:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:14,876:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.47798400e-01 -2.30669505e-01 -2.17324410e-01 -1.99178806e-01
 -1.91283833e-01 -1.89189458e-01 -1.87495653e-01 -1.78758440e-01
 -1.71173755e-01 -1.67765261e-01 -1.65621639e-01 -1.65496507e-01
 -1.62035646e-01 -1.58486908e-01 -1.58434744e-01 -1.55759335e-01
 -1.49746701e-01 -1.49604063e-01 -1.48475552e-01 -1.45490866e-01
 -1.35668623e-01 -1.30913061e-01 -1.29595251e-01 -1.26531428e-01
 -1.25659067e-01 -1.23731996e-01 -1.22952515e-01 -1.19392942e-01
 -1.18682085e-01 -1.18386049e-01 -1.17605026e-01 -1.17074242e-01
 -1.14668068e-01 -1.11765822e-01 -1.11383184e-01 -1.09851291e-01
 -1.08374651e-01 -1.06130250e-01 -1.03624891e-01 -1.03545211e-01
 -1.00471408e-01 -1.00379268e-01 -9.97021726e-02 -9.94246044e-02
 -9.61983737e-02 -9.55970758e-02 -9.27955981e-02 -8.84105166e-02
 -8.61899729e-02 -8.53368318e-02 -8.42936399e-02 -8.24404548e-02
 -7.58453678e-02 -6.79048209e-02 -6.76253578e-02 -6.75060340e-02
 -6.71232860e-02 -6.59614630e-02 -5.88690240e-02 -5.78925769e-02
 -5.55909671e-02 -5.29230812e-02 -4.59792575e-02 -4.41198773e-02
 -4.35528838e-02 -4.32220866e-02 -4.26501575e-02 -4.25593556e-02
 -3.83588719e-02 -3.83575362e-02 -3.77230364e-02 -3.65344607e-02
 -3.43268063e-02 -3.40223644e-02 -3.15192110e-02 -3.10270888e-02
 -3.03866179e-02 -3.01793051e-02 -2.86209195e-02 -2.79697242e-02
 -2.71652655e-02 -2.51561972e-02 -1.94724216e-02 -1.91535903e-02
 -1.78782996e-02 -1.73956578e-02 -1.46163353e-02 -1.40095036e-02
 -1.38530177e-02 -1.15160044e-02 -9.84211179e-03 -6.87144409e-03
 -5.69863410e-03 -2.67124845e-03 -1.22645967e-03 -6.81395139e-04
 -1.35200315e-04 -1.23079295e-04  1.85473517e-03  2.05171942e-03
  2.87195410e-03  2.88991593e-03  4.04778759e-03  4.10292102e-03
  5.33874397e-03  6.81858363e-03  6.86856332e-03  7.73962931e-03
  8.56255118e-03  9.54887668e-03  1.03426317e-02  1.10745145e-02
  1.12046109e-02  1.27554235e-02  1.59965872e-02  1.78265084e-02
  1.92801332e-02  1.96504008e-02  2.23488182e-02  2.24693118e-02
  2.25526708e-02  2.64569169e-02  2.81197361e-02  2.94143972e-02
  2.95374009e-02  3.14246563e-02  3.20100143e-02  3.26893678e-02
  3.36660939e-02  3.37763280e-02  3.38856995e-02  3.45458483e-02
  3.96229080e-02  4.02866987e-02  4.05451013e-02  4.19985725e-02
  4.20288680e-02  4.23783680e-02  4.43395400e-02  4.50253207e-02
  4.52219630e-02  4.52961821e-02  4.68440691e-02  5.04200044e-02
  5.09619882e-02  5.26079890e-02  5.36569245e-02  5.40841892e-02
  5.51455367e-02  5.70201064e-02  5.70525389e-02  5.81179191e-02
  5.85238621e-02  5.89243186e-02  5.92613152e-02  5.98463822e-02
  6.32354624e-02  6.36917106e-02  6.48517754e-02  6.52711495e-02
  6.56117628e-02  6.64181899e-02  6.78566271e-02  6.80179530e-02
  7.00936486e-02  7.22551347e-02  7.50904065e-02  7.57318143e-02
  7.62925906e-02  7.67973110e-02  7.71677698e-02  7.80661570e-02
  7.80928743e-02  8.03498841e-02  8.38270986e-02  8.44579907e-02
  8.45414469e-02  8.50696436e-02  8.63350564e-02  8.84389967e-02
  8.93637189e-02  8.97252733e-02  8.99488957e-02  9.05454176e-02
  9.15396443e-02  9.33394134e-02  9.44682148e-02  9.61409554e-02
  9.62594883e-02  9.80300913e-02  9.80787365e-02  9.81556056e-02
  1.00111076e-01  1.00221855e-01  1.01279503e-01  1.05613527e-01
  1.07760744e-01  1.08575573e-01  1.10239564e-01  1.10486020e-01
  1.10643062e-01  1.12618939e-01  1.15035210e-01  1.17674791e-01
  1.18212253e-01  1.18676572e-01  1.21125065e-01  1.24238177e-01
  1.25242645e-01  1.26690323e-01  1.28647679e-01  1.29502978e-01
  1.30984576e-01  1.33673456e-01  1.34193509e-01  1.34228299e-01
  1.34392969e-01  1.34881407e-01  1.37079482e-01  1.43127863e-01
  1.43545463e-01  1.43582729e-01  1.44652160e-01  1.45995177e-01
  1.46326274e-01  1.46356266e-01  1.46628917e-01  1.46723430e-01
  1.50855834e-01  1.52240774e-01  1.52389833e-01  1.53251995e-01
  1.54348029e-01  1.54413884e-01  1.54873169e-01  1.55262998e-01
  1.57458583e-01  1.60082433e-01  1.60580120e-01  1.61070207e-01
  1.61786224e-01  1.64005106e-01  1.64023517e-01  1.64488620e-01
  1.64824046e-01  1.64961179e-01  1.65001184e-01  1.65055937e-01
  1.69358481e-01  1.70028525e-01  1.70084140e-01  1.70085384e-01
  1.72702106e-01  1.73215145e-01  1.73817911e-01  1.75592355e-01
  1.76138853e-01  1.76165965e-01  1.76224422e-01  1.76846975e-01
  1.81999005e-01  1.82259027e-01  1.82972832e-01  1.87825318e-01
  1.88446019e-01  1.88528833e-01  1.93185456e-01  1.93407397e-01
  1.94410047e-01  1.95363212e-01  1.95426951e-01  1.97101374e-01
  1.97627233e-01  1.98167250e-01  2.03203856e-01  2.03402657e-01
  2.03959951e-01  2.06751335e-01  2.08239362e-01  2.08762299e-01
  2.09243628e-01  2.09508589e-01  2.12311859e-01  2.13717154e-01
  2.14233635e-01  2.14405407e-01  2.14546854e-01  2.14745591e-01
  2.20186472e-01  2.20920346e-01  2.21349736e-01  2.25292083e-01
  2.28577464e-01  2.29291552e-01  2.29848030e-01  2.30473168e-01
  2.31944154e-01  2.38241450e-01  2.41062049e-01  2.41498936e-01
  2.41621133e-01  2.42366318e-01  2.43557845e-01  2.43759614e-01
  2.44182346e-01  2.45622935e-01  2.46320625e-01  2.47725946e-01
  2.49006865e-01  2.51996817e-01  2.52063315e-01  2.52298031e-01
  2.52555675e-01  2.55180346e-01  2.55937206e-01  2.57344904e-01
  2.57804420e-01  2.60880626e-01  2.64743517e-01  2.65155944e-01
  2.65623799e-01  2.66793332e-01  2.66925538e-01  2.67141546e-01
  2.67263580e-01  2.70990021e-01  2.71465108e-01  2.72262053e-01
  2.72661439e-01  2.73153721e-01  2.73531076e-01  2.73845804e-01
  2.74380470e-01  2.74519034e-01  2.79614225e-01  2.81435270e-01
  2.82787421e-01  2.86798893e-01  2.87868681e-01  2.92221817e-01
  2.92312656e-01  2.94412844e-01  2.98211330e-01  2.98836349e-01
  3.00482579e-01  3.01389955e-01  3.03011439e-01  3.03964078e-01
  3.05625936e-01  3.06984161e-01  3.07336108e-01  3.07364681e-01
  3.08495089e-01  3.11611762e-01  3.15685756e-01  3.17719211e-01
  3.20938080e-01  3.22231946e-01  3.24225896e-01  3.26805514e-01
  3.27351390e-01  3.27575555e-01  3.29056894e-01  3.32662305e-01
  3.39544445e-01  3.42729147e-01  3.47801412e-01  3.51327870e-01
  3.52620380e-01  3.54557512e-01  3.55271611e-01  3.55566039e-01
  3.56203651e-01  3.61150716e-01  3.61306904e-01  3.64719420e-01
  3.65421696e-01  3.65960537e-01  3.67889466e-01  3.71022542e-01
  3.72239815e-01  3.75512413e-01  3.80591302e-01  3.83257013e-01
  3.85157870e-01  3.86574283e-01  3.87003329e-01  3.89397698e-01
  3.89610908e-01  3.91217369e-01  3.96909354e-01  3.97382087e-01
  3.98712315e-01  4.00629245e-01  4.01988696e-01  4.02219149e-01
  4.06465155e-01  4.06521530e-01  4.08609031e-01  4.12614440e-01
  4.15076433e-01  4.16780220e-01  4.22334777e-01  4.23150233e-01
  4.23438596e-01  4.26427761e-01  4.29182446e-01  4.30066499e-01
  4.30265118e-01  4.39050398e-01  4.39609623e-01  4.39622239e-01
  4.42017291e-01  4.43750820e-01  4.43986072e-01  4.45172928e-01
  4.49684013e-01  4.52678438e-01  4.55460957e-01  4.55715449e-01
  4.56578388e-01  4.58606177e-01  4.60326438e-01  4.60388264e-01
  4.61217234e-01  4.64155154e-01  4.64565741e-01  4.68204482e-01
  4.68905496e-01  4.71695730e-01  4.71903589e-01  4.72401048e-01
  4.76537116e-01  4.78864645e-01  4.89462068e-01  4.91946025e-01
  5.05498199e-01  5.06629779e-01  5.07109784e-01  5.13220172e-01
  5.17453082e-01  5.17538800e-01  5.17621180e-01  5.18004117e-01
  5.20890298e-01  5.22935800e-01  5.24813358e-01  5.27614734e-01
  5.27768165e-01  5.31681723e-01  5.33429645e-01  5.35755977e-01
  5.39903781e-01  5.41280589e-01  5.42980733e-01  5.43475277e-01
  5.47656917e-01  5.47853633e-01  5.51561063e-01  5.55312901e-01
  5.60589614e-01  5.65097892e-01  5.68274188e-01  5.73400710e-01
  5.79395842e-01  5.83446751e-01  5.84937499e-01  5.90581397e-01
  5.91207147e-01  5.91573418e-01  5.93023454e-01  6.03577877e-01
  6.06468068e-01  6.08625510e-01  6.09626351e-01  6.14875244e-01
  6.16504197e-01  6.19428731e-01  6.24403558e-01  6.30664914e-01
  6.32258040e-01  6.44768403e-01  6.52093363e-01  6.55098063e-01
  6.74473215e-01  6.84467096e-01  6.98453211e-01  7.09590099e-01
  7.15970513e-01  7.17441881e-01  7.20620520e-01  7.43106493e-01
  7.46385167e-01]

  warnings.warn(

2022-11-03 10:51:14,894:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.80768939e-01 -1.76944092e-01 -1.76094950e-01 -1.69737848e-01
 -1.59408235e-01 -1.56804244e-01 -1.45415857e-01 -1.44408298e-01
 -1.41871868e-01 -1.37022142e-01 -1.31487327e-01 -1.30313629e-01
 -1.26593721e-01 -1.24245307e-01 -1.22746434e-01 -1.16374301e-01
 -1.15077188e-01 -1.14908540e-01 -1.11996663e-01 -1.11922989e-01
 -1.07954681e-01 -1.05961031e-01 -9.83338408e-02 -9.83328550e-02
 -9.79129664e-02 -9.36694458e-02 -9.32521902e-02 -9.25093404e-02
 -9.12384696e-02 -9.07827364e-02 -9.06633048e-02 -8.75513245e-02
 -8.69750824e-02 -8.56466476e-02 -8.53763719e-02 -8.20649905e-02
 -8.07632671e-02 -8.01272157e-02 -7.95396010e-02 -7.92492750e-02
 -7.89701862e-02 -7.86549781e-02 -7.74016451e-02 -7.24379969e-02
 -7.16765903e-02 -6.99591214e-02 -6.88024308e-02 -6.80273975e-02
 -6.51022119e-02 -6.50058132e-02 -6.45781789e-02 -6.42097027e-02
 -5.98931928e-02 -5.83601435e-02 -5.82734715e-02 -5.69505321e-02
 -5.66116816e-02 -5.55685374e-02 -5.47692876e-02 -5.42750559e-02
 -5.29171630e-02 -5.17006897e-02 -5.05609285e-02 -4.84867522e-02
 -4.76354085e-02 -4.75826190e-02 -4.27573429e-02 -4.19460702e-02
 -4.07815805e-02 -4.00769621e-02 -3.92588851e-02 -3.73226954e-02
 -3.50361929e-02 -3.40129420e-02 -3.36160569e-02 -3.25856072e-02
 -3.14741667e-02 -2.76623083e-02 -2.66764464e-02 -2.59843313e-02
 -2.51096343e-02 -2.16152251e-02 -2.11372171e-02 -2.04434844e-02
 -2.02805499e-02 -1.97148422e-02 -1.75573133e-02 -1.73366607e-02
 -1.71232070e-02 -1.42078970e-02 -1.35209699e-02 -1.33900881e-02
 -1.16748200e-02 -1.13986678e-02 -1.05987618e-02 -9.15443964e-03
 -9.10949693e-03 -8.89826910e-03 -8.18618945e-03 -7.15553179e-03
 -6.97474379e-03 -6.86682522e-03 -6.23945403e-03 -5.61875340e-03
 -5.32726484e-03 -4.79896889e-03 -4.41739002e-03 -4.18085108e-03
 -3.19209386e-03 -2.61532492e-03 -1.65160860e-03 -1.01463375e-03
 -6.17185768e-04 -4.89166583e-04 -1.69420189e-05  1.26820630e-04
  8.78756498e-04  2.04611304e-03  4.43893408e-03  4.71520451e-03
  5.75589571e-03  5.94343759e-03  7.09830915e-03  8.01042483e-03
  1.45747961e-02  1.49249515e-02  1.53301341e-02  1.62445011e-02
  1.77382679e-02  1.79449061e-02  1.88882906e-02  1.95264316e-02
  2.03147350e-02  2.04121895e-02  2.04716370e-02  2.35020326e-02
  2.35307963e-02  2.54558817e-02  2.67626964e-02  2.72481406e-02
  2.98338334e-02  3.34656292e-02  3.59927715e-02  3.66074245e-02
  3.80888497e-02  3.95037726e-02  4.21498951e-02  4.23374370e-02
  4.27889780e-02  4.35029813e-02  4.60998457e-02  4.75718014e-02
  4.91632225e-02  5.02064665e-02  5.07471318e-02  5.26013049e-02
  5.44358042e-02  5.57925900e-02  5.61184200e-02  5.71107466e-02
  5.73481994e-02  5.81089833e-02  5.93249824e-02  6.02439762e-02
  6.21491259e-02  6.25036561e-02  6.28878764e-02  6.40693990e-02
  6.45414576e-02  6.54212073e-02  6.56830294e-02  6.58436404e-02
  6.64746984e-02  6.68792213e-02  6.85834122e-02  6.88529721e-02
  6.90282131e-02  6.99841163e-02  7.03371347e-02  7.16257299e-02
  7.25279982e-02  7.26570557e-02  7.62014985e-02  7.90649610e-02
  7.91242474e-02  8.18234771e-02  8.18343444e-02  8.26062013e-02
  8.42511533e-02  8.52243616e-02  8.59772013e-02  8.75262070e-02
  8.79133817e-02  8.81187230e-02  8.82061126e-02  8.83760155e-02
  8.85812426e-02  8.96599383e-02  9.01559848e-02  9.01828925e-02
  9.04739998e-02  9.05099658e-02  9.19763123e-02  9.29239355e-02
  9.37339704e-02  9.51286925e-02  9.56886184e-02  9.60229541e-02
  9.70989532e-02  9.72110446e-02  1.00835879e-01  1.01166618e-01
  1.01468161e-01  1.03404138e-01  1.04565602e-01  1.04721831e-01
  1.05219515e-01  1.07885945e-01  1.08447240e-01  1.10352422e-01
  1.11164766e-01  1.11721835e-01  1.11834573e-01  1.12000471e-01
  1.14029688e-01  1.17909094e-01  1.18849241e-01  1.18858090e-01
  1.19412405e-01  1.19555528e-01  1.19730653e-01  1.20126829e-01
  1.20488043e-01  1.21057839e-01  1.21108852e-01  1.21729330e-01
  1.22199681e-01  1.23614096e-01  1.25335877e-01  1.25621411e-01
  1.26059033e-01  1.27261068e-01  1.28576701e-01  1.29271531e-01
  1.31032534e-01  1.31092796e-01  1.31223292e-01  1.34664160e-01
  1.35885123e-01  1.36587845e-01  1.38684317e-01  1.39004047e-01
  1.39624066e-01  1.42077957e-01  1.42407183e-01  1.42465472e-01
  1.42749765e-01  1.43007451e-01  1.43224579e-01  1.45341762e-01
  1.46312453e-01  1.47801921e-01  1.52959041e-01  1.53812071e-01
  1.54942989e-01  1.55116530e-01  1.55354598e-01  1.57541537e-01
  1.58310502e-01  1.61385969e-01  1.62651618e-01  1.66149088e-01
  1.66975020e-01  1.66990490e-01  1.67051178e-01  1.70032943e-01
  1.71134068e-01  1.73413829e-01  1.73827165e-01  1.74013143e-01
  1.74568803e-01  1.75090773e-01  1.75223717e-01  1.80048185e-01
  1.81469225e-01  1.81708073e-01  1.83260700e-01  1.84509583e-01
  1.84524911e-01  1.85146120e-01  1.89089503e-01  1.90209234e-01
  1.91059344e-01  1.91979296e-01  1.92182494e-01  1.92325308e-01
  1.92943941e-01  1.94064127e-01  1.94306285e-01  1.95212121e-01
  1.95622824e-01  1.97773738e-01  2.01093882e-01  2.01609882e-01
  2.01623691e-01  2.03588608e-01  2.04133362e-01  2.06151985e-01
  2.08561521e-01  2.10394144e-01  2.10881868e-01  2.12647380e-01
  2.14433116e-01  2.15005165e-01  2.20451603e-01  2.20471010e-01
  2.20795548e-01  2.23496795e-01  2.24345746e-01  2.25934128e-01
  2.26836636e-01  2.31400084e-01  2.31821759e-01  2.34387267e-01
  2.34427540e-01  2.38359828e-01  2.38656787e-01  2.39299970e-01
  2.39585430e-01  2.40139318e-01  2.41832045e-01  2.43874712e-01
  2.43909881e-01  2.43975060e-01  2.44722225e-01  2.45410183e-01
  2.45690423e-01  2.46796542e-01  2.48417911e-01  2.48949934e-01
  2.50511595e-01  2.50878065e-01  2.56610537e-01  2.56770673e-01
  2.57891050e-01  2.57945300e-01  2.58337395e-01  2.59535509e-01
  2.59872324e-01  2.61939993e-01  2.63252894e-01  2.64704614e-01
  2.66212525e-01  2.66859161e-01  2.67897177e-01  2.71840703e-01
  2.72529158e-01  2.72950257e-01  2.73954123e-01  2.76826403e-01
  2.79643575e-01  2.80168575e-01  2.81870162e-01  2.82199239e-01
  2.83646639e-01  2.84369103e-01  2.86009000e-01  2.88380330e-01
  2.89490809e-01  2.90192020e-01  2.90909343e-01  2.91242618e-01
  2.97625736e-01  3.00207292e-01  3.01439702e-01  3.03904690e-01
  3.04482999e-01  3.04833465e-01  3.09354575e-01  3.13677116e-01
  3.17557323e-01  3.18302208e-01  3.20706765e-01  3.23221824e-01
  3.25630102e-01  3.26691656e-01  3.27814052e-01  3.30963035e-01
  3.33214915e-01  3.34258921e-01  3.36160741e-01  3.39135259e-01
  3.39172969e-01  3.39308088e-01  3.39374414e-01  3.40861029e-01
  3.41115259e-01  3.41743380e-01  3.42198331e-01  3.44072077e-01
  3.44526975e-01  3.47986275e-01  3.49046582e-01  3.49173700e-01
  3.49214360e-01  3.49895279e-01  3.51557531e-01  3.55042865e-01
  3.59244071e-01  3.62360391e-01  3.62990985e-01  3.63965849e-01
  3.64139553e-01  3.67624531e-01  3.73352789e-01  3.74824983e-01
  3.74927395e-01  3.76252113e-01  3.80652757e-01  3.82567011e-01
  3.86288376e-01  3.89132128e-01  3.94994953e-01  4.02408874e-01
  4.02933568e-01  4.05976588e-01  4.06339799e-01  4.06413457e-01
  4.12199521e-01  4.22659914e-01  4.23976751e-01  4.28509641e-01
  4.29983339e-01  4.32205948e-01  4.32672109e-01  4.35274611e-01
  4.35362721e-01  4.35902295e-01  4.36267011e-01  4.37532937e-01
  4.38916563e-01  4.39101600e-01  4.40507704e-01  4.40859207e-01
  4.45243758e-01  4.46067019e-01  4.51503865e-01  4.53555091e-01
  4.53566667e-01  4.55020112e-01  4.56650531e-01  4.56762806e-01
  4.57852495e-01  4.58040850e-01  4.59465763e-01  4.60662513e-01
  4.61231104e-01  4.61562396e-01  4.62164064e-01  4.63509849e-01
  4.64097297e-01  4.66041301e-01  4.71972159e-01  4.74027163e-01
  4.74358945e-01  4.77486823e-01  4.83362043e-01  4.84667417e-01
  4.87941007e-01  4.88231376e-01  4.89603442e-01  4.90625406e-01
  4.91895953e-01  5.09970262e-01  5.10085010e-01  5.10321174e-01
  5.10883275e-01  5.16018038e-01  5.32314600e-01  5.37215661e-01
  5.39222077e-01  5.39532587e-01  5.52129723e-01  5.56092361e-01
  5.57185217e-01  5.59063261e-01  5.60109633e-01  5.82719395e-01
  5.83691979e-01  5.84923302e-01  5.85416239e-01  6.10423122e-01]

  warnings.warn(

2022-11-03 10:51:14,931:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.25215833 -0.25134809 -0.24895768 -0.22240619 -0.21087535 -0.20455812
 -0.18886421 -0.17588412 -0.17451384 -0.17360519 -0.17083086 -0.16696586
 -0.16631461 -0.16414082 -0.16309599 -0.16045261 -0.15818669 -0.15578546
 -0.15157861 -0.15092443 -0.14824862 -0.14798673 -0.13233098 -0.13027291
 -0.12928762 -0.12795847 -0.12754963 -0.12113985 -0.12006115 -0.11926561
 -0.11901334 -0.11844004 -0.11772792 -0.11769607 -0.11691757 -0.1164303
 -0.11527883 -0.11416045 -0.11397564 -0.11397219 -0.112405   -0.11178769
 -0.11117165 -0.10445553 -0.10264056 -0.10152566 -0.0993899  -0.09717634
 -0.09680936 -0.09341198 -0.09335935 -0.09272137 -0.09057587 -0.08966721
 -0.08752047 -0.08597341 -0.08585112 -0.08042723 -0.07972344 -0.07950905
 -0.07792419 -0.07732731 -0.07604126 -0.074647   -0.07395387 -0.0724564
 -0.07103245 -0.06621951 -0.06503117 -0.06489667 -0.06481654 -0.06309214
 -0.06169938 -0.06148931 -0.05962917 -0.0588205  -0.05713486 -0.05673918
 -0.05482646 -0.05423924 -0.05300294 -0.05194655 -0.0492609  -0.04913797
 -0.04872218 -0.04795987 -0.04697659 -0.04668778 -0.04497527 -0.04469639
 -0.04279366 -0.03947346 -0.03883606 -0.03831505 -0.03386057 -0.03170603
 -0.0315294  -0.03110091 -0.02925721 -0.02899012 -0.02892424 -0.02704867
 -0.02413087 -0.02395673 -0.0236779  -0.02108558 -0.02020395 -0.01901356
 -0.01121813 -0.00902786 -0.00801959 -0.00674783 -0.00401424 -0.00132385
 -0.00114797  0.00163727  0.00507232  0.00544871  0.0060225   0.00638231
  0.00692238  0.00819622  0.00887532  0.01025502  0.01171842  0.01591303
  0.0182812   0.01872074  0.01954934  0.02034737  0.02179854  0.02575125
  0.02665827  0.02751588  0.02824799  0.03017734  0.03070908  0.03160914
  0.03187422  0.03448704  0.03498918  0.03539877  0.03667809  0.03718824
  0.03947232  0.03970613  0.04195243  0.04200333  0.04330923  0.04614348
  0.05003292  0.05072331  0.05227089  0.05227854  0.05274383  0.05321209
  0.0540105   0.05451163  0.05651474  0.05802957  0.05876426  0.0591403
  0.06007318  0.06187408  0.0623817   0.06284925  0.06419937  0.06464143
  0.06497555  0.06716624  0.06784182  0.06957365  0.0699626   0.07177197
  0.07184376  0.07314184  0.07547448  0.07593376  0.07793812  0.08038925
  0.0808502   0.08605341  0.08652379  0.08701879  0.09056483  0.09128098
  0.0917017   0.09184495  0.09256178  0.09300704  0.09833407  0.09989326
  0.10442626  0.10498777  0.10592374  0.10883405  0.1091539   0.11051274
  0.11078145  0.1122711   0.1126986   0.11285253  0.11421872  0.11641727
  0.11679046  0.11681439  0.12257399  0.12468566  0.1272044   0.12829142
  0.12901667  0.12909456  0.12976563  0.13130593  0.13248125  0.13252577
  0.13303499  0.1347556   0.13486017  0.13522393  0.13543399  0.13683592
  0.13684948  0.13696645  0.13814062  0.13819136  0.13845736  0.14089677
  0.14122895  0.14132602  0.1426289   0.14465337  0.14683691  0.14736424
  0.14758027  0.14777445  0.149406    0.15023911  0.15168662  0.15226801
  0.15291995  0.15736522  0.15882075  0.15951214  0.16001976  0.16069855
  0.16099641  0.16368894  0.16645616  0.16646792  0.16751626  0.16762335
  0.16858525  0.16979517  0.17084624  0.17417106  0.1745902   0.17868799
  0.17963706  0.18215862  0.1826781   0.18428758  0.18429274  0.1854212
  0.18728967  0.18874765  0.19273006  0.19391071  0.1960719   0.19628307
  0.19755016  0.19760994  0.1997922   0.20008487  0.20380665  0.20415545
  0.20435288  0.20679706  0.20681747  0.20790929  0.20849985  0.21481616
  0.21481748  0.21485689  0.21499439  0.21871821  0.22164763  0.22346814
  0.22356577  0.22585863  0.23072466  0.23244286  0.23919663  0.23976129
  0.24000437  0.24043216  0.24171534  0.24192946  0.2454301   0.24828513
  0.24954028  0.25307215  0.25402061  0.25403095  0.25454733  0.25476768
  0.25672082  0.25787095  0.25839488  0.25907941  0.26247365  0.26421648
  0.26608808  0.26645264  0.26687703  0.26779944  0.26850788  0.26914982
  0.27062024  0.27213212  0.27280442  0.27670518  0.27704384  0.28190597
  0.28309498  0.28477423  0.28711739  0.28734168  0.28788328  0.28944184
  0.29045298  0.29108727  0.29134302  0.29574313  0.2977766   0.29908559
  0.30061903  0.30841556  0.31244444  0.31464635  0.31547674  0.31814836
  0.31931322  0.31958057  0.32176472  0.32418328  0.32573685  0.32858676
  0.3292076   0.33075612  0.33701035  0.3375421   0.33755044  0.34245242
  0.34619738  0.34698958  0.34736089  0.34784786  0.35169214  0.35336169
  0.35594059  0.3575957   0.35848146  0.35944001  0.3594543   0.36007667
  0.36011444  0.36442442  0.36665198  0.36851372  0.36895036  0.36911631
  0.37280532  0.37410859  0.37442434  0.37491293  0.38258754  0.38577437
  0.38873823  0.38974813  0.39055411  0.39123144  0.3913172   0.39503972
  0.39504947  0.3955074   0.39888093  0.39928136  0.39998111  0.40022742
  0.40088633  0.40431421  0.40638561  0.40777783  0.41008504  0.41390818
  0.41442973  0.41483067  0.42058439  0.42153836  0.42255078  0.43080861
  0.43461146  0.43560151  0.43750505  0.43990549  0.44605605  0.44941619
  0.44963497  0.45085416  0.45226326  0.46013599  0.46159707  0.46246543
  0.46502583  0.46648476  0.46835917  0.46967134  0.46995115  0.47074795
  0.47174751  0.47264075  0.47585203  0.47600172  0.47894306  0.48478555
  0.4875819   0.49084524  0.49129353  0.49298964  0.49304548  0.49346612
  0.49410083  0.49513952  0.49538949  0.49678436  0.49696791  0.4974829
  0.49847642  0.49953592  0.50392562  0.50532707  0.51189264  0.51682404
  0.5185145   0.51860109  0.5196813   0.52024156  0.52122838  0.52198558
  0.52208151  0.52816808  0.53000525  0.53273487  0.54428903  0.54996383
  0.56095645  0.56136869  0.56233881  0.56248242  0.56272404  0.56620792
  0.56809791  0.56817354  0.57478155  0.57542631  0.57598454  0.5778423
  0.58151196  0.58233501  0.58513912  0.59084397  0.5943566   0.60231036
  0.60531965  0.62000729  0.62520243  0.62729547  0.64096122  0.64600705
  0.6537898   0.66796744  0.66871549  0.66935935  0.67054458  0.6728359
  0.67576115  0.68626337  0.69019574  0.69188943  0.6971961   0.70816468
  0.72778969]

  warnings.warn(

2022-11-03 10:51:14,993:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.39524621e-01 -2.31546458e-01 -2.24615287e-01 -2.20844719e-01
 -1.89123816e-01 -1.83344230e-01 -1.83199297e-01 -1.82123678e-01
 -1.79690813e-01 -1.77532267e-01 -1.76995993e-01 -1.62371298e-01
 -1.59192481e-01 -1.59111561e-01 -1.58810351e-01 -1.54328214e-01
 -1.52201690e-01 -1.48409018e-01 -1.45535799e-01 -1.44456836e-01
 -1.44336839e-01 -1.43571767e-01 -1.39304441e-01 -1.31900678e-01
 -1.30501333e-01 -1.23561913e-01 -1.23364625e-01 -1.22846670e-01
 -1.22164355e-01 -1.20388744e-01 -1.19755698e-01 -1.17943605e-01
 -1.13692215e-01 -1.13139030e-01 -1.12284829e-01 -1.12085610e-01
 -1.07879572e-01 -1.07363234e-01 -1.03247168e-01 -1.01600517e-01
 -1.00829490e-01 -9.59602431e-02 -9.53448270e-02 -9.31046956e-02
 -9.16028000e-02 -9.03390767e-02 -8.79374021e-02 -8.74508113e-02
 -8.65187354e-02 -8.51218676e-02 -8.47321561e-02 -8.15178982e-02
 -7.90315902e-02 -7.54492055e-02 -7.39186074e-02 -7.29596559e-02
 -7.21288283e-02 -7.07128440e-02 -6.91425040e-02 -6.83577223e-02
 -6.71903730e-02 -6.51625689e-02 -6.40006772e-02 -6.10391283e-02
 -5.84352921e-02 -5.82612545e-02 -5.29412478e-02 -5.25569996e-02
 -5.18073584e-02 -5.05854747e-02 -4.91561161e-02 -4.64284040e-02
 -4.28581698e-02 -4.17675512e-02 -4.14317225e-02 -3.99230839e-02
 -3.98550855e-02 -3.90278832e-02 -3.70056824e-02 -3.49299265e-02
 -3.44272208e-02 -3.21792176e-02 -3.01798084e-02 -2.88282039e-02
 -2.84624006e-02 -2.76912180e-02 -2.49530546e-02 -2.39320674e-02
 -2.19480406e-02 -1.82420117e-02 -1.68019993e-02 -1.56201907e-02
 -1.40731462e-02 -1.20297683e-02 -1.19840849e-02 -1.13488115e-02
 -9.34329112e-03 -8.79272738e-03 -7.20918625e-03 -5.48789886e-03
 -4.18382692e-03 -3.63788458e-03 -2.65875325e-03 -1.42999395e-03
 -2.56291936e-04  9.47550340e-05  2.42887922e-03  3.90522290e-03
  6.97112966e-03  7.35899529e-03  7.51885996e-03  8.83985462e-03
  9.99410738e-03  1.03756233e-02  1.08953171e-02  1.11508558e-02
  1.29236128e-02  1.46093663e-02  1.47873751e-02  1.54495946e-02
  1.57819805e-02  1.94144937e-02  1.97093454e-02  1.98643307e-02
  2.03665106e-02  2.15124192e-02  2.25623673e-02  2.97964161e-02
  2.99044360e-02  3.29856779e-02  3.31344156e-02  3.34008612e-02
  3.37769025e-02  3.47962945e-02  3.54199504e-02  3.84872363e-02
  3.85971737e-02  3.92815939e-02  4.22002221e-02  4.22903775e-02
  4.27587771e-02  4.50141669e-02  4.59009317e-02  5.14101264e-02
  5.16197881e-02  5.25758188e-02  5.45108610e-02  5.71789730e-02
  5.89968953e-02  5.98481328e-02  6.04642785e-02  6.20818142e-02
  6.27014413e-02  6.50287967e-02  6.54007250e-02  6.73841410e-02
  6.93815228e-02  7.14540600e-02  7.17751816e-02  7.47464115e-02
  7.54297495e-02  8.04481580e-02  8.22657195e-02  8.43042770e-02
  8.44140229e-02  8.67329964e-02  8.70736611e-02  8.79168781e-02
  9.03846707e-02  9.19578944e-02  9.22511583e-02  9.23534005e-02
  9.30917397e-02  9.38795171e-02  9.38962197e-02  9.39197563e-02
  9.46775375e-02  9.65682255e-02  9.91819320e-02  1.00090437e-01
  1.00363418e-01  1.00744874e-01  1.01153285e-01  1.02921975e-01
  1.03150730e-01  1.05474841e-01  1.05656326e-01  1.10084172e-01
  1.11006303e-01  1.11297556e-01  1.12300695e-01  1.12386278e-01
  1.12650148e-01  1.13131493e-01  1.13262891e-01  1.13399922e-01
  1.14159393e-01  1.15143455e-01  1.17142021e-01  1.18057342e-01
  1.19773128e-01  1.20478582e-01  1.26462830e-01  1.26984069e-01
  1.27734164e-01  1.29608788e-01  1.30106197e-01  1.30282425e-01
  1.32232112e-01  1.36518268e-01  1.36725172e-01  1.37360902e-01
  1.39957033e-01  1.40948145e-01  1.42084772e-01  1.45735943e-01
  1.45781827e-01  1.46657922e-01  1.46836318e-01  1.47430465e-01
  1.47586919e-01  1.47794570e-01  1.48331725e-01  1.48841669e-01
  1.50030664e-01  1.52464349e-01  1.53606678e-01  1.54996882e-01
  1.57470339e-01  1.58827466e-01  1.63051970e-01  1.63986577e-01
  1.64757118e-01  1.65603171e-01  1.66445729e-01  1.69344072e-01
  1.70830053e-01  1.73396595e-01  1.73518000e-01  1.74293278e-01
  1.75453279e-01  1.78105867e-01  1.78450154e-01  1.79071507e-01
  1.80621631e-01  1.84117383e-01  1.84694219e-01  1.86450272e-01
  1.90742937e-01  1.90776704e-01  1.93972507e-01  1.94043094e-01
  1.95563017e-01  1.96711893e-01  1.97238942e-01  2.00464239e-01
  2.04024247e-01  2.13185905e-01  2.16044147e-01  2.16120143e-01
  2.17630007e-01  2.18353994e-01  2.20853956e-01  2.22201740e-01
  2.24806017e-01  2.25973523e-01  2.27259998e-01  2.27381399e-01
  2.27451062e-01  2.32660127e-01  2.32692437e-01  2.36804595e-01
  2.37344312e-01  2.38169984e-01  2.40326753e-01  2.40758192e-01
  2.42302805e-01  2.43730362e-01  2.45422578e-01  2.49208014e-01
  2.52920939e-01  2.54194379e-01  2.54439692e-01  2.54596751e-01
  2.57018823e-01  2.59102935e-01  2.62025793e-01  2.62961770e-01
  2.64177829e-01  2.64295235e-01  2.64610906e-01  2.66210816e-01
  2.67980663e-01  2.70151815e-01  2.70868313e-01  2.71066053e-01
  2.74152574e-01  2.75994547e-01  2.78234317e-01  2.78644517e-01
  2.82482709e-01  2.83224737e-01  2.85454031e-01  2.87190290e-01
  2.87834852e-01  2.88533476e-01  2.88790283e-01  2.89290466e-01
  2.90283489e-01  2.92783792e-01  2.95237608e-01  2.95626537e-01
  2.98635900e-01  2.99123807e-01  2.99704158e-01  3.02269287e-01
  3.02370579e-01  3.05058583e-01  3.06208200e-01  3.06516173e-01
  3.14480137e-01  3.15272426e-01  3.15990894e-01  3.18431518e-01
  3.19353658e-01  3.22018682e-01  3.24040937e-01  3.26215353e-01
  3.27544399e-01  3.28219759e-01  3.29081666e-01  3.34561154e-01
  3.35241019e-01  3.36832062e-01  3.38467464e-01  3.41070605e-01
  3.41727073e-01  3.41937320e-01  3.42409974e-01  3.43071204e-01
  3.44161564e-01  3.46414441e-01  3.48108168e-01  3.52298342e-01
  3.54653722e-01  3.55442368e-01  3.61807687e-01  3.62360334e-01
  3.63615758e-01  3.70304612e-01  3.71182767e-01  3.71986352e-01
  3.76169859e-01  3.76599920e-01  3.76740343e-01  3.77116532e-01
  3.79260838e-01  3.82978027e-01  3.83515857e-01  3.84346872e-01
  3.85559610e-01  3.89441963e-01  3.90287191e-01  3.91817839e-01
  3.93614928e-01  3.94809885e-01  3.98245666e-01  3.98409284e-01
  3.99879422e-01  4.01722492e-01  4.03073141e-01  4.05727397e-01
  4.08513596e-01  4.09402415e-01  4.09646416e-01  4.10405430e-01
  4.11582791e-01  4.12260370e-01  4.13840238e-01  4.15083590e-01
  4.15311914e-01  4.16292739e-01  4.16850586e-01  4.20694793e-01
  4.21699667e-01  4.24354272e-01  4.26629689e-01  4.34298290e-01
  4.36897814e-01  4.37446366e-01  4.37568002e-01  4.39663026e-01
  4.40306852e-01  4.42010195e-01  4.42152276e-01  4.42619278e-01
  4.48163653e-01  4.49049977e-01  4.49061849e-01  4.49435601e-01
  4.50944504e-01  4.50979199e-01  4.52125065e-01  4.52530183e-01
  4.55278734e-01  4.55910563e-01  4.58939815e-01  4.64280229e-01
  4.65652490e-01  4.67105558e-01  4.70237743e-01  4.80024391e-01
  4.80272329e-01  4.82306056e-01  4.82908569e-01  4.88283710e-01
  4.89539107e-01  4.90473493e-01  4.91224337e-01  4.91542020e-01
  4.93737700e-01  4.97866674e-01  5.02174789e-01  5.05734759e-01
  5.06254012e-01  5.07423258e-01  5.10114609e-01  5.13085534e-01
  5.15939287e-01  5.18377494e-01  5.19581095e-01  5.23415566e-01
  5.26905415e-01  5.29356617e-01  5.35826908e-01  5.38654435e-01
  5.39119128e-01  5.43879225e-01  5.45757529e-01  5.53030955e-01
  5.53724397e-01  5.54824969e-01  5.55111936e-01  5.55993562e-01
  5.56196479e-01  5.60689573e-01  5.65071399e-01  5.66825638e-01
  5.68896809e-01  5.70455644e-01  5.71188480e-01  5.71190933e-01
  5.75894689e-01  5.75954738e-01  5.82558591e-01  5.82612772e-01
  5.88069136e-01  5.90362674e-01  5.91428500e-01  5.92886782e-01
  5.93470676e-01  6.01081018e-01  6.03387674e-01  6.03812762e-01
  6.05147047e-01  6.06533508e-01  6.10678044e-01  6.12697125e-01
  6.15028534e-01  6.17989205e-01  6.20709053e-01  6.20812316e-01
  6.22499736e-01  6.24398698e-01  6.24566536e-01  6.27804810e-01
  6.31996263e-01  6.32477094e-01  6.39906331e-01  6.44420986e-01
  6.47507833e-01  6.48714745e-01  6.55568117e-01  6.77589122e-01
  6.95859249e-01  7.10669978e-01  7.18825743e-01  7.19764826e-01
  7.34479771e-01  7.36108641e-01  7.52101062e-01  7.57025230e-01
  7.59549973e-01]

  warnings.warn(

2022-11-03 10:51:15,121:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.77456344e-01 -1.76133950e-01 -1.62624612e-01 -1.60611694e-01
 -1.46390918e-01 -1.38930062e-01 -1.38106189e-01 -1.28704278e-01
 -1.27408644e-01 -1.25737282e-01 -1.25227453e-01 -1.22812413e-01
 -1.20701605e-01 -1.18765284e-01 -1.15488193e-01 -1.13140693e-01
 -1.12358704e-01 -1.08482769e-01 -1.07885968e-01 -1.06169014e-01
 -9.97480221e-02 -9.88806432e-02 -9.82356006e-02 -9.59997179e-02
 -9.56156937e-02 -9.46272547e-02 -9.39078193e-02 -9.35378077e-02
 -9.11455006e-02 -8.96304055e-02 -8.42616554e-02 -8.41010405e-02
 -8.32975297e-02 -8.15740483e-02 -7.84250083e-02 -7.67061748e-02
 -7.63280912e-02 -7.53047426e-02 -7.45556904e-02 -7.24832285e-02
 -7.16184068e-02 -7.11248597e-02 -7.09951311e-02 -7.05333315e-02
 -6.32387304e-02 -6.07336055e-02 -5.86631231e-02 -5.84935411e-02
 -5.61044897e-02 -5.55495221e-02 -5.42642552e-02 -5.38981882e-02
 -5.34995423e-02 -5.19288053e-02 -5.16027712e-02 -5.06749807e-02
 -5.06138546e-02 -4.98508445e-02 -4.96411894e-02 -4.93973854e-02
 -4.82581977e-02 -4.55568683e-02 -4.48274355e-02 -4.46879652e-02
 -4.21305225e-02 -4.18185316e-02 -3.97755758e-02 -3.79032008e-02
 -3.70142364e-02 -3.35635124e-02 -3.33587806e-02 -3.32757820e-02
 -3.02514180e-02 -2.69583869e-02 -2.45628047e-02 -2.43613519e-02
 -2.32662909e-02 -2.03274909e-02 -1.95562320e-02 -1.91966959e-02
 -1.80516200e-02 -1.73281512e-02 -1.70969235e-02 -1.51535217e-02
 -1.48702286e-02 -1.33849584e-02 -1.22448811e-02 -1.22258146e-02
 -1.01888013e-02 -9.91946537e-03 -9.82710843e-03 -9.75750880e-03
 -8.61275676e-03 -7.27698888e-03 -5.01756437e-03 -1.92683051e-03
  2.04100965e-04  8.66066624e-04  3.40935472e-03  4.41020597e-03
  5.10442839e-03  5.20753066e-03  5.92539292e-03  7.31305877e-03
  8.30489063e-03  1.06396962e-02  1.21627451e-02  1.26563159e-02
  1.36039487e-02  1.41186468e-02  1.48003470e-02  1.54318000e-02
  1.56451937e-02  1.61195335e-02  1.67283303e-02  1.85527518e-02
  1.88609485e-02  1.90109830e-02  1.92937011e-02  2.19932392e-02
  2.31082630e-02  2.75978997e-02  2.89825757e-02  2.92097082e-02
  3.01270682e-02  3.05787729e-02  3.12592223e-02  3.29180800e-02
  3.30400290e-02  3.46431577e-02  3.54004352e-02  3.54565998e-02
  3.77532933e-02  3.89814271e-02  4.06001985e-02  4.09420456e-02
  4.20687083e-02  4.45583115e-02  4.47527980e-02  4.54277025e-02
  4.56458605e-02  4.58400628e-02  4.60248742e-02  4.61890287e-02
  4.64795767e-02  4.71053036e-02  4.93106360e-02  4.95275582e-02
  5.04807710e-02  5.13620834e-02  5.15080361e-02  5.29921376e-02
  5.43305054e-02  5.54613231e-02  5.61129859e-02  5.63713557e-02
  5.67349928e-02  5.68609986e-02  5.72356580e-02  5.88304247e-02
  6.15639434e-02  6.17585244e-02  6.26415976e-02  6.30749582e-02
  6.32563909e-02  6.33944213e-02  6.38460031e-02  6.40456045e-02
  6.45072346e-02  6.45785253e-02  6.59626109e-02  6.61092104e-02
  6.67265301e-02  6.71912188e-02  6.76959773e-02  6.83441521e-02
  6.86298097e-02  6.96269591e-02  6.96746266e-02  7.05187752e-02
  7.07914241e-02  7.08823059e-02  7.09266519e-02  7.10457433e-02
  7.23295514e-02  7.23638645e-02  7.24463488e-02  7.25252423e-02
  7.31764720e-02  7.40151625e-02  7.41298346e-02  7.45853043e-02
  7.58860014e-02  7.63368602e-02  7.73616705e-02  7.87431336e-02
  7.94083245e-02  7.98112716e-02  8.07715725e-02  8.09959384e-02
  8.12543769e-02  8.13169369e-02  8.16127024e-02  8.31121067e-02
  8.34509785e-02  8.42941617e-02  8.60967915e-02  8.65963783e-02
  8.67539633e-02  8.69601785e-02  8.75550310e-02  8.77751479e-02
  8.84482254e-02  8.90540882e-02  9.03832604e-02  9.05600470e-02
  9.47077146e-02  9.47368170e-02  9.50754128e-02  9.51999337e-02
  9.55905576e-02  9.65101510e-02  9.73259870e-02  9.87437190e-02
  9.88433684e-02  9.89559456e-02  1.03465751e-01  1.03834588e-01
  1.05410672e-01  1.05894371e-01  1.06367468e-01  1.07442217e-01
  1.09229907e-01  1.09603434e-01  1.09678999e-01  1.10246096e-01
  1.10319349e-01  1.10643837e-01  1.10711657e-01  1.11884948e-01
  1.12204854e-01  1.14306882e-01  1.16051189e-01  1.16061332e-01
  1.17886435e-01  1.20203951e-01  1.21324271e-01  1.24325312e-01
  1.24382377e-01  1.24843337e-01  1.25538870e-01  1.27009323e-01
  1.28868881e-01  1.29698831e-01  1.30020937e-01  1.30543176e-01
  1.32313927e-01  1.35289449e-01  1.37757156e-01  1.38932317e-01
  1.39745241e-01  1.40708364e-01  1.40927514e-01  1.41258708e-01
  1.42638536e-01  1.43739143e-01  1.46238319e-01  1.46531878e-01
  1.48807840e-01  1.51512937e-01  1.53771475e-01  1.63360006e-01
  1.63720906e-01  1.68112679e-01  1.69727045e-01  1.69848834e-01
  1.70705976e-01  1.71216397e-01  1.71671664e-01  1.71950794e-01
  1.73522808e-01  1.75118003e-01  1.78876032e-01  1.79059379e-01
  1.81288307e-01  1.81828994e-01  1.82238411e-01  1.83403194e-01
  1.84609439e-01  1.88735191e-01  1.88932964e-01  1.90156283e-01
  1.90532387e-01  1.91650967e-01  1.91783479e-01  1.94531688e-01
  1.95490450e-01  1.95869739e-01  1.97016537e-01  1.97338939e-01
  1.97392092e-01  1.99887588e-01  2.00588431e-01  2.01051501e-01
  2.01368832e-01  2.01769512e-01  2.02025054e-01  2.02469734e-01
  2.02611715e-01  2.03310009e-01  2.04148687e-01  2.04331262e-01
  2.06355788e-01  2.06543397e-01  2.07733484e-01  2.07934382e-01
  2.10280042e-01  2.11015802e-01  2.11639526e-01  2.11710201e-01
  2.11961698e-01  2.12439400e-01  2.13133664e-01  2.14991516e-01
  2.16138832e-01  2.16785275e-01  2.22296562e-01  2.22953986e-01
  2.23020855e-01  2.25025271e-01  2.26025397e-01  2.27516245e-01
  2.27802478e-01  2.29088542e-01  2.32068245e-01  2.39490608e-01
  2.41252548e-01  2.41906718e-01  2.42959269e-01  2.46266607e-01
  2.46654165e-01  2.48422998e-01  2.48978320e-01  2.50328938e-01
  2.50940153e-01  2.52761649e-01  2.58383048e-01  2.59440270e-01
  2.61162924e-01  2.61848169e-01  2.63133918e-01  2.63148154e-01
  2.63171261e-01  2.63510635e-01  2.64471114e-01  2.64943661e-01
  2.65013422e-01  2.65370235e-01  2.65710847e-01  2.66098037e-01
  2.67455379e-01  2.68682420e-01  2.68953163e-01  2.69551683e-01
  2.69656393e-01  2.70426298e-01  2.78354675e-01  2.80171332e-01
  2.81316314e-01  2.81317761e-01  2.83995845e-01  2.84891835e-01
  2.85243946e-01  2.86942436e-01  2.89597107e-01  2.90211120e-01
  2.90730825e-01  2.91440514e-01  2.91732489e-01  2.92068173e-01
  2.94095301e-01  2.97603684e-01  3.00610996e-01  3.02929657e-01
  3.03369687e-01  3.03967625e-01  3.04932177e-01  3.04939866e-01
  3.05701712e-01  3.06557393e-01  3.07544385e-01  3.11237923e-01
  3.12120445e-01  3.14158489e-01  3.14347706e-01  3.15904668e-01
  3.17603639e-01  3.21102094e-01  3.22332502e-01  3.22407215e-01
  3.22655091e-01  3.23583931e-01  3.25573382e-01  3.29568041e-01
  3.30995529e-01  3.31471222e-01  3.32036331e-01  3.33730618e-01
  3.34229079e-01  3.35183175e-01  3.35425420e-01  3.35833378e-01
  3.36499195e-01  3.37344372e-01  3.38141255e-01  3.39157845e-01
  3.39371985e-01  3.43915942e-01  3.44491965e-01  3.48536166e-01
  3.48971663e-01  3.49225196e-01  3.50007942e-01  3.51109263e-01
  3.59343744e-01  3.60069443e-01  3.64866770e-01  3.65556177e-01
  3.67108750e-01  3.67148796e-01  3.68183045e-01  3.68832053e-01
  3.71643531e-01  3.74834630e-01  3.81780036e-01  3.81878973e-01
  3.84255879e-01  3.89077378e-01  3.90317420e-01  3.92698337e-01
  3.96602178e-01  4.00086719e-01  4.04567429e-01  4.05344661e-01
  4.07185571e-01  4.09462799e-01  4.09620256e-01  4.10468617e-01
  4.10982117e-01  4.11960574e-01  4.13700205e-01  4.15166161e-01
  4.20864720e-01  4.24864596e-01  4.28229470e-01  4.28441423e-01
  4.29905460e-01  4.30223563e-01  4.31068932e-01  4.35414510e-01
  4.36043691e-01  4.41133295e-01  4.44656604e-01  4.45411627e-01
  4.45629428e-01  4.46692991e-01  4.47592832e-01  4.50689495e-01
  4.51143911e-01  4.59526954e-01  4.65315777e-01  4.70705259e-01
  4.72847778e-01  4.74830639e-01  4.75228929e-01  4.75600660e-01
  4.83068728e-01  4.85519853e-01  4.86532252e-01  4.91193547e-01
  4.95435517e-01  5.01753204e-01  5.11715571e-01  5.13867955e-01
  5.16326789e-01  5.27767047e-01  5.28604380e-01  5.29484359e-01
  5.37726570e-01  5.47105168e-01  5.50450378e-01  5.50586158e-01
  5.50641213e-01]

  warnings.warn(

2022-11-03 10:51:15,136:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.52502506e-01 -1.32588501e-01 -1.23955031e-01 -1.23295936e-01
 -1.22872301e-01 -1.17034433e-01 -1.14273374e-01 -1.13620875e-01
 -1.13476775e-01 -1.12980321e-01 -1.10154214e-01 -1.09666895e-01
 -1.07102945e-01 -1.06868778e-01 -1.05662855e-01 -1.02812632e-01
 -1.01981804e-01 -9.94650943e-02 -9.66283520e-02 -9.44605499e-02
 -9.29228897e-02 -9.21387463e-02 -9.20616278e-02 -8.68282726e-02
 -8.17195422e-02 -8.10254719e-02 -8.09103358e-02 -7.95472639e-02
 -7.73558178e-02 -7.68306679e-02 -7.50063374e-02 -7.49799601e-02
 -7.32423411e-02 -7.09762472e-02 -6.97614246e-02 -6.88856987e-02
 -6.83354184e-02 -6.67902151e-02 -6.18264537e-02 -6.15408113e-02
 -6.07514254e-02 -6.03450587e-02 -5.98090651e-02 -5.97276113e-02
 -5.73762612e-02 -5.28608376e-02 -4.87337503e-02 -4.68212921e-02
 -4.64943260e-02 -4.43191501e-02 -4.29371859e-02 -4.11291988e-02
 -3.92981898e-02 -3.86844526e-02 -3.72565843e-02 -3.57711307e-02
 -3.38540235e-02 -3.31650529e-02 -3.31549403e-02 -3.28516077e-02
 -3.26143363e-02 -3.21186841e-02 -3.00169267e-02 -2.98060705e-02
 -2.92593996e-02 -2.92445738e-02 -2.88493613e-02 -2.83234658e-02
 -2.65631744e-02 -2.49182176e-02 -2.41452602e-02 -2.40829824e-02
 -2.34929212e-02 -2.34250065e-02 -2.30243676e-02 -2.25228881e-02
 -2.24359776e-02 -2.10288944e-02 -2.08575638e-02 -2.07514705e-02
 -1.97844323e-02 -1.88742424e-02 -1.68724455e-02 -1.62814810e-02
 -1.57818299e-02 -1.52545023e-02 -1.47730867e-02 -1.47187335e-02
 -1.43884376e-02 -1.43310449e-02 -1.35019712e-02 -1.31907490e-02
 -1.22440539e-02 -1.18221224e-02 -1.17710840e-02 -7.36508208e-03
 -6.49211534e-03 -4.12925664e-03 -3.08755035e-03 -1.63740092e-03
 -1.25219370e-03 -1.15552886e-03 -7.55694027e-04 -7.50837364e-04
  4.72081402e-04  1.59320327e-03  1.88468097e-03  2.53819689e-03
  2.81401634e-03  4.79189297e-03  5.05936400e-03  6.19990100e-03
  7.55036711e-03  7.64007147e-03  8.42543722e-03  8.81288354e-03
  8.97327161e-03  1.08464851e-02  1.23371527e-02  1.25366550e-02
  1.36731200e-02  1.44068104e-02  1.58657869e-02  1.64430963e-02
  1.69564110e-02  1.69786290e-02  2.01295557e-02  2.11862481e-02
  2.13301276e-02  2.37438831e-02  2.52450065e-02  2.52877555e-02
  2.57418852e-02  2.82265731e-02  2.89939425e-02  2.92352484e-02
  2.94200820e-02  3.27841000e-02  3.28308006e-02  3.35083755e-02
  3.47034718e-02  3.60503772e-02  3.69096574e-02  3.73820485e-02
  3.74948667e-02  3.92630905e-02  3.98684259e-02  4.01539527e-02
  4.09699769e-02  4.11109166e-02  4.16423917e-02  4.16725126e-02
  4.29384367e-02  4.31822301e-02  4.44894628e-02  4.60950121e-02
  4.61747607e-02  4.74953116e-02  4.78240855e-02  4.90094593e-02
  5.12263372e-02  5.14980539e-02  5.18350369e-02  5.34590629e-02
  5.42046474e-02  5.42424014e-02  5.47218445e-02  5.55279806e-02
  5.60786438e-02  5.63300666e-02  5.65179253e-02  5.99671797e-02
  6.34287647e-02  6.38370141e-02  6.42484160e-02  6.46747905e-02
  6.48128606e-02  6.58576704e-02  6.62996935e-02  6.94587502e-02
  6.96201761e-02  7.11093231e-02  7.15342021e-02  7.33620782e-02
  7.51924844e-02  7.52589318e-02  7.53652754e-02  7.54438643e-02
  7.58488368e-02  7.59676475e-02  7.65537335e-02  7.68959485e-02
  7.72055652e-02  7.80441491e-02  7.83663906e-02  7.84223596e-02
  7.88203584e-02  8.02895413e-02  8.12404843e-02  8.19218546e-02
  8.38410421e-02  8.66323247e-02  8.69165852e-02  8.71717098e-02
  8.72165839e-02  8.87438518e-02  8.90314075e-02  9.05258083e-02
  9.13369757e-02  9.17636112e-02  9.24588380e-02  9.28894600e-02
  9.36193118e-02  9.37151255e-02  9.45768752e-02  9.67782561e-02
  9.68276364e-02  9.75247723e-02  9.78303106e-02  9.98725023e-02
  9.98937528e-02  1.00090619e-01  1.01670792e-01  1.02438381e-01
  1.03972152e-01  1.04029063e-01  1.04092199e-01  1.05116341e-01
  1.05198891e-01  1.06196836e-01  1.06846981e-01  1.07285557e-01
  1.07448936e-01  1.08639306e-01  1.08987105e-01  1.10671659e-01
  1.10682972e-01  1.10922774e-01  1.11363844e-01  1.13297172e-01
  1.14192801e-01  1.14543578e-01  1.17674409e-01  1.18906039e-01
  1.20728235e-01  1.21328314e-01  1.22267207e-01  1.24048682e-01
  1.24212558e-01  1.25632828e-01  1.26292404e-01  1.26593920e-01
  1.26631169e-01  1.27010343e-01  1.27766362e-01  1.31113359e-01
  1.33027923e-01  1.33446911e-01  1.33964451e-01  1.34078702e-01
  1.35513759e-01  1.35824565e-01  1.36575882e-01  1.37277871e-01
  1.37371487e-01  1.38786457e-01  1.38881537e-01  1.38957615e-01
  1.39000265e-01  1.41120234e-01  1.43690446e-01  1.43779830e-01
  1.44416892e-01  1.44594582e-01  1.44919649e-01  1.45046447e-01
  1.46357765e-01  1.46556287e-01  1.48793119e-01  1.48883568e-01
  1.48944371e-01  1.49000693e-01  1.50153467e-01  1.51467293e-01
  1.53017500e-01  1.54343582e-01  1.54591736e-01  1.54815747e-01
  1.55202443e-01  1.59415357e-01  1.59692088e-01  1.61925772e-01
  1.63561509e-01  1.65694990e-01  1.65898336e-01  1.69130781e-01
  1.69418620e-01  1.70764211e-01  1.72231723e-01  1.72872523e-01
  1.73956663e-01  1.74528901e-01  1.77118384e-01  1.78452172e-01
  1.83176758e-01  1.85324470e-01  1.85689470e-01  1.86700907e-01
  1.87419192e-01  1.88794911e-01  1.89889270e-01  1.90613480e-01
  1.92089522e-01  1.92706079e-01  1.96267650e-01  1.96660003e-01
  1.98349689e-01  2.00726859e-01  2.01186141e-01  2.01523737e-01
  2.01888055e-01  2.05115212e-01  2.05681207e-01  2.06437967e-01
  2.06575535e-01  2.06622170e-01  2.08523888e-01  2.08528779e-01
  2.08537881e-01  2.10365709e-01  2.13478245e-01  2.14248319e-01
  2.14551624e-01  2.15135934e-01  2.15762559e-01  2.19015482e-01
  2.23396317e-01  2.26032947e-01  2.26272701e-01  2.26669645e-01
  2.27440847e-01  2.30031686e-01  2.31095937e-01  2.32581762e-01
  2.32700575e-01  2.33056804e-01  2.35064929e-01  2.35247886e-01
  2.35748923e-01  2.36602501e-01  2.37410358e-01  2.39010041e-01
  2.40822828e-01  2.42244793e-01  2.42599227e-01  2.43122370e-01
  2.44547899e-01  2.45017133e-01  2.46380054e-01  2.51352087e-01
  2.51689567e-01  2.54835042e-01  2.57112089e-01  2.58439942e-01
  2.58585418e-01  2.61856006e-01  2.62723762e-01  2.64759720e-01
  2.65139303e-01  2.67791832e-01  2.71827444e-01  2.71947938e-01
  2.72881233e-01  2.73429212e-01  2.73785953e-01  2.75727268e-01
  2.75937510e-01  2.78221729e-01  2.78666335e-01  2.79698804e-01
  2.79831196e-01  2.80048301e-01  2.83122695e-01  2.83600560e-01
  2.84876869e-01  2.87024394e-01  2.88755297e-01  2.91069334e-01
  2.92224552e-01  2.92521560e-01  2.92814941e-01  2.96117418e-01
  2.96647448e-01  2.96873705e-01  2.96922837e-01  2.97547317e-01
  2.98006987e-01  2.98791669e-01  2.99961541e-01  3.03065817e-01
  3.03518001e-01  3.04900169e-01  3.07554742e-01  3.08842159e-01
  3.09730072e-01  3.12426074e-01  3.13483752e-01  3.14355682e-01
  3.16672040e-01  3.16862914e-01  3.18921260e-01  3.19447033e-01
  3.21683151e-01  3.22056055e-01  3.24817300e-01  3.25282851e-01
  3.25415938e-01  3.27067958e-01  3.30382791e-01  3.34980881e-01
  3.35295274e-01  3.36648855e-01  3.38030496e-01  3.43218939e-01
  3.43253914e-01  3.45946290e-01  3.46883347e-01  3.50582743e-01
  3.52471360e-01  3.54972821e-01  3.58609920e-01  3.60281238e-01
  3.62045262e-01  3.62658847e-01  3.63342649e-01  3.63905083e-01
  3.65306178e-01  3.69991220e-01  3.72473235e-01  3.75854506e-01
  3.80633162e-01  3.80959881e-01  3.82711331e-01  3.85573340e-01
  3.86212644e-01  3.87612736e-01  3.91447397e-01  3.91999403e-01
  3.94713738e-01  3.96176206e-01  3.96308524e-01  3.96634470e-01
  3.97003940e-01  3.99035908e-01  3.99200554e-01  4.01260610e-01
  4.01515218e-01  4.03601547e-01  4.03994941e-01  4.04187965e-01
  4.04423300e-01  4.11339156e-01  4.11381773e-01  4.13035019e-01
  4.14806281e-01  4.14891893e-01  4.15231009e-01  4.19679285e-01
  4.22203037e-01  4.22228832e-01  4.22301995e-01  4.23891756e-01
  4.23997026e-01  4.24920487e-01  4.25456662e-01  4.26270077e-01
  4.30256237e-01  4.47144751e-01  4.47252407e-01  4.47535844e-01
  4.54342762e-01  4.54941486e-01  4.60136148e-01  4.64465839e-01
  4.86809159e-01  4.93777675e-01  4.98194344e-01  5.01556339e-01
  5.06121861e-01  5.07284039e-01  5.10522393e-01  5.15871630e-01
  5.19402374e-01]

  warnings.warn(

2022-11-03 10:51:15,407:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.23126416e-01 -2.19615078e-01 -2.17083819e-01 -2.03753273e-01
 -1.68333604e-01 -1.63923197e-01 -1.63823042e-01 -1.56957295e-01
 -1.54100372e-01 -1.53804573e-01 -1.52702686e-01 -1.51580086e-01
 -1.41701072e-01 -1.32971803e-01 -1.22595443e-01 -1.21807074e-01
 -1.20734999e-01 -1.18403927e-01 -1.14529489e-01 -1.12469935e-01
 -1.11086425e-01 -1.06847165e-01 -1.06126378e-01 -1.05346726e-01
 -1.04069909e-01 -1.03647861e-01 -1.02495142e-01 -9.92409446e-02
 -9.90281359e-02 -9.77675282e-02 -9.67283731e-02 -9.63000967e-02
 -9.20781353e-02 -9.14983423e-02 -9.14326844e-02 -8.99341428e-02
 -8.93943025e-02 -8.66770285e-02 -8.59538788e-02 -8.52212696e-02
 -8.49433291e-02 -8.45913388e-02 -7.78987967e-02 -7.73109244e-02
 -7.55640042e-02 -7.23766810e-02 -7.23514446e-02 -7.18581053e-02
 -7.11416394e-02 -7.08005250e-02 -7.05602359e-02 -7.03286522e-02
 -6.96717004e-02 -6.90715790e-02 -6.83477506e-02 -6.81130507e-02
 -6.76518338e-02 -6.62249366e-02 -6.35806074e-02 -6.27769352e-02
 -6.01720007e-02 -5.87333458e-02 -5.65435325e-02 -5.37808230e-02
 -5.37609914e-02 -5.24874864e-02 -5.12435945e-02 -5.09737727e-02
 -4.88200120e-02 -4.62130666e-02 -4.46491942e-02 -4.34519959e-02
 -4.28251642e-02 -4.01537817e-02 -3.88816617e-02 -3.88700635e-02
 -3.84308028e-02 -3.73100399e-02 -3.70515460e-02 -3.67465627e-02
 -3.62150674e-02 -3.52244111e-02 -3.39325846e-02 -3.34955241e-02
 -3.30388612e-02 -3.26805068e-02 -3.08259800e-02 -3.04926525e-02
 -2.82243745e-02 -2.73483774e-02 -2.60405299e-02 -2.50157702e-02
 -2.49985778e-02 -2.35306227e-02 -2.02278862e-02 -1.96887735e-02
 -1.96845597e-02 -1.90565785e-02 -1.59421378e-02 -1.55207057e-02
 -1.40343595e-02 -1.37877249e-02 -1.33964531e-02 -1.13909505e-02
 -1.05273530e-02 -1.03374070e-02 -1.00445649e-02 -9.00536214e-03
 -6.56809484e-03 -4.73771876e-03 -4.54027885e-03 -3.21588492e-03
 -1.92365514e-03  7.57567799e-05  4.34247045e-04  6.21091709e-03
  6.22709979e-03  6.41745108e-03  6.92321185e-03  7.08581460e-03
  7.62443153e-03  9.72118308e-03  1.00102266e-02  1.02893549e-02
  1.13914357e-02  1.19197074e-02  1.44705992e-02  1.94544261e-02
  2.08338957e-02  2.26334018e-02  2.51950354e-02  2.57051020e-02
  2.59335664e-02  2.61003854e-02  2.94961855e-02  3.25439225e-02
  3.25570383e-02  3.39752123e-02  3.43237810e-02  3.44041155e-02
  3.54569043e-02  3.58645757e-02  3.64160221e-02  3.69132012e-02
  3.77469605e-02  3.77487690e-02  3.78959877e-02  3.97849830e-02
  4.00051699e-02  4.16430346e-02  4.17662914e-02  4.33122262e-02
  4.41662110e-02  4.68582556e-02  4.77225570e-02  5.05465171e-02
  5.18888587e-02  5.31144321e-02  5.31371394e-02  5.70504734e-02
  5.70898820e-02  5.74816076e-02  5.75338450e-02  6.18253399e-02
  6.28325425e-02  6.28771486e-02  6.47235445e-02  6.50521229e-02
  6.63182290e-02  6.70781242e-02  6.72158882e-02  6.88845097e-02
  7.05505906e-02  7.15331557e-02  7.25058387e-02  7.28526414e-02
  7.30352852e-02  7.30852585e-02  7.38059307e-02  7.38754420e-02
  7.49862004e-02  7.64272724e-02  7.76307593e-02  7.97792622e-02
  7.99001944e-02  8.01912161e-02  8.04344794e-02  8.16857348e-02
  8.36488372e-02  8.40369173e-02  8.45053012e-02  8.56466545e-02
  8.57357954e-02  8.71742193e-02  8.84693994e-02  8.96515840e-02
  8.98638106e-02  9.07408203e-02  9.08166367e-02  9.13637701e-02
  9.23380560e-02  9.29138362e-02  9.33135966e-02  9.37987860e-02
  9.42993176e-02  9.56433639e-02  9.88709808e-02  9.96737756e-02
  1.01172770e-01  1.01733922e-01  1.03251028e-01  1.05381007e-01
  1.08672816e-01  1.09641166e-01  1.09647783e-01  1.09676797e-01
  1.09863020e-01  1.13298247e-01  1.14653475e-01  1.14867507e-01
  1.14953149e-01  1.14986071e-01  1.15966410e-01  1.16143822e-01
  1.16669099e-01  1.17656574e-01  1.17659109e-01  1.19168404e-01
  1.20629081e-01  1.20812234e-01  1.21250406e-01  1.22989263e-01
  1.23559567e-01  1.25436427e-01  1.25636710e-01  1.26094523e-01
  1.27444103e-01  1.27587162e-01  1.28338666e-01  1.28712912e-01
  1.29708580e-01  1.30173631e-01  1.30354708e-01  1.33565613e-01
  1.34124890e-01  1.34270781e-01  1.34281797e-01  1.34771267e-01
  1.35036127e-01  1.35580105e-01  1.38445185e-01  1.39297190e-01
  1.39429113e-01  1.41154887e-01  1.43664622e-01  1.43902737e-01
  1.43987838e-01  1.45712115e-01  1.45733859e-01  1.46231892e-01
  1.51469365e-01  1.52158187e-01  1.52987624e-01  1.54577011e-01
  1.55365981e-01  1.56486483e-01  1.56694242e-01  1.63869654e-01
  1.65360861e-01  1.66168719e-01  1.66341143e-01  1.67269998e-01
  1.67731148e-01  1.69780992e-01  1.70255424e-01  1.70322678e-01
  1.70400071e-01  1.70565534e-01  1.71799753e-01  1.73364087e-01
  1.74409186e-01  1.76035560e-01  1.78704126e-01  1.79381743e-01
  1.80583026e-01  1.80608846e-01  1.80810066e-01  1.81252806e-01
  1.85677939e-01  1.86629384e-01  1.87514535e-01  1.88323280e-01
  1.89555205e-01  1.89983416e-01  1.91745186e-01  1.92338721e-01
  1.93578940e-01  1.93765610e-01  1.96556572e-01  1.97376569e-01
  1.97485113e-01  1.98395289e-01  1.99998134e-01  2.00156282e-01
  2.00228777e-01  2.00281911e-01  2.01870980e-01  2.05808375e-01
  2.07091197e-01  2.11238141e-01  2.11256045e-01  2.11357161e-01
  2.12104306e-01  2.13375720e-01  2.14871889e-01  2.17581698e-01
  2.18133130e-01  2.18424225e-01  2.20044540e-01  2.21001855e-01
  2.24237264e-01  2.24542460e-01  2.25156626e-01  2.25170396e-01
  2.25437277e-01  2.30806013e-01  2.31329833e-01  2.33899721e-01
  2.34589122e-01  2.36461118e-01  2.38381851e-01  2.38960110e-01
  2.40572985e-01  2.41309193e-01  2.43176025e-01  2.43835616e-01
  2.46220029e-01  2.47024601e-01  2.48609824e-01  2.50038363e-01
  2.51004014e-01  2.52023390e-01  2.52161320e-01  2.52520557e-01
  2.54921574e-01  2.56078167e-01  2.63097349e-01  2.63815668e-01
  2.64689407e-01  2.67372812e-01  2.67974882e-01  2.68426145e-01
  2.71097814e-01  2.71179986e-01  2.71213338e-01  2.72591136e-01
  2.73761478e-01  2.74610493e-01  2.74838217e-01  2.75584618e-01
  2.76390469e-01  2.79290090e-01  2.80870204e-01  2.81620691e-01
  2.82393589e-01  2.82874779e-01  2.86635308e-01  2.86851077e-01
  2.87309813e-01  2.87605907e-01  2.89717654e-01  2.90762348e-01
  2.90858066e-01  2.91787408e-01  2.96739201e-01  2.96783213e-01
  2.98884242e-01  3.01885338e-01  3.03105329e-01  3.03518720e-01
  3.06792259e-01  3.07133527e-01  3.07831013e-01  3.08538904e-01
  3.08756798e-01  3.09696555e-01  3.10926119e-01  3.11437203e-01
  3.14660116e-01  3.15282450e-01  3.15910610e-01  3.18680684e-01
  3.20685171e-01  3.22389403e-01  3.24697386e-01  3.25534484e-01
  3.28656418e-01  3.29757468e-01  3.30231625e-01  3.33222479e-01
  3.33506337e-01  3.36527328e-01  3.37142538e-01  3.38055149e-01
  3.40115259e-01  3.41503616e-01  3.41734099e-01  3.42831922e-01
  3.44569421e-01  3.44900289e-01  3.46900824e-01  3.46967639e-01
  3.47055688e-01  3.47198523e-01  3.49029052e-01  3.52430486e-01
  3.54533838e-01  3.54982651e-01  3.55025185e-01  3.55110813e-01
  3.56402173e-01  3.59726591e-01  3.62239300e-01  3.65577504e-01
  3.70541045e-01  3.78154622e-01  3.80558267e-01  3.83101023e-01
  3.87008407e-01  3.90811770e-01  3.93369408e-01  3.93572720e-01
  3.94665316e-01  3.96620246e-01  4.00124916e-01  4.00186757e-01
  4.03843714e-01  4.04101559e-01  4.04628313e-01  4.04938406e-01
  4.05207721e-01  4.06021204e-01  4.07864055e-01  4.10265146e-01
  4.17936766e-01  4.22329081e-01  4.22658279e-01  4.27839586e-01
  4.34238659e-01  4.36329002e-01  4.36627163e-01  4.38518196e-01
  4.41090122e-01  4.41418033e-01  4.43937174e-01  4.47701314e-01
  4.51587657e-01  4.53957390e-01  4.55932483e-01  4.56901072e-01
  4.58809417e-01  4.62786767e-01  4.63824604e-01  4.68263900e-01
  4.74505637e-01  4.74854823e-01  4.76187357e-01  4.76571138e-01
  4.78084802e-01  4.78215549e-01  4.79838544e-01  4.79971973e-01
  4.81151003e-01  4.82284338e-01  4.82648775e-01  4.83772085e-01
  4.89426627e-01  4.91845584e-01  4.93115597e-01  5.02761239e-01
  5.05792572e-01  5.13712059e-01  5.15922691e-01  5.18335288e-01
  5.22291884e-01  5.25111456e-01  5.42873675e-01  5.55498934e-01
  5.62461433e-01  5.70265421e-01  5.85418742e-01  5.85970367e-01
  5.90075729e-01]

  warnings.warn(

2022-11-03 10:51:15,439:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-3.43855851e-01 -2.86529797e-01 -2.81144322e-01 -2.45630841e-01
 -2.31182977e-01 -2.27785747e-01 -2.23088815e-01 -2.13444967e-01
 -2.08940440e-01 -2.08605767e-01 -2.07572928e-01 -1.96319727e-01
 -1.93912794e-01 -1.88371879e-01 -1.86588672e-01 -1.83678033e-01
 -1.82626001e-01 -1.73380087e-01 -1.70695701e-01 -1.64724968e-01
 -1.63379630e-01 -1.62247698e-01 -1.62092585e-01 -1.57179588e-01
 -1.52291029e-01 -1.47457172e-01 -1.41836688e-01 -1.41403275e-01
 -1.40740261e-01 -1.38265522e-01 -1.36097423e-01 -1.33256532e-01
 -1.30877330e-01 -1.29570312e-01 -1.28964526e-01 -1.21737499e-01
 -1.20742318e-01 -1.18684956e-01 -1.18594230e-01 -1.18244146e-01
 -1.14367650e-01 -1.13233680e-01 -1.12686146e-01 -1.12082488e-01
 -1.08416260e-01 -1.07048071e-01 -1.04133122e-01 -1.03663458e-01
 -1.03284833e-01 -9.90007008e-02 -9.42638047e-02 -9.40497864e-02
 -9.32767032e-02 -9.31108171e-02 -9.19913321e-02 -9.03850358e-02
 -8.85371366e-02 -8.70600264e-02 -8.54559382e-02 -8.18507693e-02
 -8.13383529e-02 -8.09998578e-02 -7.98682570e-02 -7.93657825e-02
 -7.63553933e-02 -7.49725854e-02 -7.39060714e-02 -7.26162977e-02
 -7.14233702e-02 -7.08703191e-02 -7.08456614e-02 -6.94144348e-02
 -6.89818557e-02 -6.81288481e-02 -6.75022346e-02 -6.51700667e-02
 -6.45917369e-02 -6.19609761e-02 -5.53826650e-02 -5.46247850e-02
 -5.18787314e-02 -5.05596169e-02 -5.05088118e-02 -5.04737348e-02
 -4.55494900e-02 -4.38766922e-02 -4.29586735e-02 -4.23713888e-02
 -4.10638595e-02 -3.80150625e-02 -3.77208741e-02 -3.75124841e-02
 -3.71707388e-02 -3.68116264e-02 -3.55536400e-02 -3.33601286e-02
 -3.29372656e-02 -3.20105539e-02 -3.16435459e-02 -3.10382536e-02
 -3.01516402e-02 -2.90909518e-02 -2.29696411e-02 -2.21356814e-02
 -2.12662035e-02 -2.05959813e-02 -1.75652480e-02 -1.60463603e-02
 -1.55634157e-02 -1.52438164e-02 -9.89941981e-03 -8.67506610e-03
 -5.34640347e-03 -2.10486018e-03 -1.31927066e-03 -1.22690557e-03
 -4.52724459e-04  3.60153028e-03  6.89897787e-03  7.01605360e-03
  8.03013117e-03  9.29344667e-03  9.73653943e-03  1.07810810e-02
  1.10746204e-02  1.46012256e-02  1.46119077e-02  1.57360377e-02
  1.58798244e-02  1.63828289e-02  1.96045295e-02  2.23013917e-02
  2.38541191e-02  2.41687871e-02  2.71837133e-02  2.82787046e-02
  2.95658027e-02  3.07327102e-02  3.13528406e-02  3.17721256e-02
  3.56361290e-02  3.58923976e-02  3.60621506e-02  3.70309110e-02
  3.70612365e-02  3.75823206e-02  3.90998691e-02  4.35924843e-02
  4.38894604e-02  4.53158933e-02  4.75718895e-02  4.93413891e-02
  4.95173157e-02  5.11516162e-02  5.58779483e-02  5.65464548e-02
  5.71570966e-02  5.84901194e-02  5.91430148e-02  5.98989742e-02
  6.03060966e-02  6.12538509e-02  6.27976456e-02  6.54824483e-02
  6.70618035e-02  6.97186974e-02  6.97529546e-02  7.22809912e-02
  7.39981373e-02  7.46609872e-02  7.47646443e-02  7.89077719e-02
  7.96088557e-02  7.99228278e-02  8.02061098e-02  8.17114815e-02
  8.44395634e-02  8.68838554e-02  8.86835857e-02  8.90861649e-02
  8.98410504e-02  8.98462590e-02  9.03835591e-02  9.03843673e-02
  9.11842410e-02  9.47629616e-02  9.68867752e-02  9.76636221e-02
  9.81661057e-02  9.88896741e-02  9.93331682e-02  1.00031773e-01
  1.01244528e-01  1.01690634e-01  1.01799992e-01  1.02328328e-01
  1.03652438e-01  1.06317683e-01  1.06685976e-01  1.06793435e-01
  1.07372792e-01  1.07508157e-01  1.08872613e-01  1.09404160e-01
  1.09476657e-01  1.11354630e-01  1.11444281e-01  1.11692605e-01
  1.11943544e-01  1.12153508e-01  1.12472396e-01  1.19101348e-01
  1.19392340e-01  1.21492017e-01  1.22610097e-01  1.23127004e-01
  1.27082415e-01  1.27158166e-01  1.27206319e-01  1.29519160e-01
  1.30096585e-01  1.32227911e-01  1.33489313e-01  1.35311906e-01
  1.35947668e-01  1.38540496e-01  1.40402482e-01  1.41117979e-01
  1.41371949e-01  1.42079766e-01  1.42086455e-01  1.42691854e-01
  1.43874971e-01  1.44648657e-01  1.47917097e-01  1.49503902e-01
  1.50651571e-01  1.50898812e-01  1.50903641e-01  1.51797799e-01
  1.54346844e-01  1.55036012e-01  1.55194210e-01  1.55195707e-01
  1.59915915e-01  1.61181262e-01  1.61899151e-01  1.63482904e-01
  1.65202177e-01  1.65243334e-01  1.65433446e-01  1.65784850e-01
  1.72736423e-01  1.72824603e-01  1.72850431e-01  1.73293154e-01
  1.74859678e-01  1.75770929e-01  1.76873779e-01  1.77699939e-01
  1.78360694e-01  1.78825845e-01  1.82797356e-01  1.84408584e-01
  1.91802827e-01  1.92674615e-01  1.94423515e-01  1.94953864e-01
  1.95129666e-01  1.98746278e-01  1.99487630e-01  2.00823015e-01
  2.02183598e-01  2.02576257e-01  2.04119165e-01  2.05137253e-01
  2.09360295e-01  2.10334073e-01  2.10983872e-01  2.11998650e-01
  2.12100558e-01  2.14727684e-01  2.15572236e-01  2.18215851e-01
  2.20665635e-01  2.20760488e-01  2.24380988e-01  2.24748198e-01
  2.27134772e-01  2.28856096e-01  2.30818164e-01  2.31211337e-01
  2.31354277e-01  2.31886704e-01  2.32338532e-01  2.33099111e-01
  2.35867884e-01  2.36827422e-01  2.41489516e-01  2.42602567e-01
  2.43284604e-01  2.45200092e-01  2.46540353e-01  2.50410722e-01
  2.50631254e-01  2.53491240e-01  2.54106181e-01  2.54476297e-01
  2.54546186e-01  2.55886064e-01  2.55907041e-01  2.56594556e-01
  2.57283152e-01  2.57343276e-01  2.57886171e-01  2.63652700e-01
  2.67957711e-01  2.70628666e-01  2.71680642e-01  2.72281929e-01
  2.73274081e-01  2.78002014e-01  2.80156770e-01  2.80871626e-01
  2.83103963e-01  2.83154597e-01  2.83210319e-01  2.84436649e-01
  2.84675720e-01  2.86226959e-01  2.88346520e-01  2.89687156e-01
  2.89734525e-01  2.94651700e-01  2.94906733e-01  3.04485566e-01
  3.07022831e-01  3.07806900e-01  3.12062243e-01  3.13623372e-01
  3.14019400e-01  3.16675287e-01  3.17020652e-01  3.20993008e-01
  3.21148122e-01  3.22006849e-01  3.23384988e-01  3.24068115e-01
  3.24162959e-01  3.30520916e-01  3.35084824e-01  3.37042652e-01
  3.41417337e-01  3.42997636e-01  3.47893741e-01  3.50128318e-01
  3.53625202e-01  3.57851108e-01  3.59899199e-01  3.62892798e-01
  3.66063882e-01  3.66413843e-01  3.68652744e-01  3.70219612e-01
  3.70918133e-01  3.76610841e-01  3.76887998e-01  3.77262010e-01
  3.78354624e-01  3.79875334e-01  3.80268198e-01  3.81264067e-01
  3.84312286e-01  3.84492017e-01  3.96209969e-01  3.96457226e-01
  3.96705909e-01  4.00108294e-01  4.03689165e-01  4.05431603e-01
  4.14731439e-01  4.15012670e-01  4.15976067e-01  4.17899473e-01
  4.18374140e-01  4.20029319e-01  4.25906174e-01  4.27335068e-01
  4.30310103e-01  4.33267922e-01  4.33670077e-01  4.33850071e-01
  4.40751798e-01  4.41505187e-01  4.42044314e-01  4.44068261e-01
  4.46153555e-01  4.48454076e-01  4.51710669e-01  4.52887759e-01
  4.54825137e-01  4.54905197e-01  4.58257903e-01  4.61398839e-01
  4.66404406e-01  4.67328433e-01  4.67729920e-01  4.68138786e-01
  4.70979273e-01  4.72022913e-01  4.79347173e-01  4.80573353e-01
  4.85911390e-01  4.87348982e-01  4.87699368e-01  4.94933479e-01
  4.95251223e-01  4.98442856e-01  5.02836733e-01  5.05248615e-01
  5.09737913e-01  5.10120874e-01  5.10473261e-01  5.10565296e-01
  5.11900383e-01  5.12010071e-01  5.12311798e-01  5.16073629e-01
  5.17291507e-01  5.20088880e-01  5.20336730e-01  5.22164839e-01
  5.23949470e-01  5.27894576e-01  5.29137900e-01  5.31517451e-01
  5.34546703e-01  5.42646674e-01  5.43352020e-01  5.47577429e-01
  5.53519278e-01  5.59253285e-01  5.67014559e-01  5.68629236e-01
  5.69798797e-01  5.69995559e-01  5.70908884e-01  5.71927409e-01
  5.74251658e-01  5.77525317e-01  5.77603712e-01  5.87903395e-01
  5.88295989e-01  5.89300516e-01  5.90430219e-01  5.93160981e-01
  5.94831580e-01  5.96017087e-01  5.98045925e-01  6.04226335e-01
  6.04640916e-01  6.09784839e-01  6.12633021e-01  6.13639918e-01
  6.16684507e-01  6.17622399e-01  6.19432829e-01  6.23251600e-01
  6.23571953e-01  6.30890606e-01  6.30964755e-01  6.35181660e-01
  6.47503578e-01  6.51731125e-01  6.54579211e-01  6.55062005e-01
  6.56044372e-01  6.58778002e-01  6.61424434e-01  6.62642456e-01
  6.67750960e-01  6.68821563e-01  6.70967804e-01  6.74025216e-01
  6.81673558e-01  6.82484863e-01  6.83327026e-01  6.92748810e-01
  6.95711672e-01  6.96743031e-01  6.97136674e-01  6.98445461e-01
  7.02147262e-01]

  warnings.warn(

2022-11-03 10:51:17,172:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:17,342:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:18,014:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.25935314 -0.22886796 -0.19909864 -0.19373132 -0.18881728 -0.18570603
 -0.17415481 -0.16451431 -0.16406108 -0.16205373 -0.15471423 -0.15344119
 -0.14999578 -0.14855287 -0.14161787 -0.13850339 -0.13033392 -0.12957808
 -0.12774978 -0.1267445  -0.1237257  -0.12277793 -0.12146526 -0.11970715
 -0.11823284 -0.1142652  -0.11418819 -0.11327263 -0.11142504 -0.10740241
 -0.10711875 -0.1046569  -0.1041006  -0.10033598 -0.10032755 -0.09997511
 -0.09947929 -0.09804215 -0.09613335 -0.09592637 -0.09374562 -0.09368915
 -0.09118561 -0.09109426 -0.09108776 -0.09101429 -0.09062707 -0.08849435
 -0.08758819 -0.08593071 -0.08438072 -0.08421623 -0.08413824 -0.08223052
 -0.08097094 -0.07802596 -0.07732122 -0.07706095 -0.07449971 -0.07184622
 -0.06759833 -0.06670952 -0.06433544 -0.0643021  -0.06420932 -0.06030831
 -0.05805428 -0.05761125 -0.05538606 -0.05431815 -0.05392552 -0.05206279
 -0.04988365 -0.04958763 -0.04887618 -0.04611721 -0.04596188 -0.04566102
 -0.04413918 -0.04226859 -0.03949708 -0.03945012 -0.0369658  -0.03664499
 -0.03555183 -0.03475482 -0.03360029 -0.03228564 -0.03116889 -0.02877164
 -0.02842615 -0.02831167 -0.02486134 -0.02001174 -0.01842925 -0.01715269
 -0.0144173  -0.01437348 -0.01279816 -0.01268827 -0.00565102 -0.00448639
 -0.00302776  0.0011658   0.0014891   0.00177898  0.00319781  0.00348202
  0.00360742  0.00462237  0.005231    0.00680235  0.00762329  0.00806559
  0.0081881   0.00933932  0.01213309  0.01257723  0.01368198  0.01373817
  0.01511152  0.01642398  0.01744927  0.01877279  0.01930337  0.01968256
  0.01997104  0.02052726  0.02209439  0.02337325  0.02341905  0.02526753
  0.02620127  0.02770363  0.02846492  0.02938234  0.02968067  0.03214183
  0.0346618   0.03648733  0.03728321  0.03741032  0.03836467  0.04106878
  0.04138276  0.04234364  0.04235243  0.04249538  0.04254441  0.04342291
  0.0511053   0.05120757  0.05160212  0.05188366  0.05290008  0.05461256
  0.05471481  0.05548022  0.05558768  0.05564959  0.05636599  0.05938991
  0.05979396  0.06186897  0.06306634  0.06343058  0.06348933  0.06390495
  0.06583693  0.0667745   0.06710154  0.06874906  0.07055943  0.07078872
  0.07167628  0.07482771  0.07644384  0.07663549  0.07817266  0.07852377
  0.078894    0.07913127  0.07932732  0.08015486  0.08116876  0.0815337
  0.08155332  0.08158585  0.08178938  0.08179115  0.08236919  0.08326443
  0.08535556  0.08592027  0.08765089  0.08837867  0.0884046   0.08984206
  0.09016528  0.0903257   0.09166818  0.09216314  0.09221311  0.09247545
  0.09343762  0.09434802  0.09544861  0.09701456  0.09718812  0.09969133
  0.10022946  0.10155192  0.10524502  0.10567931  0.10585471  0.10671307
  0.10787162  0.1089186   0.1100637   0.11032385  0.1103432   0.11125494
  0.11171359  0.1117447   0.11236376  0.11348073  0.11373921  0.11407436
  0.11435143  0.11731933  0.12238344  0.1227047   0.12366456  0.12470846
  0.12642132  0.12801003  0.12883318  0.13175682  0.13745255  0.13835879
  0.13951634  0.13964245  0.1401304   0.1405054   0.14096278  0.14170743
  0.14191806  0.14270227  0.14321325  0.14425295  0.14459463  0.14658496
  0.14695275  0.14902794  0.14964949  0.14990024  0.1531514   0.15345743
  0.15478544  0.15631976  0.16241292  0.16628957  0.16644739  0.16727541
  0.16882721  0.16890792  0.17014886  0.1749347   0.17579717  0.17650799
  0.17852904  0.1788767   0.18038953  0.18250624  0.18304986  0.18530635
  0.18563181  0.18684652  0.18692494  0.1902152   0.19023567  0.19385417
  0.19489143  0.19492984  0.19518308  0.19786659  0.1982863   0.2017715
  0.20223993  0.20342401  0.20578135  0.20598274  0.20807794  0.21020393
  0.21055641  0.21115964  0.21123621  0.21178887  0.21197481  0.21233643
  0.21245533  0.21432628  0.21602909  0.21766976  0.21839309  0.21969032
  0.21987665  0.22406267  0.22414462  0.22503167  0.22572657  0.229172
  0.23032249  0.23332398  0.23416762  0.23541561  0.23586141  0.23684768
  0.23768041  0.23860425  0.24263924  0.24536664  0.24614215  0.24871402
  0.25296254  0.25441769  0.25556772  0.25584702  0.25774306  0.25853971
  0.25958524  0.25962165  0.26126297  0.26146755  0.26369037  0.26486752
  0.26527029  0.26597182  0.26837933  0.26924003  0.27217809  0.27225908
  0.27299891  0.27311652  0.27461185  0.27994588  0.28070895  0.2852798
  0.28566827  0.28711738  0.28788651  0.28828068  0.28954115  0.29496147
  0.29657673  0.29664823  0.29788069  0.29995857  0.30078484  0.30417486
  0.30971031  0.31067113  0.31176105  0.3123565   0.31292643  0.31325438
  0.31630821  0.31709278  0.31928964  0.32020887  0.32113928  0.33040823
  0.33689222  0.33724265  0.33784256  0.34085911  0.34246125  0.34261665
  0.34600059  0.35002995  0.3503412   0.35091313  0.35265949  0.35271345
  0.35562236  0.35581405  0.35730996  0.35765958  0.35812443  0.36025017
  0.36148817  0.3620474   0.36378198  0.36537513  0.36592648  0.3665782
  0.37024079  0.37039885  0.37115522  0.37143717  0.37143961  0.37394274
  0.37428184  0.37428236  0.375895    0.37695567  0.37735879  0.38157967
  0.3852152   0.38730234  0.3958599   0.39656809  0.39792912  0.40030006
  0.40102098  0.40260048  0.40380503  0.40421698  0.40512962  0.40970643
  0.41049319  0.41251699  0.41420026  0.41629948  0.41706617  0.41857318
  0.41878726  0.41966332  0.41971697  0.42475169  0.42520683  0.4290287
  0.43044421  0.43688048  0.43944558  0.44037438  0.44236291  0.44277953
  0.44885322  0.45474691  0.45835022  0.46279211  0.46314493  0.46460395
  0.46520336  0.465482    0.47280357  0.47344223  0.47439954  0.47669847
  0.4771229   0.47944388  0.48108444  0.48196683  0.48255532  0.48505467
  0.48601687  0.49359728  0.49523195  0.49524338  0.49697915  0.49727843
  0.49736825  0.50233322  0.50412371  0.50535467  0.50630342  0.50800641
  0.51127637  0.51172251  0.51185989  0.51382664  0.52064891  0.53071341
  0.5363281   0.54383327  0.54473772  0.55776923  0.57123548  0.57357877
  0.5810613   0.58646474  0.60006431  0.61288454  0.61807147  0.61927888
  0.61987398  0.62057183  0.62294258  0.62719501  0.62833633  0.63900107
  0.64013915]

  warnings.warn(

2022-11-03 10:51:18,046:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.23352009 -0.21775838 -0.20311982 -0.19994159 -0.19744981 -0.19577474
 -0.18980012 -0.17945344 -0.17660442 -0.16813306 -0.15741536 -0.15209184
 -0.1514875  -0.15029532 -0.14946484 -0.14868733 -0.14867859 -0.14038282
 -0.1400508  -0.13915165 -0.13650341 -0.13353786 -0.13317646 -0.13280608
 -0.12737339 -0.12404101 -0.12302308 -0.12219105 -0.11745656 -0.11473712
 -0.11404015 -0.1129592  -0.1108083  -0.10970872 -0.10922754 -0.10715345
 -0.10318619 -0.10269237 -0.09745912 -0.09696177 -0.09649052 -0.09395318
 -0.09216035 -0.08858752 -0.08848821 -0.08809661 -0.08420951 -0.08385181
 -0.08237359 -0.08166291 -0.08146015 -0.07611052 -0.07573504 -0.07548292
 -0.07503324 -0.07435154 -0.07411678 -0.07331122 -0.07079142 -0.06989308
 -0.06786176 -0.06764327 -0.06610995 -0.06598989 -0.06463007 -0.06441302
 -0.05813765 -0.05757504 -0.05391172 -0.05328908 -0.0531816  -0.05209605
 -0.05040693 -0.04968132 -0.04756766 -0.04725821 -0.0472431  -0.04677338
 -0.04521396 -0.0440529  -0.04238826 -0.03989175 -0.03944803 -0.03908057
 -0.03547845 -0.03472078 -0.03304414 -0.03179277 -0.02808897 -0.02797236
 -0.0278925  -0.02681387 -0.02657807 -0.02332722 -0.02074844 -0.02070357
 -0.02012191 -0.01848045 -0.01778367 -0.01714381 -0.01533087 -0.014111
 -0.00937649 -0.00533395 -0.00442594 -0.00388156 -0.00144357  0.00292765
  0.00345142  0.0039962   0.00410255  0.00415945  0.00911729  0.01084365
  0.01279134  0.01298908  0.01483818  0.01632361  0.01944948  0.02090512
  0.02244538  0.02358452  0.02458407  0.02496768  0.02620858  0.03123994
  0.03124026  0.03153598  0.03240292  0.03296248  0.0333443   0.03410764
  0.03713555  0.03970951  0.04003809  0.04048322  0.0422809   0.04242583
  0.04322424  0.04339707  0.04376634  0.04738109  0.04906333  0.04907295
  0.04943244  0.05092032  0.05092062  0.05097833  0.05209231  0.05323245
  0.05455467  0.05644454  0.05782435  0.05840565  0.05842317  0.06174487
  0.06269667  0.0630508   0.06305564  0.0632052   0.06337114  0.06406551
  0.0664094   0.06670637  0.06921573  0.07241936  0.07678609  0.07735915
  0.0775396   0.07862345  0.07873728  0.07943401  0.08037311  0.08050776
  0.08148006  0.08282633  0.0830697   0.08341626  0.08350978  0.08540692
  0.08575648  0.08649577  0.08806618  0.09240526  0.0924128   0.09291307
  0.09372316  0.096144    0.09772195  0.09790382  0.09979112  0.10013279
  0.10117623  0.10476105  0.10515416  0.10620005  0.10737041  0.10884935
  0.11146567  0.11231154  0.11372567  0.11538286  0.11590452  0.11705335
  0.11755569  0.11809972  0.11997337  0.1221959   0.1268476   0.12910995
  0.12924928  0.12937959  0.12938921  0.13000317  0.13108789  0.13574468
  0.13591397  0.13629118  0.13818817  0.14153752  0.14244128  0.14311734
  0.14333917  0.14341515  0.1481108   0.14923768  0.14969848  0.15191366
  0.15395155  0.1566919   0.16417785  0.16621433  0.1662658   0.16647797
  0.16655651  0.16714917  0.16996247  0.17340807  0.17547315  0.17588497
  0.17700613  0.17712834  0.17743632  0.17886734  0.17974757  0.18018287
  0.1824832   0.18325771  0.1832649   0.18360428  0.18474884  0.19345821
  0.19548214  0.19712958  0.1976656   0.19783684  0.20097684  0.20235905
  0.20287434  0.20541129  0.20708741  0.20855137  0.21442588  0.21449241
  0.21482342  0.2152181   0.21641117  0.21652839  0.21741274  0.22146297
  0.22175906  0.221856    0.22231211  0.22304626  0.22353302  0.2262822
  0.22772572  0.22848385  0.22895416  0.23397998  0.23531515  0.2360197
  0.2365673   0.23948266  0.24108177  0.24285789  0.24351827  0.24784409
  0.24888908  0.24956856  0.25292892  0.25709056  0.25772563  0.25845919
  0.25864932  0.26621263  0.26680076  0.26689898  0.26743608  0.2690759
  0.26933062  0.2697426   0.27070863  0.27298878  0.27494755  0.27639853
  0.27734092  0.28133511  0.28152922  0.28261871  0.28764125  0.28946341
  0.29312404  0.29692614  0.29925396  0.29957959  0.30201278  0.30208829
  0.30230099  0.30367706  0.30423537  0.30430017  0.30651801  0.30671907
  0.30754261  0.30867948  0.30936222  0.31001974  0.31126096  0.31461453
  0.31549534  0.31854159  0.31924908  0.3209186   0.32154793  0.32204885
  0.32268653  0.32983848  0.3300335   0.33330801  0.3382024   0.34213129
  0.34394616  0.34476156  0.34685323  0.35002332  0.35062915  0.35094727
  0.3526636   0.35267663  0.353789    0.35744286  0.35985757  0.36104308
  0.36163462  0.36699891  0.37045976  0.37112562  0.37265042  0.37280373
  0.37285692  0.37774104  0.38240492  0.38295869  0.38540485  0.3883112
  0.39078894  0.39129643  0.39684167  0.39825877  0.40025895  0.40152642
  0.40331839  0.40595758  0.40664075  0.40737867  0.41135474  0.41224833
  0.41580081  0.41999397  0.42051045  0.42111769  0.42620394  0.42836706
  0.43085355  0.43294213  0.43319244  0.43320446  0.43338119  0.43691459
  0.43974728  0.43980649  0.44337149  0.44407374  0.44426528  0.44438687
  0.4462944   0.45628268  0.45727141  0.46007465  0.46899502  0.46922884
  0.46977183  0.47130547  0.47180065  0.47261036  0.47488267  0.47624819
  0.48503132  0.48534355  0.4867369   0.49055491  0.49146151  0.49366074
  0.49492283  0.49668477  0.49794969  0.50275517  0.5031809   0.5044654
  0.50518157  0.50913634  0.51101486  0.51207718  0.51330177  0.51422476
  0.51490878  0.51663551  0.51722995  0.52011541  0.52031409  0.52044597
  0.52251711  0.5240473   0.52662902  0.53501024  0.53952435  0.53982274
  0.54054113  0.54479229  0.54534482  0.54990072  0.55350227  0.55682057
  0.56544978  0.56736968  0.56776563  0.57166734  0.57274947  0.57737678
  0.58362588  0.5864078   0.58826968  0.58987818  0.59036189  0.59216279
  0.59330236  0.59617021  0.59857133  0.60553442  0.605953    0.61029633
  0.6106321   0.61088956  0.61148144  0.61434496  0.6170134   0.62230575
  0.62386062  0.62401816  0.62493421  0.62682838  0.62819085  0.62972504
  0.6320261   0.63295923  0.63486601  0.63492278  0.63507381  0.65790804
  0.66833287  0.67269922  0.67712168  0.67720127  0.68864188  0.68898131
  0.69558967  0.70072978  0.71455274  0.71600503  0.71721647  0.74198091
  0.7505446 ]

  warnings.warn(

2022-11-03 10:51:18,048:INFO:Calculating mean and std
2022-11-03 10:51:18,048:INFO:Creating metrics dataframe
2022-11-03 10:51:18,057:INFO:Uploading results into container
2022-11-03 10:51:18,057:INFO:Uploading model into container now
2022-11-03 10:51:18,057:INFO:master_model_container: 19
2022-11-03 10:51:18,057:INFO:display_container: 2
2022-11-03 10:51:18,065:INFO:HuberRegressor()
2022-11-03 10:51:18,065:INFO:create_model() successfully completed......................................
2022-11-03 10:51:18,323:WARNING:create_model() for HuberRegressor() raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:51:18,323:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:51:18,323:INFO:Initializing create_model()
2022-11-03 10:51:18,323:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:51:18,323:INFO:Checking exceptions
2022-11-03 10:51:18,339:INFO:Importing libraries
2022-11-03 10:51:18,339:INFO:Copying training dataset
2022-11-03 10:51:18,355:INFO:Defining folds
2022-11-03 10:51:18,355:INFO:Declaring metric variables
2022-11-03 10:51:18,355:INFO:Importing untrained model
2022-11-03 10:51:18,355:INFO:Huber Regressor Imported successfully
2022-11-03 10:51:18,355:INFO:Starting cross validation
2022-11-03 10:51:18,370:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:51:21,803:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:21,886:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:21,905:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:21,969:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:22,184:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:22,203:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:22,257:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:22,319:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:23,018:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.77456344e-01 -1.76133950e-01 -1.62624612e-01 -1.60611694e-01
 -1.46390918e-01 -1.38930062e-01 -1.38106189e-01 -1.28704278e-01
 -1.27408644e-01 -1.25737282e-01 -1.25227453e-01 -1.22812413e-01
 -1.20701605e-01 -1.18765284e-01 -1.15488193e-01 -1.13140693e-01
 -1.12358704e-01 -1.08482769e-01 -1.07885968e-01 -1.06169014e-01
 -9.97480221e-02 -9.88806432e-02 -9.82356006e-02 -9.59997179e-02
 -9.56156937e-02 -9.46272547e-02 -9.39078193e-02 -9.35378077e-02
 -9.11455006e-02 -8.96304055e-02 -8.42616554e-02 -8.41010405e-02
 -8.32975297e-02 -8.15740483e-02 -7.84250083e-02 -7.67061748e-02
 -7.63280912e-02 -7.53047426e-02 -7.45556904e-02 -7.24832285e-02
 -7.16184068e-02 -7.11248597e-02 -7.09951311e-02 -7.05333315e-02
 -6.32387304e-02 -6.07336055e-02 -5.86631231e-02 -5.84935411e-02
 -5.61044897e-02 -5.55495221e-02 -5.42642552e-02 -5.38981882e-02
 -5.34995423e-02 -5.19288053e-02 -5.16027712e-02 -5.06749807e-02
 -5.06138546e-02 -4.98508445e-02 -4.96411894e-02 -4.93973854e-02
 -4.82581977e-02 -4.55568683e-02 -4.48274355e-02 -4.46879652e-02
 -4.21305225e-02 -4.18185316e-02 -3.97755758e-02 -3.79032008e-02
 -3.70142364e-02 -3.35635124e-02 -3.33587806e-02 -3.32757820e-02
 -3.02514180e-02 -2.69583869e-02 -2.45628047e-02 -2.43613519e-02
 -2.32662909e-02 -2.03274909e-02 -1.95562320e-02 -1.91966959e-02
 -1.80516200e-02 -1.73281512e-02 -1.70969235e-02 -1.51535217e-02
 -1.48702286e-02 -1.33849584e-02 -1.22448811e-02 -1.22258146e-02
 -1.01888013e-02 -9.91946537e-03 -9.82710843e-03 -9.75750880e-03
 -8.61275676e-03 -7.27698888e-03 -5.01756437e-03 -1.92683051e-03
  2.04100965e-04  8.66066624e-04  3.40935472e-03  4.41020597e-03
  5.10442839e-03  5.20753066e-03  5.92539292e-03  7.31305877e-03
  8.30489063e-03  1.06396962e-02  1.21627451e-02  1.26563159e-02
  1.36039487e-02  1.41186468e-02  1.48003470e-02  1.54318000e-02
  1.56451937e-02  1.61195335e-02  1.67283303e-02  1.85527518e-02
  1.88609485e-02  1.90109830e-02  1.92937011e-02  2.19932392e-02
  2.31082630e-02  2.75978997e-02  2.89825757e-02  2.92097082e-02
  3.01270682e-02  3.05787729e-02  3.12592223e-02  3.29180800e-02
  3.30400290e-02  3.46431577e-02  3.54004352e-02  3.54565998e-02
  3.77532933e-02  3.89814271e-02  4.06001985e-02  4.09420456e-02
  4.20687083e-02  4.45583115e-02  4.47527980e-02  4.54277025e-02
  4.56458605e-02  4.58400628e-02  4.60248742e-02  4.61890287e-02
  4.64795767e-02  4.71053036e-02  4.93106360e-02  4.95275582e-02
  5.04807710e-02  5.13620834e-02  5.15080361e-02  5.29921376e-02
  5.43305054e-02  5.54613231e-02  5.61129859e-02  5.63713557e-02
  5.67349928e-02  5.68609986e-02  5.72356580e-02  5.88304247e-02
  6.15639434e-02  6.17585244e-02  6.26415976e-02  6.30749582e-02
  6.32563909e-02  6.33944213e-02  6.38460031e-02  6.40456045e-02
  6.45072346e-02  6.45785253e-02  6.59626109e-02  6.61092104e-02
  6.67265301e-02  6.71912188e-02  6.76959773e-02  6.83441521e-02
  6.86298097e-02  6.96269591e-02  6.96746266e-02  7.05187752e-02
  7.07914241e-02  7.08823059e-02  7.09266519e-02  7.10457433e-02
  7.23295514e-02  7.23638645e-02  7.24463488e-02  7.25252423e-02
  7.31764720e-02  7.40151625e-02  7.41298346e-02  7.45853043e-02
  7.58860014e-02  7.63368602e-02  7.73616705e-02  7.87431336e-02
  7.94083245e-02  7.98112716e-02  8.07715725e-02  8.09959384e-02
  8.12543769e-02  8.13169369e-02  8.16127024e-02  8.31121067e-02
  8.34509785e-02  8.42941617e-02  8.60967915e-02  8.65963783e-02
  8.67539633e-02  8.69601785e-02  8.75550310e-02  8.77751479e-02
  8.84482254e-02  8.90540882e-02  9.03832604e-02  9.05600470e-02
  9.47077146e-02  9.47368170e-02  9.50754128e-02  9.51999337e-02
  9.55905576e-02  9.65101510e-02  9.73259870e-02  9.87437190e-02
  9.88433684e-02  9.89559456e-02  1.03465751e-01  1.03834588e-01
  1.05410672e-01  1.05894371e-01  1.06367468e-01  1.07442217e-01
  1.09229907e-01  1.09603434e-01  1.09678999e-01  1.10246096e-01
  1.10319349e-01  1.10643837e-01  1.10711657e-01  1.11884948e-01
  1.12204854e-01  1.14306882e-01  1.16051189e-01  1.16061332e-01
  1.17886435e-01  1.20203951e-01  1.21324271e-01  1.24325312e-01
  1.24382377e-01  1.24843337e-01  1.25538870e-01  1.27009323e-01
  1.28868881e-01  1.29698831e-01  1.30020937e-01  1.30543176e-01
  1.32313927e-01  1.35289449e-01  1.37757156e-01  1.38932317e-01
  1.39745241e-01  1.40708364e-01  1.40927514e-01  1.41258708e-01
  1.42638536e-01  1.43739143e-01  1.46238319e-01  1.46531878e-01
  1.48807840e-01  1.51512937e-01  1.53771475e-01  1.63360006e-01
  1.63720906e-01  1.68112679e-01  1.69727045e-01  1.69848834e-01
  1.70705976e-01  1.71216397e-01  1.71671664e-01  1.71950794e-01
  1.73522808e-01  1.75118003e-01  1.78876032e-01  1.79059379e-01
  1.81288307e-01  1.81828994e-01  1.82238411e-01  1.83403194e-01
  1.84609439e-01  1.88735191e-01  1.88932964e-01  1.90156283e-01
  1.90532387e-01  1.91650967e-01  1.91783479e-01  1.94531688e-01
  1.95490450e-01  1.95869739e-01  1.97016537e-01  1.97338939e-01
  1.97392092e-01  1.99887588e-01  2.00588431e-01  2.01051501e-01
  2.01368832e-01  2.01769512e-01  2.02025054e-01  2.02469734e-01
  2.02611715e-01  2.03310009e-01  2.04148687e-01  2.04331262e-01
  2.06355788e-01  2.06543397e-01  2.07733484e-01  2.07934382e-01
  2.10280042e-01  2.11015802e-01  2.11639526e-01  2.11710201e-01
  2.11961698e-01  2.12439400e-01  2.13133664e-01  2.14991516e-01
  2.16138832e-01  2.16785275e-01  2.22296562e-01  2.22953986e-01
  2.23020855e-01  2.25025271e-01  2.26025397e-01  2.27516245e-01
  2.27802478e-01  2.29088542e-01  2.32068245e-01  2.39490608e-01
  2.41252548e-01  2.41906718e-01  2.42959269e-01  2.46266607e-01
  2.46654165e-01  2.48422998e-01  2.48978320e-01  2.50328938e-01
  2.50940153e-01  2.52761649e-01  2.58383048e-01  2.59440270e-01
  2.61162924e-01  2.61848169e-01  2.63133918e-01  2.63148154e-01
  2.63171261e-01  2.63510635e-01  2.64471114e-01  2.64943661e-01
  2.65013422e-01  2.65370235e-01  2.65710847e-01  2.66098037e-01
  2.67455379e-01  2.68682420e-01  2.68953163e-01  2.69551683e-01
  2.69656393e-01  2.70426298e-01  2.78354675e-01  2.80171332e-01
  2.81316314e-01  2.81317761e-01  2.83995845e-01  2.84891835e-01
  2.85243946e-01  2.86942436e-01  2.89597107e-01  2.90211120e-01
  2.90730825e-01  2.91440514e-01  2.91732489e-01  2.92068173e-01
  2.94095301e-01  2.97603684e-01  3.00610996e-01  3.02929657e-01
  3.03369687e-01  3.03967625e-01  3.04932177e-01  3.04939866e-01
  3.05701712e-01  3.06557393e-01  3.07544385e-01  3.11237923e-01
  3.12120445e-01  3.14158489e-01  3.14347706e-01  3.15904668e-01
  3.17603639e-01  3.21102094e-01  3.22332502e-01  3.22407215e-01
  3.22655091e-01  3.23583931e-01  3.25573382e-01  3.29568041e-01
  3.30995529e-01  3.31471222e-01  3.32036331e-01  3.33730618e-01
  3.34229079e-01  3.35183175e-01  3.35425420e-01  3.35833378e-01
  3.36499195e-01  3.37344372e-01  3.38141255e-01  3.39157845e-01
  3.39371985e-01  3.43915942e-01  3.44491965e-01  3.48536166e-01
  3.48971663e-01  3.49225196e-01  3.50007942e-01  3.51109263e-01
  3.59343744e-01  3.60069443e-01  3.64866770e-01  3.65556177e-01
  3.67108750e-01  3.67148796e-01  3.68183045e-01  3.68832053e-01
  3.71643531e-01  3.74834630e-01  3.81780036e-01  3.81878973e-01
  3.84255879e-01  3.89077378e-01  3.90317420e-01  3.92698337e-01
  3.96602178e-01  4.00086719e-01  4.04567429e-01  4.05344661e-01
  4.07185571e-01  4.09462799e-01  4.09620256e-01  4.10468617e-01
  4.10982117e-01  4.11960574e-01  4.13700205e-01  4.15166161e-01
  4.20864720e-01  4.24864596e-01  4.28229470e-01  4.28441423e-01
  4.29905460e-01  4.30223563e-01  4.31068932e-01  4.35414510e-01
  4.36043691e-01  4.41133295e-01  4.44656604e-01  4.45411627e-01
  4.45629428e-01  4.46692991e-01  4.47592832e-01  4.50689495e-01
  4.51143911e-01  4.59526954e-01  4.65315777e-01  4.70705259e-01
  4.72847778e-01  4.74830639e-01  4.75228929e-01  4.75600660e-01
  4.83068728e-01  4.85519853e-01  4.86532252e-01  4.91193547e-01
  4.95435517e-01  5.01753204e-01  5.11715571e-01  5.13867955e-01
  5.16326789e-01  5.27767047e-01  5.28604380e-01  5.29484359e-01
  5.37726570e-01  5.47105168e-01  5.50450378e-01  5.50586158e-01
  5.50641213e-01]

  warnings.warn(

2022-11-03 10:51:23,078:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-3.43855851e-01 -2.86529797e-01 -2.81144322e-01 -2.45630841e-01
 -2.31182977e-01 -2.27785747e-01 -2.23088815e-01 -2.13444967e-01
 -2.08940440e-01 -2.08605767e-01 -2.07572928e-01 -1.96319727e-01
 -1.93912794e-01 -1.88371879e-01 -1.86588672e-01 -1.83678033e-01
 -1.82626001e-01 -1.73380087e-01 -1.70695701e-01 -1.64724968e-01
 -1.63379630e-01 -1.62247698e-01 -1.62092585e-01 -1.57179588e-01
 -1.52291029e-01 -1.47457172e-01 -1.41836688e-01 -1.41403275e-01
 -1.40740261e-01 -1.38265522e-01 -1.36097423e-01 -1.33256532e-01
 -1.30877330e-01 -1.29570312e-01 -1.28964526e-01 -1.21737499e-01
 -1.20742318e-01 -1.18684956e-01 -1.18594230e-01 -1.18244146e-01
 -1.14367650e-01 -1.13233680e-01 -1.12686146e-01 -1.12082488e-01
 -1.08416260e-01 -1.07048071e-01 -1.04133122e-01 -1.03663458e-01
 -1.03284833e-01 -9.90007008e-02 -9.42638047e-02 -9.40497864e-02
 -9.32767032e-02 -9.31108171e-02 -9.19913321e-02 -9.03850358e-02
 -8.85371366e-02 -8.70600264e-02 -8.54559382e-02 -8.18507693e-02
 -8.13383529e-02 -8.09998578e-02 -7.98682570e-02 -7.93657825e-02
 -7.63553933e-02 -7.49725854e-02 -7.39060714e-02 -7.26162977e-02
 -7.14233702e-02 -7.08703191e-02 -7.08456614e-02 -6.94144348e-02
 -6.89818557e-02 -6.81288481e-02 -6.75022346e-02 -6.51700667e-02
 -6.45917369e-02 -6.19609761e-02 -5.53826650e-02 -5.46247850e-02
 -5.18787314e-02 -5.05596169e-02 -5.05088118e-02 -5.04737348e-02
 -4.55494900e-02 -4.38766922e-02 -4.29586735e-02 -4.23713888e-02
 -4.10638595e-02 -3.80150625e-02 -3.77208741e-02 -3.75124841e-02
 -3.71707388e-02 -3.68116264e-02 -3.55536400e-02 -3.33601286e-02
 -3.29372656e-02 -3.20105539e-02 -3.16435459e-02 -3.10382536e-02
 -3.01516402e-02 -2.90909518e-02 -2.29696411e-02 -2.21356814e-02
 -2.12662035e-02 -2.05959813e-02 -1.75652480e-02 -1.60463603e-02
 -1.55634157e-02 -1.52438164e-02 -9.89941981e-03 -8.67506610e-03
 -5.34640347e-03 -2.10486018e-03 -1.31927066e-03 -1.22690557e-03
 -4.52724459e-04  3.60153028e-03  6.89897787e-03  7.01605360e-03
  8.03013117e-03  9.29344667e-03  9.73653943e-03  1.07810810e-02
  1.10746204e-02  1.46012256e-02  1.46119077e-02  1.57360377e-02
  1.58798244e-02  1.63828289e-02  1.96045295e-02  2.23013917e-02
  2.38541191e-02  2.41687871e-02  2.71837133e-02  2.82787046e-02
  2.95658027e-02  3.07327102e-02  3.13528406e-02  3.17721256e-02
  3.56361290e-02  3.58923976e-02  3.60621506e-02  3.70309110e-02
  3.70612365e-02  3.75823206e-02  3.90998691e-02  4.35924843e-02
  4.38894604e-02  4.53158933e-02  4.75718895e-02  4.93413891e-02
  4.95173157e-02  5.11516162e-02  5.58779483e-02  5.65464548e-02
  5.71570966e-02  5.84901194e-02  5.91430148e-02  5.98989742e-02
  6.03060966e-02  6.12538509e-02  6.27976456e-02  6.54824483e-02
  6.70618035e-02  6.97186974e-02  6.97529546e-02  7.22809912e-02
  7.39981373e-02  7.46609872e-02  7.47646443e-02  7.89077719e-02
  7.96088557e-02  7.99228278e-02  8.02061098e-02  8.17114815e-02
  8.44395634e-02  8.68838554e-02  8.86835857e-02  8.90861649e-02
  8.98410504e-02  8.98462590e-02  9.03835591e-02  9.03843673e-02
  9.11842410e-02  9.47629616e-02  9.68867752e-02  9.76636221e-02
  9.81661057e-02  9.88896741e-02  9.93331682e-02  1.00031773e-01
  1.01244528e-01  1.01690634e-01  1.01799992e-01  1.02328328e-01
  1.03652438e-01  1.06317683e-01  1.06685976e-01  1.06793435e-01
  1.07372792e-01  1.07508157e-01  1.08872613e-01  1.09404160e-01
  1.09476657e-01  1.11354630e-01  1.11444281e-01  1.11692605e-01
  1.11943544e-01  1.12153508e-01  1.12472396e-01  1.19101348e-01
  1.19392340e-01  1.21492017e-01  1.22610097e-01  1.23127004e-01
  1.27082415e-01  1.27158166e-01  1.27206319e-01  1.29519160e-01
  1.30096585e-01  1.32227911e-01  1.33489313e-01  1.35311906e-01
  1.35947668e-01  1.38540496e-01  1.40402482e-01  1.41117979e-01
  1.41371949e-01  1.42079766e-01  1.42086455e-01  1.42691854e-01
  1.43874971e-01  1.44648657e-01  1.47917097e-01  1.49503902e-01
  1.50651571e-01  1.50898812e-01  1.50903641e-01  1.51797799e-01
  1.54346844e-01  1.55036012e-01  1.55194210e-01  1.55195707e-01
  1.59915915e-01  1.61181262e-01  1.61899151e-01  1.63482904e-01
  1.65202177e-01  1.65243334e-01  1.65433446e-01  1.65784850e-01
  1.72736423e-01  1.72824603e-01  1.72850431e-01  1.73293154e-01
  1.74859678e-01  1.75770929e-01  1.76873779e-01  1.77699939e-01
  1.78360694e-01  1.78825845e-01  1.82797356e-01  1.84408584e-01
  1.91802827e-01  1.92674615e-01  1.94423515e-01  1.94953864e-01
  1.95129666e-01  1.98746278e-01  1.99487630e-01  2.00823015e-01
  2.02183598e-01  2.02576257e-01  2.04119165e-01  2.05137253e-01
  2.09360295e-01  2.10334073e-01  2.10983872e-01  2.11998650e-01
  2.12100558e-01  2.14727684e-01  2.15572236e-01  2.18215851e-01
  2.20665635e-01  2.20760488e-01  2.24380988e-01  2.24748198e-01
  2.27134772e-01  2.28856096e-01  2.30818164e-01  2.31211337e-01
  2.31354277e-01  2.31886704e-01  2.32338532e-01  2.33099111e-01
  2.35867884e-01  2.36827422e-01  2.41489516e-01  2.42602567e-01
  2.43284604e-01  2.45200092e-01  2.46540353e-01  2.50410722e-01
  2.50631254e-01  2.53491240e-01  2.54106181e-01  2.54476297e-01
  2.54546186e-01  2.55886064e-01  2.55907041e-01  2.56594556e-01
  2.57283152e-01  2.57343276e-01  2.57886171e-01  2.63652700e-01
  2.67957711e-01  2.70628666e-01  2.71680642e-01  2.72281929e-01
  2.73274081e-01  2.78002014e-01  2.80156770e-01  2.80871626e-01
  2.83103963e-01  2.83154597e-01  2.83210319e-01  2.84436649e-01
  2.84675720e-01  2.86226959e-01  2.88346520e-01  2.89687156e-01
  2.89734525e-01  2.94651700e-01  2.94906733e-01  3.04485566e-01
  3.07022831e-01  3.07806900e-01  3.12062243e-01  3.13623372e-01
  3.14019400e-01  3.16675287e-01  3.17020652e-01  3.20993008e-01
  3.21148122e-01  3.22006849e-01  3.23384988e-01  3.24068115e-01
  3.24162959e-01  3.30520916e-01  3.35084824e-01  3.37042652e-01
  3.41417337e-01  3.42997636e-01  3.47893741e-01  3.50128318e-01
  3.53625202e-01  3.57851108e-01  3.59899199e-01  3.62892798e-01
  3.66063882e-01  3.66413843e-01  3.68652744e-01  3.70219612e-01
  3.70918133e-01  3.76610841e-01  3.76887998e-01  3.77262010e-01
  3.78354624e-01  3.79875334e-01  3.80268198e-01  3.81264067e-01
  3.84312286e-01  3.84492017e-01  3.96209969e-01  3.96457226e-01
  3.96705909e-01  4.00108294e-01  4.03689165e-01  4.05431603e-01
  4.14731439e-01  4.15012670e-01  4.15976067e-01  4.17899473e-01
  4.18374140e-01  4.20029319e-01  4.25906174e-01  4.27335068e-01
  4.30310103e-01  4.33267922e-01  4.33670077e-01  4.33850071e-01
  4.40751798e-01  4.41505187e-01  4.42044314e-01  4.44068261e-01
  4.46153555e-01  4.48454076e-01  4.51710669e-01  4.52887759e-01
  4.54825137e-01  4.54905197e-01  4.58257903e-01  4.61398839e-01
  4.66404406e-01  4.67328433e-01  4.67729920e-01  4.68138786e-01
  4.70979273e-01  4.72022913e-01  4.79347173e-01  4.80573353e-01
  4.85911390e-01  4.87348982e-01  4.87699368e-01  4.94933479e-01
  4.95251223e-01  4.98442856e-01  5.02836733e-01  5.05248615e-01
  5.09737913e-01  5.10120874e-01  5.10473261e-01  5.10565296e-01
  5.11900383e-01  5.12010071e-01  5.12311798e-01  5.16073629e-01
  5.17291507e-01  5.20088880e-01  5.20336730e-01  5.22164839e-01
  5.23949470e-01  5.27894576e-01  5.29137900e-01  5.31517451e-01
  5.34546703e-01  5.42646674e-01  5.43352020e-01  5.47577429e-01
  5.53519278e-01  5.59253285e-01  5.67014559e-01  5.68629236e-01
  5.69798797e-01  5.69995559e-01  5.70908884e-01  5.71927409e-01
  5.74251658e-01  5.77525317e-01  5.77603712e-01  5.87903395e-01
  5.88295989e-01  5.89300516e-01  5.90430219e-01  5.93160981e-01
  5.94831580e-01  5.96017087e-01  5.98045925e-01  6.04226335e-01
  6.04640916e-01  6.09784839e-01  6.12633021e-01  6.13639918e-01
  6.16684507e-01  6.17622399e-01  6.19432829e-01  6.23251600e-01
  6.23571953e-01  6.30890606e-01  6.30964755e-01  6.35181660e-01
  6.47503578e-01  6.51731125e-01  6.54579211e-01  6.55062005e-01
  6.56044372e-01  6.58778002e-01  6.61424434e-01  6.62642456e-01
  6.67750960e-01  6.68821563e-01  6.70967804e-01  6.74025216e-01
  6.81673558e-01  6.82484863e-01  6.83327026e-01  6.92748810e-01
  6.95711672e-01  6.96743031e-01  6.97136674e-01  6.98445461e-01
  7.02147262e-01]

  warnings.warn(

2022-11-03 10:51:23,167:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.25215833 -0.25134809 -0.24895768 -0.22240619 -0.21087535 -0.20455812
 -0.18886421 -0.17588412 -0.17451384 -0.17360519 -0.17083086 -0.16696586
 -0.16631461 -0.16414082 -0.16309599 -0.16045261 -0.15818669 -0.15578546
 -0.15157861 -0.15092443 -0.14824862 -0.14798673 -0.13233098 -0.13027291
 -0.12928762 -0.12795847 -0.12754963 -0.12113985 -0.12006115 -0.11926561
 -0.11901334 -0.11844004 -0.11772792 -0.11769607 -0.11691757 -0.1164303
 -0.11527883 -0.11416045 -0.11397564 -0.11397219 -0.112405   -0.11178769
 -0.11117165 -0.10445553 -0.10264056 -0.10152566 -0.0993899  -0.09717634
 -0.09680936 -0.09341198 -0.09335935 -0.09272137 -0.09057587 -0.08966721
 -0.08752047 -0.08597341 -0.08585112 -0.08042723 -0.07972344 -0.07950905
 -0.07792419 -0.07732731 -0.07604126 -0.074647   -0.07395387 -0.0724564
 -0.07103245 -0.06621951 -0.06503117 -0.06489667 -0.06481654 -0.06309214
 -0.06169938 -0.06148931 -0.05962917 -0.0588205  -0.05713486 -0.05673918
 -0.05482646 -0.05423924 -0.05300294 -0.05194655 -0.0492609  -0.04913797
 -0.04872218 -0.04795987 -0.04697659 -0.04668778 -0.04497527 -0.04469639
 -0.04279366 -0.03947346 -0.03883606 -0.03831505 -0.03386057 -0.03170603
 -0.0315294  -0.03110091 -0.02925721 -0.02899012 -0.02892424 -0.02704867
 -0.02413087 -0.02395673 -0.0236779  -0.02108558 -0.02020395 -0.01901356
 -0.01121813 -0.00902786 -0.00801959 -0.00674783 -0.00401424 -0.00132385
 -0.00114797  0.00163727  0.00507232  0.00544871  0.0060225   0.00638231
  0.00692238  0.00819622  0.00887532  0.01025502  0.01171842  0.01591303
  0.0182812   0.01872074  0.01954934  0.02034737  0.02179854  0.02575125
  0.02665827  0.02751588  0.02824799  0.03017734  0.03070908  0.03160914
  0.03187422  0.03448704  0.03498918  0.03539877  0.03667809  0.03718824
  0.03947232  0.03970613  0.04195243  0.04200333  0.04330923  0.04614348
  0.05003292  0.05072331  0.05227089  0.05227854  0.05274383  0.05321209
  0.0540105   0.05451163  0.05651474  0.05802957  0.05876426  0.0591403
  0.06007318  0.06187408  0.0623817   0.06284925  0.06419937  0.06464143
  0.06497555  0.06716624  0.06784182  0.06957365  0.0699626   0.07177197
  0.07184376  0.07314184  0.07547448  0.07593376  0.07793812  0.08038925
  0.0808502   0.08605341  0.08652379  0.08701879  0.09056483  0.09128098
  0.0917017   0.09184495  0.09256178  0.09300704  0.09833407  0.09989326
  0.10442626  0.10498777  0.10592374  0.10883405  0.1091539   0.11051274
  0.11078145  0.1122711   0.1126986   0.11285253  0.11421872  0.11641727
  0.11679046  0.11681439  0.12257399  0.12468566  0.1272044   0.12829142
  0.12901667  0.12909456  0.12976563  0.13130593  0.13248125  0.13252577
  0.13303499  0.1347556   0.13486017  0.13522393  0.13543399  0.13683592
  0.13684948  0.13696645  0.13814062  0.13819136  0.13845736  0.14089677
  0.14122895  0.14132602  0.1426289   0.14465337  0.14683691  0.14736424
  0.14758027  0.14777445  0.149406    0.15023911  0.15168662  0.15226801
  0.15291995  0.15736522  0.15882075  0.15951214  0.16001976  0.16069855
  0.16099641  0.16368894  0.16645616  0.16646792  0.16751626  0.16762335
  0.16858525  0.16979517  0.17084624  0.17417106  0.1745902   0.17868799
  0.17963706  0.18215862  0.1826781   0.18428758  0.18429274  0.1854212
  0.18728967  0.18874765  0.19273006  0.19391071  0.1960719   0.19628307
  0.19755016  0.19760994  0.1997922   0.20008487  0.20380665  0.20415545
  0.20435288  0.20679706  0.20681747  0.20790929  0.20849985  0.21481616
  0.21481748  0.21485689  0.21499439  0.21871821  0.22164763  0.22346814
  0.22356577  0.22585863  0.23072466  0.23244286  0.23919663  0.23976129
  0.24000437  0.24043216  0.24171534  0.24192946  0.2454301   0.24828513
  0.24954028  0.25307215  0.25402061  0.25403095  0.25454733  0.25476768
  0.25672082  0.25787095  0.25839488  0.25907941  0.26247365  0.26421648
  0.26608808  0.26645264  0.26687703  0.26779944  0.26850788  0.26914982
  0.27062024  0.27213212  0.27280442  0.27670518  0.27704384  0.28190597
  0.28309498  0.28477423  0.28711739  0.28734168  0.28788328  0.28944184
  0.29045298  0.29108727  0.29134302  0.29574313  0.2977766   0.29908559
  0.30061903  0.30841556  0.31244444  0.31464635  0.31547674  0.31814836
  0.31931322  0.31958057  0.32176472  0.32418328  0.32573685  0.32858676
  0.3292076   0.33075612  0.33701035  0.3375421   0.33755044  0.34245242
  0.34619738  0.34698958  0.34736089  0.34784786  0.35169214  0.35336169
  0.35594059  0.3575957   0.35848146  0.35944001  0.3594543   0.36007667
  0.36011444  0.36442442  0.36665198  0.36851372  0.36895036  0.36911631
  0.37280532  0.37410859  0.37442434  0.37491293  0.38258754  0.38577437
  0.38873823  0.38974813  0.39055411  0.39123144  0.3913172   0.39503972
  0.39504947  0.3955074   0.39888093  0.39928136  0.39998111  0.40022742
  0.40088633  0.40431421  0.40638561  0.40777783  0.41008504  0.41390818
  0.41442973  0.41483067  0.42058439  0.42153836  0.42255078  0.43080861
  0.43461146  0.43560151  0.43750505  0.43990549  0.44605605  0.44941619
  0.44963497  0.45085416  0.45226326  0.46013599  0.46159707  0.46246543
  0.46502583  0.46648476  0.46835917  0.46967134  0.46995115  0.47074795
  0.47174751  0.47264075  0.47585203  0.47600172  0.47894306  0.48478555
  0.4875819   0.49084524  0.49129353  0.49298964  0.49304548  0.49346612
  0.49410083  0.49513952  0.49538949  0.49678436  0.49696791  0.4974829
  0.49847642  0.49953592  0.50392562  0.50532707  0.51189264  0.51682404
  0.5185145   0.51860109  0.5196813   0.52024156  0.52122838  0.52198558
  0.52208151  0.52816808  0.53000525  0.53273487  0.54428903  0.54996383
  0.56095645  0.56136869  0.56233881  0.56248242  0.56272404  0.56620792
  0.56809791  0.56817354  0.57478155  0.57542631  0.57598454  0.5778423
  0.58151196  0.58233501  0.58513912  0.59084397  0.5943566   0.60231036
  0.60531965  0.62000729  0.62520243  0.62729547  0.64096122  0.64600705
  0.6537898   0.66796744  0.66871549  0.66935935  0.67054458  0.6728359
  0.67576115  0.68626337  0.69019574  0.69188943  0.6971961   0.70816468
  0.72778969]

  warnings.warn(

2022-11-03 10:51:23,287:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.52502506e-01 -1.32588501e-01 -1.23955031e-01 -1.23295936e-01
 -1.22872301e-01 -1.17034433e-01 -1.14273374e-01 -1.13620875e-01
 -1.13476775e-01 -1.12980321e-01 -1.10154214e-01 -1.09666895e-01
 -1.07102945e-01 -1.06868778e-01 -1.05662855e-01 -1.02812632e-01
 -1.01981804e-01 -9.94650943e-02 -9.66283520e-02 -9.44605499e-02
 -9.29228897e-02 -9.21387463e-02 -9.20616278e-02 -8.68282726e-02
 -8.17195422e-02 -8.10254719e-02 -8.09103358e-02 -7.95472639e-02
 -7.73558178e-02 -7.68306679e-02 -7.50063374e-02 -7.49799601e-02
 -7.32423411e-02 -7.09762472e-02 -6.97614246e-02 -6.88856987e-02
 -6.83354184e-02 -6.67902151e-02 -6.18264537e-02 -6.15408113e-02
 -6.07514254e-02 -6.03450587e-02 -5.98090651e-02 -5.97276113e-02
 -5.73762612e-02 -5.28608376e-02 -4.87337503e-02 -4.68212921e-02
 -4.64943260e-02 -4.43191501e-02 -4.29371859e-02 -4.11291988e-02
 -3.92981898e-02 -3.86844526e-02 -3.72565843e-02 -3.57711307e-02
 -3.38540235e-02 -3.31650529e-02 -3.31549403e-02 -3.28516077e-02
 -3.26143363e-02 -3.21186841e-02 -3.00169267e-02 -2.98060705e-02
 -2.92593996e-02 -2.92445738e-02 -2.88493613e-02 -2.83234658e-02
 -2.65631744e-02 -2.49182176e-02 -2.41452602e-02 -2.40829824e-02
 -2.34929212e-02 -2.34250065e-02 -2.30243676e-02 -2.25228881e-02
 -2.24359776e-02 -2.10288944e-02 -2.08575638e-02 -2.07514705e-02
 -1.97844323e-02 -1.88742424e-02 -1.68724455e-02 -1.62814810e-02
 -1.57818299e-02 -1.52545023e-02 -1.47730867e-02 -1.47187335e-02
 -1.43884376e-02 -1.43310449e-02 -1.35019712e-02 -1.31907490e-02
 -1.22440539e-02 -1.18221224e-02 -1.17710840e-02 -7.36508208e-03
 -6.49211534e-03 -4.12925664e-03 -3.08755035e-03 -1.63740092e-03
 -1.25219370e-03 -1.15552886e-03 -7.55694027e-04 -7.50837364e-04
  4.72081402e-04  1.59320327e-03  1.88468097e-03  2.53819689e-03
  2.81401634e-03  4.79189297e-03  5.05936400e-03  6.19990100e-03
  7.55036711e-03  7.64007147e-03  8.42543722e-03  8.81288354e-03
  8.97327161e-03  1.08464851e-02  1.23371527e-02  1.25366550e-02
  1.36731200e-02  1.44068104e-02  1.58657869e-02  1.64430963e-02
  1.69564110e-02  1.69786290e-02  2.01295557e-02  2.11862481e-02
  2.13301276e-02  2.37438831e-02  2.52450065e-02  2.52877555e-02
  2.57418852e-02  2.82265731e-02  2.89939425e-02  2.92352484e-02
  2.94200820e-02  3.27841000e-02  3.28308006e-02  3.35083755e-02
  3.47034718e-02  3.60503772e-02  3.69096574e-02  3.73820485e-02
  3.74948667e-02  3.92630905e-02  3.98684259e-02  4.01539527e-02
  4.09699769e-02  4.11109166e-02  4.16423917e-02  4.16725126e-02
  4.29384367e-02  4.31822301e-02  4.44894628e-02  4.60950121e-02
  4.61747607e-02  4.74953116e-02  4.78240855e-02  4.90094593e-02
  5.12263372e-02  5.14980539e-02  5.18350369e-02  5.34590629e-02
  5.42046474e-02  5.42424014e-02  5.47218445e-02  5.55279806e-02
  5.60786438e-02  5.63300666e-02  5.65179253e-02  5.99671797e-02
  6.34287647e-02  6.38370141e-02  6.42484160e-02  6.46747905e-02
  6.48128606e-02  6.58576704e-02  6.62996935e-02  6.94587502e-02
  6.96201761e-02  7.11093231e-02  7.15342021e-02  7.33620782e-02
  7.51924844e-02  7.52589318e-02  7.53652754e-02  7.54438643e-02
  7.58488368e-02  7.59676475e-02  7.65537335e-02  7.68959485e-02
  7.72055652e-02  7.80441491e-02  7.83663906e-02  7.84223596e-02
  7.88203584e-02  8.02895413e-02  8.12404843e-02  8.19218546e-02
  8.38410421e-02  8.66323247e-02  8.69165852e-02  8.71717098e-02
  8.72165839e-02  8.87438518e-02  8.90314075e-02  9.05258083e-02
  9.13369757e-02  9.17636112e-02  9.24588380e-02  9.28894600e-02
  9.36193118e-02  9.37151255e-02  9.45768752e-02  9.67782561e-02
  9.68276364e-02  9.75247723e-02  9.78303106e-02  9.98725023e-02
  9.98937528e-02  1.00090619e-01  1.01670792e-01  1.02438381e-01
  1.03972152e-01  1.04029063e-01  1.04092199e-01  1.05116341e-01
  1.05198891e-01  1.06196836e-01  1.06846981e-01  1.07285557e-01
  1.07448936e-01  1.08639306e-01  1.08987105e-01  1.10671659e-01
  1.10682972e-01  1.10922774e-01  1.11363844e-01  1.13297172e-01
  1.14192801e-01  1.14543578e-01  1.17674409e-01  1.18906039e-01
  1.20728235e-01  1.21328314e-01  1.22267207e-01  1.24048682e-01
  1.24212558e-01  1.25632828e-01  1.26292404e-01  1.26593920e-01
  1.26631169e-01  1.27010343e-01  1.27766362e-01  1.31113359e-01
  1.33027923e-01  1.33446911e-01  1.33964451e-01  1.34078702e-01
  1.35513759e-01  1.35824565e-01  1.36575882e-01  1.37277871e-01
  1.37371487e-01  1.38786457e-01  1.38881537e-01  1.38957615e-01
  1.39000265e-01  1.41120234e-01  1.43690446e-01  1.43779830e-01
  1.44416892e-01  1.44594582e-01  1.44919649e-01  1.45046447e-01
  1.46357765e-01  1.46556287e-01  1.48793119e-01  1.48883568e-01
  1.48944371e-01  1.49000693e-01  1.50153467e-01  1.51467293e-01
  1.53017500e-01  1.54343582e-01  1.54591736e-01  1.54815747e-01
  1.55202443e-01  1.59415357e-01  1.59692088e-01  1.61925772e-01
  1.63561509e-01  1.65694990e-01  1.65898336e-01  1.69130781e-01
  1.69418620e-01  1.70764211e-01  1.72231723e-01  1.72872523e-01
  1.73956663e-01  1.74528901e-01  1.77118384e-01  1.78452172e-01
  1.83176758e-01  1.85324470e-01  1.85689470e-01  1.86700907e-01
  1.87419192e-01  1.88794911e-01  1.89889270e-01  1.90613480e-01
  1.92089522e-01  1.92706079e-01  1.96267650e-01  1.96660003e-01
  1.98349689e-01  2.00726859e-01  2.01186141e-01  2.01523737e-01
  2.01888055e-01  2.05115212e-01  2.05681207e-01  2.06437967e-01
  2.06575535e-01  2.06622170e-01  2.08523888e-01  2.08528779e-01
  2.08537881e-01  2.10365709e-01  2.13478245e-01  2.14248319e-01
  2.14551624e-01  2.15135934e-01  2.15762559e-01  2.19015482e-01
  2.23396317e-01  2.26032947e-01  2.26272701e-01  2.26669645e-01
  2.27440847e-01  2.30031686e-01  2.31095937e-01  2.32581762e-01
  2.32700575e-01  2.33056804e-01  2.35064929e-01  2.35247886e-01
  2.35748923e-01  2.36602501e-01  2.37410358e-01  2.39010041e-01
  2.40822828e-01  2.42244793e-01  2.42599227e-01  2.43122370e-01
  2.44547899e-01  2.45017133e-01  2.46380054e-01  2.51352087e-01
  2.51689567e-01  2.54835042e-01  2.57112089e-01  2.58439942e-01
  2.58585418e-01  2.61856006e-01  2.62723762e-01  2.64759720e-01
  2.65139303e-01  2.67791832e-01  2.71827444e-01  2.71947938e-01
  2.72881233e-01  2.73429212e-01  2.73785953e-01  2.75727268e-01
  2.75937510e-01  2.78221729e-01  2.78666335e-01  2.79698804e-01
  2.79831196e-01  2.80048301e-01  2.83122695e-01  2.83600560e-01
  2.84876869e-01  2.87024394e-01  2.88755297e-01  2.91069334e-01
  2.92224552e-01  2.92521560e-01  2.92814941e-01  2.96117418e-01
  2.96647448e-01  2.96873705e-01  2.96922837e-01  2.97547317e-01
  2.98006987e-01  2.98791669e-01  2.99961541e-01  3.03065817e-01
  3.03518001e-01  3.04900169e-01  3.07554742e-01  3.08842159e-01
  3.09730072e-01  3.12426074e-01  3.13483752e-01  3.14355682e-01
  3.16672040e-01  3.16862914e-01  3.18921260e-01  3.19447033e-01
  3.21683151e-01  3.22056055e-01  3.24817300e-01  3.25282851e-01
  3.25415938e-01  3.27067958e-01  3.30382791e-01  3.34980881e-01
  3.35295274e-01  3.36648855e-01  3.38030496e-01  3.43218939e-01
  3.43253914e-01  3.45946290e-01  3.46883347e-01  3.50582743e-01
  3.52471360e-01  3.54972821e-01  3.58609920e-01  3.60281238e-01
  3.62045262e-01  3.62658847e-01  3.63342649e-01  3.63905083e-01
  3.65306178e-01  3.69991220e-01  3.72473235e-01  3.75854506e-01
  3.80633162e-01  3.80959881e-01  3.82711331e-01  3.85573340e-01
  3.86212644e-01  3.87612736e-01  3.91447397e-01  3.91999403e-01
  3.94713738e-01  3.96176206e-01  3.96308524e-01  3.96634470e-01
  3.97003940e-01  3.99035908e-01  3.99200554e-01  4.01260610e-01
  4.01515218e-01  4.03601547e-01  4.03994941e-01  4.04187965e-01
  4.04423300e-01  4.11339156e-01  4.11381773e-01  4.13035019e-01
  4.14806281e-01  4.14891893e-01  4.15231009e-01  4.19679285e-01
  4.22203037e-01  4.22228832e-01  4.22301995e-01  4.23891756e-01
  4.23997026e-01  4.24920487e-01  4.25456662e-01  4.26270077e-01
  4.30256237e-01  4.47144751e-01  4.47252407e-01  4.47535844e-01
  4.54342762e-01  4.54941486e-01  4.60136148e-01  4.64465839e-01
  4.86809159e-01  4.93777675e-01  4.98194344e-01  5.01556339e-01
  5.06121861e-01  5.07284039e-01  5.10522393e-01  5.15871630e-01
  5.19402374e-01]

  warnings.warn(

2022-11-03 10:51:23,446:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.80768939e-01 -1.76944092e-01 -1.76094950e-01 -1.69737848e-01
 -1.59408235e-01 -1.56804244e-01 -1.45415857e-01 -1.44408298e-01
 -1.41871868e-01 -1.37022142e-01 -1.31487327e-01 -1.30313629e-01
 -1.26593721e-01 -1.24245307e-01 -1.22746434e-01 -1.16374301e-01
 -1.15077188e-01 -1.14908540e-01 -1.11996663e-01 -1.11922989e-01
 -1.07954681e-01 -1.05961031e-01 -9.83338408e-02 -9.83328550e-02
 -9.79129664e-02 -9.36694458e-02 -9.32521902e-02 -9.25093404e-02
 -9.12384696e-02 -9.07827364e-02 -9.06633048e-02 -8.75513245e-02
 -8.69750824e-02 -8.56466476e-02 -8.53763719e-02 -8.20649905e-02
 -8.07632671e-02 -8.01272157e-02 -7.95396010e-02 -7.92492750e-02
 -7.89701862e-02 -7.86549781e-02 -7.74016451e-02 -7.24379969e-02
 -7.16765903e-02 -6.99591214e-02 -6.88024308e-02 -6.80273975e-02
 -6.51022119e-02 -6.50058132e-02 -6.45781789e-02 -6.42097027e-02
 -5.98931928e-02 -5.83601435e-02 -5.82734715e-02 -5.69505321e-02
 -5.66116816e-02 -5.55685374e-02 -5.47692876e-02 -5.42750559e-02
 -5.29171630e-02 -5.17006897e-02 -5.05609285e-02 -4.84867522e-02
 -4.76354085e-02 -4.75826190e-02 -4.27573429e-02 -4.19460702e-02
 -4.07815805e-02 -4.00769621e-02 -3.92588851e-02 -3.73226954e-02
 -3.50361929e-02 -3.40129420e-02 -3.36160569e-02 -3.25856072e-02
 -3.14741667e-02 -2.76623083e-02 -2.66764464e-02 -2.59843313e-02
 -2.51096343e-02 -2.16152251e-02 -2.11372171e-02 -2.04434844e-02
 -2.02805499e-02 -1.97148422e-02 -1.75573133e-02 -1.73366607e-02
 -1.71232070e-02 -1.42078970e-02 -1.35209699e-02 -1.33900881e-02
 -1.16748200e-02 -1.13986678e-02 -1.05987618e-02 -9.15443964e-03
 -9.10949693e-03 -8.89826910e-03 -8.18618945e-03 -7.15553179e-03
 -6.97474379e-03 -6.86682522e-03 -6.23945403e-03 -5.61875340e-03
 -5.32726484e-03 -4.79896889e-03 -4.41739002e-03 -4.18085108e-03
 -3.19209386e-03 -2.61532492e-03 -1.65160860e-03 -1.01463375e-03
 -6.17185768e-04 -4.89166583e-04 -1.69420189e-05  1.26820630e-04
  8.78756498e-04  2.04611304e-03  4.43893408e-03  4.71520451e-03
  5.75589571e-03  5.94343759e-03  7.09830915e-03  8.01042483e-03
  1.45747961e-02  1.49249515e-02  1.53301341e-02  1.62445011e-02
  1.77382679e-02  1.79449061e-02  1.88882906e-02  1.95264316e-02
  2.03147350e-02  2.04121895e-02  2.04716370e-02  2.35020326e-02
  2.35307963e-02  2.54558817e-02  2.67626964e-02  2.72481406e-02
  2.98338334e-02  3.34656292e-02  3.59927715e-02  3.66074245e-02
  3.80888497e-02  3.95037726e-02  4.21498951e-02  4.23374370e-02
  4.27889780e-02  4.35029813e-02  4.60998457e-02  4.75718014e-02
  4.91632225e-02  5.02064665e-02  5.07471318e-02  5.26013049e-02
  5.44358042e-02  5.57925900e-02  5.61184200e-02  5.71107466e-02
  5.73481994e-02  5.81089833e-02  5.93249824e-02  6.02439762e-02
  6.21491259e-02  6.25036561e-02  6.28878764e-02  6.40693990e-02
  6.45414576e-02  6.54212073e-02  6.56830294e-02  6.58436404e-02
  6.64746984e-02  6.68792213e-02  6.85834122e-02  6.88529721e-02
  6.90282131e-02  6.99841163e-02  7.03371347e-02  7.16257299e-02
  7.25279982e-02  7.26570557e-02  7.62014985e-02  7.90649610e-02
  7.91242474e-02  8.18234771e-02  8.18343444e-02  8.26062013e-02
  8.42511533e-02  8.52243616e-02  8.59772013e-02  8.75262070e-02
  8.79133817e-02  8.81187230e-02  8.82061126e-02  8.83760155e-02
  8.85812426e-02  8.96599383e-02  9.01559848e-02  9.01828925e-02
  9.04739998e-02  9.05099658e-02  9.19763123e-02  9.29239355e-02
  9.37339704e-02  9.51286925e-02  9.56886184e-02  9.60229541e-02
  9.70989532e-02  9.72110446e-02  1.00835879e-01  1.01166618e-01
  1.01468161e-01  1.03404138e-01  1.04565602e-01  1.04721831e-01
  1.05219515e-01  1.07885945e-01  1.08447240e-01  1.10352422e-01
  1.11164766e-01  1.11721835e-01  1.11834573e-01  1.12000471e-01
  1.14029688e-01  1.17909094e-01  1.18849241e-01  1.18858090e-01
  1.19412405e-01  1.19555528e-01  1.19730653e-01  1.20126829e-01
  1.20488043e-01  1.21057839e-01  1.21108852e-01  1.21729330e-01
  1.22199681e-01  1.23614096e-01  1.25335877e-01  1.25621411e-01
  1.26059033e-01  1.27261068e-01  1.28576701e-01  1.29271531e-01
  1.31032534e-01  1.31092796e-01  1.31223292e-01  1.34664160e-01
  1.35885123e-01  1.36587845e-01  1.38684317e-01  1.39004047e-01
  1.39624066e-01  1.42077957e-01  1.42407183e-01  1.42465472e-01
  1.42749765e-01  1.43007451e-01  1.43224579e-01  1.45341762e-01
  1.46312453e-01  1.47801921e-01  1.52959041e-01  1.53812071e-01
  1.54942989e-01  1.55116530e-01  1.55354598e-01  1.57541537e-01
  1.58310502e-01  1.61385969e-01  1.62651618e-01  1.66149088e-01
  1.66975020e-01  1.66990490e-01  1.67051178e-01  1.70032943e-01
  1.71134068e-01  1.73413829e-01  1.73827165e-01  1.74013143e-01
  1.74568803e-01  1.75090773e-01  1.75223717e-01  1.80048185e-01
  1.81469225e-01  1.81708073e-01  1.83260700e-01  1.84509583e-01
  1.84524911e-01  1.85146120e-01  1.89089503e-01  1.90209234e-01
  1.91059344e-01  1.91979296e-01  1.92182494e-01  1.92325308e-01
  1.92943941e-01  1.94064127e-01  1.94306285e-01  1.95212121e-01
  1.95622824e-01  1.97773738e-01  2.01093882e-01  2.01609882e-01
  2.01623691e-01  2.03588608e-01  2.04133362e-01  2.06151985e-01
  2.08561521e-01  2.10394144e-01  2.10881868e-01  2.12647380e-01
  2.14433116e-01  2.15005165e-01  2.20451603e-01  2.20471010e-01
  2.20795548e-01  2.23496795e-01  2.24345746e-01  2.25934128e-01
  2.26836636e-01  2.31400084e-01  2.31821759e-01  2.34387267e-01
  2.34427540e-01  2.38359828e-01  2.38656787e-01  2.39299970e-01
  2.39585430e-01  2.40139318e-01  2.41832045e-01  2.43874712e-01
  2.43909881e-01  2.43975060e-01  2.44722225e-01  2.45410183e-01
  2.45690423e-01  2.46796542e-01  2.48417911e-01  2.48949934e-01
  2.50511595e-01  2.50878065e-01  2.56610537e-01  2.56770673e-01
  2.57891050e-01  2.57945300e-01  2.58337395e-01  2.59535509e-01
  2.59872324e-01  2.61939993e-01  2.63252894e-01  2.64704614e-01
  2.66212525e-01  2.66859161e-01  2.67897177e-01  2.71840703e-01
  2.72529158e-01  2.72950257e-01  2.73954123e-01  2.76826403e-01
  2.79643575e-01  2.80168575e-01  2.81870162e-01  2.82199239e-01
  2.83646639e-01  2.84369103e-01  2.86009000e-01  2.88380330e-01
  2.89490809e-01  2.90192020e-01  2.90909343e-01  2.91242618e-01
  2.97625736e-01  3.00207292e-01  3.01439702e-01  3.03904690e-01
  3.04482999e-01  3.04833465e-01  3.09354575e-01  3.13677116e-01
  3.17557323e-01  3.18302208e-01  3.20706765e-01  3.23221824e-01
  3.25630102e-01  3.26691656e-01  3.27814052e-01  3.30963035e-01
  3.33214915e-01  3.34258921e-01  3.36160741e-01  3.39135259e-01
  3.39172969e-01  3.39308088e-01  3.39374414e-01  3.40861029e-01
  3.41115259e-01  3.41743380e-01  3.42198331e-01  3.44072077e-01
  3.44526975e-01  3.47986275e-01  3.49046582e-01  3.49173700e-01
  3.49214360e-01  3.49895279e-01  3.51557531e-01  3.55042865e-01
  3.59244071e-01  3.62360391e-01  3.62990985e-01  3.63965849e-01
  3.64139553e-01  3.67624531e-01  3.73352789e-01  3.74824983e-01
  3.74927395e-01  3.76252113e-01  3.80652757e-01  3.82567011e-01
  3.86288376e-01  3.89132128e-01  3.94994953e-01  4.02408874e-01
  4.02933568e-01  4.05976588e-01  4.06339799e-01  4.06413457e-01
  4.12199521e-01  4.22659914e-01  4.23976751e-01  4.28509641e-01
  4.29983339e-01  4.32205948e-01  4.32672109e-01  4.35274611e-01
  4.35362721e-01  4.35902295e-01  4.36267011e-01  4.37532937e-01
  4.38916563e-01  4.39101600e-01  4.40507704e-01  4.40859207e-01
  4.45243758e-01  4.46067019e-01  4.51503865e-01  4.53555091e-01
  4.53566667e-01  4.55020112e-01  4.56650531e-01  4.56762806e-01
  4.57852495e-01  4.58040850e-01  4.59465763e-01  4.60662513e-01
  4.61231104e-01  4.61562396e-01  4.62164064e-01  4.63509849e-01
  4.64097297e-01  4.66041301e-01  4.71972159e-01  4.74027163e-01
  4.74358945e-01  4.77486823e-01  4.83362043e-01  4.84667417e-01
  4.87941007e-01  4.88231376e-01  4.89603442e-01  4.90625406e-01
  4.91895953e-01  5.09970262e-01  5.10085010e-01  5.10321174e-01
  5.10883275e-01  5.16018038e-01  5.32314600e-01  5.37215661e-01
  5.39222077e-01  5.39532587e-01  5.52129723e-01  5.56092361e-01
  5.57185217e-01  5.59063261e-01  5.60109633e-01  5.82719395e-01
  5.83691979e-01  5.84923302e-01  5.85416239e-01  6.10423122e-01]

  warnings.warn(

2022-11-03 10:51:23,519:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.39524621e-01 -2.31546458e-01 -2.24615287e-01 -2.20844719e-01
 -1.89123816e-01 -1.83344230e-01 -1.83199297e-01 -1.82123678e-01
 -1.79690813e-01 -1.77532267e-01 -1.76995993e-01 -1.62371298e-01
 -1.59192481e-01 -1.59111561e-01 -1.58810351e-01 -1.54328214e-01
 -1.52201690e-01 -1.48409018e-01 -1.45535799e-01 -1.44456836e-01
 -1.44336839e-01 -1.43571767e-01 -1.39304441e-01 -1.31900678e-01
 -1.30501333e-01 -1.23561913e-01 -1.23364625e-01 -1.22846670e-01
 -1.22164355e-01 -1.20388744e-01 -1.19755698e-01 -1.17943605e-01
 -1.13692215e-01 -1.13139030e-01 -1.12284829e-01 -1.12085610e-01
 -1.07879572e-01 -1.07363234e-01 -1.03247168e-01 -1.01600517e-01
 -1.00829490e-01 -9.59602431e-02 -9.53448270e-02 -9.31046956e-02
 -9.16028000e-02 -9.03390767e-02 -8.79374021e-02 -8.74508113e-02
 -8.65187354e-02 -8.51218676e-02 -8.47321561e-02 -8.15178982e-02
 -7.90315902e-02 -7.54492055e-02 -7.39186074e-02 -7.29596559e-02
 -7.21288283e-02 -7.07128440e-02 -6.91425040e-02 -6.83577223e-02
 -6.71903730e-02 -6.51625689e-02 -6.40006772e-02 -6.10391283e-02
 -5.84352921e-02 -5.82612545e-02 -5.29412478e-02 -5.25569996e-02
 -5.18073584e-02 -5.05854747e-02 -4.91561161e-02 -4.64284040e-02
 -4.28581698e-02 -4.17675512e-02 -4.14317225e-02 -3.99230839e-02
 -3.98550855e-02 -3.90278832e-02 -3.70056824e-02 -3.49299265e-02
 -3.44272208e-02 -3.21792176e-02 -3.01798084e-02 -2.88282039e-02
 -2.84624006e-02 -2.76912180e-02 -2.49530546e-02 -2.39320674e-02
 -2.19480406e-02 -1.82420117e-02 -1.68019993e-02 -1.56201907e-02
 -1.40731462e-02 -1.20297683e-02 -1.19840849e-02 -1.13488115e-02
 -9.34329112e-03 -8.79272738e-03 -7.20918625e-03 -5.48789886e-03
 -4.18382692e-03 -3.63788458e-03 -2.65875325e-03 -1.42999395e-03
 -2.56291936e-04  9.47550340e-05  2.42887922e-03  3.90522290e-03
  6.97112966e-03  7.35899529e-03  7.51885996e-03  8.83985462e-03
  9.99410738e-03  1.03756233e-02  1.08953171e-02  1.11508558e-02
  1.29236128e-02  1.46093663e-02  1.47873751e-02  1.54495946e-02
  1.57819805e-02  1.94144937e-02  1.97093454e-02  1.98643307e-02
  2.03665106e-02  2.15124192e-02  2.25623673e-02  2.97964161e-02
  2.99044360e-02  3.29856779e-02  3.31344156e-02  3.34008612e-02
  3.37769025e-02  3.47962945e-02  3.54199504e-02  3.84872363e-02
  3.85971737e-02  3.92815939e-02  4.22002221e-02  4.22903775e-02
  4.27587771e-02  4.50141669e-02  4.59009317e-02  5.14101264e-02
  5.16197881e-02  5.25758188e-02  5.45108610e-02  5.71789730e-02
  5.89968953e-02  5.98481328e-02  6.04642785e-02  6.20818142e-02
  6.27014413e-02  6.50287967e-02  6.54007250e-02  6.73841410e-02
  6.93815228e-02  7.14540600e-02  7.17751816e-02  7.47464115e-02
  7.54297495e-02  8.04481580e-02  8.22657195e-02  8.43042770e-02
  8.44140229e-02  8.67329964e-02  8.70736611e-02  8.79168781e-02
  9.03846707e-02  9.19578944e-02  9.22511583e-02  9.23534005e-02
  9.30917397e-02  9.38795171e-02  9.38962197e-02  9.39197563e-02
  9.46775375e-02  9.65682255e-02  9.91819320e-02  1.00090437e-01
  1.00363418e-01  1.00744874e-01  1.01153285e-01  1.02921975e-01
  1.03150730e-01  1.05474841e-01  1.05656326e-01  1.10084172e-01
  1.11006303e-01  1.11297556e-01  1.12300695e-01  1.12386278e-01
  1.12650148e-01  1.13131493e-01  1.13262891e-01  1.13399922e-01
  1.14159393e-01  1.15143455e-01  1.17142021e-01  1.18057342e-01
  1.19773128e-01  1.20478582e-01  1.26462830e-01  1.26984069e-01
  1.27734164e-01  1.29608788e-01  1.30106197e-01  1.30282425e-01
  1.32232112e-01  1.36518268e-01  1.36725172e-01  1.37360902e-01
  1.39957033e-01  1.40948145e-01  1.42084772e-01  1.45735943e-01
  1.45781827e-01  1.46657922e-01  1.46836318e-01  1.47430465e-01
  1.47586919e-01  1.47794570e-01  1.48331725e-01  1.48841669e-01
  1.50030664e-01  1.52464349e-01  1.53606678e-01  1.54996882e-01
  1.57470339e-01  1.58827466e-01  1.63051970e-01  1.63986577e-01
  1.64757118e-01  1.65603171e-01  1.66445729e-01  1.69344072e-01
  1.70830053e-01  1.73396595e-01  1.73518000e-01  1.74293278e-01
  1.75453279e-01  1.78105867e-01  1.78450154e-01  1.79071507e-01
  1.80621631e-01  1.84117383e-01  1.84694219e-01  1.86450272e-01
  1.90742937e-01  1.90776704e-01  1.93972507e-01  1.94043094e-01
  1.95563017e-01  1.96711893e-01  1.97238942e-01  2.00464239e-01
  2.04024247e-01  2.13185905e-01  2.16044147e-01  2.16120143e-01
  2.17630007e-01  2.18353994e-01  2.20853956e-01  2.22201740e-01
  2.24806017e-01  2.25973523e-01  2.27259998e-01  2.27381399e-01
  2.27451062e-01  2.32660127e-01  2.32692437e-01  2.36804595e-01
  2.37344312e-01  2.38169984e-01  2.40326753e-01  2.40758192e-01
  2.42302805e-01  2.43730362e-01  2.45422578e-01  2.49208014e-01
  2.52920939e-01  2.54194379e-01  2.54439692e-01  2.54596751e-01
  2.57018823e-01  2.59102935e-01  2.62025793e-01  2.62961770e-01
  2.64177829e-01  2.64295235e-01  2.64610906e-01  2.66210816e-01
  2.67980663e-01  2.70151815e-01  2.70868313e-01  2.71066053e-01
  2.74152574e-01  2.75994547e-01  2.78234317e-01  2.78644517e-01
  2.82482709e-01  2.83224737e-01  2.85454031e-01  2.87190290e-01
  2.87834852e-01  2.88533476e-01  2.88790283e-01  2.89290466e-01
  2.90283489e-01  2.92783792e-01  2.95237608e-01  2.95626537e-01
  2.98635900e-01  2.99123807e-01  2.99704158e-01  3.02269287e-01
  3.02370579e-01  3.05058583e-01  3.06208200e-01  3.06516173e-01
  3.14480137e-01  3.15272426e-01  3.15990894e-01  3.18431518e-01
  3.19353658e-01  3.22018682e-01  3.24040937e-01  3.26215353e-01
  3.27544399e-01  3.28219759e-01  3.29081666e-01  3.34561154e-01
  3.35241019e-01  3.36832062e-01  3.38467464e-01  3.41070605e-01
  3.41727073e-01  3.41937320e-01  3.42409974e-01  3.43071204e-01
  3.44161564e-01  3.46414441e-01  3.48108168e-01  3.52298342e-01
  3.54653722e-01  3.55442368e-01  3.61807687e-01  3.62360334e-01
  3.63615758e-01  3.70304612e-01  3.71182767e-01  3.71986352e-01
  3.76169859e-01  3.76599920e-01  3.76740343e-01  3.77116532e-01
  3.79260838e-01  3.82978027e-01  3.83515857e-01  3.84346872e-01
  3.85559610e-01  3.89441963e-01  3.90287191e-01  3.91817839e-01
  3.93614928e-01  3.94809885e-01  3.98245666e-01  3.98409284e-01
  3.99879422e-01  4.01722492e-01  4.03073141e-01  4.05727397e-01
  4.08513596e-01  4.09402415e-01  4.09646416e-01  4.10405430e-01
  4.11582791e-01  4.12260370e-01  4.13840238e-01  4.15083590e-01
  4.15311914e-01  4.16292739e-01  4.16850586e-01  4.20694793e-01
  4.21699667e-01  4.24354272e-01  4.26629689e-01  4.34298290e-01
  4.36897814e-01  4.37446366e-01  4.37568002e-01  4.39663026e-01
  4.40306852e-01  4.42010195e-01  4.42152276e-01  4.42619278e-01
  4.48163653e-01  4.49049977e-01  4.49061849e-01  4.49435601e-01
  4.50944504e-01  4.50979199e-01  4.52125065e-01  4.52530183e-01
  4.55278734e-01  4.55910563e-01  4.58939815e-01  4.64280229e-01
  4.65652490e-01  4.67105558e-01  4.70237743e-01  4.80024391e-01
  4.80272329e-01  4.82306056e-01  4.82908569e-01  4.88283710e-01
  4.89539107e-01  4.90473493e-01  4.91224337e-01  4.91542020e-01
  4.93737700e-01  4.97866674e-01  5.02174789e-01  5.05734759e-01
  5.06254012e-01  5.07423258e-01  5.10114609e-01  5.13085534e-01
  5.15939287e-01  5.18377494e-01  5.19581095e-01  5.23415566e-01
  5.26905415e-01  5.29356617e-01  5.35826908e-01  5.38654435e-01
  5.39119128e-01  5.43879225e-01  5.45757529e-01  5.53030955e-01
  5.53724397e-01  5.54824969e-01  5.55111936e-01  5.55993562e-01
  5.56196479e-01  5.60689573e-01  5.65071399e-01  5.66825638e-01
  5.68896809e-01  5.70455644e-01  5.71188480e-01  5.71190933e-01
  5.75894689e-01  5.75954738e-01  5.82558591e-01  5.82612772e-01
  5.88069136e-01  5.90362674e-01  5.91428500e-01  5.92886782e-01
  5.93470676e-01  6.01081018e-01  6.03387674e-01  6.03812762e-01
  6.05147047e-01  6.06533508e-01  6.10678044e-01  6.12697125e-01
  6.15028534e-01  6.17989205e-01  6.20709053e-01  6.20812316e-01
  6.22499736e-01  6.24398698e-01  6.24566536e-01  6.27804810e-01
  6.31996263e-01  6.32477094e-01  6.39906331e-01  6.44420986e-01
  6.47507833e-01  6.48714745e-01  6.55568117e-01  6.77589122e-01
  6.95859249e-01  7.10669978e-01  7.18825743e-01  7.19764826e-01
  7.34479771e-01  7.36108641e-01  7.52101062e-01  7.57025230e-01
  7.59549973e-01]

  warnings.warn(

2022-11-03 10:51:23,535:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.47798400e-01 -2.30669505e-01 -2.17324410e-01 -1.99178806e-01
 -1.91283833e-01 -1.89189458e-01 -1.87495653e-01 -1.78758440e-01
 -1.71173755e-01 -1.67765261e-01 -1.65621639e-01 -1.65496507e-01
 -1.62035646e-01 -1.58486908e-01 -1.58434744e-01 -1.55759335e-01
 -1.49746701e-01 -1.49604063e-01 -1.48475552e-01 -1.45490866e-01
 -1.35668623e-01 -1.30913061e-01 -1.29595251e-01 -1.26531428e-01
 -1.25659067e-01 -1.23731996e-01 -1.22952515e-01 -1.19392942e-01
 -1.18682085e-01 -1.18386049e-01 -1.17605026e-01 -1.17074242e-01
 -1.14668068e-01 -1.11765822e-01 -1.11383184e-01 -1.09851291e-01
 -1.08374651e-01 -1.06130250e-01 -1.03624891e-01 -1.03545211e-01
 -1.00471408e-01 -1.00379268e-01 -9.97021726e-02 -9.94246044e-02
 -9.61983737e-02 -9.55970758e-02 -9.27955981e-02 -8.84105166e-02
 -8.61899729e-02 -8.53368318e-02 -8.42936399e-02 -8.24404548e-02
 -7.58453678e-02 -6.79048209e-02 -6.76253578e-02 -6.75060340e-02
 -6.71232860e-02 -6.59614630e-02 -5.88690240e-02 -5.78925769e-02
 -5.55909671e-02 -5.29230812e-02 -4.59792575e-02 -4.41198773e-02
 -4.35528838e-02 -4.32220866e-02 -4.26501575e-02 -4.25593556e-02
 -3.83588719e-02 -3.83575362e-02 -3.77230364e-02 -3.65344607e-02
 -3.43268063e-02 -3.40223644e-02 -3.15192110e-02 -3.10270888e-02
 -3.03866179e-02 -3.01793051e-02 -2.86209195e-02 -2.79697242e-02
 -2.71652655e-02 -2.51561972e-02 -1.94724216e-02 -1.91535903e-02
 -1.78782996e-02 -1.73956578e-02 -1.46163353e-02 -1.40095036e-02
 -1.38530177e-02 -1.15160044e-02 -9.84211179e-03 -6.87144409e-03
 -5.69863410e-03 -2.67124845e-03 -1.22645967e-03 -6.81395139e-04
 -1.35200315e-04 -1.23079295e-04  1.85473517e-03  2.05171942e-03
  2.87195410e-03  2.88991593e-03  4.04778759e-03  4.10292102e-03
  5.33874397e-03  6.81858363e-03  6.86856332e-03  7.73962931e-03
  8.56255118e-03  9.54887668e-03  1.03426317e-02  1.10745145e-02
  1.12046109e-02  1.27554235e-02  1.59965872e-02  1.78265084e-02
  1.92801332e-02  1.96504008e-02  2.23488182e-02  2.24693118e-02
  2.25526708e-02  2.64569169e-02  2.81197361e-02  2.94143972e-02
  2.95374009e-02  3.14246563e-02  3.20100143e-02  3.26893678e-02
  3.36660939e-02  3.37763280e-02  3.38856995e-02  3.45458483e-02
  3.96229080e-02  4.02866987e-02  4.05451013e-02  4.19985725e-02
  4.20288680e-02  4.23783680e-02  4.43395400e-02  4.50253207e-02
  4.52219630e-02  4.52961821e-02  4.68440691e-02  5.04200044e-02
  5.09619882e-02  5.26079890e-02  5.36569245e-02  5.40841892e-02
  5.51455367e-02  5.70201064e-02  5.70525389e-02  5.81179191e-02
  5.85238621e-02  5.89243186e-02  5.92613152e-02  5.98463822e-02
  6.32354624e-02  6.36917106e-02  6.48517754e-02  6.52711495e-02
  6.56117628e-02  6.64181899e-02  6.78566271e-02  6.80179530e-02
  7.00936486e-02  7.22551347e-02  7.50904065e-02  7.57318143e-02
  7.62925906e-02  7.67973110e-02  7.71677698e-02  7.80661570e-02
  7.80928743e-02  8.03498841e-02  8.38270986e-02  8.44579907e-02
  8.45414469e-02  8.50696436e-02  8.63350564e-02  8.84389967e-02
  8.93637189e-02  8.97252733e-02  8.99488957e-02  9.05454176e-02
  9.15396443e-02  9.33394134e-02  9.44682148e-02  9.61409554e-02
  9.62594883e-02  9.80300913e-02  9.80787365e-02  9.81556056e-02
  1.00111076e-01  1.00221855e-01  1.01279503e-01  1.05613527e-01
  1.07760744e-01  1.08575573e-01  1.10239564e-01  1.10486020e-01
  1.10643062e-01  1.12618939e-01  1.15035210e-01  1.17674791e-01
  1.18212253e-01  1.18676572e-01  1.21125065e-01  1.24238177e-01
  1.25242645e-01  1.26690323e-01  1.28647679e-01  1.29502978e-01
  1.30984576e-01  1.33673456e-01  1.34193509e-01  1.34228299e-01
  1.34392969e-01  1.34881407e-01  1.37079482e-01  1.43127863e-01
  1.43545463e-01  1.43582729e-01  1.44652160e-01  1.45995177e-01
  1.46326274e-01  1.46356266e-01  1.46628917e-01  1.46723430e-01
  1.50855834e-01  1.52240774e-01  1.52389833e-01  1.53251995e-01
  1.54348029e-01  1.54413884e-01  1.54873169e-01  1.55262998e-01
  1.57458583e-01  1.60082433e-01  1.60580120e-01  1.61070207e-01
  1.61786224e-01  1.64005106e-01  1.64023517e-01  1.64488620e-01
  1.64824046e-01  1.64961179e-01  1.65001184e-01  1.65055937e-01
  1.69358481e-01  1.70028525e-01  1.70084140e-01  1.70085384e-01
  1.72702106e-01  1.73215145e-01  1.73817911e-01  1.75592355e-01
  1.76138853e-01  1.76165965e-01  1.76224422e-01  1.76846975e-01
  1.81999005e-01  1.82259027e-01  1.82972832e-01  1.87825318e-01
  1.88446019e-01  1.88528833e-01  1.93185456e-01  1.93407397e-01
  1.94410047e-01  1.95363212e-01  1.95426951e-01  1.97101374e-01
  1.97627233e-01  1.98167250e-01  2.03203856e-01  2.03402657e-01
  2.03959951e-01  2.06751335e-01  2.08239362e-01  2.08762299e-01
  2.09243628e-01  2.09508589e-01  2.12311859e-01  2.13717154e-01
  2.14233635e-01  2.14405407e-01  2.14546854e-01  2.14745591e-01
  2.20186472e-01  2.20920346e-01  2.21349736e-01  2.25292083e-01
  2.28577464e-01  2.29291552e-01  2.29848030e-01  2.30473168e-01
  2.31944154e-01  2.38241450e-01  2.41062049e-01  2.41498936e-01
  2.41621133e-01  2.42366318e-01  2.43557845e-01  2.43759614e-01
  2.44182346e-01  2.45622935e-01  2.46320625e-01  2.47725946e-01
  2.49006865e-01  2.51996817e-01  2.52063315e-01  2.52298031e-01
  2.52555675e-01  2.55180346e-01  2.55937206e-01  2.57344904e-01
  2.57804420e-01  2.60880626e-01  2.64743517e-01  2.65155944e-01
  2.65623799e-01  2.66793332e-01  2.66925538e-01  2.67141546e-01
  2.67263580e-01  2.70990021e-01  2.71465108e-01  2.72262053e-01
  2.72661439e-01  2.73153721e-01  2.73531076e-01  2.73845804e-01
  2.74380470e-01  2.74519034e-01  2.79614225e-01  2.81435270e-01
  2.82787421e-01  2.86798893e-01  2.87868681e-01  2.92221817e-01
  2.92312656e-01  2.94412844e-01  2.98211330e-01  2.98836349e-01
  3.00482579e-01  3.01389955e-01  3.03011439e-01  3.03964078e-01
  3.05625936e-01  3.06984161e-01  3.07336108e-01  3.07364681e-01
  3.08495089e-01  3.11611762e-01  3.15685756e-01  3.17719211e-01
  3.20938080e-01  3.22231946e-01  3.24225896e-01  3.26805514e-01
  3.27351390e-01  3.27575555e-01  3.29056894e-01  3.32662305e-01
  3.39544445e-01  3.42729147e-01  3.47801412e-01  3.51327870e-01
  3.52620380e-01  3.54557512e-01  3.55271611e-01  3.55566039e-01
  3.56203651e-01  3.61150716e-01  3.61306904e-01  3.64719420e-01
  3.65421696e-01  3.65960537e-01  3.67889466e-01  3.71022542e-01
  3.72239815e-01  3.75512413e-01  3.80591302e-01  3.83257013e-01
  3.85157870e-01  3.86574283e-01  3.87003329e-01  3.89397698e-01
  3.89610908e-01  3.91217369e-01  3.96909354e-01  3.97382087e-01
  3.98712315e-01  4.00629245e-01  4.01988696e-01  4.02219149e-01
  4.06465155e-01  4.06521530e-01  4.08609031e-01  4.12614440e-01
  4.15076433e-01  4.16780220e-01  4.22334777e-01  4.23150233e-01
  4.23438596e-01  4.26427761e-01  4.29182446e-01  4.30066499e-01
  4.30265118e-01  4.39050398e-01  4.39609623e-01  4.39622239e-01
  4.42017291e-01  4.43750820e-01  4.43986072e-01  4.45172928e-01
  4.49684013e-01  4.52678438e-01  4.55460957e-01  4.55715449e-01
  4.56578388e-01  4.58606177e-01  4.60326438e-01  4.60388264e-01
  4.61217234e-01  4.64155154e-01  4.64565741e-01  4.68204482e-01
  4.68905496e-01  4.71695730e-01  4.71903589e-01  4.72401048e-01
  4.76537116e-01  4.78864645e-01  4.89462068e-01  4.91946025e-01
  5.05498199e-01  5.06629779e-01  5.07109784e-01  5.13220172e-01
  5.17453082e-01  5.17538800e-01  5.17621180e-01  5.18004117e-01
  5.20890298e-01  5.22935800e-01  5.24813358e-01  5.27614734e-01
  5.27768165e-01  5.31681723e-01  5.33429645e-01  5.35755977e-01
  5.39903781e-01  5.41280589e-01  5.42980733e-01  5.43475277e-01
  5.47656917e-01  5.47853633e-01  5.51561063e-01  5.55312901e-01
  5.60589614e-01  5.65097892e-01  5.68274188e-01  5.73400710e-01
  5.79395842e-01  5.83446751e-01  5.84937499e-01  5.90581397e-01
  5.91207147e-01  5.91573418e-01  5.93023454e-01  6.03577877e-01
  6.06468068e-01  6.08625510e-01  6.09626351e-01  6.14875244e-01
  6.16504197e-01  6.19428731e-01  6.24403558e-01  6.30664914e-01
  6.32258040e-01  6.44768403e-01  6.52093363e-01  6.55098063e-01
  6.74473215e-01  6.84467096e-01  6.98453211e-01  7.09590099e-01
  7.15970513e-01  7.17441881e-01  7.20620520e-01  7.43106493e-01
  7.46385167e-01]

  warnings.warn(

2022-11-03 10:51:23,587:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.23126416e-01 -2.19615078e-01 -2.17083819e-01 -2.03753273e-01
 -1.68333604e-01 -1.63923197e-01 -1.63823042e-01 -1.56957295e-01
 -1.54100372e-01 -1.53804573e-01 -1.52702686e-01 -1.51580086e-01
 -1.41701072e-01 -1.32971803e-01 -1.22595443e-01 -1.21807074e-01
 -1.20734999e-01 -1.18403927e-01 -1.14529489e-01 -1.12469935e-01
 -1.11086425e-01 -1.06847165e-01 -1.06126378e-01 -1.05346726e-01
 -1.04069909e-01 -1.03647861e-01 -1.02495142e-01 -9.92409446e-02
 -9.90281359e-02 -9.77675282e-02 -9.67283731e-02 -9.63000967e-02
 -9.20781353e-02 -9.14983423e-02 -9.14326844e-02 -8.99341428e-02
 -8.93943025e-02 -8.66770285e-02 -8.59538788e-02 -8.52212696e-02
 -8.49433291e-02 -8.45913388e-02 -7.78987967e-02 -7.73109244e-02
 -7.55640042e-02 -7.23766810e-02 -7.23514446e-02 -7.18581053e-02
 -7.11416394e-02 -7.08005250e-02 -7.05602359e-02 -7.03286522e-02
 -6.96717004e-02 -6.90715790e-02 -6.83477506e-02 -6.81130507e-02
 -6.76518338e-02 -6.62249366e-02 -6.35806074e-02 -6.27769352e-02
 -6.01720007e-02 -5.87333458e-02 -5.65435325e-02 -5.37808230e-02
 -5.37609914e-02 -5.24874864e-02 -5.12435945e-02 -5.09737727e-02
 -4.88200120e-02 -4.62130666e-02 -4.46491942e-02 -4.34519959e-02
 -4.28251642e-02 -4.01537817e-02 -3.88816617e-02 -3.88700635e-02
 -3.84308028e-02 -3.73100399e-02 -3.70515460e-02 -3.67465627e-02
 -3.62150674e-02 -3.52244111e-02 -3.39325846e-02 -3.34955241e-02
 -3.30388612e-02 -3.26805068e-02 -3.08259800e-02 -3.04926525e-02
 -2.82243745e-02 -2.73483774e-02 -2.60405299e-02 -2.50157702e-02
 -2.49985778e-02 -2.35306227e-02 -2.02278862e-02 -1.96887735e-02
 -1.96845597e-02 -1.90565785e-02 -1.59421378e-02 -1.55207057e-02
 -1.40343595e-02 -1.37877249e-02 -1.33964531e-02 -1.13909505e-02
 -1.05273530e-02 -1.03374070e-02 -1.00445649e-02 -9.00536214e-03
 -6.56809484e-03 -4.73771876e-03 -4.54027885e-03 -3.21588492e-03
 -1.92365514e-03  7.57567799e-05  4.34247045e-04  6.21091709e-03
  6.22709979e-03  6.41745108e-03  6.92321185e-03  7.08581460e-03
  7.62443153e-03  9.72118308e-03  1.00102266e-02  1.02893549e-02
  1.13914357e-02  1.19197074e-02  1.44705992e-02  1.94544261e-02
  2.08338957e-02  2.26334018e-02  2.51950354e-02  2.57051020e-02
  2.59335664e-02  2.61003854e-02  2.94961855e-02  3.25439225e-02
  3.25570383e-02  3.39752123e-02  3.43237810e-02  3.44041155e-02
  3.54569043e-02  3.58645757e-02  3.64160221e-02  3.69132012e-02
  3.77469605e-02  3.77487690e-02  3.78959877e-02  3.97849830e-02
  4.00051699e-02  4.16430346e-02  4.17662914e-02  4.33122262e-02
  4.41662110e-02  4.68582556e-02  4.77225570e-02  5.05465171e-02
  5.18888587e-02  5.31144321e-02  5.31371394e-02  5.70504734e-02
  5.70898820e-02  5.74816076e-02  5.75338450e-02  6.18253399e-02
  6.28325425e-02  6.28771486e-02  6.47235445e-02  6.50521229e-02
  6.63182290e-02  6.70781242e-02  6.72158882e-02  6.88845097e-02
  7.05505906e-02  7.15331557e-02  7.25058387e-02  7.28526414e-02
  7.30352852e-02  7.30852585e-02  7.38059307e-02  7.38754420e-02
  7.49862004e-02  7.64272724e-02  7.76307593e-02  7.97792622e-02
  7.99001944e-02  8.01912161e-02  8.04344794e-02  8.16857348e-02
  8.36488372e-02  8.40369173e-02  8.45053012e-02  8.56466545e-02
  8.57357954e-02  8.71742193e-02  8.84693994e-02  8.96515840e-02
  8.98638106e-02  9.07408203e-02  9.08166367e-02  9.13637701e-02
  9.23380560e-02  9.29138362e-02  9.33135966e-02  9.37987860e-02
  9.42993176e-02  9.56433639e-02  9.88709808e-02  9.96737756e-02
  1.01172770e-01  1.01733922e-01  1.03251028e-01  1.05381007e-01
  1.08672816e-01  1.09641166e-01  1.09647783e-01  1.09676797e-01
  1.09863020e-01  1.13298247e-01  1.14653475e-01  1.14867507e-01
  1.14953149e-01  1.14986071e-01  1.15966410e-01  1.16143822e-01
  1.16669099e-01  1.17656574e-01  1.17659109e-01  1.19168404e-01
  1.20629081e-01  1.20812234e-01  1.21250406e-01  1.22989263e-01
  1.23559567e-01  1.25436427e-01  1.25636710e-01  1.26094523e-01
  1.27444103e-01  1.27587162e-01  1.28338666e-01  1.28712912e-01
  1.29708580e-01  1.30173631e-01  1.30354708e-01  1.33565613e-01
  1.34124890e-01  1.34270781e-01  1.34281797e-01  1.34771267e-01
  1.35036127e-01  1.35580105e-01  1.38445185e-01  1.39297190e-01
  1.39429113e-01  1.41154887e-01  1.43664622e-01  1.43902737e-01
  1.43987838e-01  1.45712115e-01  1.45733859e-01  1.46231892e-01
  1.51469365e-01  1.52158187e-01  1.52987624e-01  1.54577011e-01
  1.55365981e-01  1.56486483e-01  1.56694242e-01  1.63869654e-01
  1.65360861e-01  1.66168719e-01  1.66341143e-01  1.67269998e-01
  1.67731148e-01  1.69780992e-01  1.70255424e-01  1.70322678e-01
  1.70400071e-01  1.70565534e-01  1.71799753e-01  1.73364087e-01
  1.74409186e-01  1.76035560e-01  1.78704126e-01  1.79381743e-01
  1.80583026e-01  1.80608846e-01  1.80810066e-01  1.81252806e-01
  1.85677939e-01  1.86629384e-01  1.87514535e-01  1.88323280e-01
  1.89555205e-01  1.89983416e-01  1.91745186e-01  1.92338721e-01
  1.93578940e-01  1.93765610e-01  1.96556572e-01  1.97376569e-01
  1.97485113e-01  1.98395289e-01  1.99998134e-01  2.00156282e-01
  2.00228777e-01  2.00281911e-01  2.01870980e-01  2.05808375e-01
  2.07091197e-01  2.11238141e-01  2.11256045e-01  2.11357161e-01
  2.12104306e-01  2.13375720e-01  2.14871889e-01  2.17581698e-01
  2.18133130e-01  2.18424225e-01  2.20044540e-01  2.21001855e-01
  2.24237264e-01  2.24542460e-01  2.25156626e-01  2.25170396e-01
  2.25437277e-01  2.30806013e-01  2.31329833e-01  2.33899721e-01
  2.34589122e-01  2.36461118e-01  2.38381851e-01  2.38960110e-01
  2.40572985e-01  2.41309193e-01  2.43176025e-01  2.43835616e-01
  2.46220029e-01  2.47024601e-01  2.48609824e-01  2.50038363e-01
  2.51004014e-01  2.52023390e-01  2.52161320e-01  2.52520557e-01
  2.54921574e-01  2.56078167e-01  2.63097349e-01  2.63815668e-01
  2.64689407e-01  2.67372812e-01  2.67974882e-01  2.68426145e-01
  2.71097814e-01  2.71179986e-01  2.71213338e-01  2.72591136e-01
  2.73761478e-01  2.74610493e-01  2.74838217e-01  2.75584618e-01
  2.76390469e-01  2.79290090e-01  2.80870204e-01  2.81620691e-01
  2.82393589e-01  2.82874779e-01  2.86635308e-01  2.86851077e-01
  2.87309813e-01  2.87605907e-01  2.89717654e-01  2.90762348e-01
  2.90858066e-01  2.91787408e-01  2.96739201e-01  2.96783213e-01
  2.98884242e-01  3.01885338e-01  3.03105329e-01  3.03518720e-01
  3.06792259e-01  3.07133527e-01  3.07831013e-01  3.08538904e-01
  3.08756798e-01  3.09696555e-01  3.10926119e-01  3.11437203e-01
  3.14660116e-01  3.15282450e-01  3.15910610e-01  3.18680684e-01
  3.20685171e-01  3.22389403e-01  3.24697386e-01  3.25534484e-01
  3.28656418e-01  3.29757468e-01  3.30231625e-01  3.33222479e-01
  3.33506337e-01  3.36527328e-01  3.37142538e-01  3.38055149e-01
  3.40115259e-01  3.41503616e-01  3.41734099e-01  3.42831922e-01
  3.44569421e-01  3.44900289e-01  3.46900824e-01  3.46967639e-01
  3.47055688e-01  3.47198523e-01  3.49029052e-01  3.52430486e-01
  3.54533838e-01  3.54982651e-01  3.55025185e-01  3.55110813e-01
  3.56402173e-01  3.59726591e-01  3.62239300e-01  3.65577504e-01
  3.70541045e-01  3.78154622e-01  3.80558267e-01  3.83101023e-01
  3.87008407e-01  3.90811770e-01  3.93369408e-01  3.93572720e-01
  3.94665316e-01  3.96620246e-01  4.00124916e-01  4.00186757e-01
  4.03843714e-01  4.04101559e-01  4.04628313e-01  4.04938406e-01
  4.05207721e-01  4.06021204e-01  4.07864055e-01  4.10265146e-01
  4.17936766e-01  4.22329081e-01  4.22658279e-01  4.27839586e-01
  4.34238659e-01  4.36329002e-01  4.36627163e-01  4.38518196e-01
  4.41090122e-01  4.41418033e-01  4.43937174e-01  4.47701314e-01
  4.51587657e-01  4.53957390e-01  4.55932483e-01  4.56901072e-01
  4.58809417e-01  4.62786767e-01  4.63824604e-01  4.68263900e-01
  4.74505637e-01  4.74854823e-01  4.76187357e-01  4.76571138e-01
  4.78084802e-01  4.78215549e-01  4.79838544e-01  4.79971973e-01
  4.81151003e-01  4.82284338e-01  4.82648775e-01  4.83772085e-01
  4.89426627e-01  4.91845584e-01  4.93115597e-01  5.02761239e-01
  5.05792572e-01  5.13712059e-01  5.15922691e-01  5.18335288e-01
  5.22291884e-01  5.25111456e-01  5.42873675e-01  5.55498934e-01
  5.62461433e-01  5.70265421e-01  5.85418742e-01  5.85970367e-01
  5.90075729e-01]

  warnings.warn(

2022-11-03 10:51:25,405:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:25,574:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-03 10:51:26,204:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.25935314 -0.22886796 -0.19909864 -0.19373132 -0.18881728 -0.18570603
 -0.17415481 -0.16451431 -0.16406108 -0.16205373 -0.15471423 -0.15344119
 -0.14999578 -0.14855287 -0.14161787 -0.13850339 -0.13033392 -0.12957808
 -0.12774978 -0.1267445  -0.1237257  -0.12277793 -0.12146526 -0.11970715
 -0.11823284 -0.1142652  -0.11418819 -0.11327263 -0.11142504 -0.10740241
 -0.10711875 -0.1046569  -0.1041006  -0.10033598 -0.10032755 -0.09997511
 -0.09947929 -0.09804215 -0.09613335 -0.09592637 -0.09374562 -0.09368915
 -0.09118561 -0.09109426 -0.09108776 -0.09101429 -0.09062707 -0.08849435
 -0.08758819 -0.08593071 -0.08438072 -0.08421623 -0.08413824 -0.08223052
 -0.08097094 -0.07802596 -0.07732122 -0.07706095 -0.07449971 -0.07184622
 -0.06759833 -0.06670952 -0.06433544 -0.0643021  -0.06420932 -0.06030831
 -0.05805428 -0.05761125 -0.05538606 -0.05431815 -0.05392552 -0.05206279
 -0.04988365 -0.04958763 -0.04887618 -0.04611721 -0.04596188 -0.04566102
 -0.04413918 -0.04226859 -0.03949708 -0.03945012 -0.0369658  -0.03664499
 -0.03555183 -0.03475482 -0.03360029 -0.03228564 -0.03116889 -0.02877164
 -0.02842615 -0.02831167 -0.02486134 -0.02001174 -0.01842925 -0.01715269
 -0.0144173  -0.01437348 -0.01279816 -0.01268827 -0.00565102 -0.00448639
 -0.00302776  0.0011658   0.0014891   0.00177898  0.00319781  0.00348202
  0.00360742  0.00462237  0.005231    0.00680235  0.00762329  0.00806559
  0.0081881   0.00933932  0.01213309  0.01257723  0.01368198  0.01373817
  0.01511152  0.01642398  0.01744927  0.01877279  0.01930337  0.01968256
  0.01997104  0.02052726  0.02209439  0.02337325  0.02341905  0.02526753
  0.02620127  0.02770363  0.02846492  0.02938234  0.02968067  0.03214183
  0.0346618   0.03648733  0.03728321  0.03741032  0.03836467  0.04106878
  0.04138276  0.04234364  0.04235243  0.04249538  0.04254441  0.04342291
  0.0511053   0.05120757  0.05160212  0.05188366  0.05290008  0.05461256
  0.05471481  0.05548022  0.05558768  0.05564959  0.05636599  0.05938991
  0.05979396  0.06186897  0.06306634  0.06343058  0.06348933  0.06390495
  0.06583693  0.0667745   0.06710154  0.06874906  0.07055943  0.07078872
  0.07167628  0.07482771  0.07644384  0.07663549  0.07817266  0.07852377
  0.078894    0.07913127  0.07932732  0.08015486  0.08116876  0.0815337
  0.08155332  0.08158585  0.08178938  0.08179115  0.08236919  0.08326443
  0.08535556  0.08592027  0.08765089  0.08837867  0.0884046   0.08984206
  0.09016528  0.0903257   0.09166818  0.09216314  0.09221311  0.09247545
  0.09343762  0.09434802  0.09544861  0.09701456  0.09718812  0.09969133
  0.10022946  0.10155192  0.10524502  0.10567931  0.10585471  0.10671307
  0.10787162  0.1089186   0.1100637   0.11032385  0.1103432   0.11125494
  0.11171359  0.1117447   0.11236376  0.11348073  0.11373921  0.11407436
  0.11435143  0.11731933  0.12238344  0.1227047   0.12366456  0.12470846
  0.12642132  0.12801003  0.12883318  0.13175682  0.13745255  0.13835879
  0.13951634  0.13964245  0.1401304   0.1405054   0.14096278  0.14170743
  0.14191806  0.14270227  0.14321325  0.14425295  0.14459463  0.14658496
  0.14695275  0.14902794  0.14964949  0.14990024  0.1531514   0.15345743
  0.15478544  0.15631976  0.16241292  0.16628957  0.16644739  0.16727541
  0.16882721  0.16890792  0.17014886  0.1749347   0.17579717  0.17650799
  0.17852904  0.1788767   0.18038953  0.18250624  0.18304986  0.18530635
  0.18563181  0.18684652  0.18692494  0.1902152   0.19023567  0.19385417
  0.19489143  0.19492984  0.19518308  0.19786659  0.1982863   0.2017715
  0.20223993  0.20342401  0.20578135  0.20598274  0.20807794  0.21020393
  0.21055641  0.21115964  0.21123621  0.21178887  0.21197481  0.21233643
  0.21245533  0.21432628  0.21602909  0.21766976  0.21839309  0.21969032
  0.21987665  0.22406267  0.22414462  0.22503167  0.22572657  0.229172
  0.23032249  0.23332398  0.23416762  0.23541561  0.23586141  0.23684768
  0.23768041  0.23860425  0.24263924  0.24536664  0.24614215  0.24871402
  0.25296254  0.25441769  0.25556772  0.25584702  0.25774306  0.25853971
  0.25958524  0.25962165  0.26126297  0.26146755  0.26369037  0.26486752
  0.26527029  0.26597182  0.26837933  0.26924003  0.27217809  0.27225908
  0.27299891  0.27311652  0.27461185  0.27994588  0.28070895  0.2852798
  0.28566827  0.28711738  0.28788651  0.28828068  0.28954115  0.29496147
  0.29657673  0.29664823  0.29788069  0.29995857  0.30078484  0.30417486
  0.30971031  0.31067113  0.31176105  0.3123565   0.31292643  0.31325438
  0.31630821  0.31709278  0.31928964  0.32020887  0.32113928  0.33040823
  0.33689222  0.33724265  0.33784256  0.34085911  0.34246125  0.34261665
  0.34600059  0.35002995  0.3503412   0.35091313  0.35265949  0.35271345
  0.35562236  0.35581405  0.35730996  0.35765958  0.35812443  0.36025017
  0.36148817  0.3620474   0.36378198  0.36537513  0.36592648  0.3665782
  0.37024079  0.37039885  0.37115522  0.37143717  0.37143961  0.37394274
  0.37428184  0.37428236  0.375895    0.37695567  0.37735879  0.38157967
  0.3852152   0.38730234  0.3958599   0.39656809  0.39792912  0.40030006
  0.40102098  0.40260048  0.40380503  0.40421698  0.40512962  0.40970643
  0.41049319  0.41251699  0.41420026  0.41629948  0.41706617  0.41857318
  0.41878726  0.41966332  0.41971697  0.42475169  0.42520683  0.4290287
  0.43044421  0.43688048  0.43944558  0.44037438  0.44236291  0.44277953
  0.44885322  0.45474691  0.45835022  0.46279211  0.46314493  0.46460395
  0.46520336  0.465482    0.47280357  0.47344223  0.47439954  0.47669847
  0.4771229   0.47944388  0.48108444  0.48196683  0.48255532  0.48505467
  0.48601687  0.49359728  0.49523195  0.49524338  0.49697915  0.49727843
  0.49736825  0.50233322  0.50412371  0.50535467  0.50630342  0.50800641
  0.51127637  0.51172251  0.51185989  0.51382664  0.52064891  0.53071341
  0.5363281   0.54383327  0.54473772  0.55776923  0.57123548  0.57357877
  0.5810613   0.58646474  0.60006431  0.61288454  0.61807147  0.61927888
  0.61987398  0.62057183  0.62294258  0.62719501  0.62833633  0.63900107
  0.64013915]

  warnings.warn(

2022-11-03 10:51:26,318:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.23352009 -0.21775838 -0.20311982 -0.19994159 -0.19744981 -0.19577474
 -0.18980012 -0.17945344 -0.17660442 -0.16813306 -0.15741536 -0.15209184
 -0.1514875  -0.15029532 -0.14946484 -0.14868733 -0.14867859 -0.14038282
 -0.1400508  -0.13915165 -0.13650341 -0.13353786 -0.13317646 -0.13280608
 -0.12737339 -0.12404101 -0.12302308 -0.12219105 -0.11745656 -0.11473712
 -0.11404015 -0.1129592  -0.1108083  -0.10970872 -0.10922754 -0.10715345
 -0.10318619 -0.10269237 -0.09745912 -0.09696177 -0.09649052 -0.09395318
 -0.09216035 -0.08858752 -0.08848821 -0.08809661 -0.08420951 -0.08385181
 -0.08237359 -0.08166291 -0.08146015 -0.07611052 -0.07573504 -0.07548292
 -0.07503324 -0.07435154 -0.07411678 -0.07331122 -0.07079142 -0.06989308
 -0.06786176 -0.06764327 -0.06610995 -0.06598989 -0.06463007 -0.06441302
 -0.05813765 -0.05757504 -0.05391172 -0.05328908 -0.0531816  -0.05209605
 -0.05040693 -0.04968132 -0.04756766 -0.04725821 -0.0472431  -0.04677338
 -0.04521396 -0.0440529  -0.04238826 -0.03989175 -0.03944803 -0.03908057
 -0.03547845 -0.03472078 -0.03304414 -0.03179277 -0.02808897 -0.02797236
 -0.0278925  -0.02681387 -0.02657807 -0.02332722 -0.02074844 -0.02070357
 -0.02012191 -0.01848045 -0.01778367 -0.01714381 -0.01533087 -0.014111
 -0.00937649 -0.00533395 -0.00442594 -0.00388156 -0.00144357  0.00292765
  0.00345142  0.0039962   0.00410255  0.00415945  0.00911729  0.01084365
  0.01279134  0.01298908  0.01483818  0.01632361  0.01944948  0.02090512
  0.02244538  0.02358452  0.02458407  0.02496768  0.02620858  0.03123994
  0.03124026  0.03153598  0.03240292  0.03296248  0.0333443   0.03410764
  0.03713555  0.03970951  0.04003809  0.04048322  0.0422809   0.04242583
  0.04322424  0.04339707  0.04376634  0.04738109  0.04906333  0.04907295
  0.04943244  0.05092032  0.05092062  0.05097833  0.05209231  0.05323245
  0.05455467  0.05644454  0.05782435  0.05840565  0.05842317  0.06174487
  0.06269667  0.0630508   0.06305564  0.0632052   0.06337114  0.06406551
  0.0664094   0.06670637  0.06921573  0.07241936  0.07678609  0.07735915
  0.0775396   0.07862345  0.07873728  0.07943401  0.08037311  0.08050776
  0.08148006  0.08282633  0.0830697   0.08341626  0.08350978  0.08540692
  0.08575648  0.08649577  0.08806618  0.09240526  0.0924128   0.09291307
  0.09372316  0.096144    0.09772195  0.09790382  0.09979112  0.10013279
  0.10117623  0.10476105  0.10515416  0.10620005  0.10737041  0.10884935
  0.11146567  0.11231154  0.11372567  0.11538286  0.11590452  0.11705335
  0.11755569  0.11809972  0.11997337  0.1221959   0.1268476   0.12910995
  0.12924928  0.12937959  0.12938921  0.13000317  0.13108789  0.13574468
  0.13591397  0.13629118  0.13818817  0.14153752  0.14244128  0.14311734
  0.14333917  0.14341515  0.1481108   0.14923768  0.14969848  0.15191366
  0.15395155  0.1566919   0.16417785  0.16621433  0.1662658   0.16647797
  0.16655651  0.16714917  0.16996247  0.17340807  0.17547315  0.17588497
  0.17700613  0.17712834  0.17743632  0.17886734  0.17974757  0.18018287
  0.1824832   0.18325771  0.1832649   0.18360428  0.18474884  0.19345821
  0.19548214  0.19712958  0.1976656   0.19783684  0.20097684  0.20235905
  0.20287434  0.20541129  0.20708741  0.20855137  0.21442588  0.21449241
  0.21482342  0.2152181   0.21641117  0.21652839  0.21741274  0.22146297
  0.22175906  0.221856    0.22231211  0.22304626  0.22353302  0.2262822
  0.22772572  0.22848385  0.22895416  0.23397998  0.23531515  0.2360197
  0.2365673   0.23948266  0.24108177  0.24285789  0.24351827  0.24784409
  0.24888908  0.24956856  0.25292892  0.25709056  0.25772563  0.25845919
  0.25864932  0.26621263  0.26680076  0.26689898  0.26743608  0.2690759
  0.26933062  0.2697426   0.27070863  0.27298878  0.27494755  0.27639853
  0.27734092  0.28133511  0.28152922  0.28261871  0.28764125  0.28946341
  0.29312404  0.29692614  0.29925396  0.29957959  0.30201278  0.30208829
  0.30230099  0.30367706  0.30423537  0.30430017  0.30651801  0.30671907
  0.30754261  0.30867948  0.30936222  0.31001974  0.31126096  0.31461453
  0.31549534  0.31854159  0.31924908  0.3209186   0.32154793  0.32204885
  0.32268653  0.32983848  0.3300335   0.33330801  0.3382024   0.34213129
  0.34394616  0.34476156  0.34685323  0.35002332  0.35062915  0.35094727
  0.3526636   0.35267663  0.353789    0.35744286  0.35985757  0.36104308
  0.36163462  0.36699891  0.37045976  0.37112562  0.37265042  0.37280373
  0.37285692  0.37774104  0.38240492  0.38295869  0.38540485  0.3883112
  0.39078894  0.39129643  0.39684167  0.39825877  0.40025895  0.40152642
  0.40331839  0.40595758  0.40664075  0.40737867  0.41135474  0.41224833
  0.41580081  0.41999397  0.42051045  0.42111769  0.42620394  0.42836706
  0.43085355  0.43294213  0.43319244  0.43320446  0.43338119  0.43691459
  0.43974728  0.43980649  0.44337149  0.44407374  0.44426528  0.44438687
  0.4462944   0.45628268  0.45727141  0.46007465  0.46899502  0.46922884
  0.46977183  0.47130547  0.47180065  0.47261036  0.47488267  0.47624819
  0.48503132  0.48534355  0.4867369   0.49055491  0.49146151  0.49366074
  0.49492283  0.49668477  0.49794969  0.50275517  0.5031809   0.5044654
  0.50518157  0.50913634  0.51101486  0.51207718  0.51330177  0.51422476
  0.51490878  0.51663551  0.51722995  0.52011541  0.52031409  0.52044597
  0.52251711  0.5240473   0.52662902  0.53501024  0.53952435  0.53982274
  0.54054113  0.54479229  0.54534482  0.54990072  0.55350227  0.55682057
  0.56544978  0.56736968  0.56776563  0.57166734  0.57274947  0.57737678
  0.58362588  0.5864078   0.58826968  0.58987818  0.59036189  0.59216279
  0.59330236  0.59617021  0.59857133  0.60553442  0.605953    0.61029633
  0.6106321   0.61088956  0.61148144  0.61434496  0.6170134   0.62230575
  0.62386062  0.62401816  0.62493421  0.62682838  0.62819085  0.62972504
  0.6320261   0.63295923  0.63486601  0.63492278  0.63507381  0.65790804
  0.66833287  0.67269922  0.67712168  0.67720127  0.68864188  0.68898131
  0.69558967  0.70072978  0.71455274  0.71600503  0.71721647  0.74198091
  0.7505446 ]

  warnings.warn(

2022-11-03 10:51:26,318:INFO:Calculating mean and std
2022-11-03 10:51:26,318:INFO:Creating metrics dataframe
2022-11-03 10:51:26,334:INFO:Uploading results into container
2022-11-03 10:51:26,334:INFO:Uploading model into container now
2022-11-03 10:51:26,334:INFO:master_model_container: 20
2022-11-03 10:51:26,342:INFO:display_container: 2
2022-11-03 10:51:26,342:INFO:HuberRegressor()
2022-11-03 10:51:26,342:INFO:create_model() successfully completed......................................
2022-11-03 10:51:26,619:ERROR:create_model() for HuberRegressor() raised an exception or returned all 0.0:
2022-11-03 10:51:26,619:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:51:26,619:INFO:Initializing K Neighbors Regressor
2022-11-03 10:51:26,619:INFO:Total runtime is 2.8451534509658813 minutes
2022-11-03 10:51:26,619:INFO:SubProcess create_model() called ==================================
2022-11-03 10:51:26,619:INFO:Initializing create_model()
2022-11-03 10:51:26,619:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:51:26,619:INFO:Checking exceptions
2022-11-03 10:51:26,619:INFO:Importing libraries
2022-11-03 10:51:26,619:INFO:Copying training dataset
2022-11-03 10:51:26,634:INFO:Defining folds
2022-11-03 10:51:26,634:INFO:Declaring metric variables
2022-11-03 10:51:26,634:INFO:Importing untrained model
2022-11-03 10:51:26,634:INFO:K Neighbors Regressor Imported successfully
2022-11-03 10:51:26,634:INFO:Starting cross validation
2022-11-03 10:51:26,650:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:51:31,414:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:31,535:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:31,535:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:31,602:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:31,696:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:31,760:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:31,760:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:31,865:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:34,236:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:34,326:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:34,326:INFO:Calculating mean and std
2022-11-03 10:51:34,326:INFO:Creating metrics dataframe
2022-11-03 10:51:34,342:INFO:Uploading results into container
2022-11-03 10:51:34,342:INFO:Uploading model into container now
2022-11-03 10:51:34,342:INFO:master_model_container: 21
2022-11-03 10:51:34,342:INFO:display_container: 2
2022-11-03 10:51:34,342:INFO:KNeighborsRegressor(n_jobs=-1)
2022-11-03 10:51:34,342:INFO:create_model() successfully completed......................................
2022-11-03 10:51:34,632:WARNING:create_model() for KNeighborsRegressor(n_jobs=-1) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:51:34,632:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:51:34,632:INFO:Initializing create_model()
2022-11-03 10:51:34,632:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:51:34,632:INFO:Checking exceptions
2022-11-03 10:51:34,649:INFO:Importing libraries
2022-11-03 10:51:34,649:INFO:Copying training dataset
2022-11-03 10:51:34,665:INFO:Defining folds
2022-11-03 10:51:34,665:INFO:Declaring metric variables
2022-11-03 10:51:34,665:INFO:Importing untrained model
2022-11-03 10:51:34,665:INFO:K Neighbors Regressor Imported successfully
2022-11-03 10:51:34,665:INFO:Starting cross validation
2022-11-03 10:51:34,673:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:51:38,922:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:38,938:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:38,994:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:39,026:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:39,310:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:39,320:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:39,404:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:39,504:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:41,900:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:41,941:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  warnings.warn(

2022-11-03 10:51:41,957:INFO:Calculating mean and std
2022-11-03 10:51:41,957:INFO:Creating metrics dataframe
2022-11-03 10:51:41,968:INFO:Uploading results into container
2022-11-03 10:51:41,968:INFO:Uploading model into container now
2022-11-03 10:51:41,977:INFO:master_model_container: 22
2022-11-03 10:51:41,977:INFO:display_container: 2
2022-11-03 10:51:41,977:INFO:KNeighborsRegressor(n_jobs=-1)
2022-11-03 10:51:41,977:INFO:create_model() successfully completed......................................
2022-11-03 10:51:42,263:ERROR:create_model() for KNeighborsRegressor(n_jobs=-1) raised an exception or returned all 0.0:
2022-11-03 10:51:42,265:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:51:42,265:INFO:Initializing Decision Tree Regressor
2022-11-03 10:51:42,265:INFO:Total runtime is 3.1059231837590535 minutes
2022-11-03 10:51:42,265:INFO:SubProcess create_model() called ==================================
2022-11-03 10:51:42,265:INFO:Initializing create_model()
2022-11-03 10:51:42,265:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:51:42,265:INFO:Checking exceptions
2022-11-03 10:51:42,265:INFO:Importing libraries
2022-11-03 10:51:42,265:INFO:Copying training dataset
2022-11-03 10:51:42,296:INFO:Defining folds
2022-11-03 10:51:42,296:INFO:Declaring metric variables
2022-11-03 10:51:42,296:INFO:Importing untrained model
2022-11-03 10:51:42,296:INFO:Decision Tree Regressor Imported successfully
2022-11-03 10:51:42,296:INFO:Starting cross validation
2022-11-03 10:51:42,305:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:51:46,554:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:46,585:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:46,601:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:46,632:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:46,756:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:46,798:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:46,884:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:46,928:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:49,568:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:49,671:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:49,674:INFO:Calculating mean and std
2022-11-03 10:51:49,674:INFO:Creating metrics dataframe
2022-11-03 10:51:49,683:INFO:Uploading results into container
2022-11-03 10:51:49,683:INFO:Uploading model into container now
2022-11-03 10:51:49,683:INFO:master_model_container: 23
2022-11-03 10:51:49,683:INFO:display_container: 2
2022-11-03 10:51:49,683:INFO:DecisionTreeRegressor(random_state=4411)
2022-11-03 10:51:49,683:INFO:create_model() successfully completed......................................
2022-11-03 10:51:49,921:WARNING:create_model() for DecisionTreeRegressor(random_state=4411) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:51:49,921:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:51:49,921:INFO:Initializing create_model()
2022-11-03 10:51:49,921:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:51:49,921:INFO:Checking exceptions
2022-11-03 10:51:49,937:INFO:Importing libraries
2022-11-03 10:51:49,937:INFO:Copying training dataset
2022-11-03 10:51:49,942:INFO:Defining folds
2022-11-03 10:51:49,942:INFO:Declaring metric variables
2022-11-03 10:51:49,942:INFO:Importing untrained model
2022-11-03 10:51:49,942:INFO:Decision Tree Regressor Imported successfully
2022-11-03 10:51:49,942:INFO:Starting cross validation
2022-11-03 10:51:49,955:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:51:54,090:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:54,123:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:54,187:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:54,243:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:54,345:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:54,462:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:54,473:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:54,539:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:56,930:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:56,946:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 163, in inverse_transform
    return self.classes_[y]
IndexError: arrays used as indices must be of integer (or boolean) type

  warnings.warn(

2022-11-03 10:51:56,946:INFO:Calculating mean and std
2022-11-03 10:51:56,946:INFO:Creating metrics dataframe
2022-11-03 10:51:56,963:INFO:Uploading results into container
2022-11-03 10:51:56,963:INFO:Uploading model into container now
2022-11-03 10:51:56,963:INFO:master_model_container: 24
2022-11-03 10:51:56,978:INFO:display_container: 2
2022-11-03 10:51:56,978:INFO:DecisionTreeRegressor(random_state=4411)
2022-11-03 10:51:56,978:INFO:create_model() successfully completed......................................
2022-11-03 10:51:57,239:ERROR:create_model() for DecisionTreeRegressor(random_state=4411) raised an exception or returned all 0.0:
2022-11-03 10:51:57,239:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:51:57,239:INFO:Initializing Random Forest Regressor
2022-11-03 10:51:57,239:INFO:Total runtime is 3.355487557252248 minutes
2022-11-03 10:51:57,239:INFO:SubProcess create_model() called ==================================
2022-11-03 10:51:57,239:INFO:Initializing create_model()
2022-11-03 10:51:57,239:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:51:57,239:INFO:Checking exceptions
2022-11-03 10:51:57,239:INFO:Importing libraries
2022-11-03 10:51:57,239:INFO:Copying training dataset
2022-11-03 10:51:57,255:INFO:Defining folds
2022-11-03 10:51:57,255:INFO:Declaring metric variables
2022-11-03 10:51:57,255:INFO:Importing untrained model
2022-11-03 10:51:57,255:INFO:Random Forest Regressor Imported successfully
2022-11-03 10:51:57,255:INFO:Starting cross validation
2022-11-03 10:51:57,263:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:52:06,553:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.36 0.37 0.38 0.39 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57
 0.58 0.6  0.61 0.62 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72 0.73
 0.74 0.75 0.76 0.77 0.78 0.8  0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89
 0.9  0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:06,697:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.55 0.56 0.57 0.58
 0.59 0.6  0.61 0.62 0.64 0.65 0.66 0.68 0.69 0.7  0.71 0.72 0.73 0.74
 0.75 0.76 0.77 0.78 0.79 0.81 0.82 0.83 0.87 0.9  0.91 0.92 0.93 0.94
 0.95 0.96 0.97 0.98]

  warnings.warn(

2022-11-03 10:52:07,184:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7
 0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.81 0.82 0.83 0.84 0.85
 0.87 0.88 0.9  0.92 0.94 0.97]

  warnings.warn(

2022-11-03 10:52:07,185:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.67 0.68 0.69 0.7  0.72
 0.73 0.74 0.75 0.76 0.77 0.79 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88
 0.89 0.9  0.91 0.92 0.95 0.97 0.99]

  warnings.warn(

2022-11-03 10:52:07,203:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.38 0.39 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57
 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.69 0.7  0.71 0.72
 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8  0.83 0.84 0.85 0.86 0.89 0.9
 0.92 0.93 0.94 0.95 0.96 0.98]

  warnings.warn(

2022-11-03 10:52:07,266:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7
 0.71 0.72 0.73 0.74 0.75 0.77 0.78 0.81 0.82 0.83 0.84 0.86 0.87 0.89
 0.9  0.91 0.92 0.94 0.96 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:07,293:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.67 0.68 0.69 0.7  0.71
 0.73 0.74 0.75 0.76 0.77 0.79 0.8  0.81 0.82 0.83 0.85 0.87 0.88 0.89
 0.9  0.91 0.93 0.94 0.97 0.98]

  warnings.warn(

2022-11-03 10:52:07,420:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.52 0.54 0.55 0.57 0.58 0.59 0.6
 0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72 0.73 0.74
 0.75 0.76 0.77 0.78 0.8  0.82 0.83 0.84 0.85 0.86 0.88 0.89 0.9  0.91
 0.92 0.93 0.94 0.95 0.99]

  warnings.warn(

2022-11-03 10:52:11,102:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.38 0.39 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.55 0.56 0.57 0.58
 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72
 0.73 0.75 0.76 0.78 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.9
 0.92 0.93 0.96 0.98]

  warnings.warn(

2022-11-03 10:52:11,173:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.58 0.59 0.6  0.61 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71
 0.72 0.73 0.75 0.76 0.77 0.78 0.8  0.81 0.82 0.83 0.84 0.85 0.86 0.87
 0.88 0.89 0.9  0.91 0.93 0.94 0.96 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:11,173:INFO:Calculating mean and std
2022-11-03 10:52:11,173:INFO:Creating metrics dataframe
2022-11-03 10:52:11,189:INFO:Uploading results into container
2022-11-03 10:52:11,189:INFO:Uploading model into container now
2022-11-03 10:52:11,189:INFO:master_model_container: 25
2022-11-03 10:52:11,189:INFO:display_container: 2
2022-11-03 10:52:11,189:INFO:RandomForestRegressor(n_jobs=-1, random_state=4411)
2022-11-03 10:52:11,189:INFO:create_model() successfully completed......................................
2022-11-03 10:52:11,438:WARNING:create_model() for RandomForestRegressor(n_jobs=-1, random_state=4411) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:52:11,438:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:52:11,438:INFO:Initializing create_model()
2022-11-03 10:52:11,438:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:52:11,438:INFO:Checking exceptions
2022-11-03 10:52:11,455:INFO:Importing libraries
2022-11-03 10:52:11,455:INFO:Copying training dataset
2022-11-03 10:52:11,463:INFO:Defining folds
2022-11-03 10:52:11,463:INFO:Declaring metric variables
2022-11-03 10:52:11,471:INFO:Importing untrained model
2022-11-03 10:52:11,471:INFO:Random Forest Regressor Imported successfully
2022-11-03 10:52:11,471:INFO:Starting cross validation
2022-11-03 10:52:11,471:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:52:20,945:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.55 0.56 0.57 0.58
 0.59 0.6  0.61 0.62 0.64 0.65 0.66 0.68 0.69 0.7  0.71 0.72 0.73 0.74
 0.75 0.76 0.77 0.78 0.79 0.81 0.82 0.83 0.87 0.9  0.91 0.92 0.93 0.94
 0.95 0.96 0.97 0.98]

  warnings.warn(

2022-11-03 10:52:20,948:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.36 0.37 0.38 0.39 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57
 0.58 0.6  0.61 0.62 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72 0.73
 0.74 0.75 0.76 0.77 0.78 0.8  0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89
 0.9  0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:21,428:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.67 0.68 0.69 0.7  0.72
 0.73 0.74 0.75 0.76 0.77 0.79 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88
 0.89 0.9  0.91 0.92 0.95 0.97 0.99]

  warnings.warn(

2022-11-03 10:52:21,429:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.67 0.68 0.69 0.7  0.71
 0.73 0.74 0.75 0.76 0.77 0.79 0.8  0.81 0.82 0.83 0.85 0.87 0.88 0.89
 0.9  0.91 0.93 0.94 0.97 0.98]

  warnings.warn(

2022-11-03 10:52:21,487:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.52 0.54 0.55 0.57 0.58 0.59 0.6
 0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72 0.73 0.74
 0.75 0.76 0.77 0.78 0.8  0.82 0.83 0.84 0.85 0.86 0.88 0.89 0.9  0.91
 0.92 0.93 0.94 0.95 0.99]

  warnings.warn(

2022-11-03 10:52:21,570:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7
 0.71 0.72 0.73 0.74 0.75 0.77 0.78 0.81 0.82 0.83 0.84 0.86 0.87 0.89
 0.9  0.91 0.92 0.94 0.96 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:21,590:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7
 0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.81 0.82 0.83 0.84 0.85
 0.87 0.88 0.9  0.92 0.94 0.97]

  warnings.warn(

2022-11-03 10:52:21,879:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.38 0.39 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57
 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.69 0.7  0.71 0.72
 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8  0.83 0.84 0.85 0.86 0.89 0.9
 0.92 0.93 0.94 0.95 0.96 0.98]

  warnings.warn(

2022-11-03 10:52:25,294:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.58 0.59 0.6  0.61 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71
 0.72 0.73 0.75 0.76 0.77 0.78 0.8  0.81 0.82 0.83 0.84 0.85 0.86 0.87
 0.88 0.89 0.9  0.91 0.93 0.94 0.96 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:25,393:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.38 0.39 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.55 0.56 0.57 0.58
 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72
 0.73 0.75 0.76 0.78 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.9
 0.92 0.93 0.96 0.98]

  warnings.warn(

2022-11-03 10:52:25,393:INFO:Calculating mean and std
2022-11-03 10:52:25,393:INFO:Creating metrics dataframe
2022-11-03 10:52:25,409:INFO:Uploading results into container
2022-11-03 10:52:25,409:INFO:Uploading model into container now
2022-11-03 10:52:25,409:INFO:master_model_container: 26
2022-11-03 10:52:25,409:INFO:display_container: 2
2022-11-03 10:52:25,409:INFO:RandomForestRegressor(n_jobs=-1, random_state=4411)
2022-11-03 10:52:25,409:INFO:create_model() successfully completed......................................
2022-11-03 10:52:25,682:ERROR:create_model() for RandomForestRegressor(n_jobs=-1, random_state=4411) raised an exception or returned all 0.0:
2022-11-03 10:52:25,682:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:52:25,682:INFO:Initializing Extra Trees Regressor
2022-11-03 10:52:25,682:INFO:Total runtime is 3.8295353293418883 minutes
2022-11-03 10:52:25,682:INFO:SubProcess create_model() called ==================================
2022-11-03 10:52:25,682:INFO:Initializing create_model()
2022-11-03 10:52:25,682:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:52:25,682:INFO:Checking exceptions
2022-11-03 10:52:25,690:INFO:Importing libraries
2022-11-03 10:52:25,690:INFO:Copying training dataset
2022-11-03 10:52:25,698:INFO:Defining folds
2022-11-03 10:52:25,698:INFO:Declaring metric variables
2022-11-03 10:52:25,698:INFO:Importing untrained model
2022-11-03 10:52:25,698:INFO:Extra Trees Regressor Imported successfully
2022-11-03 10:52:25,698:INFO:Starting cross validation
2022-11-03 10:52:25,706:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:52:35,547:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.67 0.69 0.71 0.72 0.74
 0.75 0.77 0.79 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.9  0.91
 0.92 0.94 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:35,576:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57
 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.72
 0.74 0.75 0.76 0.78 0.79 0.8  0.81 0.82 0.84 0.86 0.87 0.88 0.89 0.9
 0.91 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:35,946:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.39 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57
 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71
 0.72 0.73 0.74 0.75 0.77 0.78 0.79 0.81 0.82 0.83 0.84 0.85 0.86 0.87
 0.88 0.89 0.9  0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:36,156:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29
 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.39 0.4  0.41 0.42 0.43 0.44
 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.58 0.59
 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.72 0.74 0.76
 0.77 0.78 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.92
 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:36,277:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.37 0.38 0.39 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.58
 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72
 0.73 0.74 0.75 0.77 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.87 0.88 0.89
 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:36,307:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.59 0.6  0.61 0.63 0.64 0.65 0.67 0.68 0.69 0.7  0.71 0.72 0.73
 0.74 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88
 0.89 0.9  0.92 0.93 0.94 0.95 0.96 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:36,344:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.42 0.44
 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57 0.58
 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72
 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.86
 0.87 0.88 0.89 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:36,385:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42 0.44
 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57 0.58
 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.72 0.73 0.74
 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.84 0.85 0.86 0.87 0.88 0.9
 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:39,804:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57
 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.67 0.68 0.69 0.7  0.71 0.73 0.74
 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.89
 0.9  0.91 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:39,995:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.27 0.28 0.29
 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.54 0.55 0.56 0.57 0.58
 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72
 0.73 0.74 0.75 0.76 0.78 0.79 0.8  0.81 0.83 0.84 0.85 0.86 0.89 0.9
 0.91 0.92 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:39,995:INFO:Calculating mean and std
2022-11-03 10:52:39,995:INFO:Creating metrics dataframe
2022-11-03 10:52:40,011:INFO:Uploading results into container
2022-11-03 10:52:40,011:INFO:Uploading model into container now
2022-11-03 10:52:40,011:INFO:master_model_container: 27
2022-11-03 10:52:40,011:INFO:display_container: 2
2022-11-03 10:52:40,011:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=4411)
2022-11-03 10:52:40,011:INFO:create_model() successfully completed......................................
2022-11-03 10:52:40,283:WARNING:create_model() for ExtraTreesRegressor(n_jobs=-1, random_state=4411) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:52:40,283:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:52:40,283:INFO:Initializing create_model()
2022-11-03 10:52:40,283:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:52:40,283:INFO:Checking exceptions
2022-11-03 10:52:40,294:INFO:Importing libraries
2022-11-03 10:52:40,294:INFO:Copying training dataset
2022-11-03 10:52:40,294:INFO:Defining folds
2022-11-03 10:52:40,294:INFO:Declaring metric variables
2022-11-03 10:52:40,294:INFO:Importing untrained model
2022-11-03 10:52:40,294:INFO:Extra Trees Regressor Imported successfully
2022-11-03 10:52:40,294:INFO:Starting cross validation
2022-11-03 10:52:40,310:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:52:50,042:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57
 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.72
 0.74 0.75 0.76 0.78 0.79 0.8  0.81 0.82 0.84 0.86 0.87 0.88 0.89 0.9
 0.91 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:50,188:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.28 0.29
 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.39 0.4  0.41 0.42 0.43 0.44
 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.58 0.59
 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.72 0.74 0.76
 0.77 0.78 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.92
 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:50,626:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.39 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57
 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71
 0.72 0.73 0.74 0.75 0.77 0.78 0.79 0.81 0.82 0.83 0.84 0.85 0.86 0.87
 0.88 0.89 0.9  0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:50,749:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.67 0.69 0.71 0.72 0.74
 0.75 0.77 0.79 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.9  0.91
 0.92 0.94 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:50,825:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.42 0.44
 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57 0.58
 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72
 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.86
 0.87 0.88 0.89 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:50,825:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42 0.44
 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57 0.58
 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.72 0.73 0.74
 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.84 0.85 0.86 0.87 0.88 0.9
 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:50,948:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.37 0.38 0.39 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.58
 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72
 0.73 0.74 0.75 0.77 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.87 0.88 0.89
 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:51,127:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42
 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56
 0.57 0.59 0.6  0.61 0.63 0.64 0.65 0.67 0.68 0.69 0.7  0.71 0.72 0.73
 0.74 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88
 0.89 0.9  0.92 0.93 0.94 0.95 0.96 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:54,489:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.27 0.28 0.29
 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.54 0.55 0.56 0.57 0.58
 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72
 0.73 0.74 0.75 0.76 0.78 0.79 0.8  0.81 0.83 0.84 0.85 0.86 0.89 0.9
 0.91 0.92 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:54,504:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14
 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28
 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.4  0.41 0.42 0.43
 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57
 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.67 0.68 0.69 0.7  0.71 0.73 0.74
 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.89
 0.9  0.91 0.93 0.94 0.95 0.96 0.97 0.98 0.99]

  warnings.warn(

2022-11-03 10:52:54,512:INFO:Calculating mean and std
2022-11-03 10:52:54,512:INFO:Creating metrics dataframe
2022-11-03 10:52:54,512:INFO:Uploading results into container
2022-11-03 10:52:54,512:INFO:Uploading model into container now
2022-11-03 10:52:54,512:INFO:master_model_container: 28
2022-11-03 10:52:54,512:INFO:display_container: 2
2022-11-03 10:52:54,512:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=4411)
2022-11-03 10:52:54,512:INFO:create_model() successfully completed......................................
2022-11-03 10:52:54,776:ERROR:create_model() for ExtraTreesRegressor(n_jobs=-1, random_state=4411) raised an exception or returned all 0.0:
2022-11-03 10:52:54,776:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:52:54,776:INFO:Initializing AdaBoost Regressor
2022-11-03 10:52:54,776:INFO:Total runtime is 4.314450812339783 minutes
2022-11-03 10:52:54,776:INFO:SubProcess create_model() called ==================================
2022-11-03 10:52:54,776:INFO:Initializing create_model()
2022-11-03 10:52:54,776:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:52:54,776:INFO:Checking exceptions
2022-11-03 10:52:54,792:INFO:Importing libraries
2022-11-03 10:52:54,792:INFO:Copying training dataset
2022-11-03 10:52:54,792:INFO:Defining folds
2022-11-03 10:52:54,792:INFO:Declaring metric variables
2022-11-03 10:52:54,792:INFO:Importing untrained model
2022-11-03 10:52:54,792:INFO:AdaBoost Regressor Imported successfully
2022-11-03 10:52:54,792:INFO:Starting cross validation
2022-11-03 10:52:54,808:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:52:59,297:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.03512015 0.05986904 0.06382979 0.07352941 0.07636888 0.07648184
 0.11674347 0.12552301 0.13218391 0.18518519 0.19602273 0.21230769
 0.25877193 0.29597198 0.32185629 0.34180791 0.34567901 0.36544586
 0.3808933  0.38567839 0.43085714 0.4606414  0.52176309 0.5230179
 0.52459016 0.5511811  0.56666667 0.56839422 0.67085954]

  warnings.warn(

2022-11-03 10:52:59,314:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.03353659 0.04516129 0.0952381  0.09701493 0.12807882 0.15968586
 0.19529412 0.20743034 0.21279213 0.25139665 0.25641026 0.26174497
 0.29898219 0.31152648 0.3141994  0.32512953 0.33696563 0.37866667
 0.40727273 0.41069627 0.41644562 0.41726619 0.43853428 0.44180791
 0.46839729 0.47058824 0.47315436 0.47344633 0.48434622 0.50141643
 0.50674663 0.58289963 0.60668185 0.61327014 0.64028777]

  warnings.warn(

2022-11-03 10:52:59,330:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.05446927 0.05728643 0.06584362 0.07179487 0.09276438 0.09508716
 0.12009804 0.12587413 0.17127072 0.17665615 0.18656716 0.2
 0.21264368 0.23591549 0.25897436 0.27809798 0.30946292 0.32373114
 0.36129032 0.3693901  0.3826087  0.39307049 0.40582524 0.42664093
 0.43181818 0.44418331 0.44966443 0.46153846 0.46685879 0.4828125
 0.4905765  0.50996016 0.53181818 0.5544186  0.57971014 0.58108108
 0.58499525 0.58774834 0.63274336 0.63865546]

  warnings.warn(

2022-11-03 10:52:59,331:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.03623188 0.06010929 0.07025411 0.07053942 0.08561644 0.08607595
 0.10602094 0.14559387 0.15229885 0.1827957  0.19928826 0.23562152
 0.2731377  0.27643172 0.27717391 0.29392971 0.31087584 0.32511737
 0.3491656  0.3601359  0.37467018 0.4031746  0.40452617 0.41813602
 0.42407407 0.43323996 0.44537815 0.46076146 0.4717608  0.48481013
 0.49899531 0.50249861 0.57692308 0.62392344 0.65339578 0.68992248
 0.7122807 ]

  warnings.warn(

2022-11-03 10:52:59,380:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.04225352 0.04599407 0.05714286 0.0609319  0.06338028 0.07132244
 0.08401084 0.09090909 0.1025641  0.12435897 0.13218391 0.14038877
 0.22352941 0.24242424 0.25358324 0.27681027 0.30581867 0.30856334
 0.31843575 0.32894737 0.34130435 0.34812287 0.35332894 0.35469613
 0.3908046  0.39530333 0.39937435 0.40112994 0.4081287  0.41111111
 0.41447368 0.42369021 0.42962963 0.42997543 0.44705882 0.44919786
 0.47744581 0.48786718 0.50547046 0.51061093 0.51131222 0.51526032
 0.53639847 0.58740831 0.59042553 0.59390048 0.59953524 0.60281065
 0.64073227]

  warnings.warn(

2022-11-03 10:52:59,412:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.02891933 0.09814324 0.13470681 0.1787234  0.17961165 0.21184919
 0.21359223 0.23478261 0.30024814 0.3040293  0.31375    0.34664401
 0.34971098 0.3528     0.36470588 0.36929461 0.37288136 0.37878788
 0.38935108 0.41755319 0.4338843  0.43655413 0.45588235 0.46294307
 0.47204969 0.47682119 0.5106383  0.53691275 0.56630582 0.56999372
 0.58577778 0.59107535 0.61811723 0.6196868  0.63474388]

  warnings.warn(

2022-11-03 10:52:59,460:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.05637982 0.05837321 0.06177606 0.06179775 0.07194245 0.08163265
 0.1312336  0.17514124 0.18778626 0.20261438 0.21621622 0.25
 0.27013177 0.27067669 0.2887078  0.29859155 0.30384615 0.33510638
 0.3375     0.35348837 0.37343358 0.3890411  0.43548387 0.44052863
 0.44501718 0.44921875 0.45347468 0.45692308 0.46081871 0.4718845
 0.50157729 0.56130268 0.58615264 0.6037182  0.65880218]

  warnings.warn(

2022-11-03 10:52:59,581:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01056338 0.03974895 0.0441989  0.05787781 0.06567164 0.06610169
 0.09177215 0.09738717 0.12179487 0.22123894 0.23938224 0.24
 0.24699828 0.27682927 0.30284728 0.30338983 0.30363636 0.30670927
 0.30838816 0.31263858 0.38558559 0.4025641  0.43558282 0.43771828
 0.44788732 0.45762712 0.46226415 0.47457627 0.47663866 0.50813008
 0.50849443 0.52747253 0.53763441 0.58607498 0.59809028 0.62537166
 0.64143426 0.67053364 0.68552036]

  warnings.warn(

2022-11-03 10:53:02,088:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01901743 0.03187251 0.07017544 0.089701   0.11940299 0.12045889
 0.15436242 0.1910828  0.24930748 0.2605042  0.28415301 0.28544601
 0.31081081 0.39737991 0.42769231 0.43256997 0.44422311 0.45004669
 0.48115942 0.49758454 0.53968254 0.60606061 0.61135371 0.69285714]

  warnings.warn(

2022-11-03 10:53:02,244:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.02188184 0.03501946 0.039801   0.05371901 0.06896552 0.09090909
 0.09462916 0.09935897 0.10606061 0.18585859 0.20575221 0.24444444
 0.25224215 0.2628866  0.26375176 0.26573427 0.28220859 0.30666667
 0.3078203  0.34470588 0.3755144  0.38948995 0.39150943 0.4
 0.40376266 0.41193182 0.42307692 0.43141593 0.43349754 0.47826087
 0.4964986  0.53459119 0.58801142 0.60730594 0.62700661 0.63694268]

  warnings.warn(

2022-11-03 10:53:02,260:INFO:Calculating mean and std
2022-11-03 10:53:02,261:INFO:Creating metrics dataframe
2022-11-03 10:53:02,269:INFO:Uploading results into container
2022-11-03 10:53:02,269:INFO:Uploading model into container now
2022-11-03 10:53:02,269:INFO:master_model_container: 29
2022-11-03 10:53:02,269:INFO:display_container: 2
2022-11-03 10:53:02,269:INFO:AdaBoostRegressor(random_state=4411)
2022-11-03 10:53:02,269:INFO:create_model() successfully completed......................................
2022-11-03 10:53:02,518:WARNING:create_model() for AdaBoostRegressor(random_state=4411) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:53:02,518:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:53:02,518:INFO:Initializing create_model()
2022-11-03 10:53:02,518:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:53:02,518:INFO:Checking exceptions
2022-11-03 10:53:02,534:INFO:Importing libraries
2022-11-03 10:53:02,534:INFO:Copying training dataset
2022-11-03 10:53:02,550:INFO:Defining folds
2022-11-03 10:53:02,550:INFO:Declaring metric variables
2022-11-03 10:53:02,550:INFO:Importing untrained model
2022-11-03 10:53:02,550:INFO:AdaBoost Regressor Imported successfully
2022-11-03 10:53:02,550:INFO:Starting cross validation
2022-11-03 10:53:02,550:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:53:06,995:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.03353659 0.04516129 0.0952381  0.09701493 0.12807882 0.15968586
 0.19529412 0.20743034 0.21279213 0.25139665 0.25641026 0.26174497
 0.29898219 0.31152648 0.3141994  0.32512953 0.33696563 0.37866667
 0.40727273 0.41069627 0.41644562 0.41726619 0.43853428 0.44180791
 0.46839729 0.47058824 0.47315436 0.47344633 0.48434622 0.50141643
 0.50674663 0.58289963 0.60668185 0.61327014 0.64028777]

  warnings.warn(

2022-11-03 10:53:07,130:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.03623188 0.06010929 0.07025411 0.07053942 0.08561644 0.08607595
 0.10602094 0.14559387 0.15229885 0.1827957  0.19928826 0.23562152
 0.2731377  0.27643172 0.27717391 0.29392971 0.31087584 0.32511737
 0.3491656  0.3601359  0.37467018 0.4031746  0.40452617 0.41813602
 0.42407407 0.43323996 0.44537815 0.46076146 0.4717608  0.48481013
 0.49899531 0.50249861 0.57692308 0.62392344 0.65339578 0.68992248
 0.7122807 ]

  warnings.warn(

2022-11-03 10:53:07,146:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.04225352 0.04599407 0.05714286 0.0609319  0.06338028 0.07132244
 0.08401084 0.09090909 0.1025641  0.12435897 0.13218391 0.14038877
 0.22352941 0.24242424 0.25358324 0.27681027 0.30581867 0.30856334
 0.31843575 0.32894737 0.34130435 0.34812287 0.35332894 0.35469613
 0.3908046  0.39530333 0.39937435 0.40112994 0.4081287  0.41111111
 0.41447368 0.42369021 0.42962963 0.42997543 0.44705882 0.44919786
 0.47744581 0.48786718 0.50547046 0.51061093 0.51131222 0.51526032
 0.53639847 0.58740831 0.59042553 0.59390048 0.59953524 0.60281065
 0.64073227]

  warnings.warn(

2022-11-03 10:53:07,146:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.03512015 0.05986904 0.06382979 0.07352941 0.07636888 0.07648184
 0.11674347 0.12552301 0.13218391 0.18518519 0.19602273 0.21230769
 0.25877193 0.29597198 0.32185629 0.34180791 0.34567901 0.36544586
 0.3808933  0.38567839 0.43085714 0.4606414  0.52176309 0.5230179
 0.52459016 0.5511811  0.56666667 0.56839422 0.67085954]

  warnings.warn(

2022-11-03 10:53:07,169:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.05637982 0.05837321 0.06177606 0.06179775 0.07194245 0.08163265
 0.1312336  0.17514124 0.18778626 0.20261438 0.21621622 0.25
 0.27013177 0.27067669 0.2887078  0.29859155 0.30384615 0.33510638
 0.3375     0.35348837 0.37343358 0.3890411  0.43548387 0.44052863
 0.44501718 0.44921875 0.45347468 0.45692308 0.46081871 0.4718845
 0.50157729 0.56130268 0.58615264 0.6037182  0.65880218]

  warnings.warn(

2022-11-03 10:53:07,213:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.02891933 0.09814324 0.13470681 0.1787234  0.17961165 0.21184919
 0.21359223 0.23478261 0.30024814 0.3040293  0.31375    0.34664401
 0.34971098 0.3528     0.36470588 0.36929461 0.37288136 0.37878788
 0.38935108 0.41755319 0.4338843  0.43655413 0.45588235 0.46294307
 0.47204969 0.47682119 0.5106383  0.53691275 0.56630582 0.56999372
 0.58577778 0.59107535 0.61811723 0.6196868  0.63474388]

  warnings.warn(

2022-11-03 10:53:07,227:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01056338 0.03974895 0.0441989  0.05787781 0.06567164 0.06610169
 0.09177215 0.09738717 0.12179487 0.22123894 0.23938224 0.24
 0.24699828 0.27682927 0.30284728 0.30338983 0.30363636 0.30670927
 0.30838816 0.31263858 0.38558559 0.4025641  0.43558282 0.43771828
 0.44788732 0.45762712 0.46226415 0.47457627 0.47663866 0.50813008
 0.50849443 0.52747253 0.53763441 0.58607498 0.59809028 0.62537166
 0.64143426 0.67053364 0.68552036]

  warnings.warn(

2022-11-03 10:53:07,298:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.05446927 0.05728643 0.06584362 0.07179487 0.09276438 0.09508716
 0.12009804 0.12587413 0.17127072 0.17665615 0.18656716 0.2
 0.21264368 0.23591549 0.25897436 0.27809798 0.30946292 0.32373114
 0.36129032 0.3693901  0.3826087  0.39307049 0.40582524 0.42664093
 0.43181818 0.44418331 0.44966443 0.46153846 0.46685879 0.4828125
 0.4905765  0.50996016 0.53181818 0.5544186  0.57971014 0.58108108
 0.58499525 0.58774834 0.63274336 0.63865546]

  warnings.warn(

2022-11-03 10:53:09,993:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01901743 0.03187251 0.07017544 0.089701   0.11940299 0.12045889
 0.15436242 0.1910828  0.24930748 0.2605042  0.28415301 0.28544601
 0.31081081 0.39737991 0.42769231 0.43256997 0.44422311 0.45004669
 0.48115942 0.49758454 0.53968254 0.60606061 0.61135371 0.69285714]

  warnings.warn(

2022-11-03 10:53:10,042:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.02188184 0.03501946 0.039801   0.05371901 0.06896552 0.09090909
 0.09462916 0.09935897 0.10606061 0.18585859 0.20575221 0.24444444
 0.25224215 0.2628866  0.26375176 0.26573427 0.28220859 0.30666667
 0.3078203  0.34470588 0.3755144  0.38948995 0.39150943 0.4
 0.40376266 0.41193182 0.42307692 0.43141593 0.43349754 0.47826087
 0.4964986  0.53459119 0.58801142 0.60730594 0.62700661 0.63694268]

  warnings.warn(

2022-11-03 10:53:10,042:INFO:Calculating mean and std
2022-11-03 10:53:10,042:INFO:Creating metrics dataframe
2022-11-03 10:53:10,059:INFO:Uploading results into container
2022-11-03 10:53:10,059:INFO:Uploading model into container now
2022-11-03 10:53:10,059:INFO:master_model_container: 30
2022-11-03 10:53:10,059:INFO:display_container: 2
2022-11-03 10:53:10,067:INFO:AdaBoostRegressor(random_state=4411)
2022-11-03 10:53:10,067:INFO:create_model() successfully completed......................................
2022-11-03 10:53:10,331:ERROR:create_model() for AdaBoostRegressor(random_state=4411) raised an exception or returned all 0.0:
2022-11-03 10:53:10,331:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:53:10,331:INFO:Initializing Gradient Boosting Regressor
2022-11-03 10:53:10,331:INFO:Total runtime is 4.5736964305241905 minutes
2022-11-03 10:53:10,331:INFO:SubProcess create_model() called ==================================
2022-11-03 10:53:10,331:INFO:Initializing create_model()
2022-11-03 10:53:10,331:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:53:10,331:INFO:Checking exceptions
2022-11-03 10:53:10,331:INFO:Importing libraries
2022-11-03 10:53:10,331:INFO:Copying training dataset
2022-11-03 10:53:10,347:INFO:Defining folds
2022-11-03 10:53:10,347:INFO:Declaring metric variables
2022-11-03 10:53:10,347:INFO:Importing untrained model
2022-11-03 10:53:10,347:INFO:Gradient Boosting Regressor Imported successfully
2022-11-03 10:53:10,363:INFO:Starting cross validation
2022-11-03 10:53:10,363:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:53:16,747:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-9.49123978e-02 -3.27667021e-02 -3.17736934e-02 -3.00370420e-02
 -2.98976112e-02 -2.31488522e-02 -2.22964421e-02 -2.06006759e-02
 -2.02233435e-02 -1.83831750e-02 -1.80294352e-02 -1.79754313e-02
 -1.75325395e-02 -1.74904482e-02 -1.56372833e-02 -1.52457712e-02
 -1.37688106e-02 -1.35827739e-02 -1.28409592e-02 -1.20342478e-02
 -1.19060431e-02 -1.14871686e-02 -1.14336550e-02 -1.13937037e-02
 -1.09932468e-02 -1.07206980e-02 -1.03667511e-02 -9.74283365e-03
 -9.46511308e-03 -9.19192143e-03 -8.61631963e-03 -8.29902001e-03
 -8.13830796e-03 -7.85458318e-03 -7.76964830e-03 -7.55972145e-03
 -7.38524560e-03 -7.26851182e-03 -7.24197173e-03 -7.04942406e-03
 -6.65792301e-03 -6.52426530e-03 -5.91025394e-03 -5.73168272e-03
 -5.72496050e-03 -5.66027012e-03 -5.52247812e-03 -5.38603153e-03
 -5.22656153e-03 -5.05523994e-03 -4.89328147e-03 -4.65558547e-03
 -3.61015582e-03 -3.42794688e-03 -2.67293941e-03 -1.91372057e-03
 -1.43432982e-03 -1.75071652e-04  3.92227041e-04  4.88602357e-04
  6.73168675e-04  1.20972420e-03  1.90264864e-03  2.84119911e-03
  3.00341476e-03  3.07541952e-03  3.20855596e-03  3.69757020e-03
  3.90231118e-03  4.21727612e-03  4.57108448e-03  5.75428460e-03
  6.31369925e-03  6.50667030e-03  6.53439138e-03  6.66750762e-03
  7.30587726e-03  7.39957575e-03  7.92676578e-03  9.10129029e-03
  9.19564156e-03  9.34357108e-03  9.72679182e-03  1.10280254e-02
  1.12056935e-02  1.12568798e-02  1.12951575e-02  1.14161537e-02
  1.14762462e-02  1.31368091e-02  1.41221201e-02  1.43701022e-02
  1.54528654e-02  1.56733290e-02  1.58193383e-02  1.69870469e-02
  1.69890460e-02  1.86799733e-02  2.24297032e-02  2.24788656e-02
  2.27519583e-02  2.56915466e-02  2.57151857e-02  2.66418014e-02
  2.70981504e-02  2.72445244e-02  2.72630939e-02  2.87482326e-02
  2.88900467e-02  2.90995182e-02  2.99056744e-02  3.02554432e-02
  3.28540666e-02  3.34117187e-02  3.73728890e-02  4.18958636e-02
  4.27670591e-02  4.30306686e-02  4.30483048e-02  4.39810146e-02
  4.39931197e-02  4.42784783e-02  4.50433753e-02  4.69828327e-02
  4.72137704e-02  4.80226866e-02  4.90835320e-02  5.07217246e-02
  5.11534116e-02  5.18229413e-02  5.22583345e-02  5.60209875e-02
  5.61184643e-02  5.86485629e-02  6.35825404e-02  6.42146868e-02
  6.68487932e-02  7.04829751e-02  7.20599509e-02  7.42382535e-02
  7.48814338e-02  7.52319947e-02  7.77096299e-02  8.00600061e-02
  8.04335139e-02  8.19940952e-02  8.22734145e-02  8.23677736e-02
  8.37127143e-02  8.53107420e-02  8.94192369e-02  9.02772131e-02
  9.03217383e-02  9.07250337e-02  9.18453348e-02  9.25617890e-02
  9.29952762e-02  9.32827174e-02  9.41288230e-02  9.48369774e-02
  9.56749241e-02  9.79960298e-02  9.95177352e-02  1.01377597e-01
  1.02113019e-01  1.02728571e-01  1.06110478e-01  1.06422534e-01
  1.08923373e-01  1.09229158e-01  1.10023875e-01  1.11762754e-01
  1.13148737e-01  1.14460308e-01  1.15815177e-01  1.16122228e-01
  1.17656861e-01  1.18011739e-01  1.18994894e-01  1.20248357e-01
  1.21243617e-01  1.21483099e-01  1.23643253e-01  1.24785785e-01
  1.25031314e-01  1.25807721e-01  1.27508636e-01  1.28095285e-01
  1.29128491e-01  1.29267290e-01  1.29306895e-01  1.29688711e-01
  1.29824791e-01  1.30472594e-01  1.31790400e-01  1.32592983e-01
  1.32813966e-01  1.36168846e-01  1.37334206e-01  1.37776708e-01
  1.41012167e-01  1.42044495e-01  1.42711636e-01  1.43124964e-01
  1.43192599e-01  1.43315563e-01  1.43589976e-01  1.43927260e-01
  1.47117410e-01  1.50774481e-01  1.53903281e-01  1.54062201e-01
  1.54816312e-01  1.55229577e-01  1.55350965e-01  1.56719797e-01
  1.57022234e-01  1.57192576e-01  1.62215068e-01  1.63625256e-01
  1.65761107e-01  1.66451688e-01  1.66925941e-01  1.68727452e-01
  1.75782659e-01  1.77164110e-01  1.83364605e-01  1.84693854e-01
  1.85124461e-01  1.86801201e-01  1.87444600e-01  1.87742927e-01
  1.88658112e-01  1.89443420e-01  1.89715786e-01  1.93000006e-01
  1.93983026e-01  1.94004564e-01  1.94629578e-01  1.94888592e-01
  1.96465186e-01  1.97415332e-01  1.98416045e-01  1.99433003e-01
  1.99478863e-01  2.00709098e-01  2.01934361e-01  2.02720807e-01
  2.04937412e-01  2.06171793e-01  2.06563319e-01  2.07430909e-01
  2.09508533e-01  2.11132018e-01  2.12036926e-01  2.12073356e-01
  2.12840553e-01  2.15558943e-01  2.16739723e-01  2.16791885e-01
  2.17797891e-01  2.19261406e-01  2.24659162e-01  2.24778859e-01
  2.27109612e-01  2.29794301e-01  2.30580503e-01  2.31197276e-01
  2.31736121e-01  2.34740553e-01  2.34784112e-01  2.40040179e-01
  2.43215899e-01  2.43777829e-01  2.43886870e-01  2.44727371e-01
  2.46552234e-01  2.46910469e-01  2.49661456e-01  2.50394270e-01
  2.50921606e-01  2.56263657e-01  2.57323985e-01  2.59798935e-01
  2.60098462e-01  2.60501653e-01  2.60709804e-01  2.61323493e-01
  2.61655106e-01  2.64127826e-01  2.67276420e-01  2.67404679e-01
  2.71500489e-01  2.72051051e-01  2.76497580e-01  2.79165549e-01
  2.87119049e-01  2.88174533e-01  2.89415559e-01  2.89931913e-01
  2.92630237e-01  2.94040956e-01  2.97008831e-01  2.97448439e-01
  3.03477435e-01  3.06066923e-01  3.08873323e-01  3.10097803e-01
  3.10841634e-01  3.11499050e-01  3.12697953e-01  3.14339671e-01
  3.15341253e-01  3.16791796e-01  3.19138458e-01  3.22683862e-01
  3.26780335e-01  3.27596835e-01  3.29575067e-01  3.29648632e-01
  3.34308498e-01  3.34727794e-01  3.38779926e-01  3.40868302e-01
  3.43326404e-01  3.47092572e-01  3.51151320e-01  3.53586299e-01
  3.54918388e-01  3.58073705e-01  3.60600557e-01  3.65641091e-01
  3.66096548e-01  3.68282743e-01  3.68665358e-01  3.70715901e-01
  3.73231120e-01  3.76245132e-01  3.80461893e-01  3.81754604e-01
  3.83270375e-01  3.84972996e-01  3.85650343e-01  3.86344276e-01
  3.87089187e-01  3.89500645e-01  3.89554973e-01  3.92749879e-01
  4.00887026e-01  4.02245959e-01  4.06481165e-01  4.08299705e-01
  4.11959822e-01  4.14315791e-01  4.16450170e-01  4.18453288e-01
  4.22403997e-01  4.23851281e-01  4.27768843e-01  4.27910665e-01
  4.28668041e-01  4.30441452e-01  4.31174372e-01  4.32087234e-01
  4.33524840e-01  4.37074556e-01  4.37700783e-01  4.40133339e-01
  4.40871448e-01  4.46350776e-01  4.48058059e-01  4.50800084e-01
  4.51286798e-01  4.52943869e-01  4.53244020e-01  4.56978482e-01
  4.57587231e-01  4.58867718e-01  4.58906942e-01  4.67624480e-01
  4.72256196e-01  4.82326935e-01  4.83156057e-01  4.83940814e-01
  4.85555278e-01  4.86361180e-01  4.91789976e-01  4.92387164e-01
  4.96207398e-01  5.01310960e-01  5.01726329e-01  5.01803239e-01
  5.07704013e-01  5.07974216e-01  5.08874452e-01  5.09644413e-01
  5.14603397e-01  5.21663157e-01  5.21838558e-01  5.22360146e-01
  5.29334752e-01  5.29924309e-01  5.31218059e-01  5.34859836e-01
  5.34954630e-01  5.35848718e-01  5.45974570e-01  5.48869061e-01
  5.49297204e-01  5.58922890e-01  5.60076349e-01  5.61391197e-01
  5.61684503e-01  5.61803163e-01  5.70619834e-01  5.71626712e-01
  5.73235869e-01  5.73575562e-01  5.75819589e-01  5.79669412e-01
  5.86289232e-01  5.98155512e-01  5.98654403e-01  5.99947742e-01
  6.06555547e-01  6.14438366e-01  6.15955868e-01  6.16492245e-01
  6.18676389e-01  6.22818654e-01  6.27083686e-01  6.30883634e-01
  6.34871438e-01  6.39022461e-01  6.40405804e-01  6.49051407e-01
  6.50497188e-01  6.54511488e-01  6.57686445e-01  6.61782225e-01
  6.73432902e-01  6.77895644e-01  6.80898026e-01  6.95344024e-01
  6.97522871e-01  7.04906561e-01  7.05964260e-01  7.11577255e-01
  7.12019109e-01  7.13040735e-01  7.29308738e-01  7.34029605e-01
  7.37883769e-01  7.42138007e-01  7.48220604e-01  7.54842618e-01
  7.55086769e-01  7.55451696e-01  7.60268655e-01  7.60703316e-01
  7.64787021e-01  7.65760538e-01  7.69194050e-01  7.89147878e-01
  7.96068984e-01  8.15349703e-01  8.16085642e-01  8.38206860e-01
  8.40421659e-01  8.40508498e-01  8.42603969e-01  8.47280004e-01
  8.51706560e-01  8.83519701e-01  8.96083194e-01  9.00319472e-01
  9.00585100e-01  9.09820562e-01  9.09869245e-01  9.29009300e-01
  9.41723125e-01  9.57654132e-01  1.07089488e+00]

  warnings.warn(

2022-11-03 10:53:16,863:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-3.55145608e-02 -2.91982978e-02 -2.71880420e-02 -2.27656383e-02
 -2.18616118e-02 -2.09314821e-02 -1.92367744e-02 -1.91363411e-02
 -1.83760305e-02 -1.74578508e-02 -1.73951503e-02 -1.60529435e-02
 -1.54755903e-02 -1.50310622e-02 -1.33030426e-02 -1.31720668e-02
 -1.29206022e-02 -1.28678764e-02 -1.22899749e-02 -1.20119241e-02
 -1.19385410e-02 -1.19199328e-02 -1.14735034e-02 -1.06291659e-02
 -1.05092650e-02 -1.03087640e-02 -9.65253051e-03 -8.75826200e-03
 -7.05078383e-03 -6.08140159e-03 -5.83433623e-03 -5.49948594e-03
 -4.47844748e-03 -4.25056577e-03 -4.14954932e-03 -4.10933771e-03
 -4.01334597e-03 -3.59775051e-03 -3.43313570e-03 -3.29587094e-03
 -3.21164794e-03 -3.11498378e-03 -2.14052095e-03 -1.84549356e-03
 -1.69130175e-03 -4.52181971e-04  5.16124012e-05  4.08133731e-04
  6.67333104e-04  1.02324006e-03  1.36986672e-03  1.58830983e-03
  1.76328778e-03  2.56778051e-03  2.79482118e-03  2.89872106e-03
  2.98252691e-03  3.38459904e-03  3.41920244e-03  3.67260591e-03
  3.86339067e-03  4.02768167e-03  4.24727070e-03  4.37975616e-03
  5.74106360e-03  5.81428666e-03  5.84743114e-03  5.90250673e-03
  6.72682547e-03  6.75196224e-03  7.07451699e-03  7.41882666e-03
  7.47864710e-03  7.63298655e-03  7.71219820e-03  7.92210285e-03
  7.99393125e-03  8.19300726e-03  8.41616831e-03  8.47390166e-03
  9.18165689e-03  9.32900816e-03  1.04608094e-02  1.05153273e-02
  1.05849047e-02  1.07518919e-02  1.08862436e-02  1.09021965e-02
  1.12926599e-02  1.14081275e-02  1.16605216e-02  1.32118164e-02
  1.44836615e-02  1.51162398e-02  1.52999333e-02  1.55081134e-02
  1.55508127e-02  1.60119078e-02  1.62369491e-02  1.64592217e-02
  1.74180686e-02  2.17185636e-02  2.30832643e-02  2.34116879e-02
  2.47646698e-02  2.59180766e-02  2.65311252e-02  2.68503784e-02
  2.74261642e-02  2.83302934e-02  3.20815615e-02  3.21150482e-02
  3.24377523e-02  3.27236071e-02  3.29077694e-02  3.29737752e-02
  3.40815197e-02  3.51193273e-02  3.76458688e-02  4.05897539e-02
  4.07984484e-02  4.13435874e-02  4.14986217e-02  4.43350579e-02
  4.71804303e-02  4.73142632e-02  4.79739454e-02  4.92706041e-02
  4.95237463e-02  5.08269743e-02  5.15317674e-02  5.32611824e-02
  5.38275201e-02  5.44016386e-02  5.66956944e-02  5.76845424e-02
  5.81213929e-02  5.92365183e-02  6.06275900e-02  6.07552296e-02
  6.16491901e-02  6.48274262e-02  6.49431789e-02  6.50736319e-02
  6.68694884e-02  6.82794472e-02  6.98097931e-02  7.17354654e-02
  7.29999247e-02  7.58244269e-02  7.62105308e-02  7.66181745e-02
  7.66570259e-02  7.73488601e-02  7.74017565e-02  7.85250711e-02
  8.00561839e-02  8.36774399e-02  8.43038114e-02  8.43683571e-02
  8.46819322e-02  8.55601651e-02  8.58806080e-02  8.78771630e-02
  8.99904924e-02  9.03162187e-02  9.08609981e-02  9.09792862e-02
  9.18165208e-02  9.18954164e-02  9.24252785e-02  9.36946423e-02
  9.38220263e-02  9.64003132e-02  9.76758563e-02  9.78821563e-02
  9.86677850e-02  9.90512700e-02  1.00593328e-01  1.02023388e-01
  1.02330031e-01  1.02553126e-01  1.04247574e-01  1.05869783e-01
  1.07702195e-01  1.08282855e-01  1.10684321e-01  1.11691412e-01
  1.12647181e-01  1.12836602e-01  1.15102417e-01  1.15424516e-01
  1.17497366e-01  1.25778562e-01  1.26036513e-01  1.26982081e-01
  1.31565959e-01  1.32167580e-01  1.32867621e-01  1.32966020e-01
  1.33886685e-01  1.34282943e-01  1.35133813e-01  1.37679344e-01
  1.39307628e-01  1.43456956e-01  1.43457704e-01  1.44677255e-01
  1.46944773e-01  1.48589935e-01  1.50049778e-01  1.51350022e-01
  1.55990059e-01  1.56505178e-01  1.56620203e-01  1.58344579e-01
  1.59626715e-01  1.59660893e-01  1.60228863e-01  1.60720173e-01
  1.60947023e-01  1.62399750e-01  1.70523714e-01  1.74570895e-01
  1.76775654e-01  1.80480238e-01  1.81596909e-01  1.84048495e-01
  1.88442911e-01  1.89810821e-01  1.93025034e-01  1.94439197e-01
  1.95827244e-01  1.96769262e-01  1.98581842e-01  1.98995883e-01
  2.01126461e-01  2.01509815e-01  2.01536057e-01  2.02334133e-01
  2.02765129e-01  2.03172773e-01  2.03692246e-01  2.03933514e-01
  2.09192574e-01  2.11319595e-01  2.12768153e-01  2.16057137e-01
  2.17160912e-01  2.17830544e-01  2.18624779e-01  2.21503224e-01
  2.22382088e-01  2.26533314e-01  2.28235535e-01  2.29635230e-01
  2.29745189e-01  2.33564010e-01  2.37694635e-01  2.38114680e-01
  2.40230518e-01  2.41740737e-01  2.42388074e-01  2.45073019e-01
  2.47657072e-01  2.47849378e-01  2.48088315e-01  2.50386403e-01
  2.53077299e-01  2.53252639e-01  2.53342233e-01  2.56173758e-01
  2.56899309e-01  2.57652154e-01  2.61263272e-01  2.62397624e-01
  2.64068727e-01  2.66947272e-01  2.70341814e-01  2.77256705e-01
  2.77905203e-01  2.78961909e-01  2.79548394e-01  2.80499463e-01
  2.82087513e-01  2.82455755e-01  2.83954643e-01  2.85008634e-01
  2.85887594e-01  2.86285324e-01  2.86287944e-01  2.88781279e-01
  2.88785679e-01  2.89593722e-01  2.90000864e-01  2.91207913e-01
  2.91810694e-01  2.93854579e-01  2.95156742e-01  2.98525942e-01
  3.01773013e-01  3.02698265e-01  3.05121235e-01  3.06208909e-01
  3.09055017e-01  3.11004046e-01  3.11915274e-01  3.18630821e-01
  3.18708257e-01  3.21049380e-01  3.25254472e-01  3.26068261e-01
  3.27788938e-01  3.29388113e-01  3.30227137e-01  3.31288939e-01
  3.31781385e-01  3.33039487e-01  3.33389481e-01  3.39126527e-01
  3.39306040e-01  3.39775232e-01  3.39775849e-01  3.47202761e-01
  3.51150770e-01  3.52757895e-01  3.53608212e-01  3.54989334e-01
  3.58448940e-01  3.58923406e-01  3.60064640e-01  3.63537056e-01
  3.64234950e-01  3.69199172e-01  3.74521219e-01  3.77023655e-01
  3.82000918e-01  3.82262714e-01  3.85405924e-01  3.86044917e-01
  3.89693136e-01  3.91553350e-01  3.94488321e-01  3.97058095e-01
  3.99152432e-01  4.00726342e-01  4.02014108e-01  4.04769638e-01
  4.04948519e-01  4.07971564e-01  4.10634267e-01  4.15476185e-01
  4.16541498e-01  4.16832739e-01  4.21547449e-01  4.22428375e-01
  4.22960361e-01  4.24499387e-01  4.26487361e-01  4.31783128e-01
  4.33819055e-01  4.34363418e-01  4.38045433e-01  4.41882599e-01
  4.43815404e-01  4.46164262e-01  4.47188624e-01  4.49981880e-01
  4.57008878e-01  4.57971617e-01  4.63357990e-01  4.66829370e-01
  4.70860381e-01  4.78797580e-01  4.79972735e-01  4.80828941e-01
  4.81165446e-01  4.85536128e-01  4.89220124e-01  4.90442703e-01
  4.90662684e-01  4.91471648e-01  4.95854640e-01  4.98524697e-01
  4.98941173e-01  5.03329551e-01  5.03390258e-01  5.03677231e-01
  5.05762120e-01  5.06574947e-01  5.08051494e-01  5.16100399e-01
  5.17919258e-01  5.18298494e-01  5.19102165e-01  5.21686733e-01
  5.23432318e-01  5.25384669e-01  5.27965895e-01  5.28443420e-01
  5.33919208e-01  5.35685011e-01  5.37815643e-01  5.38764370e-01
  5.41020102e-01  5.45384916e-01  5.48071222e-01  5.50719630e-01
  5.51177321e-01  5.55359545e-01  5.55950944e-01  5.58503722e-01
  5.58894425e-01  5.62586592e-01  5.64014332e-01  5.71489086e-01
  5.74260390e-01  5.75460658e-01  5.81813529e-01  5.86017007e-01
  5.89256556e-01  5.90416671e-01  5.92351583e-01  5.96661416e-01
  6.00496748e-01  6.00985118e-01  6.05166458e-01  6.07611933e-01
  6.16694649e-01  6.17253740e-01  6.21943318e-01  6.23768988e-01
  6.41306358e-01  6.42591224e-01  6.48773452e-01  6.52785978e-01
  6.62326100e-01  6.66962378e-01  6.67467134e-01  6.67761141e-01
  6.69597816e-01  6.72021253e-01  6.72651134e-01  6.75730407e-01
  6.76959766e-01  6.77896179e-01  6.87310734e-01  6.89590077e-01
  6.96215452e-01  7.02056451e-01  7.07958609e-01  7.10814638e-01
  7.13894462e-01  7.16499106e-01  7.18006050e-01  7.19321566e-01
  7.19361377e-01  7.21803562e-01  7.22009200e-01  7.22497880e-01
  7.26306371e-01  7.31972109e-01  7.41019515e-01  7.43579637e-01
  7.44506403e-01  7.46043902e-01  7.47354934e-01  7.52955416e-01
  7.57767742e-01  7.62032453e-01  7.65556509e-01  7.67856264e-01
  7.68225342e-01  7.77136294e-01  7.82542080e-01  7.90878807e-01
  7.97183482e-01  8.01924901e-01  8.08118418e-01  8.15802229e-01
  8.43960700e-01  8.44718725e-01  8.83668949e-01  8.84377977e-01
  8.89268541e-01  8.91153775e-01  9.10560122e-01  9.28116275e-01
  9.61122736e-01]

  warnings.warn(

2022-11-03 10:53:16,883:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-5.57334486e-02 -3.21444891e-02 -3.17932888e-02 -3.10696045e-02
 -2.34577059e-02 -2.21080166e-02 -2.19552316e-02 -2.17879470e-02
 -2.14573577e-02 -2.12354998e-02 -1.92985703e-02 -1.73816260e-02
 -1.71598470e-02 -1.67765895e-02 -1.65281002e-02 -1.59570328e-02
 -1.58318832e-02 -1.48045672e-02 -1.43400433e-02 -1.42527266e-02
 -1.41673658e-02 -1.39080003e-02 -1.34817275e-02 -1.25291588e-02
 -1.24506773e-02 -1.23433924e-02 -1.23422089e-02 -1.09305118e-02
 -1.06977753e-02 -1.01749886e-02 -8.02736351e-03 -7.72324535e-03
 -7.66486880e-03 -7.56844399e-03 -6.37550952e-03 -5.99735744e-03
 -5.95819791e-03 -5.49309836e-03 -4.86907471e-03 -4.30555686e-03
 -3.42455134e-03 -3.18404644e-03 -3.01973171e-03 -2.01914833e-03
 -1.99877874e-03 -1.60939457e-03 -9.69800218e-04 -4.86484306e-04
 -3.32938154e-04 -6.21307417e-05  1.63483820e-03  1.85290655e-03
  2.25102817e-03  2.33719103e-03  2.52601536e-03  2.66213681e-03
  2.81326411e-03  2.87847145e-03  4.77186811e-03  4.90390009e-03
  4.91642962e-03  5.30973489e-03  6.35885231e-03  6.95510735e-03
  7.06390560e-03  7.12540113e-03  7.19533617e-03  7.78501184e-03
  7.81253195e-03  8.10747063e-03  8.56508977e-03  9.58963663e-03
  9.78625156e-03  9.85131931e-03  1.11851106e-02  1.13501202e-02
  1.26081744e-02  1.27571005e-02  1.38720967e-02  1.47919483e-02
  1.70410898e-02  1.85202108e-02  1.85536034e-02  1.89487771e-02
  2.13147945e-02  2.27028292e-02  2.49065307e-02  2.50677506e-02
  2.51415963e-02  2.57745620e-02  2.63262297e-02  2.70652726e-02
  2.72463856e-02  2.81292400e-02  2.88001504e-02  2.89395303e-02
  3.00229739e-02  3.00941798e-02  3.09661953e-02  3.14215304e-02
  3.37011664e-02  3.39476409e-02  3.45965562e-02  3.53945128e-02
  3.60665680e-02  3.63082977e-02  3.66088448e-02  3.77625202e-02
  3.79210869e-02  3.82697158e-02  3.83899356e-02  3.87004874e-02
  3.97059881e-02  3.99852871e-02  4.07123479e-02  4.12048151e-02
  4.22938144e-02  4.30138653e-02  4.32246988e-02  4.37207046e-02
  4.42426454e-02  4.88883293e-02  4.97435928e-02  5.04306601e-02
  5.08077006e-02  5.16495504e-02  5.19580472e-02  5.28852150e-02
  5.30700297e-02  5.47446809e-02  5.57227866e-02  5.69774743e-02
  5.70744403e-02  5.76037640e-02  5.85043649e-02  5.87268943e-02
  5.97249689e-02  6.03925921e-02  6.11264402e-02  6.11908810e-02
  6.15588218e-02  6.17919513e-02  6.18461438e-02  6.22048045e-02
  6.34594181e-02  6.34942128e-02  6.35492885e-02  6.40842593e-02
  6.50395812e-02  6.54467929e-02  6.63774248e-02  6.68085206e-02
  6.80935429e-02  7.00712895e-02  7.07254810e-02  7.12538731e-02
  7.13869661e-02  7.27629175e-02  7.30044067e-02  7.32852588e-02
  7.44645180e-02  7.49901586e-02  7.65666472e-02  7.69278479e-02
  7.75773702e-02  7.75981455e-02  8.00409341e-02  8.31929657e-02
  8.34116247e-02  8.58113077e-02  8.66276786e-02  8.74419377e-02
  8.80395665e-02  8.83718274e-02  8.95205483e-02  9.00945070e-02
  9.15098450e-02  9.22291577e-02  9.29793004e-02  9.34283707e-02
  9.34823413e-02  9.41916369e-02  9.66846783e-02  9.71213758e-02
  9.79641785e-02  9.85480643e-02  1.00163260e-01  1.00979079e-01
  1.01627244e-01  1.03482671e-01  1.03707045e-01  1.04611661e-01
  1.07194169e-01  1.08628899e-01  1.08895532e-01  1.10275502e-01
  1.12237332e-01  1.14990078e-01  1.18730048e-01  1.19457729e-01
  1.21092548e-01  1.21540944e-01  1.21691314e-01  1.22218807e-01
  1.23705110e-01  1.23940911e-01  1.24887983e-01  1.26967728e-01
  1.29161209e-01  1.30938881e-01  1.32006956e-01  1.32046989e-01
  1.36635391e-01  1.37484201e-01  1.40048563e-01  1.42832166e-01
  1.43596221e-01  1.44986186e-01  1.46419762e-01  1.46999664e-01
  1.51258291e-01  1.53126154e-01  1.54396692e-01  1.54801102e-01
  1.55587919e-01  1.56162538e-01  1.56308970e-01  1.57872596e-01
  1.59434095e-01  1.59671206e-01  1.60156847e-01  1.60523739e-01
  1.63072532e-01  1.63452230e-01  1.63772688e-01  1.63837390e-01
  1.65381718e-01  1.66676204e-01  1.67781651e-01  1.69573308e-01
  1.70715282e-01  1.75060684e-01  1.75135695e-01  1.75929246e-01
  1.76943906e-01  1.77999468e-01  1.79013205e-01  1.79083899e-01
  1.79115991e-01  1.82611394e-01  1.87850468e-01  1.90856554e-01
  1.92852434e-01  1.93070256e-01  1.93264563e-01  1.95623017e-01
  1.96647417e-01  1.97425436e-01  1.98248494e-01  2.00446066e-01
  2.02533215e-01  2.03280673e-01  2.03441426e-01  2.03770208e-01
  2.04417858e-01  2.08575877e-01  2.09424806e-01  2.11206386e-01
  2.11859197e-01  2.12518596e-01  2.13042854e-01  2.14508137e-01
  2.17109154e-01  2.19307343e-01  2.19688886e-01  2.21239771e-01
  2.22838408e-01  2.23897727e-01  2.26500355e-01  2.26579192e-01
  2.28541233e-01  2.28776606e-01  2.29477837e-01  2.29691851e-01
  2.32206980e-01  2.32951506e-01  2.35412749e-01  2.37774896e-01
  2.43162437e-01  2.45768359e-01  2.45978492e-01  2.48217161e-01
  2.49101504e-01  2.51463785e-01  2.51912314e-01  2.54557638e-01
  2.54703888e-01  2.60129955e-01  2.66407555e-01  2.66908340e-01
  2.67317666e-01  2.69340122e-01  2.72884907e-01  2.72911162e-01
  2.74252712e-01  2.76243577e-01  2.79794030e-01  2.81069669e-01
  2.81379231e-01  2.83300799e-01  2.83447584e-01  2.83448430e-01
  2.83860730e-01  2.92306523e-01  2.92818099e-01  2.94994937e-01
  2.95999624e-01  2.97976803e-01  2.98235265e-01  3.01027118e-01
  3.02880715e-01  3.04627273e-01  3.05865496e-01  3.06624808e-01
  3.08697218e-01  3.11899923e-01  3.12561508e-01  3.14465849e-01
  3.14763531e-01  3.18598430e-01  3.21836792e-01  3.23354902e-01
  3.24631417e-01  3.25937150e-01  3.25946300e-01  3.26450447e-01
  3.28729289e-01  3.29823687e-01  3.30138138e-01  3.32538786e-01
  3.33622347e-01  3.34044928e-01  3.34859608e-01  3.35665063e-01
  3.48203272e-01  3.49171516e-01  3.49764452e-01  3.56587057e-01
  3.62314269e-01  3.62691099e-01  3.63502989e-01  3.64076227e-01
  3.64816267e-01  3.65006334e-01  3.68871457e-01  3.69997908e-01
  3.72043566e-01  3.75089014e-01  3.77196870e-01  3.78887364e-01
  3.79145841e-01  3.80199327e-01  3.83945322e-01  3.86267479e-01
  3.86787327e-01  3.87406529e-01  3.88637790e-01  3.89214887e-01
  3.91481767e-01  4.00065568e-01  4.01247690e-01  4.02729465e-01
  4.05472198e-01  4.08760656e-01  4.11132734e-01  4.15036579e-01
  4.17643837e-01  4.18734082e-01  4.23613011e-01  4.25934556e-01
  4.28942644e-01  4.31989051e-01  4.34833202e-01  4.38617783e-01
  4.43747557e-01  4.44019231e-01  4.48976407e-01  4.50437144e-01
  4.57346706e-01  4.57558351e-01  4.62986004e-01  4.63588119e-01
  4.64519433e-01  4.70508808e-01  4.71174579e-01  4.73093360e-01
  4.73307051e-01  4.75379075e-01  4.81191549e-01  4.82143239e-01
  4.83235725e-01  4.85810951e-01  4.87021459e-01  4.89629564e-01
  4.94036771e-01  4.98086320e-01  4.98348092e-01  4.99408181e-01
  5.00967081e-01  5.02950803e-01  5.05657756e-01  5.06720160e-01
  5.07363829e-01  5.11986679e-01  5.17519527e-01  5.19027313e-01
  5.23024315e-01  5.23562997e-01  5.23685743e-01  5.26354360e-01
  5.29655739e-01  5.70364476e-01  5.72751885e-01  5.73047342e-01
  5.81068668e-01  5.83394988e-01  5.86305460e-01  6.02285671e-01
  6.03538551e-01  6.04591317e-01  6.08896771e-01  6.13090097e-01
  6.13482869e-01  6.17311675e-01  6.19927466e-01  6.20424184e-01
  6.24014641e-01  6.24112232e-01  6.30221097e-01  6.31445328e-01
  6.32225144e-01  6.33215820e-01  6.38439225e-01  6.41559690e-01
  6.41682525e-01  6.44665203e-01  6.47118410e-01  6.49185286e-01
  6.53122825e-01  6.53868112e-01  6.59087406e-01  6.60682845e-01
  6.72891762e-01  6.73598638e-01  6.76455056e-01  6.82082787e-01
  6.83573508e-01  6.84143752e-01  6.90649865e-01  6.94252848e-01
  6.99065579e-01  7.02355026e-01  7.11878958e-01  7.13727595e-01
  7.14964875e-01  7.20586000e-01  7.21612181e-01  7.29500729e-01
  7.34311497e-01  7.34371806e-01  7.35220001e-01  7.35385470e-01
  7.41224006e-01  7.70195208e-01  7.71122862e-01  7.72895654e-01
  7.78051727e-01  7.82520171e-01  7.89157578e-01  8.14551750e-01
  8.15502556e-01  8.21850434e-01  8.31091711e-01  8.39609394e-01
  8.56562393e-01  9.10532442e-01  9.32244723e-01  9.41499689e-01
  9.48849093e-01  9.53867180e-01  9.58697470e-01  9.68744746e-01]

  warnings.warn(

2022-11-03 10:53:17,098:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-5.69298941e-02 -3.25590803e-02 -3.14484359e-02 -3.06759580e-02
 -2.59880021e-02 -2.54878277e-02 -2.54710180e-02 -2.53757248e-02
 -2.53268276e-02 -2.48616956e-02 -2.42473071e-02 -2.38366276e-02
 -2.31937818e-02 -2.22993094e-02 -2.21107258e-02 -2.07012655e-02
 -2.05226075e-02 -2.03788964e-02 -1.89507655e-02 -1.81803974e-02
 -1.77507696e-02 -1.69437559e-02 -1.65387479e-02 -1.60352212e-02
 -1.59035405e-02 -1.56217348e-02 -1.54185824e-02 -1.53909216e-02
 -1.45965333e-02 -1.42864923e-02 -1.41950173e-02 -1.40620615e-02
 -1.38988664e-02 -1.29554709e-02 -1.19684635e-02 -1.13452770e-02
 -1.10356002e-02 -1.05963218e-02 -1.02177602e-02 -9.78207543e-03
 -9.66917600e-03 -9.62256274e-03 -9.50811830e-03 -9.49109596e-03
 -9.37016050e-03 -8.72292363e-03 -8.47310203e-03 -7.34303151e-03
 -6.56764938e-03 -6.48888978e-03 -6.34530379e-03 -6.15342422e-03
 -5.94596179e-03 -5.26758471e-03 -5.14275612e-03 -4.85464566e-03
 -4.50026953e-03 -4.20804496e-03 -3.69750347e-03 -3.26777558e-03
 -2.46461919e-03 -2.39674439e-03 -6.48047905e-05  1.38795386e-04
  2.48842933e-04  8.55843531e-04  1.36220789e-03  1.41431191e-03
  1.61625843e-03  1.87010503e-03  2.41518118e-03  3.77851986e-03
  3.85393076e-03  4.76592802e-03  5.23435232e-03  5.48940513e-03
  6.89019620e-03  7.16792177e-03  7.50766230e-03  7.90545993e-03
  9.54417867e-03  1.05277658e-02  1.08392464e-02  1.14962731e-02
  1.15351441e-02  1.20270620e-02  1.23935469e-02  1.26585266e-02
  1.31107967e-02  1.34683940e-02  1.40578083e-02  1.41023794e-02
  1.42892247e-02  1.52232237e-02  1.54712440e-02  1.55688354e-02
  1.60324040e-02  1.61567664e-02  1.73796392e-02  1.78458177e-02
  2.03470098e-02  2.14457656e-02  2.14775390e-02  2.14979519e-02
  2.15230918e-02  2.18817288e-02  2.21654303e-02  2.22023943e-02
  2.27451202e-02  2.40855132e-02  2.45612803e-02  2.50216020e-02
  2.58410765e-02  2.61764102e-02  2.67135564e-02  2.73448112e-02
  2.82748259e-02  2.88613227e-02  2.89070776e-02  3.05688067e-02
  3.21653885e-02  3.27181252e-02  3.37492974e-02  3.49222380e-02
  3.57932674e-02  3.77778388e-02  3.88514476e-02  3.90589581e-02
  3.91850314e-02  4.07106797e-02  4.18852954e-02  4.50070034e-02
  4.63542469e-02  4.80289021e-02  4.82114180e-02  4.95649604e-02
  5.03586406e-02  5.11342631e-02  5.40619745e-02  5.40853688e-02
  5.45928929e-02  5.53227607e-02  6.18332261e-02  6.32008576e-02
  6.41507457e-02  6.66116447e-02  6.80201293e-02  6.83192013e-02
  6.88436541e-02  7.09290642e-02  7.12756528e-02  7.14750238e-02
  7.20078413e-02  7.20954202e-02  7.36040072e-02  7.39653388e-02
  7.40507699e-02  7.46311460e-02  7.55751265e-02  7.59136003e-02
  7.73381256e-02  7.77420690e-02  7.81906186e-02  8.17174740e-02
  8.30744689e-02  8.32276584e-02  8.39105682e-02  8.42006437e-02
  8.70450089e-02  8.70746293e-02  8.76717342e-02  8.78618482e-02
  9.35795865e-02  9.45038137e-02  9.52391403e-02  9.53590527e-02
  9.61968717e-02  9.64850478e-02  9.71463771e-02  9.71601444e-02
  9.86464480e-02  1.03785165e-01  1.04165755e-01  1.05808450e-01
  1.06434559e-01  1.10099981e-01  1.10551288e-01  1.10739652e-01
  1.12822700e-01  1.15833690e-01  1.20032869e-01  1.20879124e-01
  1.21790174e-01  1.22344420e-01  1.22431721e-01  1.22706267e-01
  1.22985511e-01  1.23366136e-01  1.23504218e-01  1.24468981e-01
  1.25714505e-01  1.25988589e-01  1.29353454e-01  1.30529187e-01
  1.31108972e-01  1.31612479e-01  1.31959295e-01  1.33495691e-01
  1.34076487e-01  1.34635371e-01  1.37553205e-01  1.39245163e-01
  1.39752543e-01  1.40188648e-01  1.41242064e-01  1.42948949e-01
  1.44386725e-01  1.44862282e-01  1.46814789e-01  1.48283230e-01
  1.51087480e-01  1.51216672e-01  1.51312453e-01  1.51477013e-01
  1.51546752e-01  1.52909922e-01  1.54520554e-01  1.54728394e-01
  1.55083884e-01  1.58619382e-01  1.61642141e-01  1.62069340e-01
  1.62556953e-01  1.63228905e-01  1.63999400e-01  1.65167389e-01
  1.66067002e-01  1.68233349e-01  1.68237524e-01  1.69754149e-01
  1.70534146e-01  1.71031016e-01  1.71924253e-01  1.72013773e-01
  1.73087078e-01  1.81731149e-01  1.83414324e-01  1.83773264e-01
  1.84128317e-01  1.84377794e-01  1.84918454e-01  1.86559627e-01
  1.86943664e-01  1.90590176e-01  1.90725690e-01  1.92874969e-01
  1.93348930e-01  1.94562112e-01  1.96389751e-01  1.97636965e-01
  1.98172933e-01  1.98848931e-01  2.00951727e-01  2.01804328e-01
  2.07061985e-01  2.07158664e-01  2.07908574e-01  2.11742111e-01
  2.14463048e-01  2.16097840e-01  2.16326992e-01  2.17567100e-01
  2.18254971e-01  2.19656048e-01  2.19663008e-01  2.19927411e-01
  2.26454062e-01  2.31328853e-01  2.33432488e-01  2.35484551e-01
  2.36194243e-01  2.36564371e-01  2.37401146e-01  2.40702056e-01
  2.41080592e-01  2.47117707e-01  2.47180413e-01  2.47762777e-01
  2.48978916e-01  2.49859686e-01  2.52565919e-01  2.52767308e-01
  2.55349543e-01  2.56008317e-01  2.60212065e-01  2.62032964e-01
  2.62894123e-01  2.64347877e-01  2.65072350e-01  2.66895809e-01
  2.67459189e-01  2.78256280e-01  2.78671071e-01  2.79955772e-01
  2.81485569e-01  2.82148262e-01  2.85897762e-01  2.86152413e-01
  2.86328510e-01  2.95962099e-01  2.98691638e-01  3.00908964e-01
  3.03383960e-01  3.04806569e-01  3.11381545e-01  3.12876864e-01
  3.14798570e-01  3.19572419e-01  3.20981955e-01  3.21905693e-01
  3.24061836e-01  3.24665460e-01  3.26371326e-01  3.28692021e-01
  3.30636014e-01  3.37692388e-01  3.37859174e-01  3.41267561e-01
  3.43324626e-01  3.48441724e-01  3.49659542e-01  3.50131025e-01
  3.51661964e-01  3.52936317e-01  3.56111912e-01  3.56426144e-01
  3.58560705e-01  3.59742576e-01  3.62424812e-01  3.64695327e-01
  3.65441495e-01  3.67270561e-01  3.69348831e-01  3.72147794e-01
  3.77470893e-01  3.78053006e-01  3.82384515e-01  3.86086138e-01
  3.86453456e-01  3.87324003e-01  3.88103328e-01  3.88376639e-01
  3.89533199e-01  3.91523751e-01  3.92774457e-01  3.93099448e-01
  3.93672083e-01  4.02814763e-01  4.04025492e-01  4.04292267e-01
  4.04658761e-01  4.06107503e-01  4.06640380e-01  4.07106362e-01
  4.11179146e-01  4.14469447e-01  4.16945417e-01  4.18334159e-01
  4.21579044e-01  4.25048796e-01  4.25652663e-01  4.26126940e-01
  4.33541275e-01  4.34839424e-01  4.35478673e-01  4.35892540e-01
  4.44124816e-01  4.50792938e-01  4.56070458e-01  4.68046813e-01
  4.69007831e-01  4.71398973e-01  4.73855659e-01  4.77440132e-01
  4.82178716e-01  4.82368178e-01  4.84455164e-01  4.86882008e-01
  4.87138651e-01  4.92335516e-01  4.94998027e-01  4.95075727e-01
  4.96746543e-01  5.00965213e-01  5.01857956e-01  5.02116049e-01
  5.03129281e-01  5.03345015e-01  5.07035263e-01  5.08211971e-01
  5.11977128e-01  5.13172941e-01  5.13873061e-01  5.16092748e-01
  5.16617184e-01  5.21190062e-01  5.21640592e-01  5.22433016e-01
  5.25245076e-01  5.26715324e-01  5.30868033e-01  5.31683467e-01
  5.31731989e-01  5.36658848e-01  5.37381046e-01  5.41621168e-01
  5.43234973e-01  5.46926364e-01  5.51717239e-01  5.55776554e-01
  5.60309055e-01  5.60660599e-01  5.61561152e-01  5.63166136e-01
  5.69367308e-01  5.84384812e-01  5.94935845e-01  5.95820407e-01
  5.96254932e-01  5.96325846e-01  5.97039853e-01  5.97545891e-01
  6.00983802e-01  6.01299399e-01  6.05435817e-01  6.05925643e-01
  6.23705163e-01  6.24523417e-01  6.25180795e-01  6.28142026e-01
  6.32173704e-01  6.33843299e-01  6.36796994e-01  6.40398937e-01
  6.42702472e-01  6.47106483e-01  6.52764875e-01  6.58572288e-01
  6.62315003e-01  6.62788143e-01  6.63382996e-01  6.67068942e-01
  6.68980470e-01  6.73157933e-01  6.78826365e-01  6.88300993e-01
  6.88839843e-01  6.89048260e-01  7.02031663e-01  7.05098471e-01
  7.05634650e-01  7.23820455e-01  7.42842484e-01  7.48341746e-01
  7.49405133e-01  7.52687478e-01  7.54033681e-01  7.57042292e-01
  7.58611374e-01  7.67018438e-01  7.75132195e-01  7.75472055e-01
  7.93081822e-01  8.00905571e-01  8.04770995e-01  8.07617707e-01
  8.09482060e-01  8.13782029e-01  8.16283236e-01  8.16563343e-01
  8.17891570e-01  8.18135284e-01  8.46182779e-01  8.58194246e-01
  8.64390034e-01  8.85423265e-01  8.85675112e-01  9.10175853e-01]

  warnings.warn(

2022-11-03 10:53:17,147:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.71262366e-02 -3.59516681e-02 -3.27812809e-02 -3.04178737e-02
 -2.89986866e-02 -2.85625335e-02 -2.64010041e-02 -2.63397921e-02
 -2.60381974e-02 -2.32535281e-02 -2.30500745e-02 -2.17788557e-02
 -2.15153916e-02 -1.89696079e-02 -1.88657278e-02 -1.81635773e-02
 -1.73862614e-02 -1.66423268e-02 -1.63468007e-02 -1.59923482e-02
 -1.54651353e-02 -1.52704106e-02 -1.45067031e-02 -1.43298120e-02
 -1.37554807e-02 -1.36930611e-02 -1.14098934e-02 -1.11531855e-02
 -1.09081950e-02 -9.98312142e-03 -9.97345806e-03 -8.95470163e-03
 -8.79318753e-03 -7.84319881e-03 -7.73347966e-03 -7.27167046e-03
 -6.89572553e-03 -6.52433758e-03 -6.22997899e-03 -6.13334206e-03
 -5.82091903e-03 -5.80252224e-03 -4.94271539e-03 -4.64861857e-03
 -4.18295136e-03 -4.02781423e-03 -3.70585318e-03 -2.79719937e-03
 -2.27039074e-03 -1.73042933e-03 -1.63842358e-03 -1.55257019e-03
 -1.12996152e-03 -9.16405114e-04 -7.21542077e-04 -6.53910199e-04
 -2.79136962e-04 -7.80245212e-05  9.26245149e-05  2.81305929e-04
  2.85929183e-04  7.91940464e-04  9.48892787e-04  1.51546880e-03
  1.68891598e-03  2.24438881e-03  2.49835398e-03  2.68232501e-03
  2.69652801e-03  2.78404573e-03  2.95352118e-03  2.98990211e-03
  3.85539099e-03  4.01879230e-03  4.32364086e-03  4.88642204e-03
  5.21096157e-03  5.61080727e-03  5.83145003e-03  5.92224001e-03
  7.40699435e-03  7.40995964e-03  7.81655329e-03  7.92623507e-03
  8.26290549e-03  8.33020347e-03  8.62457322e-03  9.42577601e-03
  1.04253695e-02  1.05807535e-02  1.21768268e-02  1.40099197e-02
  1.50089343e-02  1.59605223e-02  1.70119431e-02  1.74333355e-02
  1.80099335e-02  1.81544702e-02  1.95105811e-02  1.97035443e-02
  2.00324799e-02  2.13446243e-02  2.16135170e-02  2.17204907e-02
  2.18861493e-02  2.23083917e-02  2.26114525e-02  2.29736921e-02
  2.32945557e-02  2.39570668e-02  2.40820319e-02  2.43938720e-02
  2.45395724e-02  2.51856791e-02  2.64821034e-02  2.94613539e-02
  2.98389218e-02  3.06808292e-02  3.17283114e-02  3.26085432e-02
  3.29437639e-02  3.43825325e-02  3.47857762e-02  3.49310336e-02
  3.54558880e-02  3.56894449e-02  3.62295070e-02  3.74324822e-02
  3.76774027e-02  3.92578299e-02  4.00698817e-02  4.01943822e-02
  4.12072651e-02  4.37110621e-02  4.54561703e-02  4.73411962e-02
  4.92252122e-02  5.06587382e-02  5.16552525e-02  5.32968497e-02
  5.34016372e-02  5.34694415e-02  5.52306492e-02  5.60245962e-02
  5.86855896e-02  5.87963552e-02  6.10822073e-02  6.27952605e-02
  6.31720276e-02  6.51004238e-02  6.64032123e-02  6.64799070e-02
  6.85326631e-02  7.10037876e-02  7.29219383e-02  7.73087657e-02
  7.78238879e-02  8.25985159e-02  8.46802189e-02  8.53539959e-02
  8.59835424e-02  8.90794582e-02  9.06911337e-02  9.10185289e-02
  9.18193455e-02  9.26763228e-02  9.32234062e-02  9.35768688e-02
  9.38530369e-02  9.63945970e-02  9.67824157e-02  9.81439993e-02
  9.91269961e-02  1.00743809e-01  1.01470200e-01  1.01707933e-01
  1.02228062e-01  1.04462167e-01  1.04523793e-01  1.05869558e-01
  1.09411201e-01  1.09622053e-01  1.16057602e-01  1.17053947e-01
  1.20458986e-01  1.21265966e-01  1.23371225e-01  1.25725170e-01
  1.27651686e-01  1.28199361e-01  1.31940328e-01  1.33415290e-01
  1.34057288e-01  1.36112437e-01  1.36506808e-01  1.37062041e-01
  1.37224361e-01  1.37353212e-01  1.39233061e-01  1.40717598e-01
  1.40880290e-01  1.40951719e-01  1.42486043e-01  1.42699500e-01
  1.44922721e-01  1.48728104e-01  1.50511639e-01  1.52278614e-01
  1.52638096e-01  1.52650759e-01  1.53029521e-01  1.53218982e-01
  1.53417963e-01  1.53764530e-01  1.58210634e-01  1.58987368e-01
  1.59729522e-01  1.59859915e-01  1.59923764e-01  1.64805758e-01
  1.68222766e-01  1.68557304e-01  1.69813948e-01  1.71402502e-01
  1.73094471e-01  1.73560000e-01  1.76559885e-01  1.79878018e-01
  1.81396155e-01  1.86250163e-01  1.89599530e-01  1.89959366e-01
  1.90063704e-01  1.91178980e-01  1.91539996e-01  1.91620216e-01
  1.93011713e-01  1.94625805e-01  1.97076602e-01  1.98466045e-01
  2.00875287e-01  2.00970421e-01  2.01241308e-01  2.03691231e-01
  2.07152598e-01  2.07883152e-01  2.08345271e-01  2.10714903e-01
  2.12203480e-01  2.15565178e-01  2.15837555e-01  2.22635856e-01
  2.27880809e-01  2.27967933e-01  2.29782165e-01  2.30749164e-01
  2.31040306e-01  2.33452450e-01  2.34695376e-01  2.36288126e-01
  2.37004661e-01  2.37432666e-01  2.38657426e-01  2.40420636e-01
  2.41506013e-01  2.42035826e-01  2.42563045e-01  2.42754467e-01
  2.48441099e-01  2.48982009e-01  2.49672212e-01  2.51039577e-01
  2.51242045e-01  2.51618296e-01  2.52458271e-01  2.52942353e-01
  2.53806164e-01  2.54002139e-01  2.54329091e-01  2.56183217e-01
  2.56220561e-01  2.57941676e-01  2.68261131e-01  2.71384254e-01
  2.71422785e-01  2.73355593e-01  2.74696611e-01  2.75443605e-01
  2.76958794e-01  2.78326962e-01  2.79810009e-01  2.85501393e-01
  2.85508841e-01  2.86427341e-01  2.87384661e-01  2.87937339e-01
  2.90756387e-01  2.92842772e-01  2.93079618e-01  2.93151246e-01
  2.94501540e-01  2.94932991e-01  2.98620910e-01  2.99975864e-01
  3.00637403e-01  3.07024772e-01  3.07303395e-01  3.09820216e-01
  3.10026618e-01  3.10289686e-01  3.11420355e-01  3.11745197e-01
  3.16820796e-01  3.17762067e-01  3.17974047e-01  3.19466898e-01
  3.22178079e-01  3.23957265e-01  3.26864821e-01  3.28184022e-01
  3.28443232e-01  3.31130436e-01  3.31900598e-01  3.36171574e-01
  3.38363586e-01  3.42666508e-01  3.43141817e-01  3.44069103e-01
  3.46315645e-01  3.49075956e-01  3.49938564e-01  3.57122947e-01
  3.57705747e-01  3.60997476e-01  3.63646285e-01  3.64041563e-01
  3.65696201e-01  3.70467741e-01  3.72620787e-01  3.73331204e-01
  3.74824479e-01  3.75934544e-01  3.79370653e-01  3.81481887e-01
  3.82645249e-01  3.83637516e-01  3.87610386e-01  3.91305902e-01
  3.92230037e-01  3.96569960e-01  4.02158187e-01  4.03320797e-01
  4.04136588e-01  4.06425898e-01  4.09255520e-01  4.15151329e-01
  4.17505187e-01  4.18097473e-01  4.19530631e-01  4.21670263e-01
  4.23567024e-01  4.23636624e-01  4.24248790e-01  4.26719705e-01
  4.31090168e-01  4.33264421e-01  4.40082101e-01  4.44163544e-01
  4.45941003e-01  4.48317214e-01  4.48594209e-01  4.50627922e-01
  4.55995618e-01  4.59070664e-01  4.64112918e-01  4.72366859e-01
  4.75444476e-01  4.76585392e-01  4.76830802e-01  4.76954562e-01
  4.78912227e-01  4.90324460e-01  4.91058174e-01  4.92827091e-01
  4.94567605e-01  4.96653979e-01  4.96894278e-01  4.99673579e-01
  5.05695002e-01  5.06429828e-01  5.10568243e-01  5.14098620e-01
  5.14445783e-01  5.14589814e-01  5.15069842e-01  5.15279718e-01
  5.19137975e-01  5.21894200e-01  5.23642107e-01  5.24201823e-01
  5.25569622e-01  5.26278423e-01  5.29791321e-01  5.31313648e-01
  5.34066201e-01  5.34896651e-01  5.37801259e-01  5.39401135e-01
  5.43475963e-01  5.46062164e-01  5.46911599e-01  5.47144735e-01
  5.48051829e-01  5.50384679e-01  5.52231232e-01  5.54980565e-01
  5.55468821e-01  5.55925678e-01  5.69860065e-01  5.74132196e-01
  5.74973825e-01  5.78935220e-01  5.85504465e-01  5.88083766e-01
  5.88287785e-01  5.89340362e-01  5.89807213e-01  5.92195582e-01
  5.92297846e-01  5.95354399e-01  5.96866198e-01  5.98252412e-01
  6.07079942e-01  6.11231635e-01  6.14642335e-01  6.15456802e-01
  6.21923376e-01  6.25358342e-01  6.26894867e-01  6.30000514e-01
  6.36133488e-01  6.42672420e-01  6.46309658e-01  6.47948175e-01
  6.51875524e-01  6.55545391e-01  6.57547454e-01  6.61977128e-01
  6.67476475e-01  6.70461914e-01  6.71595551e-01  6.71678652e-01
  6.74459590e-01  6.75939865e-01  6.99565892e-01  7.04416031e-01
  7.08369014e-01  7.09788639e-01  7.12108647e-01  7.14199641e-01
  7.21522831e-01  7.27094049e-01  7.49866659e-01  7.52499049e-01
  7.52631407e-01  7.53248476e-01  7.59339912e-01  7.69351884e-01
  7.86347979e-01  7.98103251e-01  8.00749016e-01  8.15385270e-01
  8.16810914e-01  8.23222913e-01  8.25265635e-01  8.28494035e-01
  8.39966634e-01  8.40636702e-01  8.45375356e-01  8.49388214e-01
  8.50842536e-01  8.54690975e-01  8.55743230e-01  8.72786186e-01
  8.74065920e-01  8.83495700e-01  8.99223056e-01  9.15499009e-01
  9.43965927e-01  9.84020142e-01]

  warnings.warn(

2022-11-03 10:53:17,147:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.47294645e-02 -3.58878569e-02 -3.50408104e-02 -3.48233320e-02
 -3.42484349e-02 -3.22777280e-02 -3.08318540e-02 -3.03121698e-02
 -2.88210798e-02 -2.45734726e-02 -2.43794929e-02 -2.41583224e-02
 -2.25372000e-02 -2.24558871e-02 -2.16777017e-02 -2.08343171e-02
 -1.87631715e-02 -1.80626119e-02 -1.80311935e-02 -1.78047175e-02
 -1.73109089e-02 -1.70340698e-02 -1.66296468e-02 -1.64792139e-02
 -1.58186206e-02 -1.55105238e-02 -1.43807146e-02 -1.33707411e-02
 -1.33057012e-02 -1.11357375e-02 -1.07718542e-02 -1.05405691e-02
 -1.05241236e-02 -9.76223439e-03 -9.53616860e-03 -7.82559169e-03
 -6.49330473e-03 -6.10783495e-03 -5.66466116e-03 -5.32067944e-03
 -4.69618739e-03 -4.57115964e-03 -4.06081150e-03 -3.82809394e-03
 -3.38860853e-03 -3.35223963e-03 -3.15764357e-03 -2.99494800e-03
 -2.99454108e-03 -2.07535618e-03 -1.99895730e-03 -1.76173064e-03
 -1.62333848e-03 -1.49070203e-03 -1.24739811e-03 -1.00346889e-03
 -5.43176969e-04 -5.22498111e-04 -1.27142042e-04  1.00202624e-04
  5.83760588e-04  8.39229540e-04  1.20230407e-03  1.61002459e-03
  2.52549743e-03  2.54685122e-03  2.95238561e-03  3.22771239e-03
  3.47367006e-03  4.06856165e-03  4.35401142e-03  5.58337255e-03
  6.21008502e-03  7.07766980e-03  7.31379094e-03  7.89217648e-03
  7.89997817e-03  7.91211431e-03  7.98392614e-03  8.06845250e-03
  8.78104867e-03  1.00604944e-02  1.01488053e-02  1.08576717e-02
  1.19151160e-02  1.20290907e-02  1.25747125e-02  1.31931468e-02
  1.32938218e-02  1.37389266e-02  1.42023525e-02  1.49345743e-02
  1.55959751e-02  1.56177464e-02  1.57559673e-02  1.60873516e-02
  1.64706776e-02  1.67132709e-02  1.70627211e-02  1.80423577e-02
  1.86360790e-02  1.94537849e-02  1.98399340e-02  2.03803609e-02
  2.10210741e-02  2.16792609e-02  2.36646673e-02  2.43750914e-02
  2.44779315e-02  2.49834469e-02  2.50192915e-02  2.55070963e-02
  2.64369200e-02  2.65050104e-02  2.66410549e-02  2.68734043e-02
  2.85440195e-02  2.94352108e-02  2.94957742e-02  3.36084699e-02
  3.57955371e-02  3.64976332e-02  3.66150166e-02  3.97766486e-02
  3.98972101e-02  3.99928054e-02  4.01731775e-02  4.15470526e-02
  4.28085427e-02  4.66233409e-02  4.99322587e-02  5.01063601e-02
  5.17531957e-02  5.21682365e-02  5.23748941e-02  5.25716564e-02
  5.33527258e-02  5.41074075e-02  5.56472667e-02  5.75668037e-02
  5.85328194e-02  6.04263978e-02  6.14658754e-02  6.20378655e-02
  6.47410881e-02  6.49834122e-02  6.71808039e-02  6.72755313e-02
  6.73161819e-02  6.92445119e-02  7.12615019e-02  7.15555163e-02
  7.28274585e-02  7.40341827e-02  7.75793585e-02  7.90467125e-02
  7.90709981e-02  7.91523352e-02  8.07667437e-02  8.09314639e-02
  8.11226340e-02  8.19202985e-02  8.24809369e-02  8.45965591e-02
  8.77317607e-02  8.83199780e-02  8.86086582e-02  8.94467430e-02
  9.04304018e-02  9.10982485e-02  9.55736517e-02  9.56090505e-02
  9.63407005e-02  9.64101283e-02  9.82638064e-02  9.88479675e-02
  9.91776404e-02  1.00487507e-01  1.00521000e-01  1.00657736e-01
  1.00994344e-01  1.02309278e-01  1.02431433e-01  1.04244883e-01
  1.05075789e-01  1.06881992e-01  1.07994650e-01  1.13812564e-01
  1.14298539e-01  1.14495894e-01  1.19669483e-01  1.20304207e-01
  1.21282724e-01  1.23972947e-01  1.25361049e-01  1.28523354e-01
  1.29687438e-01  1.29863269e-01  1.31536107e-01  1.37750914e-01
  1.37783108e-01  1.38497907e-01  1.38795226e-01  1.41347247e-01
  1.42065619e-01  1.43023822e-01  1.43154510e-01  1.43268770e-01
  1.44662143e-01  1.46463591e-01  1.48917626e-01  1.49920020e-01
  1.49969901e-01  1.51804702e-01  1.54636235e-01  1.56083821e-01
  1.59334427e-01  1.60162743e-01  1.60803533e-01  1.64726806e-01
  1.65163191e-01  1.65215379e-01  1.67139077e-01  1.68807790e-01
  1.69069366e-01  1.70810791e-01  1.70885677e-01  1.71071690e-01
  1.71367985e-01  1.74767530e-01  1.78230799e-01  1.78639161e-01
  1.78718625e-01  1.79922572e-01  1.80214179e-01  1.80635085e-01
  1.81055665e-01  1.82929275e-01  1.83431621e-01  1.87523690e-01
  1.92676428e-01  1.92913996e-01  1.93097345e-01  1.98348872e-01
  2.00901213e-01  2.01147177e-01  2.06221957e-01  2.06645666e-01
  2.07039551e-01  2.09901606e-01  2.10555672e-01  2.15003909e-01
  2.15581647e-01  2.17004993e-01  2.18206553e-01  2.18283509e-01
  2.23974351e-01  2.24855083e-01  2.26415125e-01  2.27637212e-01
  2.28418834e-01  2.29219926e-01  2.30063259e-01  2.30165166e-01
  2.32758079e-01  2.33763899e-01  2.33958763e-01  2.34429532e-01
  2.34906107e-01  2.37778943e-01  2.38799168e-01  2.39121024e-01
  2.42686539e-01  2.43143932e-01  2.45178158e-01  2.46634101e-01
  2.47300604e-01  2.47561926e-01  2.48060751e-01  2.48097299e-01
  2.49339850e-01  2.49444918e-01  2.51301082e-01  2.51778279e-01
  2.52841656e-01  2.53055679e-01  2.53922441e-01  2.54087891e-01
  2.55645159e-01  2.55673587e-01  2.55786436e-01  2.56987232e-01
  2.58858268e-01  2.59080576e-01  2.61172852e-01  2.61477007e-01
  2.64349383e-01  2.66689691e-01  2.68802961e-01  2.70934490e-01
  2.71033737e-01  2.71929051e-01  2.76539595e-01  2.77770355e-01
  2.78391861e-01  2.79626044e-01  2.80064563e-01  2.80873674e-01
  2.81415167e-01  2.82229259e-01  2.83619343e-01  2.84254240e-01
  2.85204598e-01  2.86435552e-01  2.92297622e-01  2.94401586e-01
  2.97160911e-01  2.99199349e-01  3.01097424e-01  3.01628189e-01
  3.12936527e-01  3.14614572e-01  3.19518121e-01  3.26755499e-01
  3.29250239e-01  3.36340887e-01  3.37614136e-01  3.37640675e-01
  3.38208129e-01  3.38663536e-01  3.41153838e-01  3.43357707e-01
  3.45823458e-01  3.47012390e-01  3.47466741e-01  3.48655306e-01
  3.52653412e-01  3.55553419e-01  3.57467270e-01  3.58063861e-01
  3.59186203e-01  3.63269450e-01  3.73085739e-01  3.76686568e-01
  3.78385314e-01  3.83575972e-01  3.83883832e-01  3.85306494e-01
  3.87097219e-01  3.89014673e-01  3.89754811e-01  3.91573387e-01
  3.91842242e-01  3.94889276e-01  3.96655724e-01  4.00225186e-01
  4.00329997e-01  4.00560762e-01  4.03654635e-01  4.05544196e-01
  4.05822805e-01  4.05844439e-01  4.06041901e-01  4.12637958e-01
  4.12835330e-01  4.15114942e-01  4.15836092e-01  4.17626218e-01
  4.25572643e-01  4.32878264e-01  4.35243484e-01  4.37453043e-01
  4.38464078e-01  4.39734269e-01  4.40400871e-01  4.41834573e-01
  4.42072697e-01  4.44642144e-01  4.48217286e-01  4.52438524e-01
  4.55087288e-01  4.58933532e-01  4.64949088e-01  4.65567985e-01
  4.65969692e-01  4.68003427e-01  4.68821913e-01  4.70525863e-01
  4.74723319e-01  4.84894957e-01  4.87131858e-01  4.88656463e-01
  4.97129435e-01  4.98470937e-01  4.99382366e-01  5.00612450e-01
  5.09421265e-01  5.10448019e-01  5.12112462e-01  5.18083634e-01
  5.18764910e-01  5.24287593e-01  5.26008729e-01  5.26278694e-01
  5.28991818e-01  5.33273033e-01  5.34118188e-01  5.34789551e-01
  5.35554699e-01  5.36151586e-01  5.38213357e-01  5.42302352e-01
  5.50007270e-01  5.52347911e-01  5.62225993e-01  5.63088200e-01
  5.65004913e-01  5.67570737e-01  5.68986841e-01  5.81532602e-01
  5.87284368e-01  5.96881649e-01  5.98419161e-01  6.06240968e-01
  6.09341529e-01  6.10453602e-01  6.10653927e-01  6.14916148e-01
  6.22945444e-01  6.23200920e-01  6.35421590e-01  6.39430753e-01
  6.42915628e-01  6.44148159e-01  6.46600412e-01  6.51002115e-01
  6.52175771e-01  6.58213595e-01  6.66657595e-01  6.69842358e-01
  6.70879559e-01  6.80546391e-01  6.81713730e-01  6.82442130e-01
  6.84316541e-01  6.93011901e-01  7.00228869e-01  7.02258506e-01
  7.02395827e-01  7.11386581e-01  7.19448862e-01  7.21635754e-01
  7.23909565e-01  7.27869867e-01  7.32018245e-01  7.44822328e-01
  7.46909054e-01  7.48142638e-01  7.49453336e-01  7.52100263e-01
  7.53927945e-01  7.66022300e-01  7.73262157e-01  7.75559560e-01
  7.82575510e-01  7.83879159e-01  7.87800361e-01  7.92809843e-01
  7.93885638e-01  7.94908805e-01  8.17551805e-01  8.18106876e-01
  8.21871685e-01  8.27926434e-01  8.79677968e-01  8.84243099e-01
  8.87118796e-01  8.88444345e-01  8.90038962e-01  9.00787672e-01
  9.04401421e-01  9.15946413e-01  9.34453000e-01  9.47932663e-01
  9.73366273e-01]

  warnings.warn(

2022-11-03 10:53:17,270:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.55610438e-02 -3.61790842e-02 -3.17093152e-02 -3.17082090e-02
 -3.13326181e-02 -2.91297222e-02 -2.89802980e-02 -2.79857254e-02
 -2.49986655e-02 -2.42459864e-02 -2.23990646e-02 -2.11044894e-02
 -2.01961686e-02 -1.97009359e-02 -1.82330651e-02 -1.63750397e-02
 -1.63369440e-02 -1.60855429e-02 -1.60755312e-02 -1.60397793e-02
 -1.54447550e-02 -1.47550406e-02 -1.39978239e-02 -1.35108790e-02
 -1.32498732e-02 -1.28216845e-02 -1.23689475e-02 -1.18946583e-02
 -1.15767919e-02 -1.07153438e-02 -1.04905512e-02 -1.02885163e-02
 -9.29951153e-03 -9.07963151e-03 -8.86057324e-03 -8.72229950e-03
 -8.44336764e-03 -8.25576119e-03 -7.72228714e-03 -7.44617665e-03
 -7.22518907e-03 -6.94993032e-03 -6.15279405e-03 -6.02977208e-03
 -5.66743436e-03 -5.06038433e-03 -4.82873315e-03 -4.56132944e-03
 -3.54487323e-03 -3.48586458e-03 -2.74687806e-03 -2.56929489e-03
 -2.08068293e-03 -1.92172940e-03 -1.04785558e-03 -1.03537292e-03
 -9.57013232e-04  5.15646246e-05  1.70638363e-04  4.19508446e-04
  1.51519967e-03  1.55448195e-03  1.60918574e-03  1.76797658e-03
  2.29995250e-03  2.37944542e-03  2.42395874e-03  3.31488860e-03
  3.33097614e-03  3.86268170e-03  4.31864609e-03  4.50414580e-03
  4.75176477e-03  4.96055731e-03  5.28944079e-03  5.34990879e-03
  5.42752646e-03  5.45753076e-03  5.92711268e-03  6.03027006e-03
  6.33483364e-03  7.15162196e-03  7.68028320e-03  7.86528918e-03
  7.91866242e-03  8.19898142e-03  8.33420519e-03  8.45081136e-03
  9.82582223e-03  1.01005450e-02  1.08094481e-02  1.08159901e-02
  1.11234268e-02  1.16593976e-02  1.18326205e-02  1.20266919e-02
  1.22667669e-02  1.46429671e-02  1.48311858e-02  1.48621682e-02
  1.51803076e-02  1.51909779e-02  1.52282000e-02  1.53111399e-02
  1.56718322e-02  1.59861511e-02  1.67700942e-02  1.68748226e-02
  1.83104277e-02  1.85565018e-02  1.87563190e-02  1.88272109e-02
  1.95161276e-02  1.95824659e-02  1.97088146e-02  1.99362470e-02
  2.11766676e-02  2.22889350e-02  2.25487021e-02  2.60029807e-02
  2.61467432e-02  2.68274884e-02  2.69271558e-02  2.74490748e-02
  2.84238796e-02  2.87055748e-02  2.92679644e-02  2.96850700e-02
  3.06329158e-02  3.12913052e-02  3.13495920e-02  3.16059429e-02
  3.19879094e-02  3.21196892e-02  3.38148197e-02  3.39875556e-02
  3.44708298e-02  3.71231413e-02  3.78279317e-02  4.39245769e-02
  4.61377686e-02  4.65738038e-02  4.83250750e-02  4.99536443e-02
  5.00176474e-02  5.02564394e-02  5.03431954e-02  5.07579135e-02
  5.07909394e-02  5.08376928e-02  5.12098048e-02  5.39164540e-02
  5.54904925e-02  5.58127699e-02  5.78043847e-02  5.85816833e-02
  5.95235363e-02  6.03341946e-02  6.14075613e-02  6.16551591e-02
  6.17663170e-02  6.27618728e-02  6.36939139e-02  6.38723736e-02
  6.50119181e-02  6.56789715e-02  6.57632887e-02  6.89345306e-02
  7.46708587e-02  7.52598160e-02  7.65611792e-02  7.96623807e-02
  8.10718723e-02  8.20501856e-02  8.34832332e-02  8.37811958e-02
  8.43329506e-02  8.53080231e-02  8.53089729e-02  8.56463218e-02
  8.61863527e-02  8.65474899e-02  8.65689546e-02  8.71113067e-02
  8.73147380e-02  8.73355000e-02  8.92661256e-02  9.01070681e-02
  9.06325809e-02  9.12093368e-02  9.25631119e-02  9.27913343e-02
  9.31512310e-02  9.76444509e-02  9.86136685e-02  1.00804975e-01
  1.02610486e-01  1.02960238e-01  1.03633679e-01  1.03713966e-01
  1.04249072e-01  1.04552252e-01  1.04571073e-01  1.05917920e-01
  1.07660688e-01  1.08274320e-01  1.11897563e-01  1.12426702e-01
  1.12455582e-01  1.12683754e-01  1.16795339e-01  1.16881740e-01
  1.17056671e-01  1.19035482e-01  1.19869818e-01  1.22466368e-01
  1.23893509e-01  1.24939347e-01  1.25549292e-01  1.28642260e-01
  1.31175339e-01  1.32250060e-01  1.34284946e-01  1.34919721e-01
  1.39346556e-01  1.40558619e-01  1.41047387e-01  1.41470393e-01
  1.44766633e-01  1.45357566e-01  1.45924946e-01  1.46416656e-01
  1.50443142e-01  1.53098139e-01  1.54974785e-01  1.56594938e-01
  1.59289629e-01  1.59610471e-01  1.61826201e-01  1.62909779e-01
  1.63922944e-01  1.64373335e-01  1.64906341e-01  1.67261694e-01
  1.67615694e-01  1.68603377e-01  1.70629375e-01  1.72914635e-01
  1.73574836e-01  1.74617359e-01  1.74637025e-01  1.75908881e-01
  1.76620163e-01  1.77858800e-01  1.78096252e-01  1.79481277e-01
  1.83747470e-01  1.86950358e-01  1.87594095e-01  1.97774402e-01
  1.98938176e-01  2.01100318e-01  2.01247005e-01  2.08475220e-01
  2.09643609e-01  2.10730114e-01  2.13937169e-01  2.14403684e-01
  2.15559439e-01  2.15751381e-01  2.18916518e-01  2.20481672e-01
  2.23150829e-01  2.24268549e-01  2.27176254e-01  2.35924120e-01
  2.36484479e-01  2.37079556e-01  2.38641873e-01  2.38944703e-01
  2.40676893e-01  2.41095515e-01  2.47293801e-01  2.47727381e-01
  2.47986770e-01  2.51510060e-01  2.55976889e-01  2.58090490e-01
  2.58709903e-01  2.61779070e-01  2.63601347e-01  2.65160372e-01
  2.65802755e-01  2.70764275e-01  2.77309521e-01  2.77343787e-01
  2.77576575e-01  2.81936368e-01  2.84012572e-01  2.84250794e-01
  2.84345743e-01  2.86937939e-01  2.88164419e-01  2.89269930e-01
  2.90085333e-01  2.92043710e-01  2.93190760e-01  2.98994704e-01
  3.00056922e-01  3.00410695e-01  3.00731646e-01  3.01826949e-01
  3.04394169e-01  3.08319093e-01  3.11724564e-01  3.18461568e-01
  3.21750763e-01  3.24299647e-01  3.24337581e-01  3.24877371e-01
  3.25662393e-01  3.26399830e-01  3.27825428e-01  3.27891290e-01
  3.29062558e-01  3.32295935e-01  3.34150846e-01  3.34268864e-01
  3.35632077e-01  3.37973780e-01  3.39447088e-01  3.39744186e-01
  3.41080518e-01  3.44063216e-01  3.48468224e-01  3.49077276e-01
  3.49621326e-01  3.49644827e-01  3.53706280e-01  3.55366410e-01
  3.56642364e-01  3.61553686e-01  3.62081347e-01  3.62976805e-01
  3.72221432e-01  3.73433106e-01  3.75597358e-01  3.78885471e-01
  3.79748027e-01  3.85220802e-01  3.86753646e-01  3.90265535e-01
  3.90978159e-01  3.96196944e-01  3.96902211e-01  3.98156346e-01
  3.98499122e-01  3.98882565e-01  3.99562898e-01  4.01154433e-01
  4.09168289e-01  4.13417811e-01  4.15746901e-01  4.20032164e-01
  4.21840886e-01  4.22488083e-01  4.26958942e-01  4.29042295e-01
  4.30214558e-01  4.32213939e-01  4.34891871e-01  4.37810635e-01
  4.39148688e-01  4.39242368e-01  4.43266282e-01  4.44031636e-01
  4.46887751e-01  4.49592825e-01  4.50145544e-01  4.52609923e-01
  4.54227569e-01  4.58111124e-01  4.66966493e-01  4.74564910e-01
  4.75688112e-01  4.79092989e-01  4.79330627e-01  4.86731830e-01
  4.87755816e-01  4.97468111e-01  4.98568120e-01  5.00369074e-01
  5.01026964e-01  5.01524064e-01  5.01724205e-01  5.04062700e-01
  5.09088777e-01  5.15740589e-01  5.15742921e-01  5.15985572e-01
  5.16130680e-01  5.27073025e-01  5.32223160e-01  5.32594900e-01
  5.40724643e-01  5.44991058e-01  5.52500643e-01  5.53702039e-01
  5.55730688e-01  5.56728334e-01  5.64037242e-01  5.65291864e-01
  5.67269489e-01  5.68457890e-01  5.70966321e-01  5.71935653e-01
  5.72683322e-01  5.72992277e-01  5.73115093e-01  5.74450324e-01
  5.80986578e-01  5.81041629e-01  5.81347119e-01  5.83631724e-01
  5.84509925e-01  5.91538646e-01  5.94101714e-01  5.96603516e-01
  5.97875653e-01  6.00153753e-01  6.00850349e-01  6.04213216e-01
  6.05994850e-01  6.06091904e-01  6.16091263e-01  6.18553928e-01
  6.19386808e-01  6.27832018e-01  6.31062340e-01  6.34576234e-01
  6.34825474e-01  6.40269749e-01  6.45619693e-01  6.46226221e-01
  6.47483175e-01  6.48680796e-01  6.58243133e-01  6.65719316e-01
  6.65969831e-01  6.70091369e-01  6.71662798e-01  6.72839528e-01
  6.80735186e-01  6.81828725e-01  6.82058292e-01  6.90186543e-01
  6.93620262e-01  6.98739012e-01  7.00490709e-01  7.07986084e-01
  7.11681086e-01  7.16045875e-01  7.17169023e-01  7.23463191e-01
  7.26118455e-01  7.32912492e-01  7.40823983e-01  7.44759037e-01
  7.49488842e-01  7.51741734e-01  7.53581929e-01  7.54734093e-01
  7.57827657e-01  7.66188094e-01  7.66732194e-01  7.72623696e-01
  7.87767389e-01  8.07992440e-01  8.24334124e-01  8.24722326e-01
  8.41251346e-01  8.60862661e-01  8.65351394e-01  8.76383777e-01
  8.76538223e-01  8.77073071e-01  9.03606074e-01  9.68535084e-01]

  warnings.warn(

2022-11-03 10:53:17,279:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.32116069e-02 -5.94660008e-02 -4.11723983e-02 -4.00346607e-02
 -3.62530982e-02 -3.58141142e-02 -3.12052450e-02 -3.10639497e-02
 -2.97873444e-02 -2.94466014e-02 -2.89549569e-02 -2.85779698e-02
 -2.13441553e-02 -2.08887209e-02 -2.08453042e-02 -2.05221450e-02
 -2.02121342e-02 -1.82608215e-02 -1.67204377e-02 -1.66473066e-02
 -1.55140623e-02 -1.54544279e-02 -1.51856408e-02 -1.51820999e-02
 -1.48313593e-02 -1.44329602e-02 -1.41711071e-02 -1.37423731e-02
 -1.36916691e-02 -1.36451095e-02 -1.36194270e-02 -1.35597176e-02
 -1.33601657e-02 -1.31565380e-02 -1.29266073e-02 -1.29183403e-02
 -1.22674721e-02 -1.17459466e-02 -1.16830325e-02 -1.02891017e-02
 -9.19348261e-03 -8.96690785e-03 -8.86128081e-03 -8.46890525e-03
 -7.89802438e-03 -7.82529615e-03 -7.67806157e-03 -7.28217912e-03
 -6.87657667e-03 -6.72751012e-03 -6.55010953e-03 -6.40426612e-03
 -5.72238087e-03 -5.66130231e-03 -5.65271276e-03 -5.11416606e-03
 -4.65728510e-03 -4.52969762e-03 -4.11272938e-03 -3.64142383e-03
 -3.18366032e-03 -2.87371771e-03 -1.83853732e-03 -1.65495294e-03
 -1.48714772e-03 -1.67986708e-04  1.39465696e-04  7.14125439e-04
  1.46526728e-03  1.70823624e-03  2.46291107e-03  2.54321393e-03
  4.39245279e-03  4.59901361e-03  4.72992199e-03  6.15095202e-03
  6.22354392e-03  7.03179281e-03  7.40481153e-03  7.53795881e-03
  7.86026692e-03  8.35201085e-03  8.41623168e-03  9.71930285e-03
  9.73859095e-03  9.92779029e-03  1.07600023e-02  1.19698552e-02
  1.22665861e-02  1.32173043e-02  1.40085005e-02  1.44554693e-02
  1.50242943e-02  1.51179525e-02  1.51183869e-02  1.53111100e-02
  1.55930215e-02  1.73315615e-02  1.94537452e-02  2.05595803e-02
  2.06704788e-02  2.09627955e-02  2.15226509e-02  2.17029330e-02
  2.18039401e-02  2.19717661e-02  2.38418618e-02  2.43543297e-02
  2.48384120e-02  2.63142319e-02  2.72950754e-02  3.04948994e-02
  3.22526213e-02  3.25271031e-02  3.40586615e-02  3.49397483e-02
  3.69825979e-02  3.73574338e-02  3.93929955e-02  4.21220734e-02
  4.32210296e-02  4.41465043e-02  4.46229861e-02  4.66758453e-02
  4.75801268e-02  5.04138177e-02  5.05843131e-02  5.19683332e-02
  5.24426629e-02  5.34112368e-02  5.63218552e-02  6.08022350e-02
  6.29277541e-02  6.41921705e-02  6.60140782e-02  6.86692207e-02
  6.96870832e-02  7.07674096e-02  7.63043097e-02  7.76527342e-02
  7.91634853e-02  7.97134746e-02  7.98563158e-02  8.04928374e-02
  8.07639320e-02  8.15121462e-02  8.21129889e-02  8.27660563e-02
  8.28495951e-02  8.35581632e-02  8.52036587e-02  8.58062988e-02
  8.80837592e-02  8.87300680e-02  8.89364193e-02  8.96749608e-02
  9.08379504e-02  9.31297832e-02  9.49630284e-02  9.53896545e-02
  9.58273396e-02  9.76685821e-02  1.00003523e-01  1.00142332e-01
  1.00556684e-01  1.01529586e-01  1.01654113e-01  1.04288387e-01
  1.09029445e-01  1.09725064e-01  1.09898165e-01  1.10181862e-01
  1.13242395e-01  1.13499264e-01  1.13757463e-01  1.14311741e-01
  1.14481577e-01  1.15360198e-01  1.17864555e-01  1.18653654e-01
  1.19854003e-01  1.20325665e-01  1.20680921e-01  1.21431165e-01
  1.21469791e-01  1.21560245e-01  1.22170659e-01  1.23202145e-01
  1.24146254e-01  1.24269864e-01  1.26649681e-01  1.27955606e-01
  1.28111562e-01  1.31998478e-01  1.32664819e-01  1.33667618e-01
  1.34217063e-01  1.35179680e-01  1.35517058e-01  1.35940885e-01
  1.38131854e-01  1.38666406e-01  1.38907822e-01  1.39750518e-01
  1.43041342e-01  1.43548038e-01  1.44021099e-01  1.44122004e-01
  1.44514341e-01  1.45480757e-01  1.45710418e-01  1.46925717e-01
  1.47349317e-01  1.47525628e-01  1.51845327e-01  1.54788716e-01
  1.56953156e-01  1.58150077e-01  1.59389359e-01  1.59890046e-01
  1.61416382e-01  1.61985984e-01  1.63833313e-01  1.69163203e-01
  1.70182221e-01  1.70209763e-01  1.72405690e-01  1.73244239e-01
  1.75114791e-01  1.75955803e-01  1.77931775e-01  1.78976048e-01
  1.82639694e-01  1.83281641e-01  1.84681941e-01  1.84706242e-01
  1.86808051e-01  1.87013506e-01  1.88938103e-01  1.89457798e-01
  1.90489206e-01  1.91646586e-01  1.92916391e-01  1.95399120e-01
  1.95694914e-01  1.96079538e-01  1.97606852e-01  1.98811405e-01
  1.99553489e-01  2.03738843e-01  2.09050113e-01  2.12913834e-01
  2.12962523e-01  2.14759984e-01  2.15119087e-01  2.21114215e-01
  2.23007049e-01  2.24527457e-01  2.25997429e-01  2.27055842e-01
  2.29179660e-01  2.31581183e-01  2.35358847e-01  2.35468086e-01
  2.36722219e-01  2.37147828e-01  2.37470996e-01  2.38218972e-01
  2.38395631e-01  2.38631271e-01  2.40198065e-01  2.41637989e-01
  2.42136189e-01  2.42980433e-01  2.43648652e-01  2.43808300e-01
  2.49213590e-01  2.50553791e-01  2.53990265e-01  2.56744274e-01
  2.57146204e-01  2.57803449e-01  2.61764693e-01  2.64383137e-01
  2.68163733e-01  2.70068603e-01  2.72482414e-01  2.72802262e-01
  2.73597450e-01  2.73731646e-01  2.74752821e-01  2.77561145e-01
  2.78242028e-01  2.80265193e-01  2.83434095e-01  2.84295809e-01
  2.87887042e-01  2.91192284e-01  2.94979474e-01  2.95785225e-01
  2.97292867e-01  2.99613044e-01  3.02391343e-01  3.02567185e-01
  3.03062652e-01  3.05112463e-01  3.09578250e-01  3.09833944e-01
  3.18457796e-01  3.19110747e-01  3.21743108e-01  3.22088750e-01
  3.22836462e-01  3.25833251e-01  3.26264796e-01  3.26376991e-01
  3.26515645e-01  3.27238781e-01  3.27297219e-01  3.28174828e-01
  3.28223534e-01  3.30420819e-01  3.32794860e-01  3.34738012e-01
  3.39733193e-01  3.40928047e-01  3.41626971e-01  3.42861441e-01
  3.44153092e-01  3.44436810e-01  3.46470175e-01  3.46719458e-01
  3.47940965e-01  3.48632792e-01  3.49570335e-01  3.50216914e-01
  3.59684693e-01  3.60730289e-01  3.61497941e-01  3.63604324e-01
  3.65784675e-01  3.66414180e-01  3.66585363e-01  3.71789826e-01
  3.72154362e-01  3.72261066e-01  3.72936114e-01  3.73958443e-01
  3.74307908e-01  3.75509435e-01  3.76849669e-01  3.93215655e-01
  3.95225817e-01  3.95458505e-01  3.96029177e-01  3.98341628e-01
  3.98760162e-01  3.99842459e-01  4.00334151e-01  4.01253161e-01
  4.01384641e-01  4.03565944e-01  4.12193241e-01  4.12888536e-01
  4.13130991e-01  4.14120763e-01  4.16064973e-01  4.19371302e-01
  4.20313077e-01  4.20741975e-01  4.21199605e-01  4.21987577e-01
  4.26388458e-01  4.28402568e-01  4.30087499e-01  4.34141249e-01
  4.34328081e-01  4.34506255e-01  4.35485719e-01  4.41653155e-01
  4.41863467e-01  4.45665877e-01  4.48251503e-01  4.48758733e-01
  4.51059027e-01  4.53658540e-01  4.57681148e-01  4.57705567e-01
  4.61285543e-01  4.67832945e-01  4.69499011e-01  4.70542309e-01
  4.71591146e-01  4.71596816e-01  4.73175275e-01  4.74014216e-01
  4.77623901e-01  4.90875207e-01  4.95219663e-01  4.96648175e-01
  4.99934187e-01  5.05329504e-01  5.06236787e-01  5.06348543e-01
  5.09835274e-01  5.11021871e-01  5.16071316e-01  5.18398059e-01
  5.20278958e-01  5.22506343e-01  5.22513653e-01  5.24833112e-01
  5.27044921e-01  5.29598453e-01  5.32312111e-01  5.33669740e-01
  5.36342825e-01  5.37589646e-01  5.37858801e-01  5.41889780e-01
  5.44499483e-01  5.48524741e-01  5.50357070e-01  5.52381507e-01
  5.55432605e-01  5.57877901e-01  5.58319853e-01  5.62042811e-01
  5.65617974e-01  5.71945617e-01  5.75817512e-01  5.77859799e-01
  5.78510041e-01  5.90046064e-01  5.90352625e-01  6.00120682e-01
  6.03988060e-01  6.09743685e-01  6.09957101e-01  6.15076122e-01
  6.25692114e-01  6.26086699e-01  6.27832248e-01  6.31135218e-01
  6.34224595e-01  6.35995275e-01  6.37600943e-01  6.43178570e-01
  6.47360142e-01  6.48960124e-01  6.54751135e-01  6.56206325e-01
  6.57422555e-01  6.59268487e-01  6.61040518e-01  6.61229458e-01
  6.61682837e-01  6.63754617e-01  6.65460287e-01  6.81357906e-01
  6.95399360e-01  6.98863485e-01  7.08615571e-01  7.11143186e-01
  7.23386764e-01  7.31792121e-01  7.44148432e-01  7.51683522e-01
  7.53750989e-01  7.54238146e-01  7.55414068e-01  7.69112124e-01
  7.69602977e-01  7.88598070e-01  8.00260841e-01  8.06871157e-01
  8.19636488e-01  8.28641412e-01  8.40591190e-01  8.46361346e-01
  8.56895270e-01  8.58950560e-01  8.60702173e-01  8.61010864e-01
  8.66807600e-01  8.81225774e-01  8.95337520e-01  8.95766219e-01
  9.01771522e-01  9.01786151e-01  9.41604907e-01]

  warnings.warn(

2022-11-03 10:53:21,321:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.35287157e-02 -6.85901234e-02 -5.08076822e-02 -3.20296889e-02
 -3.12656639e-02 -2.81147354e-02 -2.80677514e-02 -2.67189788e-02
 -2.59400649e-02 -2.57540977e-02 -2.53481951e-02 -2.52558628e-02
 -2.46116671e-02 -2.33760931e-02 -2.26654445e-02 -2.19230234e-02
 -2.16514840e-02 -2.14127183e-02 -1.97744223e-02 -1.95698787e-02
 -1.84210238e-02 -1.76822442e-02 -1.73799137e-02 -1.70486990e-02
 -1.58725507e-02 -1.48681731e-02 -1.42119314e-02 -1.37880524e-02
 -1.27174698e-02 -1.21826836e-02 -1.13056133e-02 -1.12549173e-02
 -1.05795922e-02 -1.04095592e-02 -9.61452015e-03 -9.58609677e-03
 -9.27538698e-03 -9.26005402e-03 -8.77908779e-03 -8.61790613e-03
 -8.44787306e-03 -7.94728912e-03 -7.68553081e-03 -7.15606829e-03
 -7.14069034e-03 -7.05585333e-03 -6.80423901e-03 -6.32885356e-03
 -6.14624583e-03 -5.29324865e-03 -4.78092553e-03 -4.50208171e-03
 -4.49050691e-03 -4.37859427e-03 -4.22375285e-03 -3.73940321e-03
 -3.70605124e-03 -3.62702068e-03 -3.50505131e-03 -2.89776442e-03
 -2.85348932e-03 -2.83015056e-03 -2.79584077e-03 -2.59640208e-03
 -2.59396578e-03 -2.58970974e-03 -2.41894576e-03 -2.14891038e-03
 -2.03683946e-03 -1.88874259e-03 -1.88818641e-03 -1.64122330e-03
 -1.60490332e-03 -7.75539058e-04 -5.03255790e-04  1.71937006e-04
  1.30606622e-03  1.30815981e-03  1.32955346e-03  1.47014441e-03
  2.95982846e-03  3.23204588e-03  3.80898923e-03  4.15918812e-03
  4.75039719e-03  4.82786149e-03  4.91734701e-03  5.01722465e-03
  5.64233684e-03  5.77067534e-03  6.34915317e-03  7.38627694e-03
  7.52857685e-03  7.86489391e-03  8.09757011e-03  8.77836574e-03
  8.95016723e-03  9.49134862e-03  9.51129678e-03  9.79109257e-03
  1.00709703e-02  1.02103975e-02  1.13192806e-02  1.15464809e-02
  1.17088315e-02  1.27819409e-02  1.40053868e-02  1.41554070e-02
  1.43101034e-02  1.65332734e-02  1.68719288e-02  1.83055385e-02
  1.93445650e-02  1.94279916e-02  2.21199040e-02  2.25347565e-02
  2.35286994e-02  2.44034476e-02  2.63047040e-02  2.64673212e-02
  2.73175373e-02  2.92321763e-02  3.02249596e-02  3.06415812e-02
  3.10830366e-02  3.18494806e-02  3.18555279e-02  3.21515563e-02
  3.70377075e-02  3.90270543e-02  3.96677749e-02  4.10365273e-02
  4.26113387e-02  4.40118266e-02  4.42981993e-02  4.55718880e-02
  4.57372979e-02  4.58780386e-02  4.79804857e-02  4.80516354e-02
  4.87657675e-02  4.99425923e-02  5.08857260e-02  5.11249007e-02
  5.24999853e-02  5.27533686e-02  5.48754669e-02  5.56210764e-02
  5.57993241e-02  5.77323554e-02  6.04387129e-02  6.18613819e-02
  6.23160926e-02  6.29213252e-02  6.64124752e-02  6.65208905e-02
  6.83526883e-02  6.96455317e-02  7.05640107e-02  7.12072067e-02
  7.17494318e-02  7.28839501e-02  7.33698188e-02  7.53388199e-02
  7.57928647e-02  7.66942836e-02  7.67744145e-02  7.74909333e-02
  7.96200858e-02  8.06659657e-02  8.17780401e-02  8.56141109e-02
  8.90533214e-02  8.99845285e-02  9.48985404e-02  9.53525489e-02
  9.60475168e-02  9.64733126e-02  9.67551702e-02  9.83366803e-02
  1.01883817e-01  1.02272969e-01  1.02726065e-01  1.03344824e-01
  1.03586818e-01  1.05126091e-01  1.07392502e-01  1.09174818e-01
  1.09859797e-01  1.10625838e-01  1.10789112e-01  1.14540698e-01
  1.15151274e-01  1.15271507e-01  1.15417500e-01  1.16043405e-01
  1.16654579e-01  1.18142459e-01  1.23150540e-01  1.25284327e-01
  1.27460988e-01  1.27736599e-01  1.28717301e-01  1.30595875e-01
  1.31400277e-01  1.32575132e-01  1.34811405e-01  1.35477679e-01
  1.35594234e-01  1.39113345e-01  1.39337975e-01  1.40472008e-01
  1.41022062e-01  1.41741444e-01  1.43099909e-01  1.45558424e-01
  1.49446288e-01  1.49661968e-01  1.50136307e-01  1.51020618e-01
  1.51127917e-01  1.51439825e-01  1.53584541e-01  1.55273196e-01
  1.58457347e-01  1.59190650e-01  1.61248015e-01  1.61288235e-01
  1.62505770e-01  1.66533542e-01  1.67373415e-01  1.67571968e-01
  1.72305867e-01  1.73747989e-01  1.76417872e-01  1.77015538e-01
  1.77487259e-01  1.78364017e-01  1.80347601e-01  1.82114529e-01
  1.85194831e-01  1.86509839e-01  1.87877259e-01  1.88359312e-01
  1.90325594e-01  1.93888671e-01  1.94606500e-01  1.99627188e-01
  2.01595764e-01  2.05513147e-01  2.06137951e-01  2.07228022e-01
  2.11883919e-01  2.12979432e-01  2.15316586e-01  2.16479074e-01
  2.18575615e-01  2.19543834e-01  2.24022647e-01  2.25110688e-01
  2.28992594e-01  2.31311975e-01  2.33868172e-01  2.34135669e-01
  2.34235642e-01  2.36411420e-01  2.37147368e-01  2.38106192e-01
  2.39198229e-01  2.41867209e-01  2.42350161e-01  2.42934213e-01
  2.45156233e-01  2.45397786e-01  2.45641875e-01  2.48788323e-01
  2.56134843e-01  2.57012484e-01  2.61158365e-01  2.62394602e-01
  2.64441750e-01  2.66353999e-01  2.67287721e-01  2.68483502e-01
  2.70806451e-01  2.73069586e-01  2.74747807e-01  2.75957371e-01
  2.77967572e-01  2.80146140e-01  2.86429957e-01  2.86504171e-01
  2.87490935e-01  2.88739911e-01  2.90032902e-01  2.91006474e-01
  2.91931598e-01  2.92344850e-01  2.98520755e-01  3.00573301e-01
  3.01646585e-01  3.04383815e-01  3.04909086e-01  3.07090588e-01
  3.07385551e-01  3.08132416e-01  3.12501105e-01  3.16060183e-01
  3.17353377e-01  3.20358649e-01  3.21952051e-01  3.23308534e-01
  3.23896225e-01  3.29919578e-01  3.34070552e-01  3.35007404e-01
  3.38507368e-01  3.40388410e-01  3.40624481e-01  3.40901675e-01
  3.43368587e-01  3.44533858e-01  3.44932909e-01  3.47687007e-01
  3.47974241e-01  3.50285793e-01  3.50653456e-01  3.53555809e-01
  3.61498714e-01  3.66790730e-01  3.69676853e-01  3.70773023e-01
  3.70969019e-01  3.71263588e-01  3.74492552e-01  3.76950672e-01
  3.77529918e-01  3.80270380e-01  3.84024604e-01  3.84841954e-01
  3.87761146e-01  3.91383122e-01  3.92262182e-01  3.93525653e-01
  3.93897221e-01  3.94045545e-01  3.99856696e-01  4.00075161e-01
  4.02092631e-01  4.03346483e-01  4.04610276e-01  4.07876810e-01
  4.10158320e-01  4.16141216e-01  4.18354978e-01  4.20519285e-01
  4.22680110e-01  4.22743902e-01  4.25399306e-01  4.26623352e-01
  4.28869125e-01  4.32413659e-01  4.32901057e-01  4.33740810e-01
  4.42691530e-01  4.46254683e-01  4.46528996e-01  4.49884283e-01
  4.51971207e-01  4.56084636e-01  4.57930304e-01  4.58102912e-01
  4.64148805e-01  4.66390874e-01  4.67082031e-01  4.68188401e-01
  4.71072724e-01  4.72106314e-01  4.72843581e-01  4.73311495e-01
  4.74098983e-01  4.76542817e-01  4.76859520e-01  4.77595398e-01
  4.79500271e-01  4.79847741e-01  4.86444217e-01  4.89197472e-01
  4.99758492e-01  5.04583981e-01  5.04785987e-01  5.08214714e-01
  5.09404048e-01  5.09676647e-01  5.12671682e-01  5.15832931e-01
  5.21271204e-01  5.21643533e-01  5.31053731e-01  5.37339745e-01
  5.37429571e-01  5.40066525e-01  5.43129083e-01  5.43277837e-01
  5.47830842e-01  5.49324083e-01  5.51032178e-01  5.56602763e-01
  5.59400420e-01  5.67956248e-01  5.69231020e-01  5.72918201e-01
  5.77724093e-01  5.86178892e-01  5.87829407e-01  5.89350768e-01
  5.89600805e-01  5.90978030e-01  5.91815588e-01  5.92560657e-01
  5.94116929e-01  5.95230764e-01  5.98487287e-01  5.99777339e-01
  6.06919257e-01  6.08441737e-01  6.10222627e-01  6.12270521e-01
  6.16524588e-01  6.26978982e-01  6.32553095e-01  6.33509924e-01
  6.35533313e-01  6.37130648e-01  6.45001829e-01  6.48951579e-01
  6.52013126e-01  6.52735819e-01  6.52861092e-01  6.56032560e-01
  6.59376959e-01  6.59704741e-01  6.65953415e-01  6.67735348e-01
  6.69040127e-01  6.70556461e-01  6.70625458e-01  6.72226796e-01
  6.82593866e-01  6.83216576e-01  6.83511981e-01  6.86953943e-01
  6.89703031e-01  6.90656293e-01  6.91752226e-01  6.92191845e-01
  7.05057671e-01  7.09601221e-01  7.30551053e-01  7.44212489e-01
  7.56382837e-01  7.69617728e-01  7.78228800e-01  7.81634870e-01
  7.83659104e-01  7.84146610e-01  7.94855706e-01  8.02212437e-01
  8.13864814e-01  8.15846016e-01  8.16924727e-01  8.25769328e-01
  8.29846447e-01  8.44039946e-01  8.46842864e-01  8.66127480e-01
  8.68901899e-01  8.76824521e-01  8.90363119e-01  9.17593116e-01
  9.20718006e-01  9.22646528e-01  9.57168525e-01]

  warnings.warn(

2022-11-03 10:53:21,382:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.19546823e-02 -3.34896261e-02 -3.06190414e-02 -2.85295002e-02
 -2.40608007e-02 -2.23110183e-02 -2.16100234e-02 -2.02757654e-02
 -2.01386979e-02 -1.96964865e-02 -1.91216736e-02 -1.88056860e-02
 -1.86238668e-02 -1.78397176e-02 -1.72415147e-02 -1.69328059e-02
 -1.66226158e-02 -1.60222525e-02 -1.59852374e-02 -1.58636090e-02
 -1.46113346e-02 -1.45654257e-02 -1.43294774e-02 -1.41737995e-02
 -1.30351099e-02 -1.27043327e-02 -1.20416790e-02 -1.19510315e-02
 -1.04334993e-02 -9.05548110e-03 -8.24506258e-03 -8.18812664e-03
 -7.57916884e-03 -6.43566825e-03 -6.32900604e-03 -5.20909042e-03
 -4.77615265e-03 -3.09033789e-03 -2.72746084e-03 -2.38037843e-04
  2.40764190e-05  9.26436305e-04  1.09087907e-03  2.00503944e-03
  2.19307828e-03  2.50883627e-03  3.04948153e-03  3.12435763e-03
  3.25001523e-03  3.90750023e-03  3.99193784e-03  4.69001279e-03
  4.70036483e-03  5.45088691e-03  5.67729242e-03  5.86993009e-03
  6.80298080e-03  6.83183410e-03  7.11544820e-03  7.82655569e-03
  8.31885031e-03  8.53647989e-03  8.75750979e-03  8.96169509e-03
  9.11963253e-03  1.14133039e-02  1.15256018e-02  1.23167991e-02
  1.41658724e-02  1.69148180e-02  1.73577830e-02  1.79143527e-02
  1.79594371e-02  1.87413854e-02  1.96482971e-02  2.05241112e-02
  2.23762288e-02  2.23836883e-02  2.30975167e-02  2.37298385e-02
  2.41406798e-02  2.44260249e-02  2.49971404e-02  2.64220991e-02
  2.67807465e-02  2.72055942e-02  2.73592587e-02  2.76010685e-02
  2.89892474e-02  3.06084494e-02  3.16189649e-02  3.16483252e-02
  3.23023391e-02  3.37961558e-02  3.58034986e-02  3.68708552e-02
  3.71822205e-02  3.86543163e-02  3.88805848e-02  4.04515539e-02
  4.27379438e-02  4.29466327e-02  4.29726094e-02  4.32905811e-02
  4.48917421e-02  4.55864130e-02  4.58826913e-02  4.60210555e-02
  4.68245400e-02  4.73728522e-02  4.73915133e-02  4.75539205e-02
  4.88170388e-02  4.92725816e-02  5.06993287e-02  5.08364462e-02
  5.10758035e-02  5.15903049e-02  5.37744803e-02  5.46453387e-02
  5.63543238e-02  5.80761653e-02  6.00482794e-02  6.01994919e-02
  6.50276720e-02  6.56918497e-02  6.84398123e-02  6.97817990e-02
  7.00010965e-02  7.14302063e-02  7.27200683e-02  7.56449019e-02
  7.80387760e-02  7.93318455e-02  8.04042902e-02  8.07366648e-02
  8.09821110e-02  8.43490591e-02  8.45269798e-02  8.53711613e-02
  8.60158734e-02  8.64363993e-02  8.73214515e-02  8.91147349e-02
  8.93415083e-02  8.96057719e-02  8.96141352e-02  8.97011562e-02
  9.44970104e-02  9.52231099e-02  9.57309085e-02  9.58100761e-02
  9.62568445e-02  9.70505564e-02  9.70765439e-02  9.81966039e-02
  9.88707931e-02  9.93780825e-02  9.94049681e-02  9.96807357e-02
  1.00229552e-01  1.00894327e-01  1.01234894e-01  1.01266760e-01
  1.01811421e-01  1.04631869e-01  1.05198367e-01  1.06132016e-01
  1.06372728e-01  1.07599983e-01  1.07669571e-01  1.07976728e-01
  1.10180151e-01  1.13944532e-01  1.15703748e-01  1.15942440e-01
  1.16900786e-01  1.18165217e-01  1.23502946e-01  1.26630169e-01
  1.27585713e-01  1.28374162e-01  1.28465223e-01  1.28651081e-01
  1.29923117e-01  1.33774124e-01  1.34537910e-01  1.35509890e-01
  1.35851397e-01  1.35938046e-01  1.38433302e-01  1.41360360e-01
  1.43171542e-01  1.43301741e-01  1.43921309e-01  1.45027049e-01
  1.53127967e-01  1.53932329e-01  1.56575783e-01  1.56992079e-01
  1.59250953e-01  1.62013237e-01  1.62061128e-01  1.63364758e-01
  1.63538002e-01  1.63786658e-01  1.65261612e-01  1.65744191e-01
  1.69631930e-01  1.71123370e-01  1.71319713e-01  1.71706874e-01
  1.73332669e-01  1.73454239e-01  1.74070064e-01  1.74480669e-01
  1.76634332e-01  1.76837283e-01  1.76895736e-01  1.77264492e-01
  1.77925522e-01  1.79232198e-01  1.81477951e-01  1.83023910e-01
  1.85247277e-01  1.87096361e-01  1.87231002e-01  1.88855472e-01
  1.90968527e-01  1.91519980e-01  1.91555422e-01  1.93693021e-01
  1.98229248e-01  1.99953775e-01  2.01282518e-01  2.02176120e-01
  2.05283941e-01  2.08073187e-01  2.10632814e-01  2.11449597e-01
  2.13536583e-01  2.13555865e-01  2.14913021e-01  2.18211481e-01
  2.19469230e-01  2.19747385e-01  2.22712816e-01  2.23383949e-01
  2.25420468e-01  2.26179906e-01  2.28089109e-01  2.29339951e-01
  2.31124171e-01  2.32870032e-01  2.34877869e-01  2.34993833e-01
  2.35426709e-01  2.35562972e-01  2.39140523e-01  2.41526184e-01
  2.45026250e-01  2.45152615e-01  2.48047682e-01  2.48265068e-01
  2.50967736e-01  2.51039768e-01  2.54224764e-01  2.55236445e-01
  2.56378781e-01  2.58905574e-01  2.61265775e-01  2.61314363e-01
  2.63016020e-01  2.63614623e-01  2.65043358e-01  2.65984291e-01
  2.66862803e-01  2.70955628e-01  2.71783264e-01  2.72024565e-01
  2.77396326e-01  2.79974783e-01  2.84004606e-01  2.85573459e-01
  2.86086971e-01  2.87324117e-01  2.88102158e-01  2.89202737e-01
  2.90772675e-01  2.91208281e-01  2.91277976e-01  2.91988721e-01
  2.95279316e-01  2.95324749e-01  2.96483218e-01  2.98420294e-01
  2.98501734e-01  2.98729363e-01  2.99435930e-01  3.00275881e-01
  3.00447712e-01  3.00619988e-01  3.00951785e-01  3.04574368e-01
  3.07178412e-01  3.07672718e-01  3.10217276e-01  3.12514890e-01
  3.14269451e-01  3.19506902e-01  3.22799335e-01  3.24203522e-01
  3.24595153e-01  3.24815937e-01  3.25176769e-01  3.26021429e-01
  3.26256619e-01  3.28457630e-01  3.30483273e-01  3.38644620e-01
  3.39266818e-01  3.39536352e-01  3.41791204e-01  3.43774015e-01
  3.46188424e-01  3.50542324e-01  3.55725706e-01  3.58049176e-01
  3.62865313e-01  3.65281876e-01  3.67568977e-01  3.71041282e-01
  3.73519772e-01  3.74175774e-01  3.77712767e-01  3.78229220e-01
  3.79674834e-01  3.79802818e-01  3.82313782e-01  3.90919750e-01
  3.91465041e-01  3.91617701e-01  3.94119706e-01  3.94186908e-01
  3.94580162e-01  3.97262238e-01  3.98203964e-01  4.01971560e-01
  4.02909628e-01  4.11336982e-01  4.11914527e-01  4.12970078e-01
  4.13097999e-01  4.14286313e-01  4.14485856e-01  4.17093290e-01
  4.20556655e-01  4.22939372e-01  4.25689681e-01  4.27989790e-01
  4.32913319e-01  4.33146964e-01  4.35843610e-01  4.36109028e-01
  4.39341292e-01  4.39484386e-01  4.42010951e-01  4.42448578e-01
  4.43696372e-01  4.52454437e-01  4.54380190e-01  4.56684581e-01
  4.60177556e-01  4.63520519e-01  4.63732334e-01  4.66793615e-01
  4.73673860e-01  4.76734490e-01  4.77677122e-01  4.80884398e-01
  4.81901204e-01  4.85282550e-01  4.90361399e-01  4.91254118e-01
  4.91398996e-01  4.94053569e-01  4.94450186e-01  4.94590908e-01
  4.95950768e-01  4.96152798e-01  4.96238018e-01  4.97621502e-01
  5.00107382e-01  5.02286835e-01  5.05086121e-01  5.08643328e-01
  5.10189156e-01  5.11213350e-01  5.13186762e-01  5.20574276e-01
  5.33638210e-01  5.33904694e-01  5.37743220e-01  5.39429726e-01
  5.41347774e-01  5.43173991e-01  5.46720845e-01  5.47905271e-01
  5.53014171e-01  5.54975024e-01  5.55696817e-01  5.57170786e-01
  5.57717205e-01  5.58296354e-01  5.63279646e-01  5.64738883e-01
  5.65546326e-01  5.66294462e-01  5.67688244e-01  5.71988399e-01
  5.72647001e-01  5.75500734e-01  5.77176946e-01  5.83179136e-01
  5.85291065e-01  5.91813751e-01  5.91995281e-01  5.94206070e-01
  5.96466349e-01  5.97101853e-01  5.97838779e-01  6.06329729e-01
  6.08342735e-01  6.08922639e-01  6.09616547e-01  6.09752335e-01
  6.15906560e-01  6.21507819e-01  6.24540362e-01  6.35483539e-01
  6.35807832e-01  6.41737921e-01  6.56944262e-01  6.59007746e-01
  6.66399866e-01  6.68053764e-01  6.70274929e-01  6.70930384e-01
  6.71812880e-01  6.82061738e-01  6.97025729e-01  6.99497689e-01
  7.03194707e-01  7.05394586e-01  7.06002346e-01  7.11652806e-01
  7.14226076e-01  7.28213872e-01  7.39183733e-01  7.45064605e-01
  7.46716603e-01  7.48243842e-01  7.54835698e-01  7.55646381e-01
  7.62497343e-01  7.66033770e-01  7.68280412e-01  7.69590163e-01
  7.70572816e-01  7.91318149e-01  7.94025852e-01  7.95046301e-01
  7.96885522e-01  7.97302555e-01  8.01842998e-01  8.48245060e-01
  8.55411566e-01  8.56961458e-01  8.59104947e-01  8.72097704e-01
  9.00316396e-01  9.03244487e-01  9.59790759e-01  9.92758171e-01
  1.00223678e+00]

  warnings.warn(

2022-11-03 10:53:21,388:INFO:Calculating mean and std
2022-11-03 10:53:21,388:INFO:Creating metrics dataframe
2022-11-03 10:53:21,397:INFO:Uploading results into container
2022-11-03 10:53:21,397:INFO:Uploading model into container now
2022-11-03 10:53:21,397:INFO:master_model_container: 31
2022-11-03 10:53:21,397:INFO:display_container: 2
2022-11-03 10:53:21,397:INFO:GradientBoostingRegressor(random_state=4411)
2022-11-03 10:53:21,397:INFO:create_model() successfully completed......................................
2022-11-03 10:53:21,668:WARNING:create_model() for GradientBoostingRegressor(random_state=4411) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:53:21,668:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:53:21,668:INFO:Initializing create_model()
2022-11-03 10:53:21,668:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:53:21,668:INFO:Checking exceptions
2022-11-03 10:53:21,676:INFO:Importing libraries
2022-11-03 10:53:21,676:INFO:Copying training dataset
2022-11-03 10:53:21,684:INFO:Defining folds
2022-11-03 10:53:21,684:INFO:Declaring metric variables
2022-11-03 10:53:21,692:INFO:Importing untrained model
2022-11-03 10:53:21,692:INFO:Gradient Boosting Regressor Imported successfully
2022-11-03 10:53:21,692:INFO:Starting cross validation
2022-11-03 10:53:21,692:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:53:28,116:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-9.49123978e-02 -3.27667021e-02 -3.17736934e-02 -3.00370420e-02
 -2.98976112e-02 -2.31488522e-02 -2.22964421e-02 -2.06006759e-02
 -2.02233435e-02 -1.83831750e-02 -1.80294352e-02 -1.79754313e-02
 -1.75325395e-02 -1.74904482e-02 -1.56372833e-02 -1.52457712e-02
 -1.37688106e-02 -1.35827739e-02 -1.28409592e-02 -1.20342478e-02
 -1.19060431e-02 -1.14871686e-02 -1.14336550e-02 -1.13937037e-02
 -1.09932468e-02 -1.07206980e-02 -1.03667511e-02 -9.74283365e-03
 -9.46511308e-03 -9.19192143e-03 -8.61631963e-03 -8.29902001e-03
 -8.13830796e-03 -7.85458318e-03 -7.76964830e-03 -7.55972145e-03
 -7.38524560e-03 -7.26851182e-03 -7.24197173e-03 -7.04942406e-03
 -6.65792301e-03 -6.52426530e-03 -5.91025394e-03 -5.73168272e-03
 -5.72496050e-03 -5.66027012e-03 -5.52247812e-03 -5.38603153e-03
 -5.22656153e-03 -5.05523994e-03 -4.89328147e-03 -4.65558547e-03
 -3.61015582e-03 -3.42794688e-03 -2.67293941e-03 -1.91372057e-03
 -1.43432982e-03 -1.75071652e-04  3.92227041e-04  4.88602357e-04
  6.73168675e-04  1.20972420e-03  1.90264864e-03  2.84119911e-03
  3.00341476e-03  3.07541952e-03  3.20855596e-03  3.69757020e-03
  3.90231118e-03  4.21727612e-03  4.57108448e-03  5.75428460e-03
  6.31369925e-03  6.50667030e-03  6.53439138e-03  6.66750762e-03
  7.30587726e-03  7.39957575e-03  7.92676578e-03  9.10129029e-03
  9.19564156e-03  9.34357108e-03  9.72679182e-03  1.10280254e-02
  1.12056935e-02  1.12568798e-02  1.12951575e-02  1.14161537e-02
  1.14762462e-02  1.31368091e-02  1.41221201e-02  1.43701022e-02
  1.54528654e-02  1.56733290e-02  1.58193383e-02  1.69870469e-02
  1.69890460e-02  1.86799733e-02  2.24297032e-02  2.24788656e-02
  2.27519583e-02  2.56915466e-02  2.57151857e-02  2.66418014e-02
  2.70981504e-02  2.72445244e-02  2.72630939e-02  2.87482326e-02
  2.88900467e-02  2.90995182e-02  2.99056744e-02  3.02554432e-02
  3.28540666e-02  3.34117187e-02  3.73728890e-02  4.18958636e-02
  4.27670591e-02  4.30306686e-02  4.30483048e-02  4.39810146e-02
  4.39931197e-02  4.42784783e-02  4.50433753e-02  4.69828327e-02
  4.72137704e-02  4.80226866e-02  4.90835320e-02  5.07217246e-02
  5.11534116e-02  5.18229413e-02  5.22583345e-02  5.60209875e-02
  5.61184643e-02  5.86485629e-02  6.35825404e-02  6.42146868e-02
  6.68487932e-02  7.04829751e-02  7.20599509e-02  7.42382535e-02
  7.48814338e-02  7.52319947e-02  7.77096299e-02  8.00600061e-02
  8.04335139e-02  8.19940952e-02  8.22734145e-02  8.23677736e-02
  8.37127143e-02  8.53107420e-02  8.94192369e-02  9.02772131e-02
  9.03217383e-02  9.07250337e-02  9.18453348e-02  9.25617890e-02
  9.29952762e-02  9.32827174e-02  9.41288230e-02  9.48369774e-02
  9.56749241e-02  9.79960298e-02  9.95177352e-02  1.01377597e-01
  1.02113019e-01  1.02728571e-01  1.06110478e-01  1.06422534e-01
  1.08923373e-01  1.09229158e-01  1.10023875e-01  1.11762754e-01
  1.13148737e-01  1.14460308e-01  1.15815177e-01  1.16122228e-01
  1.17656861e-01  1.18011739e-01  1.18994894e-01  1.20248357e-01
  1.21243617e-01  1.21483099e-01  1.23643253e-01  1.24785785e-01
  1.25031314e-01  1.25807721e-01  1.27508636e-01  1.28095285e-01
  1.29128491e-01  1.29267290e-01  1.29306895e-01  1.29688711e-01
  1.29824791e-01  1.30472594e-01  1.31790400e-01  1.32592983e-01
  1.32813966e-01  1.36168846e-01  1.37334206e-01  1.37776708e-01
  1.41012167e-01  1.42044495e-01  1.42711636e-01  1.43124964e-01
  1.43192599e-01  1.43315563e-01  1.43589976e-01  1.43927260e-01
  1.47117410e-01  1.50774481e-01  1.53903281e-01  1.54062201e-01
  1.54816312e-01  1.55229577e-01  1.55350965e-01  1.56719797e-01
  1.57022234e-01  1.57192576e-01  1.62215068e-01  1.63625256e-01
  1.65761107e-01  1.66451688e-01  1.66925941e-01  1.68727452e-01
  1.75782659e-01  1.77164110e-01  1.83364605e-01  1.84693854e-01
  1.85124461e-01  1.86801201e-01  1.87444600e-01  1.87742927e-01
  1.88658112e-01  1.89443420e-01  1.89715786e-01  1.93000006e-01
  1.93983026e-01  1.94004564e-01  1.94629578e-01  1.94888592e-01
  1.96465186e-01  1.97415332e-01  1.98416045e-01  1.99433003e-01
  1.99478863e-01  2.00709098e-01  2.01934361e-01  2.02720807e-01
  2.04937412e-01  2.06171793e-01  2.06563319e-01  2.07430909e-01
  2.09508533e-01  2.11132018e-01  2.12036926e-01  2.12073356e-01
  2.12840553e-01  2.15558943e-01  2.16739723e-01  2.16791885e-01
  2.17797891e-01  2.19261406e-01  2.24659162e-01  2.24778859e-01
  2.27109612e-01  2.29794301e-01  2.30580503e-01  2.31197276e-01
  2.31736121e-01  2.34740553e-01  2.34784112e-01  2.40040179e-01
  2.43215899e-01  2.43777829e-01  2.43886870e-01  2.44727371e-01
  2.46552234e-01  2.46910469e-01  2.49661456e-01  2.50394270e-01
  2.50921606e-01  2.56263657e-01  2.57323985e-01  2.59798935e-01
  2.60098462e-01  2.60501653e-01  2.60709804e-01  2.61323493e-01
  2.61655106e-01  2.64127826e-01  2.67276420e-01  2.67404679e-01
  2.71500489e-01  2.72051051e-01  2.76497580e-01  2.79165549e-01
  2.87119049e-01  2.88174533e-01  2.89415559e-01  2.89931913e-01
  2.92630237e-01  2.94040956e-01  2.97008831e-01  2.97448439e-01
  3.03477435e-01  3.06066923e-01  3.08873323e-01  3.10097803e-01
  3.10841634e-01  3.11499050e-01  3.12697953e-01  3.14339671e-01
  3.15341253e-01  3.16791796e-01  3.19138458e-01  3.22683862e-01
  3.26780335e-01  3.27596835e-01  3.29575067e-01  3.29648632e-01
  3.34308498e-01  3.34727794e-01  3.38779926e-01  3.40868302e-01
  3.43326404e-01  3.47092572e-01  3.51151320e-01  3.53586299e-01
  3.54918388e-01  3.58073705e-01  3.60600557e-01  3.65641091e-01
  3.66096548e-01  3.68282743e-01  3.68665358e-01  3.70715901e-01
  3.73231120e-01  3.76245132e-01  3.80461893e-01  3.81754604e-01
  3.83270375e-01  3.84972996e-01  3.85650343e-01  3.86344276e-01
  3.87089187e-01  3.89500645e-01  3.89554973e-01  3.92749879e-01
  4.00887026e-01  4.02245959e-01  4.06481165e-01  4.08299705e-01
  4.11959822e-01  4.14315791e-01  4.16450170e-01  4.18453288e-01
  4.22403997e-01  4.23851281e-01  4.27768843e-01  4.27910665e-01
  4.28668041e-01  4.30441452e-01  4.31174372e-01  4.32087234e-01
  4.33524840e-01  4.37074556e-01  4.37700783e-01  4.40133339e-01
  4.40871448e-01  4.46350776e-01  4.48058059e-01  4.50800084e-01
  4.51286798e-01  4.52943869e-01  4.53244020e-01  4.56978482e-01
  4.57587231e-01  4.58867718e-01  4.58906942e-01  4.67624480e-01
  4.72256196e-01  4.82326935e-01  4.83156057e-01  4.83940814e-01
  4.85555278e-01  4.86361180e-01  4.91789976e-01  4.92387164e-01
  4.96207398e-01  5.01310960e-01  5.01726329e-01  5.01803239e-01
  5.07704013e-01  5.07974216e-01  5.08874452e-01  5.09644413e-01
  5.14603397e-01  5.21663157e-01  5.21838558e-01  5.22360146e-01
  5.29334752e-01  5.29924309e-01  5.31218059e-01  5.34859836e-01
  5.34954630e-01  5.35848718e-01  5.45974570e-01  5.48869061e-01
  5.49297204e-01  5.58922890e-01  5.60076349e-01  5.61391197e-01
  5.61684503e-01  5.61803163e-01  5.70619834e-01  5.71626712e-01
  5.73235869e-01  5.73575562e-01  5.75819589e-01  5.79669412e-01
  5.86289232e-01  5.98155512e-01  5.98654403e-01  5.99947742e-01
  6.06555547e-01  6.14438366e-01  6.15955868e-01  6.16492245e-01
  6.18676389e-01  6.22818654e-01  6.27083686e-01  6.30883634e-01
  6.34871438e-01  6.39022461e-01  6.40405804e-01  6.49051407e-01
  6.50497188e-01  6.54511488e-01  6.57686445e-01  6.61782225e-01
  6.73432902e-01  6.77895644e-01  6.80898026e-01  6.95344024e-01
  6.97522871e-01  7.04906561e-01  7.05964260e-01  7.11577255e-01
  7.12019109e-01  7.13040735e-01  7.29308738e-01  7.34029605e-01
  7.37883769e-01  7.42138007e-01  7.48220604e-01  7.54842618e-01
  7.55086769e-01  7.55451696e-01  7.60268655e-01  7.60703316e-01
  7.64787021e-01  7.65760538e-01  7.69194050e-01  7.89147878e-01
  7.96068984e-01  8.15349703e-01  8.16085642e-01  8.38206860e-01
  8.40421659e-01  8.40508498e-01  8.42603969e-01  8.47280004e-01
  8.51706560e-01  8.83519701e-01  8.96083194e-01  9.00319472e-01
  9.00585100e-01  9.09820562e-01  9.09869245e-01  9.29009300e-01
  9.41723125e-01  9.57654132e-01  1.07089488e+00]

  warnings.warn(

2022-11-03 10:53:28,211:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.71262366e-02 -3.59516681e-02 -3.27812809e-02 -3.04178737e-02
 -2.89986866e-02 -2.85625335e-02 -2.64010041e-02 -2.63397921e-02
 -2.60381974e-02 -2.32535281e-02 -2.30500745e-02 -2.17788557e-02
 -2.15153916e-02 -1.89696079e-02 -1.88657278e-02 -1.81635773e-02
 -1.73862614e-02 -1.66423268e-02 -1.63468007e-02 -1.59923482e-02
 -1.54651353e-02 -1.52704106e-02 -1.45067031e-02 -1.43298120e-02
 -1.37554807e-02 -1.36930611e-02 -1.14098934e-02 -1.11531855e-02
 -1.09081950e-02 -9.98312142e-03 -9.97345806e-03 -8.95470163e-03
 -8.79318753e-03 -7.84319881e-03 -7.73347966e-03 -7.27167046e-03
 -6.89572553e-03 -6.52433758e-03 -6.22997899e-03 -6.13334206e-03
 -5.82091903e-03 -5.80252224e-03 -4.94271539e-03 -4.64861857e-03
 -4.18295136e-03 -4.02781423e-03 -3.70585318e-03 -2.79719937e-03
 -2.27039074e-03 -1.73042933e-03 -1.63842358e-03 -1.55257019e-03
 -1.12996152e-03 -9.16405114e-04 -7.21542077e-04 -6.53910199e-04
 -2.79136962e-04 -7.80245212e-05  9.26245149e-05  2.81305929e-04
  2.85929183e-04  7.91940464e-04  9.48892787e-04  1.51546880e-03
  1.68891598e-03  2.24438881e-03  2.49835398e-03  2.68232501e-03
  2.69652801e-03  2.78404573e-03  2.95352118e-03  2.98990211e-03
  3.85539099e-03  4.01879230e-03  4.32364086e-03  4.88642204e-03
  5.21096157e-03  5.61080727e-03  5.83145003e-03  5.92224001e-03
  7.40699435e-03  7.40995964e-03  7.81655329e-03  7.92623507e-03
  8.26290549e-03  8.33020347e-03  8.62457322e-03  9.42577601e-03
  1.04253695e-02  1.05807535e-02  1.21768268e-02  1.40099197e-02
  1.50089343e-02  1.59605223e-02  1.70119431e-02  1.74333355e-02
  1.80099335e-02  1.81544702e-02  1.95105811e-02  1.97035443e-02
  2.00324799e-02  2.13446243e-02  2.16135170e-02  2.17204907e-02
  2.18861493e-02  2.23083917e-02  2.26114525e-02  2.29736921e-02
  2.32945557e-02  2.39570668e-02  2.40820319e-02  2.43938720e-02
  2.45395724e-02  2.51856791e-02  2.64821034e-02  2.94613539e-02
  2.98389218e-02  3.06808292e-02  3.17283114e-02  3.26085432e-02
  3.29437639e-02  3.43825325e-02  3.47857762e-02  3.49310336e-02
  3.54558880e-02  3.56894449e-02  3.62295070e-02  3.74324822e-02
  3.76774027e-02  3.92578299e-02  4.00698817e-02  4.01943822e-02
  4.12072651e-02  4.37110621e-02  4.54561703e-02  4.73411962e-02
  4.92252122e-02  5.06587382e-02  5.16552525e-02  5.32968497e-02
  5.34016372e-02  5.34694415e-02  5.52306492e-02  5.60245962e-02
  5.86855896e-02  5.87963552e-02  6.10822073e-02  6.27952605e-02
  6.31720276e-02  6.51004238e-02  6.64032123e-02  6.64799070e-02
  6.85326631e-02  7.10037876e-02  7.29219383e-02  7.73087657e-02
  7.78238879e-02  8.25985159e-02  8.46802189e-02  8.53539959e-02
  8.59835424e-02  8.90794582e-02  9.06911337e-02  9.10185289e-02
  9.18193455e-02  9.26763228e-02  9.32234062e-02  9.35768688e-02
  9.38530369e-02  9.63945970e-02  9.67824157e-02  9.81439993e-02
  9.91269961e-02  1.00743809e-01  1.01470200e-01  1.01707933e-01
  1.02228062e-01  1.04462167e-01  1.04523793e-01  1.05869558e-01
  1.09411201e-01  1.09622053e-01  1.16057602e-01  1.17053947e-01
  1.20458986e-01  1.21265966e-01  1.23371225e-01  1.25725170e-01
  1.27651686e-01  1.28199361e-01  1.31940328e-01  1.33415290e-01
  1.34057288e-01  1.36112437e-01  1.36506808e-01  1.37062041e-01
  1.37224361e-01  1.37353212e-01  1.39233061e-01  1.40717598e-01
  1.40880290e-01  1.40951719e-01  1.42486043e-01  1.42699500e-01
  1.44922721e-01  1.48728104e-01  1.50511639e-01  1.52278614e-01
  1.52638096e-01  1.52650759e-01  1.53029521e-01  1.53218982e-01
  1.53417963e-01  1.53764530e-01  1.58210634e-01  1.58987368e-01
  1.59729522e-01  1.59859915e-01  1.59923764e-01  1.64805758e-01
  1.68222766e-01  1.68557304e-01  1.69813948e-01  1.71402502e-01
  1.73094471e-01  1.73560000e-01  1.76559885e-01  1.79878018e-01
  1.81396155e-01  1.86250163e-01  1.89599530e-01  1.89959366e-01
  1.90063704e-01  1.91178980e-01  1.91539996e-01  1.91620216e-01
  1.93011713e-01  1.94625805e-01  1.97076602e-01  1.98466045e-01
  2.00875287e-01  2.00970421e-01  2.01241308e-01  2.03691231e-01
  2.07152598e-01  2.07883152e-01  2.08345271e-01  2.10714903e-01
  2.12203480e-01  2.15565178e-01  2.15837555e-01  2.22635856e-01
  2.27880809e-01  2.27967933e-01  2.29782165e-01  2.30749164e-01
  2.31040306e-01  2.33452450e-01  2.34695376e-01  2.36288126e-01
  2.37004661e-01  2.37432666e-01  2.38657426e-01  2.40420636e-01
  2.41506013e-01  2.42035826e-01  2.42563045e-01  2.42754467e-01
  2.48441099e-01  2.48982009e-01  2.49672212e-01  2.51039577e-01
  2.51242045e-01  2.51618296e-01  2.52458271e-01  2.52942353e-01
  2.53806164e-01  2.54002139e-01  2.54329091e-01  2.56183217e-01
  2.56220561e-01  2.57941676e-01  2.68261131e-01  2.71384254e-01
  2.71422785e-01  2.73355593e-01  2.74696611e-01  2.75443605e-01
  2.76958794e-01  2.78326962e-01  2.79810009e-01  2.85501393e-01
  2.85508841e-01  2.86427341e-01  2.87384661e-01  2.87937339e-01
  2.90756387e-01  2.92842772e-01  2.93079618e-01  2.93151246e-01
  2.94501540e-01  2.94932991e-01  2.98620910e-01  2.99975864e-01
  3.00637403e-01  3.07024772e-01  3.07303395e-01  3.09820216e-01
  3.10026618e-01  3.10289686e-01  3.11420355e-01  3.11745197e-01
  3.16820796e-01  3.17762067e-01  3.17974047e-01  3.19466898e-01
  3.22178079e-01  3.23957265e-01  3.26864821e-01  3.28184022e-01
  3.28443232e-01  3.31130436e-01  3.31900598e-01  3.36171574e-01
  3.38363586e-01  3.42666508e-01  3.43141817e-01  3.44069103e-01
  3.46315645e-01  3.49075956e-01  3.49938564e-01  3.57122947e-01
  3.57705747e-01  3.60997476e-01  3.63646285e-01  3.64041563e-01
  3.65696201e-01  3.70467741e-01  3.72620787e-01  3.73331204e-01
  3.74824479e-01  3.75934544e-01  3.79370653e-01  3.81481887e-01
  3.82645249e-01  3.83637516e-01  3.87610386e-01  3.91305902e-01
  3.92230037e-01  3.96569960e-01  4.02158187e-01  4.03320797e-01
  4.04136588e-01  4.06425898e-01  4.09255520e-01  4.15151329e-01
  4.17505187e-01  4.18097473e-01  4.19530631e-01  4.21670263e-01
  4.23567024e-01  4.23636624e-01  4.24248790e-01  4.26719705e-01
  4.31090168e-01  4.33264421e-01  4.40082101e-01  4.44163544e-01
  4.45941003e-01  4.48317214e-01  4.48594209e-01  4.50627922e-01
  4.55995618e-01  4.59070664e-01  4.64112918e-01  4.72366859e-01
  4.75444476e-01  4.76585392e-01  4.76830802e-01  4.76954562e-01
  4.78912227e-01  4.90324460e-01  4.91058174e-01  4.92827091e-01
  4.94567605e-01  4.96653979e-01  4.96894278e-01  4.99673579e-01
  5.05695002e-01  5.06429828e-01  5.10568243e-01  5.14098620e-01
  5.14445783e-01  5.14589814e-01  5.15069842e-01  5.15279718e-01
  5.19137975e-01  5.21894200e-01  5.23642107e-01  5.24201823e-01
  5.25569622e-01  5.26278423e-01  5.29791321e-01  5.31313648e-01
  5.34066201e-01  5.34896651e-01  5.37801259e-01  5.39401135e-01
  5.43475963e-01  5.46062164e-01  5.46911599e-01  5.47144735e-01
  5.48051829e-01  5.50384679e-01  5.52231232e-01  5.54980565e-01
  5.55468821e-01  5.55925678e-01  5.69860065e-01  5.74132196e-01
  5.74973825e-01  5.78935220e-01  5.85504465e-01  5.88083766e-01
  5.88287785e-01  5.89340362e-01  5.89807213e-01  5.92195582e-01
  5.92297846e-01  5.95354399e-01  5.96866198e-01  5.98252412e-01
  6.07079942e-01  6.11231635e-01  6.14642335e-01  6.15456802e-01
  6.21923376e-01  6.25358342e-01  6.26894867e-01  6.30000514e-01
  6.36133488e-01  6.42672420e-01  6.46309658e-01  6.47948175e-01
  6.51875524e-01  6.55545391e-01  6.57547454e-01  6.61977128e-01
  6.67476475e-01  6.70461914e-01  6.71595551e-01  6.71678652e-01
  6.74459590e-01  6.75939865e-01  6.99565892e-01  7.04416031e-01
  7.08369014e-01  7.09788639e-01  7.12108647e-01  7.14199641e-01
  7.21522831e-01  7.27094049e-01  7.49866659e-01  7.52499049e-01
  7.52631407e-01  7.53248476e-01  7.59339912e-01  7.69351884e-01
  7.86347979e-01  7.98103251e-01  8.00749016e-01  8.15385270e-01
  8.16810914e-01  8.23222913e-01  8.25265635e-01  8.28494035e-01
  8.39966634e-01  8.40636702e-01  8.45375356e-01  8.49388214e-01
  8.50842536e-01  8.54690975e-01  8.55743230e-01  8.72786186e-01
  8.74065920e-01  8.83495700e-01  8.99223056e-01  9.15499009e-01
  9.43965927e-01  9.84020142e-01]

  warnings.warn(

2022-11-03 10:53:28,227:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.55610438e-02 -3.61790842e-02 -3.17093152e-02 -3.17082090e-02
 -3.13326181e-02 -2.91297222e-02 -2.89802980e-02 -2.79857254e-02
 -2.49986655e-02 -2.42459864e-02 -2.23990646e-02 -2.11044894e-02
 -2.01961686e-02 -1.97009359e-02 -1.82330651e-02 -1.63750397e-02
 -1.63369440e-02 -1.60855429e-02 -1.60755312e-02 -1.60397793e-02
 -1.54447550e-02 -1.47550406e-02 -1.39978239e-02 -1.35108790e-02
 -1.32498732e-02 -1.28216845e-02 -1.23689475e-02 -1.18946583e-02
 -1.15767919e-02 -1.07153438e-02 -1.04905512e-02 -1.02885163e-02
 -9.29951153e-03 -9.07963151e-03 -8.86057324e-03 -8.72229950e-03
 -8.44336764e-03 -8.25576119e-03 -7.72228714e-03 -7.44617665e-03
 -7.22518907e-03 -6.94993032e-03 -6.15279405e-03 -6.02977208e-03
 -5.66743436e-03 -5.06038433e-03 -4.82873315e-03 -4.56132944e-03
 -3.54487323e-03 -3.48586458e-03 -2.74687806e-03 -2.56929489e-03
 -2.08068293e-03 -1.92172940e-03 -1.04785558e-03 -1.03537292e-03
 -9.57013232e-04  5.15646246e-05  1.70638363e-04  4.19508446e-04
  1.51519967e-03  1.55448195e-03  1.60918574e-03  1.76797658e-03
  2.29995250e-03  2.37944542e-03  2.42395874e-03  3.31488860e-03
  3.33097614e-03  3.86268170e-03  4.31864609e-03  4.50414580e-03
  4.75176477e-03  4.96055731e-03  5.28944079e-03  5.34990879e-03
  5.42752646e-03  5.45753076e-03  5.92711268e-03  6.03027006e-03
  6.33483364e-03  7.15162196e-03  7.68028320e-03  7.86528918e-03
  7.91866242e-03  8.19898142e-03  8.33420519e-03  8.45081136e-03
  9.82582223e-03  1.01005450e-02  1.08094481e-02  1.08159901e-02
  1.11234268e-02  1.16593976e-02  1.18326205e-02  1.20266919e-02
  1.22667669e-02  1.46429671e-02  1.48311858e-02  1.48621682e-02
  1.51803076e-02  1.51909779e-02  1.52282000e-02  1.53111399e-02
  1.56718322e-02  1.59861511e-02  1.67700942e-02  1.68748226e-02
  1.83104277e-02  1.85565018e-02  1.87563190e-02  1.88272109e-02
  1.95161276e-02  1.95824659e-02  1.97088146e-02  1.99362470e-02
  2.11766676e-02  2.22889350e-02  2.25487021e-02  2.60029807e-02
  2.61467432e-02  2.68274884e-02  2.69271558e-02  2.74490748e-02
  2.84238796e-02  2.87055748e-02  2.92679644e-02  2.96850700e-02
  3.06329158e-02  3.12913052e-02  3.13495920e-02  3.16059429e-02
  3.19879094e-02  3.21196892e-02  3.38148197e-02  3.39875556e-02
  3.44708298e-02  3.71231413e-02  3.78279317e-02  4.39245769e-02
  4.61377686e-02  4.65738038e-02  4.83250750e-02  4.99536443e-02
  5.00176474e-02  5.02564394e-02  5.03431954e-02  5.07579135e-02
  5.07909394e-02  5.08376928e-02  5.12098048e-02  5.39164540e-02
  5.54904925e-02  5.58127699e-02  5.78043847e-02  5.85816833e-02
  5.95235363e-02  6.03341946e-02  6.14075613e-02  6.16551591e-02
  6.17663170e-02  6.27618728e-02  6.36939139e-02  6.38723736e-02
  6.50119181e-02  6.56789715e-02  6.57632887e-02  6.89345306e-02
  7.46708587e-02  7.52598160e-02  7.65611792e-02  7.96623807e-02
  8.10718723e-02  8.20501856e-02  8.34832332e-02  8.37811958e-02
  8.43329506e-02  8.53080231e-02  8.53089729e-02  8.56463218e-02
  8.61863527e-02  8.65474899e-02  8.65689546e-02  8.71113067e-02
  8.73147380e-02  8.73355000e-02  8.92661256e-02  9.01070681e-02
  9.06325809e-02  9.12093368e-02  9.25631119e-02  9.27913343e-02
  9.31512310e-02  9.76444509e-02  9.86136685e-02  1.00804975e-01
  1.02610486e-01  1.02960238e-01  1.03633679e-01  1.03713966e-01
  1.04249072e-01  1.04552252e-01  1.04571073e-01  1.05917920e-01
  1.07660688e-01  1.08274320e-01  1.11897563e-01  1.12426702e-01
  1.12455582e-01  1.12683754e-01  1.16795339e-01  1.16881740e-01
  1.17056671e-01  1.19035482e-01  1.19869818e-01  1.22466368e-01
  1.23893509e-01  1.24939347e-01  1.25549292e-01  1.28642260e-01
  1.31175339e-01  1.32250060e-01  1.34284946e-01  1.34919721e-01
  1.39346556e-01  1.40558619e-01  1.41047387e-01  1.41470393e-01
  1.44766633e-01  1.45357566e-01  1.45924946e-01  1.46416656e-01
  1.50443142e-01  1.53098139e-01  1.54974785e-01  1.56594938e-01
  1.59289629e-01  1.59610471e-01  1.61826201e-01  1.62909779e-01
  1.63922944e-01  1.64373335e-01  1.64906341e-01  1.67261694e-01
  1.67615694e-01  1.68603377e-01  1.70629375e-01  1.72914635e-01
  1.73574836e-01  1.74617359e-01  1.74637025e-01  1.75908881e-01
  1.76620163e-01  1.77858800e-01  1.78096252e-01  1.79481277e-01
  1.83747470e-01  1.86950358e-01  1.87594095e-01  1.97774402e-01
  1.98938176e-01  2.01100318e-01  2.01247005e-01  2.08475220e-01
  2.09643609e-01  2.10730114e-01  2.13937169e-01  2.14403684e-01
  2.15559439e-01  2.15751381e-01  2.18916518e-01  2.20481672e-01
  2.23150829e-01  2.24268549e-01  2.27176254e-01  2.35924120e-01
  2.36484479e-01  2.37079556e-01  2.38641873e-01  2.38944703e-01
  2.40676893e-01  2.41095515e-01  2.47293801e-01  2.47727381e-01
  2.47986770e-01  2.51510060e-01  2.55976889e-01  2.58090490e-01
  2.58709903e-01  2.61779070e-01  2.63601347e-01  2.65160372e-01
  2.65802755e-01  2.70764275e-01  2.77309521e-01  2.77343787e-01
  2.77576575e-01  2.81936368e-01  2.84012572e-01  2.84250794e-01
  2.84345743e-01  2.86937939e-01  2.88164419e-01  2.89269930e-01
  2.90085333e-01  2.92043710e-01  2.93190760e-01  2.98994704e-01
  3.00056922e-01  3.00410695e-01  3.00731646e-01  3.01826949e-01
  3.04394169e-01  3.08319093e-01  3.11724564e-01  3.18461568e-01
  3.21750763e-01  3.24299647e-01  3.24337581e-01  3.24877371e-01
  3.25662393e-01  3.26399830e-01  3.27825428e-01  3.27891290e-01
  3.29062558e-01  3.32295935e-01  3.34150846e-01  3.34268864e-01
  3.35632077e-01  3.37973780e-01  3.39447088e-01  3.39744186e-01
  3.41080518e-01  3.44063216e-01  3.48468224e-01  3.49077276e-01
  3.49621326e-01  3.49644827e-01  3.53706280e-01  3.55366410e-01
  3.56642364e-01  3.61553686e-01  3.62081347e-01  3.62976805e-01
  3.72221432e-01  3.73433106e-01  3.75597358e-01  3.78885471e-01
  3.79748027e-01  3.85220802e-01  3.86753646e-01  3.90265535e-01
  3.90978159e-01  3.96196944e-01  3.96902211e-01  3.98156346e-01
  3.98499122e-01  3.98882565e-01  3.99562898e-01  4.01154433e-01
  4.09168289e-01  4.13417811e-01  4.15746901e-01  4.20032164e-01
  4.21840886e-01  4.22488083e-01  4.26958942e-01  4.29042295e-01
  4.30214558e-01  4.32213939e-01  4.34891871e-01  4.37810635e-01
  4.39148688e-01  4.39242368e-01  4.43266282e-01  4.44031636e-01
  4.46887751e-01  4.49592825e-01  4.50145544e-01  4.52609923e-01
  4.54227569e-01  4.58111124e-01  4.66966493e-01  4.74564910e-01
  4.75688112e-01  4.79092989e-01  4.79330627e-01  4.86731830e-01
  4.87755816e-01  4.97468111e-01  4.98568120e-01  5.00369074e-01
  5.01026964e-01  5.01524064e-01  5.01724205e-01  5.04062700e-01
  5.09088777e-01  5.15740589e-01  5.15742921e-01  5.15985572e-01
  5.16130680e-01  5.27073025e-01  5.32223160e-01  5.32594900e-01
  5.40724643e-01  5.44991058e-01  5.52500643e-01  5.53702039e-01
  5.55730688e-01  5.56728334e-01  5.64037242e-01  5.65291864e-01
  5.67269489e-01  5.68457890e-01  5.70966321e-01  5.71935653e-01
  5.72683322e-01  5.72992277e-01  5.73115093e-01  5.74450324e-01
  5.80986578e-01  5.81041629e-01  5.81347119e-01  5.83631724e-01
  5.84509925e-01  5.91538646e-01  5.94101714e-01  5.96603516e-01
  5.97875653e-01  6.00153753e-01  6.00850349e-01  6.04213216e-01
  6.05994850e-01  6.06091904e-01  6.16091263e-01  6.18553928e-01
  6.19386808e-01  6.27832018e-01  6.31062340e-01  6.34576234e-01
  6.34825474e-01  6.40269749e-01  6.45619693e-01  6.46226221e-01
  6.47483175e-01  6.48680796e-01  6.58243133e-01  6.65719316e-01
  6.65969831e-01  6.70091369e-01  6.71662798e-01  6.72839528e-01
  6.80735186e-01  6.81828725e-01  6.82058292e-01  6.90186543e-01
  6.93620262e-01  6.98739012e-01  7.00490709e-01  7.07986084e-01
  7.11681086e-01  7.16045875e-01  7.17169023e-01  7.23463191e-01
  7.26118455e-01  7.32912492e-01  7.40823983e-01  7.44759037e-01
  7.49488842e-01  7.51741734e-01  7.53581929e-01  7.54734093e-01
  7.57827657e-01  7.66188094e-01  7.66732194e-01  7.72623696e-01
  7.87767389e-01  8.07992440e-01  8.24334124e-01  8.24722326e-01
  8.41251346e-01  8.60862661e-01  8.65351394e-01  8.76383777e-01
  8.76538223e-01  8.77073071e-01  9.03606074e-01  9.68535084e-01]

  warnings.warn(

2022-11-03 10:53:28,246:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-5.57334486e-02 -3.21444891e-02 -3.17932888e-02 -3.10696045e-02
 -2.34577059e-02 -2.21080166e-02 -2.19552316e-02 -2.17879470e-02
 -2.14573577e-02 -2.12354998e-02 -1.92985703e-02 -1.73816260e-02
 -1.71598470e-02 -1.67765895e-02 -1.65281002e-02 -1.59570328e-02
 -1.58318832e-02 -1.48045672e-02 -1.43400433e-02 -1.42527266e-02
 -1.41673658e-02 -1.39080003e-02 -1.34817275e-02 -1.25291588e-02
 -1.24506773e-02 -1.23433924e-02 -1.23422089e-02 -1.09305118e-02
 -1.06977753e-02 -1.01749886e-02 -8.02736351e-03 -7.72324535e-03
 -7.66486880e-03 -7.56844399e-03 -6.37550952e-03 -5.99735744e-03
 -5.95819791e-03 -5.49309836e-03 -4.86907471e-03 -4.30555686e-03
 -3.42455134e-03 -3.18404644e-03 -3.01973171e-03 -2.01914833e-03
 -1.99877874e-03 -1.60939457e-03 -9.69800218e-04 -4.86484306e-04
 -3.32938154e-04 -6.21307417e-05  1.63483820e-03  1.85290655e-03
  2.25102817e-03  2.33719103e-03  2.52601536e-03  2.66213681e-03
  2.81326411e-03  2.87847145e-03  4.77186811e-03  4.90390009e-03
  4.91642962e-03  5.30973489e-03  6.35885231e-03  6.95510735e-03
  7.06390560e-03  7.12540113e-03  7.19533617e-03  7.78501184e-03
  7.81253195e-03  8.10747063e-03  8.56508977e-03  9.58963663e-03
  9.78625156e-03  9.85131931e-03  1.11851106e-02  1.13501202e-02
  1.26081744e-02  1.27571005e-02  1.38720967e-02  1.47919483e-02
  1.70410898e-02  1.85202108e-02  1.85536034e-02  1.89487771e-02
  2.13147945e-02  2.27028292e-02  2.49065307e-02  2.50677506e-02
  2.51415963e-02  2.57745620e-02  2.63262297e-02  2.70652726e-02
  2.72463856e-02  2.81292400e-02  2.88001504e-02  2.89395303e-02
  3.00229739e-02  3.00941798e-02  3.09661953e-02  3.14215304e-02
  3.37011664e-02  3.39476409e-02  3.45965562e-02  3.53945128e-02
  3.60665680e-02  3.63082977e-02  3.66088448e-02  3.77625202e-02
  3.79210869e-02  3.82697158e-02  3.83899356e-02  3.87004874e-02
  3.97059881e-02  3.99852871e-02  4.07123479e-02  4.12048151e-02
  4.22938144e-02  4.30138653e-02  4.32246988e-02  4.37207046e-02
  4.42426454e-02  4.88883293e-02  4.97435928e-02  5.04306601e-02
  5.08077006e-02  5.16495504e-02  5.19580472e-02  5.28852150e-02
  5.30700297e-02  5.47446809e-02  5.57227866e-02  5.69774743e-02
  5.70744403e-02  5.76037640e-02  5.85043649e-02  5.87268943e-02
  5.97249689e-02  6.03925921e-02  6.11264402e-02  6.11908810e-02
  6.15588218e-02  6.17919513e-02  6.18461438e-02  6.22048045e-02
  6.34594181e-02  6.34942128e-02  6.35492885e-02  6.40842593e-02
  6.50395812e-02  6.54467929e-02  6.63774248e-02  6.68085206e-02
  6.80935429e-02  7.00712895e-02  7.07254810e-02  7.12538731e-02
  7.13869661e-02  7.27629175e-02  7.30044067e-02  7.32852588e-02
  7.44645180e-02  7.49901586e-02  7.65666472e-02  7.69278479e-02
  7.75773702e-02  7.75981455e-02  8.00409341e-02  8.31929657e-02
  8.34116247e-02  8.58113077e-02  8.66276786e-02  8.74419377e-02
  8.80395665e-02  8.83718274e-02  8.95205483e-02  9.00945070e-02
  9.15098450e-02  9.22291577e-02  9.29793004e-02  9.34283707e-02
  9.34823413e-02  9.41916369e-02  9.66846783e-02  9.71213758e-02
  9.79641785e-02  9.85480643e-02  1.00163260e-01  1.00979079e-01
  1.01627244e-01  1.03482671e-01  1.03707045e-01  1.04611661e-01
  1.07194169e-01  1.08628899e-01  1.08895532e-01  1.10275502e-01
  1.12237332e-01  1.14990078e-01  1.18730048e-01  1.19457729e-01
  1.21092548e-01  1.21540944e-01  1.21691314e-01  1.22218807e-01
  1.23705110e-01  1.23940911e-01  1.24887983e-01  1.26967728e-01
  1.29161209e-01  1.30938881e-01  1.32006956e-01  1.32046989e-01
  1.36635391e-01  1.37484201e-01  1.40048563e-01  1.42832166e-01
  1.43596221e-01  1.44986186e-01  1.46419762e-01  1.46999664e-01
  1.51258291e-01  1.53126154e-01  1.54396692e-01  1.54801102e-01
  1.55587919e-01  1.56162538e-01  1.56308970e-01  1.57872596e-01
  1.59434095e-01  1.59671206e-01  1.60156847e-01  1.60523739e-01
  1.63072532e-01  1.63452230e-01  1.63772688e-01  1.63837390e-01
  1.65381718e-01  1.66676204e-01  1.67781651e-01  1.69573308e-01
  1.70715282e-01  1.75060684e-01  1.75135695e-01  1.75929246e-01
  1.76943906e-01  1.77999468e-01  1.79013205e-01  1.79083899e-01
  1.79115991e-01  1.82611394e-01  1.87850468e-01  1.90856554e-01
  1.92852434e-01  1.93070256e-01  1.93264563e-01  1.95623017e-01
  1.96647417e-01  1.97425436e-01  1.98248494e-01  2.00446066e-01
  2.02533215e-01  2.03280673e-01  2.03441426e-01  2.03770208e-01
  2.04417858e-01  2.08575877e-01  2.09424806e-01  2.11206386e-01
  2.11859197e-01  2.12518596e-01  2.13042854e-01  2.14508137e-01
  2.17109154e-01  2.19307343e-01  2.19688886e-01  2.21239771e-01
  2.22838408e-01  2.23897727e-01  2.26500355e-01  2.26579192e-01
  2.28541233e-01  2.28776606e-01  2.29477837e-01  2.29691851e-01
  2.32206980e-01  2.32951506e-01  2.35412749e-01  2.37774896e-01
  2.43162437e-01  2.45768359e-01  2.45978492e-01  2.48217161e-01
  2.49101504e-01  2.51463785e-01  2.51912314e-01  2.54557638e-01
  2.54703888e-01  2.60129955e-01  2.66407555e-01  2.66908340e-01
  2.67317666e-01  2.69340122e-01  2.72884907e-01  2.72911162e-01
  2.74252712e-01  2.76243577e-01  2.79794030e-01  2.81069669e-01
  2.81379231e-01  2.83300799e-01  2.83447584e-01  2.83448430e-01
  2.83860730e-01  2.92306523e-01  2.92818099e-01  2.94994937e-01
  2.95999624e-01  2.97976803e-01  2.98235265e-01  3.01027118e-01
  3.02880715e-01  3.04627273e-01  3.05865496e-01  3.06624808e-01
  3.08697218e-01  3.11899923e-01  3.12561508e-01  3.14465849e-01
  3.14763531e-01  3.18598430e-01  3.21836792e-01  3.23354902e-01
  3.24631417e-01  3.25937150e-01  3.25946300e-01  3.26450447e-01
  3.28729289e-01  3.29823687e-01  3.30138138e-01  3.32538786e-01
  3.33622347e-01  3.34044928e-01  3.34859608e-01  3.35665063e-01
  3.48203272e-01  3.49171516e-01  3.49764452e-01  3.56587057e-01
  3.62314269e-01  3.62691099e-01  3.63502989e-01  3.64076227e-01
  3.64816267e-01  3.65006334e-01  3.68871457e-01  3.69997908e-01
  3.72043566e-01  3.75089014e-01  3.77196870e-01  3.78887364e-01
  3.79145841e-01  3.80199327e-01  3.83945322e-01  3.86267479e-01
  3.86787327e-01  3.87406529e-01  3.88637790e-01  3.89214887e-01
  3.91481767e-01  4.00065568e-01  4.01247690e-01  4.02729465e-01
  4.05472198e-01  4.08760656e-01  4.11132734e-01  4.15036579e-01
  4.17643837e-01  4.18734082e-01  4.23613011e-01  4.25934556e-01
  4.28942644e-01  4.31989051e-01  4.34833202e-01  4.38617783e-01
  4.43747557e-01  4.44019231e-01  4.48976407e-01  4.50437144e-01
  4.57346706e-01  4.57558351e-01  4.62986004e-01  4.63588119e-01
  4.64519433e-01  4.70508808e-01  4.71174579e-01  4.73093360e-01
  4.73307051e-01  4.75379075e-01  4.81191549e-01  4.82143239e-01
  4.83235725e-01  4.85810951e-01  4.87021459e-01  4.89629564e-01
  4.94036771e-01  4.98086320e-01  4.98348092e-01  4.99408181e-01
  5.00967081e-01  5.02950803e-01  5.05657756e-01  5.06720160e-01
  5.07363829e-01  5.11986679e-01  5.17519527e-01  5.19027313e-01
  5.23024315e-01  5.23562997e-01  5.23685743e-01  5.26354360e-01
  5.29655739e-01  5.70364476e-01  5.72751885e-01  5.73047342e-01
  5.81068668e-01  5.83394988e-01  5.86305460e-01  6.02285671e-01
  6.03538551e-01  6.04591317e-01  6.08896771e-01  6.13090097e-01
  6.13482869e-01  6.17311675e-01  6.19927466e-01  6.20424184e-01
  6.24014641e-01  6.24112232e-01  6.30221097e-01  6.31445328e-01
  6.32225144e-01  6.33215820e-01  6.38439225e-01  6.41559690e-01
  6.41682525e-01  6.44665203e-01  6.47118410e-01  6.49185286e-01
  6.53122825e-01  6.53868112e-01  6.59087406e-01  6.60682845e-01
  6.72891762e-01  6.73598638e-01  6.76455056e-01  6.82082787e-01
  6.83573508e-01  6.84143752e-01  6.90649865e-01  6.94252848e-01
  6.99065579e-01  7.02355026e-01  7.11878958e-01  7.13727595e-01
  7.14964875e-01  7.20586000e-01  7.21612181e-01  7.29500729e-01
  7.34311497e-01  7.34371806e-01  7.35220001e-01  7.35385470e-01
  7.41224006e-01  7.70195208e-01  7.71122862e-01  7.72895654e-01
  7.78051727e-01  7.82520171e-01  7.89157578e-01  8.14551750e-01
  8.15502556e-01  8.21850434e-01  8.31091711e-01  8.39609394e-01
  8.56562393e-01  9.10532442e-01  9.32244723e-01  9.41499689e-01
  9.48849093e-01  9.53867180e-01  9.58697470e-01  9.68744746e-01]

  warnings.warn(

2022-11-03 10:53:28,282:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-3.55145608e-02 -2.91982978e-02 -2.71880420e-02 -2.27656383e-02
 -2.18616118e-02 -2.09314821e-02 -1.92367744e-02 -1.91363411e-02
 -1.83760305e-02 -1.74578508e-02 -1.73951503e-02 -1.60529435e-02
 -1.54755903e-02 -1.50310622e-02 -1.33030426e-02 -1.31720668e-02
 -1.29206022e-02 -1.28678764e-02 -1.22899749e-02 -1.20119241e-02
 -1.19385410e-02 -1.19199328e-02 -1.14735034e-02 -1.06291659e-02
 -1.05092650e-02 -1.03087640e-02 -9.65253051e-03 -8.75826200e-03
 -7.05078383e-03 -6.08140159e-03 -5.83433623e-03 -5.49948594e-03
 -4.47844748e-03 -4.25056577e-03 -4.14954932e-03 -4.10933771e-03
 -4.01334597e-03 -3.59775051e-03 -3.43313570e-03 -3.29587094e-03
 -3.21164794e-03 -3.11498378e-03 -2.14052095e-03 -1.84549356e-03
 -1.69130175e-03 -4.52181971e-04  5.16124012e-05  4.08133731e-04
  6.67333104e-04  1.02324006e-03  1.36986672e-03  1.58830983e-03
  1.76328778e-03  2.56778051e-03  2.79482118e-03  2.89872106e-03
  2.98252691e-03  3.38459904e-03  3.41920244e-03  3.67260591e-03
  3.86339067e-03  4.02768167e-03  4.24727070e-03  4.37975616e-03
  5.74106360e-03  5.81428666e-03  5.84743114e-03  5.90250673e-03
  6.72682547e-03  6.75196224e-03  7.07451699e-03  7.41882666e-03
  7.47864710e-03  7.63298655e-03  7.71219820e-03  7.92210285e-03
  7.99393125e-03  8.19300726e-03  8.41616831e-03  8.47390166e-03
  9.18165689e-03  9.32900816e-03  1.04608094e-02  1.05153273e-02
  1.05849047e-02  1.07518919e-02  1.08862436e-02  1.09021965e-02
  1.12926599e-02  1.14081275e-02  1.16605216e-02  1.32118164e-02
  1.44836615e-02  1.51162398e-02  1.52999333e-02  1.55081134e-02
  1.55508127e-02  1.60119078e-02  1.62369491e-02  1.64592217e-02
  1.74180686e-02  2.17185636e-02  2.30832643e-02  2.34116879e-02
  2.47646698e-02  2.59180766e-02  2.65311252e-02  2.68503784e-02
  2.74261642e-02  2.83302934e-02  3.20815615e-02  3.21150482e-02
  3.24377523e-02  3.27236071e-02  3.29077694e-02  3.29737752e-02
  3.40815197e-02  3.51193273e-02  3.76458688e-02  4.05897539e-02
  4.07984484e-02  4.13435874e-02  4.14986217e-02  4.43350579e-02
  4.71804303e-02  4.73142632e-02  4.79739454e-02  4.92706041e-02
  4.95237463e-02  5.08269743e-02  5.15317674e-02  5.32611824e-02
  5.38275201e-02  5.44016386e-02  5.66956944e-02  5.76845424e-02
  5.81213929e-02  5.92365183e-02  6.06275900e-02  6.07552296e-02
  6.16491901e-02  6.48274262e-02  6.49431789e-02  6.50736319e-02
  6.68694884e-02  6.82794472e-02  6.98097931e-02  7.17354654e-02
  7.29999247e-02  7.58244269e-02  7.62105308e-02  7.66181745e-02
  7.66570259e-02  7.73488601e-02  7.74017565e-02  7.85250711e-02
  8.00561839e-02  8.36774399e-02  8.43038114e-02  8.43683571e-02
  8.46819322e-02  8.55601651e-02  8.58806080e-02  8.78771630e-02
  8.99904924e-02  9.03162187e-02  9.08609981e-02  9.09792862e-02
  9.18165208e-02  9.18954164e-02  9.24252785e-02  9.36946423e-02
  9.38220263e-02  9.64003132e-02  9.76758563e-02  9.78821563e-02
  9.86677850e-02  9.90512700e-02  1.00593328e-01  1.02023388e-01
  1.02330031e-01  1.02553126e-01  1.04247574e-01  1.05869783e-01
  1.07702195e-01  1.08282855e-01  1.10684321e-01  1.11691412e-01
  1.12647181e-01  1.12836602e-01  1.15102417e-01  1.15424516e-01
  1.17497366e-01  1.25778562e-01  1.26036513e-01  1.26982081e-01
  1.31565959e-01  1.32167580e-01  1.32867621e-01  1.32966020e-01
  1.33886685e-01  1.34282943e-01  1.35133813e-01  1.37679344e-01
  1.39307628e-01  1.43456956e-01  1.43457704e-01  1.44677255e-01
  1.46944773e-01  1.48589935e-01  1.50049778e-01  1.51350022e-01
  1.55990059e-01  1.56505178e-01  1.56620203e-01  1.58344579e-01
  1.59626715e-01  1.59660893e-01  1.60228863e-01  1.60720173e-01
  1.60947023e-01  1.62399750e-01  1.70523714e-01  1.74570895e-01
  1.76775654e-01  1.80480238e-01  1.81596909e-01  1.84048495e-01
  1.88442911e-01  1.89810821e-01  1.93025034e-01  1.94439197e-01
  1.95827244e-01  1.96769262e-01  1.98581842e-01  1.98995883e-01
  2.01126461e-01  2.01509815e-01  2.01536057e-01  2.02334133e-01
  2.02765129e-01  2.03172773e-01  2.03692246e-01  2.03933514e-01
  2.09192574e-01  2.11319595e-01  2.12768153e-01  2.16057137e-01
  2.17160912e-01  2.17830544e-01  2.18624779e-01  2.21503224e-01
  2.22382088e-01  2.26533314e-01  2.28235535e-01  2.29635230e-01
  2.29745189e-01  2.33564010e-01  2.37694635e-01  2.38114680e-01
  2.40230518e-01  2.41740737e-01  2.42388074e-01  2.45073019e-01
  2.47657072e-01  2.47849378e-01  2.48088315e-01  2.50386403e-01
  2.53077299e-01  2.53252639e-01  2.53342233e-01  2.56173758e-01
  2.56899309e-01  2.57652154e-01  2.61263272e-01  2.62397624e-01
  2.64068727e-01  2.66947272e-01  2.70341814e-01  2.77256705e-01
  2.77905203e-01  2.78961909e-01  2.79548394e-01  2.80499463e-01
  2.82087513e-01  2.82455755e-01  2.83954643e-01  2.85008634e-01
  2.85887594e-01  2.86285324e-01  2.86287944e-01  2.88781279e-01
  2.88785679e-01  2.89593722e-01  2.90000864e-01  2.91207913e-01
  2.91810694e-01  2.93854579e-01  2.95156742e-01  2.98525942e-01
  3.01773013e-01  3.02698265e-01  3.05121235e-01  3.06208909e-01
  3.09055017e-01  3.11004046e-01  3.11915274e-01  3.18630821e-01
  3.18708257e-01  3.21049380e-01  3.25254472e-01  3.26068261e-01
  3.27788938e-01  3.29388113e-01  3.30227137e-01  3.31288939e-01
  3.31781385e-01  3.33039487e-01  3.33389481e-01  3.39126527e-01
  3.39306040e-01  3.39775232e-01  3.39775849e-01  3.47202761e-01
  3.51150770e-01  3.52757895e-01  3.53608212e-01  3.54989334e-01
  3.58448940e-01  3.58923406e-01  3.60064640e-01  3.63537056e-01
  3.64234950e-01  3.69199172e-01  3.74521219e-01  3.77023655e-01
  3.82000918e-01  3.82262714e-01  3.85405924e-01  3.86044917e-01
  3.89693136e-01  3.91553350e-01  3.94488321e-01  3.97058095e-01
  3.99152432e-01  4.00726342e-01  4.02014108e-01  4.04769638e-01
  4.04948519e-01  4.07971564e-01  4.10634267e-01  4.15476185e-01
  4.16541498e-01  4.16832739e-01  4.21547449e-01  4.22428375e-01
  4.22960361e-01  4.24499387e-01  4.26487361e-01  4.31783128e-01
  4.33819055e-01  4.34363418e-01  4.38045433e-01  4.41882599e-01
  4.43815404e-01  4.46164262e-01  4.47188624e-01  4.49981880e-01
  4.57008878e-01  4.57971617e-01  4.63357990e-01  4.66829370e-01
  4.70860381e-01  4.78797580e-01  4.79972735e-01  4.80828941e-01
  4.81165446e-01  4.85536128e-01  4.89220124e-01  4.90442703e-01
  4.90662684e-01  4.91471648e-01  4.95854640e-01  4.98524697e-01
  4.98941173e-01  5.03329551e-01  5.03390258e-01  5.03677231e-01
  5.05762120e-01  5.06574947e-01  5.08051494e-01  5.16100399e-01
  5.17919258e-01  5.18298494e-01  5.19102165e-01  5.21686733e-01
  5.23432318e-01  5.25384669e-01  5.27965895e-01  5.28443420e-01
  5.33919208e-01  5.35685011e-01  5.37815643e-01  5.38764370e-01
  5.41020102e-01  5.45384916e-01  5.48071222e-01  5.50719630e-01
  5.51177321e-01  5.55359545e-01  5.55950944e-01  5.58503722e-01
  5.58894425e-01  5.62586592e-01  5.64014332e-01  5.71489086e-01
  5.74260390e-01  5.75460658e-01  5.81813529e-01  5.86017007e-01
  5.89256556e-01  5.90416671e-01  5.92351583e-01  5.96661416e-01
  6.00496748e-01  6.00985118e-01  6.05166458e-01  6.07611933e-01
  6.16694649e-01  6.17253740e-01  6.21943318e-01  6.23768988e-01
  6.41306358e-01  6.42591224e-01  6.48773452e-01  6.52785978e-01
  6.62326100e-01  6.66962378e-01  6.67467134e-01  6.67761141e-01
  6.69597816e-01  6.72021253e-01  6.72651134e-01  6.75730407e-01
  6.76959766e-01  6.77896179e-01  6.87310734e-01  6.89590077e-01
  6.96215452e-01  7.02056451e-01  7.07958609e-01  7.10814638e-01
  7.13894462e-01  7.16499106e-01  7.18006050e-01  7.19321566e-01
  7.19361377e-01  7.21803562e-01  7.22009200e-01  7.22497880e-01
  7.26306371e-01  7.31972109e-01  7.41019515e-01  7.43579637e-01
  7.44506403e-01  7.46043902e-01  7.47354934e-01  7.52955416e-01
  7.57767742e-01  7.62032453e-01  7.65556509e-01  7.67856264e-01
  7.68225342e-01  7.77136294e-01  7.82542080e-01  7.90878807e-01
  7.97183482e-01  8.01924901e-01  8.08118418e-01  8.15802229e-01
  8.43960700e-01  8.44718725e-01  8.83668949e-01  8.84377977e-01
  8.89268541e-01  8.91153775e-01  9.10560122e-01  9.28116275e-01
  9.61122736e-01]

  warnings.warn(

2022-11-03 10:53:28,291:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.47294645e-02 -3.58878569e-02 -3.50408104e-02 -3.48233320e-02
 -3.42484349e-02 -3.22777280e-02 -3.08318540e-02 -3.03121698e-02
 -2.88210798e-02 -2.45734726e-02 -2.43794929e-02 -2.41583224e-02
 -2.25372000e-02 -2.24558871e-02 -2.16777017e-02 -2.08343171e-02
 -1.87631715e-02 -1.80626119e-02 -1.80311935e-02 -1.78047175e-02
 -1.73109089e-02 -1.70340698e-02 -1.66296468e-02 -1.64792139e-02
 -1.58186206e-02 -1.55105238e-02 -1.43807146e-02 -1.33707411e-02
 -1.33057012e-02 -1.11357375e-02 -1.07718542e-02 -1.05405691e-02
 -1.05241236e-02 -9.76223439e-03 -9.53616860e-03 -7.82559169e-03
 -6.49330473e-03 -6.10783495e-03 -5.66466116e-03 -5.32067944e-03
 -4.69618739e-03 -4.57115964e-03 -4.06081150e-03 -3.82809394e-03
 -3.38860853e-03 -3.35223963e-03 -3.15764357e-03 -2.99494800e-03
 -2.99454108e-03 -2.07535618e-03 -1.99895730e-03 -1.76173064e-03
 -1.62333848e-03 -1.49070203e-03 -1.24739811e-03 -1.00346889e-03
 -5.43176969e-04 -5.22498111e-04 -1.27142042e-04  1.00202624e-04
  5.83760588e-04  8.39229540e-04  1.20230407e-03  1.61002459e-03
  2.52549743e-03  2.54685122e-03  2.95238561e-03  3.22771239e-03
  3.47367006e-03  4.06856165e-03  4.35401142e-03  5.58337255e-03
  6.21008502e-03  7.07766980e-03  7.31379094e-03  7.89217648e-03
  7.89997817e-03  7.91211431e-03  7.98392614e-03  8.06845250e-03
  8.78104867e-03  1.00604944e-02  1.01488053e-02  1.08576717e-02
  1.19151160e-02  1.20290907e-02  1.25747125e-02  1.31931468e-02
  1.32938218e-02  1.37389266e-02  1.42023525e-02  1.49345743e-02
  1.55959751e-02  1.56177464e-02  1.57559673e-02  1.60873516e-02
  1.64706776e-02  1.67132709e-02  1.70627211e-02  1.80423577e-02
  1.86360790e-02  1.94537849e-02  1.98399340e-02  2.03803609e-02
  2.10210741e-02  2.16792609e-02  2.36646673e-02  2.43750914e-02
  2.44779315e-02  2.49834469e-02  2.50192915e-02  2.55070963e-02
  2.64369200e-02  2.65050104e-02  2.66410549e-02  2.68734043e-02
  2.85440195e-02  2.94352108e-02  2.94957742e-02  3.36084699e-02
  3.57955371e-02  3.64976332e-02  3.66150166e-02  3.97766486e-02
  3.98972101e-02  3.99928054e-02  4.01731775e-02  4.15470526e-02
  4.28085427e-02  4.66233409e-02  4.99322587e-02  5.01063601e-02
  5.17531957e-02  5.21682365e-02  5.23748941e-02  5.25716564e-02
  5.33527258e-02  5.41074075e-02  5.56472667e-02  5.75668037e-02
  5.85328194e-02  6.04263978e-02  6.14658754e-02  6.20378655e-02
  6.47410881e-02  6.49834122e-02  6.71808039e-02  6.72755313e-02
  6.73161819e-02  6.92445119e-02  7.12615019e-02  7.15555163e-02
  7.28274585e-02  7.40341827e-02  7.75793585e-02  7.90467125e-02
  7.90709981e-02  7.91523352e-02  8.07667437e-02  8.09314639e-02
  8.11226340e-02  8.19202985e-02  8.24809369e-02  8.45965591e-02
  8.77317607e-02  8.83199780e-02  8.86086582e-02  8.94467430e-02
  9.04304018e-02  9.10982485e-02  9.55736517e-02  9.56090505e-02
  9.63407005e-02  9.64101283e-02  9.82638064e-02  9.88479675e-02
  9.91776404e-02  1.00487507e-01  1.00521000e-01  1.00657736e-01
  1.00994344e-01  1.02309278e-01  1.02431433e-01  1.04244883e-01
  1.05075789e-01  1.06881992e-01  1.07994650e-01  1.13812564e-01
  1.14298539e-01  1.14495894e-01  1.19669483e-01  1.20304207e-01
  1.21282724e-01  1.23972947e-01  1.25361049e-01  1.28523354e-01
  1.29687438e-01  1.29863269e-01  1.31536107e-01  1.37750914e-01
  1.37783108e-01  1.38497907e-01  1.38795226e-01  1.41347247e-01
  1.42065619e-01  1.43023822e-01  1.43154510e-01  1.43268770e-01
  1.44662143e-01  1.46463591e-01  1.48917626e-01  1.49920020e-01
  1.49969901e-01  1.51804702e-01  1.54636235e-01  1.56083821e-01
  1.59334427e-01  1.60162743e-01  1.60803533e-01  1.64726806e-01
  1.65163191e-01  1.65215379e-01  1.67139077e-01  1.68807790e-01
  1.69069366e-01  1.70810791e-01  1.70885677e-01  1.71071690e-01
  1.71367985e-01  1.74767530e-01  1.78230799e-01  1.78639161e-01
  1.78718625e-01  1.79922572e-01  1.80214179e-01  1.80635085e-01
  1.81055665e-01  1.82929275e-01  1.83431621e-01  1.87523690e-01
  1.92676428e-01  1.92913996e-01  1.93097345e-01  1.98348872e-01
  2.00901213e-01  2.01147177e-01  2.06221957e-01  2.06645666e-01
  2.07039551e-01  2.09901606e-01  2.10555672e-01  2.15003909e-01
  2.15581647e-01  2.17004993e-01  2.18206553e-01  2.18283509e-01
  2.23974351e-01  2.24855083e-01  2.26415125e-01  2.27637212e-01
  2.28418834e-01  2.29219926e-01  2.30063259e-01  2.30165166e-01
  2.32758079e-01  2.33763899e-01  2.33958763e-01  2.34429532e-01
  2.34906107e-01  2.37778943e-01  2.38799168e-01  2.39121024e-01
  2.42686539e-01  2.43143932e-01  2.45178158e-01  2.46634101e-01
  2.47300604e-01  2.47561926e-01  2.48060751e-01  2.48097299e-01
  2.49339850e-01  2.49444918e-01  2.51301082e-01  2.51778279e-01
  2.52841656e-01  2.53055679e-01  2.53922441e-01  2.54087891e-01
  2.55645159e-01  2.55673587e-01  2.55786436e-01  2.56987232e-01
  2.58858268e-01  2.59080576e-01  2.61172852e-01  2.61477007e-01
  2.64349383e-01  2.66689691e-01  2.68802961e-01  2.70934490e-01
  2.71033737e-01  2.71929051e-01  2.76539595e-01  2.77770355e-01
  2.78391861e-01  2.79626044e-01  2.80064563e-01  2.80873674e-01
  2.81415167e-01  2.82229259e-01  2.83619343e-01  2.84254240e-01
  2.85204598e-01  2.86435552e-01  2.92297622e-01  2.94401586e-01
  2.97160911e-01  2.99199349e-01  3.01097424e-01  3.01628189e-01
  3.12936527e-01  3.14614572e-01  3.19518121e-01  3.26755499e-01
  3.29250239e-01  3.36340887e-01  3.37614136e-01  3.37640675e-01
  3.38208129e-01  3.38663536e-01  3.41153838e-01  3.43357707e-01
  3.45823458e-01  3.47012390e-01  3.47466741e-01  3.48655306e-01
  3.52653412e-01  3.55553419e-01  3.57467270e-01  3.58063861e-01
  3.59186203e-01  3.63269450e-01  3.73085739e-01  3.76686568e-01
  3.78385314e-01  3.83575972e-01  3.83883832e-01  3.85306494e-01
  3.87097219e-01  3.89014673e-01  3.89754811e-01  3.91573387e-01
  3.91842242e-01  3.94889276e-01  3.96655724e-01  4.00225186e-01
  4.00329997e-01  4.00560762e-01  4.03654635e-01  4.05544196e-01
  4.05822805e-01  4.05844439e-01  4.06041901e-01  4.12637958e-01
  4.12835330e-01  4.15114942e-01  4.15836092e-01  4.17626218e-01
  4.25572643e-01  4.32878264e-01  4.35243484e-01  4.37453043e-01
  4.38464078e-01  4.39734269e-01  4.40400871e-01  4.41834573e-01
  4.42072697e-01  4.44642144e-01  4.48217286e-01  4.52438524e-01
  4.55087288e-01  4.58933532e-01  4.64949088e-01  4.65567985e-01
  4.65969692e-01  4.68003427e-01  4.68821913e-01  4.70525863e-01
  4.74723319e-01  4.84894957e-01  4.87131858e-01  4.88656463e-01
  4.97129435e-01  4.98470937e-01  4.99382366e-01  5.00612450e-01
  5.09421265e-01  5.10448019e-01  5.12112462e-01  5.18083634e-01
  5.18764910e-01  5.24287593e-01  5.26008729e-01  5.26278694e-01
  5.28991818e-01  5.33273033e-01  5.34118188e-01  5.34789551e-01
  5.35554699e-01  5.36151586e-01  5.38213357e-01  5.42302352e-01
  5.50007270e-01  5.52347911e-01  5.62225993e-01  5.63088200e-01
  5.65004913e-01  5.67570737e-01  5.68986841e-01  5.81532602e-01
  5.87284368e-01  5.96881649e-01  5.98419161e-01  6.06240968e-01
  6.09341529e-01  6.10453602e-01  6.10653927e-01  6.14916148e-01
  6.22945444e-01  6.23200920e-01  6.35421590e-01  6.39430753e-01
  6.42915628e-01  6.44148159e-01  6.46600412e-01  6.51002115e-01
  6.52175771e-01  6.58213595e-01  6.66657595e-01  6.69842358e-01
  6.70879559e-01  6.80546391e-01  6.81713730e-01  6.82442130e-01
  6.84316541e-01  6.93011901e-01  7.00228869e-01  7.02258506e-01
  7.02395827e-01  7.11386581e-01  7.19448862e-01  7.21635754e-01
  7.23909565e-01  7.27869867e-01  7.32018245e-01  7.44822328e-01
  7.46909054e-01  7.48142638e-01  7.49453336e-01  7.52100263e-01
  7.53927945e-01  7.66022300e-01  7.73262157e-01  7.75559560e-01
  7.82575510e-01  7.83879159e-01  7.87800361e-01  7.92809843e-01
  7.93885638e-01  7.94908805e-01  8.17551805e-01  8.18106876e-01
  8.21871685e-01  8.27926434e-01  8.79677968e-01  8.84243099e-01
  8.87118796e-01  8.88444345e-01  8.90038962e-01  9.00787672e-01
  9.04401421e-01  9.15946413e-01  9.34453000e-01  9.47932663e-01
  9.73366273e-01]

  warnings.warn(

2022-11-03 10:53:28,291:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-5.69298941e-02 -3.25590803e-02 -3.14484359e-02 -3.06759580e-02
 -2.59880021e-02 -2.54878277e-02 -2.54710180e-02 -2.53757248e-02
 -2.53268276e-02 -2.48616956e-02 -2.42473071e-02 -2.38366276e-02
 -2.31937818e-02 -2.22993094e-02 -2.21107258e-02 -2.07012655e-02
 -2.05226075e-02 -2.03788964e-02 -1.89507655e-02 -1.81803974e-02
 -1.77507696e-02 -1.69437559e-02 -1.65387479e-02 -1.60352212e-02
 -1.59035405e-02 -1.56217348e-02 -1.54185824e-02 -1.53909216e-02
 -1.45965333e-02 -1.42864923e-02 -1.41950173e-02 -1.40620615e-02
 -1.38988664e-02 -1.29554709e-02 -1.19684635e-02 -1.13452770e-02
 -1.10356002e-02 -1.05963218e-02 -1.02177602e-02 -9.78207543e-03
 -9.66917600e-03 -9.62256274e-03 -9.50811830e-03 -9.49109596e-03
 -9.37016050e-03 -8.72292363e-03 -8.47310203e-03 -7.34303151e-03
 -6.56764938e-03 -6.48888978e-03 -6.34530379e-03 -6.15342422e-03
 -5.94596179e-03 -5.26758471e-03 -5.14275612e-03 -4.85464566e-03
 -4.50026953e-03 -4.20804496e-03 -3.69750347e-03 -3.26777558e-03
 -2.46461919e-03 -2.39674439e-03 -6.48047905e-05  1.38795386e-04
  2.48842933e-04  8.55843531e-04  1.36220789e-03  1.41431191e-03
  1.61625843e-03  1.87010503e-03  2.41518118e-03  3.77851986e-03
  3.85393076e-03  4.76592802e-03  5.23435232e-03  5.48940513e-03
  6.89019620e-03  7.16792177e-03  7.50766230e-03  7.90545993e-03
  9.54417867e-03  1.05277658e-02  1.08392464e-02  1.14962731e-02
  1.15351441e-02  1.20270620e-02  1.23935469e-02  1.26585266e-02
  1.31107967e-02  1.34683940e-02  1.40578083e-02  1.41023794e-02
  1.42892247e-02  1.52232237e-02  1.54712440e-02  1.55688354e-02
  1.60324040e-02  1.61567664e-02  1.73796392e-02  1.78458177e-02
  2.03470098e-02  2.14457656e-02  2.14775390e-02  2.14979519e-02
  2.15230918e-02  2.18817288e-02  2.21654303e-02  2.22023943e-02
  2.27451202e-02  2.40855132e-02  2.45612803e-02  2.50216020e-02
  2.58410765e-02  2.61764102e-02  2.67135564e-02  2.73448112e-02
  2.82748259e-02  2.88613227e-02  2.89070776e-02  3.05688067e-02
  3.21653885e-02  3.27181252e-02  3.37492974e-02  3.49222380e-02
  3.57932674e-02  3.77778388e-02  3.88514476e-02  3.90589581e-02
  3.91850314e-02  4.07106797e-02  4.18852954e-02  4.50070034e-02
  4.63542469e-02  4.80289021e-02  4.82114180e-02  4.95649604e-02
  5.03586406e-02  5.11342631e-02  5.40619745e-02  5.40853688e-02
  5.45928929e-02  5.53227607e-02  6.18332261e-02  6.32008576e-02
  6.41507457e-02  6.66116447e-02  6.80201293e-02  6.83192013e-02
  6.88436541e-02  7.09290642e-02  7.12756528e-02  7.14750238e-02
  7.20078413e-02  7.20954202e-02  7.36040072e-02  7.39653388e-02
  7.40507699e-02  7.46311460e-02  7.55751265e-02  7.59136003e-02
  7.73381256e-02  7.77420690e-02  7.81906186e-02  8.17174740e-02
  8.30744689e-02  8.32276584e-02  8.39105682e-02  8.42006437e-02
  8.70450089e-02  8.70746293e-02  8.76717342e-02  8.78618482e-02
  9.35795865e-02  9.45038137e-02  9.52391403e-02  9.53590527e-02
  9.61968717e-02  9.64850478e-02  9.71463771e-02  9.71601444e-02
  9.86464480e-02  1.03785165e-01  1.04165755e-01  1.05808450e-01
  1.06434559e-01  1.10099981e-01  1.10551288e-01  1.10739652e-01
  1.12822700e-01  1.15833690e-01  1.20032869e-01  1.20879124e-01
  1.21790174e-01  1.22344420e-01  1.22431721e-01  1.22706267e-01
  1.22985511e-01  1.23366136e-01  1.23504218e-01  1.24468981e-01
  1.25714505e-01  1.25988589e-01  1.29353454e-01  1.30529187e-01
  1.31108972e-01  1.31612479e-01  1.31959295e-01  1.33495691e-01
  1.34076487e-01  1.34635371e-01  1.37553205e-01  1.39245163e-01
  1.39752543e-01  1.40188648e-01  1.41242064e-01  1.42948949e-01
  1.44386725e-01  1.44862282e-01  1.46814789e-01  1.48283230e-01
  1.51087480e-01  1.51216672e-01  1.51312453e-01  1.51477013e-01
  1.51546752e-01  1.52909922e-01  1.54520554e-01  1.54728394e-01
  1.55083884e-01  1.58619382e-01  1.61642141e-01  1.62069340e-01
  1.62556953e-01  1.63228905e-01  1.63999400e-01  1.65167389e-01
  1.66067002e-01  1.68233349e-01  1.68237524e-01  1.69754149e-01
  1.70534146e-01  1.71031016e-01  1.71924253e-01  1.72013773e-01
  1.73087078e-01  1.81731149e-01  1.83414324e-01  1.83773264e-01
  1.84128317e-01  1.84377794e-01  1.84918454e-01  1.86559627e-01
  1.86943664e-01  1.90590176e-01  1.90725690e-01  1.92874969e-01
  1.93348930e-01  1.94562112e-01  1.96389751e-01  1.97636965e-01
  1.98172933e-01  1.98848931e-01  2.00951727e-01  2.01804328e-01
  2.07061985e-01  2.07158664e-01  2.07908574e-01  2.11742111e-01
  2.14463048e-01  2.16097840e-01  2.16326992e-01  2.17567100e-01
  2.18254971e-01  2.19656048e-01  2.19663008e-01  2.19927411e-01
  2.26454062e-01  2.31328853e-01  2.33432488e-01  2.35484551e-01
  2.36194243e-01  2.36564371e-01  2.37401146e-01  2.40702056e-01
  2.41080592e-01  2.47117707e-01  2.47180413e-01  2.47762777e-01
  2.48978916e-01  2.49859686e-01  2.52565919e-01  2.52767308e-01
  2.55349543e-01  2.56008317e-01  2.60212065e-01  2.62032964e-01
  2.62894123e-01  2.64347877e-01  2.65072350e-01  2.66895809e-01
  2.67459189e-01  2.78256280e-01  2.78671071e-01  2.79955772e-01
  2.81485569e-01  2.82148262e-01  2.85897762e-01  2.86152413e-01
  2.86328510e-01  2.95962099e-01  2.98691638e-01  3.00908964e-01
  3.03383960e-01  3.04806569e-01  3.11381545e-01  3.12876864e-01
  3.14798570e-01  3.19572419e-01  3.20981955e-01  3.21905693e-01
  3.24061836e-01  3.24665460e-01  3.26371326e-01  3.28692021e-01
  3.30636014e-01  3.37692388e-01  3.37859174e-01  3.41267561e-01
  3.43324626e-01  3.48441724e-01  3.49659542e-01  3.50131025e-01
  3.51661964e-01  3.52936317e-01  3.56111912e-01  3.56426144e-01
  3.58560705e-01  3.59742576e-01  3.62424812e-01  3.64695327e-01
  3.65441495e-01  3.67270561e-01  3.69348831e-01  3.72147794e-01
  3.77470893e-01  3.78053006e-01  3.82384515e-01  3.86086138e-01
  3.86453456e-01  3.87324003e-01  3.88103328e-01  3.88376639e-01
  3.89533199e-01  3.91523751e-01  3.92774457e-01  3.93099448e-01
  3.93672083e-01  4.02814763e-01  4.04025492e-01  4.04292267e-01
  4.04658761e-01  4.06107503e-01  4.06640380e-01  4.07106362e-01
  4.11179146e-01  4.14469447e-01  4.16945417e-01  4.18334159e-01
  4.21579044e-01  4.25048796e-01  4.25652663e-01  4.26126940e-01
  4.33541275e-01  4.34839424e-01  4.35478673e-01  4.35892540e-01
  4.44124816e-01  4.50792938e-01  4.56070458e-01  4.68046813e-01
  4.69007831e-01  4.71398973e-01  4.73855659e-01  4.77440132e-01
  4.82178716e-01  4.82368178e-01  4.84455164e-01  4.86882008e-01
  4.87138651e-01  4.92335516e-01  4.94998027e-01  4.95075727e-01
  4.96746543e-01  5.00965213e-01  5.01857956e-01  5.02116049e-01
  5.03129281e-01  5.03345015e-01  5.07035263e-01  5.08211971e-01
  5.11977128e-01  5.13172941e-01  5.13873061e-01  5.16092748e-01
  5.16617184e-01  5.21190062e-01  5.21640592e-01  5.22433016e-01
  5.25245076e-01  5.26715324e-01  5.30868033e-01  5.31683467e-01
  5.31731989e-01  5.36658848e-01  5.37381046e-01  5.41621168e-01
  5.43234973e-01  5.46926364e-01  5.51717239e-01  5.55776554e-01
  5.60309055e-01  5.60660599e-01  5.61561152e-01  5.63166136e-01
  5.69367308e-01  5.84384812e-01  5.94935845e-01  5.95820407e-01
  5.96254932e-01  5.96325846e-01  5.97039853e-01  5.97545891e-01
  6.00983802e-01  6.01299399e-01  6.05435817e-01  6.05925643e-01
  6.23705163e-01  6.24523417e-01  6.25180795e-01  6.28142026e-01
  6.32173704e-01  6.33843299e-01  6.36796994e-01  6.40398937e-01
  6.42702472e-01  6.47106483e-01  6.52764875e-01  6.58572288e-01
  6.62315003e-01  6.62788143e-01  6.63382996e-01  6.67068942e-01
  6.68980470e-01  6.73157933e-01  6.78826365e-01  6.88300993e-01
  6.88839843e-01  6.89048260e-01  7.02031663e-01  7.05098471e-01
  7.05634650e-01  7.23820455e-01  7.42842484e-01  7.48341746e-01
  7.49405133e-01  7.52687478e-01  7.54033681e-01  7.57042292e-01
  7.58611374e-01  7.67018438e-01  7.75132195e-01  7.75472055e-01
  7.93081822e-01  8.00905571e-01  8.04770995e-01  8.07617707e-01
  8.09482060e-01  8.13782029e-01  8.16283236e-01  8.16563343e-01
  8.17891570e-01  8.18135284e-01  8.46182779e-01  8.58194246e-01
  8.64390034e-01  8.85423265e-01  8.85675112e-01  9.10175853e-01]

  warnings.warn(

2022-11-03 10:53:28,418:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.32116069e-02 -5.94660008e-02 -4.11723983e-02 -4.00346607e-02
 -3.62530982e-02 -3.58141142e-02 -3.12052450e-02 -3.10639497e-02
 -2.97873444e-02 -2.94466014e-02 -2.89549569e-02 -2.85779698e-02
 -2.13441553e-02 -2.08887209e-02 -2.08453042e-02 -2.05221450e-02
 -2.02121342e-02 -1.82608215e-02 -1.67204377e-02 -1.66473066e-02
 -1.55140623e-02 -1.54544279e-02 -1.51856408e-02 -1.51820999e-02
 -1.48313593e-02 -1.44329602e-02 -1.41711071e-02 -1.37423731e-02
 -1.36916691e-02 -1.36451095e-02 -1.36194270e-02 -1.35597176e-02
 -1.33601657e-02 -1.31565380e-02 -1.29266073e-02 -1.29183403e-02
 -1.22674721e-02 -1.17459466e-02 -1.16830325e-02 -1.02891017e-02
 -9.19348261e-03 -8.96690785e-03 -8.86128081e-03 -8.46890525e-03
 -7.89802438e-03 -7.82529615e-03 -7.67806157e-03 -7.28217912e-03
 -6.87657667e-03 -6.72751012e-03 -6.55010953e-03 -6.40426612e-03
 -5.72238087e-03 -5.66130231e-03 -5.65271276e-03 -5.11416606e-03
 -4.65728510e-03 -4.52969762e-03 -4.11272938e-03 -3.64142383e-03
 -3.18366032e-03 -2.87371771e-03 -1.83853732e-03 -1.65495294e-03
 -1.48714772e-03 -1.67986708e-04  1.39465696e-04  7.14125439e-04
  1.46526728e-03  1.70823624e-03  2.46291107e-03  2.54321393e-03
  4.39245279e-03  4.59901361e-03  4.72992199e-03  6.15095202e-03
  6.22354392e-03  7.03179281e-03  7.40481153e-03  7.53795881e-03
  7.86026692e-03  8.35201085e-03  8.41623168e-03  9.71930285e-03
  9.73859095e-03  9.92779029e-03  1.07600023e-02  1.19698552e-02
  1.22665861e-02  1.32173043e-02  1.40085005e-02  1.44554693e-02
  1.50242943e-02  1.51179525e-02  1.51183869e-02  1.53111100e-02
  1.55930215e-02  1.73315615e-02  1.94537452e-02  2.05595803e-02
  2.06704788e-02  2.09627955e-02  2.15226509e-02  2.17029330e-02
  2.18039401e-02  2.19717661e-02  2.38418618e-02  2.43543297e-02
  2.48384120e-02  2.63142319e-02  2.72950754e-02  3.04948994e-02
  3.22526213e-02  3.25271031e-02  3.40586615e-02  3.49397483e-02
  3.69825979e-02  3.73574338e-02  3.93929955e-02  4.21220734e-02
  4.32210296e-02  4.41465043e-02  4.46229861e-02  4.66758453e-02
  4.75801268e-02  5.04138177e-02  5.05843131e-02  5.19683332e-02
  5.24426629e-02  5.34112368e-02  5.63218552e-02  6.08022350e-02
  6.29277541e-02  6.41921705e-02  6.60140782e-02  6.86692207e-02
  6.96870832e-02  7.07674096e-02  7.63043097e-02  7.76527342e-02
  7.91634853e-02  7.97134746e-02  7.98563158e-02  8.04928374e-02
  8.07639320e-02  8.15121462e-02  8.21129889e-02  8.27660563e-02
  8.28495951e-02  8.35581632e-02  8.52036587e-02  8.58062988e-02
  8.80837592e-02  8.87300680e-02  8.89364193e-02  8.96749608e-02
  9.08379504e-02  9.31297832e-02  9.49630284e-02  9.53896545e-02
  9.58273396e-02  9.76685821e-02  1.00003523e-01  1.00142332e-01
  1.00556684e-01  1.01529586e-01  1.01654113e-01  1.04288387e-01
  1.09029445e-01  1.09725064e-01  1.09898165e-01  1.10181862e-01
  1.13242395e-01  1.13499264e-01  1.13757463e-01  1.14311741e-01
  1.14481577e-01  1.15360198e-01  1.17864555e-01  1.18653654e-01
  1.19854003e-01  1.20325665e-01  1.20680921e-01  1.21431165e-01
  1.21469791e-01  1.21560245e-01  1.22170659e-01  1.23202145e-01
  1.24146254e-01  1.24269864e-01  1.26649681e-01  1.27955606e-01
  1.28111562e-01  1.31998478e-01  1.32664819e-01  1.33667618e-01
  1.34217063e-01  1.35179680e-01  1.35517058e-01  1.35940885e-01
  1.38131854e-01  1.38666406e-01  1.38907822e-01  1.39750518e-01
  1.43041342e-01  1.43548038e-01  1.44021099e-01  1.44122004e-01
  1.44514341e-01  1.45480757e-01  1.45710418e-01  1.46925717e-01
  1.47349317e-01  1.47525628e-01  1.51845327e-01  1.54788716e-01
  1.56953156e-01  1.58150077e-01  1.59389359e-01  1.59890046e-01
  1.61416382e-01  1.61985984e-01  1.63833313e-01  1.69163203e-01
  1.70182221e-01  1.70209763e-01  1.72405690e-01  1.73244239e-01
  1.75114791e-01  1.75955803e-01  1.77931775e-01  1.78976048e-01
  1.82639694e-01  1.83281641e-01  1.84681941e-01  1.84706242e-01
  1.86808051e-01  1.87013506e-01  1.88938103e-01  1.89457798e-01
  1.90489206e-01  1.91646586e-01  1.92916391e-01  1.95399120e-01
  1.95694914e-01  1.96079538e-01  1.97606852e-01  1.98811405e-01
  1.99553489e-01  2.03738843e-01  2.09050113e-01  2.12913834e-01
  2.12962523e-01  2.14759984e-01  2.15119087e-01  2.21114215e-01
  2.23007049e-01  2.24527457e-01  2.25997429e-01  2.27055842e-01
  2.29179660e-01  2.31581183e-01  2.35358847e-01  2.35468086e-01
  2.36722219e-01  2.37147828e-01  2.37470996e-01  2.38218972e-01
  2.38395631e-01  2.38631271e-01  2.40198065e-01  2.41637989e-01
  2.42136189e-01  2.42980433e-01  2.43648652e-01  2.43808300e-01
  2.49213590e-01  2.50553791e-01  2.53990265e-01  2.56744274e-01
  2.57146204e-01  2.57803449e-01  2.61764693e-01  2.64383137e-01
  2.68163733e-01  2.70068603e-01  2.72482414e-01  2.72802262e-01
  2.73597450e-01  2.73731646e-01  2.74752821e-01  2.77561145e-01
  2.78242028e-01  2.80265193e-01  2.83434095e-01  2.84295809e-01
  2.87887042e-01  2.91192284e-01  2.94979474e-01  2.95785225e-01
  2.97292867e-01  2.99613044e-01  3.02391343e-01  3.02567185e-01
  3.03062652e-01  3.05112463e-01  3.09578250e-01  3.09833944e-01
  3.18457796e-01  3.19110747e-01  3.21743108e-01  3.22088750e-01
  3.22836462e-01  3.25833251e-01  3.26264796e-01  3.26376991e-01
  3.26515645e-01  3.27238781e-01  3.27297219e-01  3.28174828e-01
  3.28223534e-01  3.30420819e-01  3.32794860e-01  3.34738012e-01
  3.39733193e-01  3.40928047e-01  3.41626971e-01  3.42861441e-01
  3.44153092e-01  3.44436810e-01  3.46470175e-01  3.46719458e-01
  3.47940965e-01  3.48632792e-01  3.49570335e-01  3.50216914e-01
  3.59684693e-01  3.60730289e-01  3.61497941e-01  3.63604324e-01
  3.65784675e-01  3.66414180e-01  3.66585363e-01  3.71789826e-01
  3.72154362e-01  3.72261066e-01  3.72936114e-01  3.73958443e-01
  3.74307908e-01  3.75509435e-01  3.76849669e-01  3.93215655e-01
  3.95225817e-01  3.95458505e-01  3.96029177e-01  3.98341628e-01
  3.98760162e-01  3.99842459e-01  4.00334151e-01  4.01253161e-01
  4.01384641e-01  4.03565944e-01  4.12193241e-01  4.12888536e-01
  4.13130991e-01  4.14120763e-01  4.16064973e-01  4.19371302e-01
  4.20313077e-01  4.20741975e-01  4.21199605e-01  4.21987577e-01
  4.26388458e-01  4.28402568e-01  4.30087499e-01  4.34141249e-01
  4.34328081e-01  4.34506255e-01  4.35485719e-01  4.41653155e-01
  4.41863467e-01  4.45665877e-01  4.48251503e-01  4.48758733e-01
  4.51059027e-01  4.53658540e-01  4.57681148e-01  4.57705567e-01
  4.61285543e-01  4.67832945e-01  4.69499011e-01  4.70542309e-01
  4.71591146e-01  4.71596816e-01  4.73175275e-01  4.74014216e-01
  4.77623901e-01  4.90875207e-01  4.95219663e-01  4.96648175e-01
  4.99934187e-01  5.05329504e-01  5.06236787e-01  5.06348543e-01
  5.09835274e-01  5.11021871e-01  5.16071316e-01  5.18398059e-01
  5.20278958e-01  5.22506343e-01  5.22513653e-01  5.24833112e-01
  5.27044921e-01  5.29598453e-01  5.32312111e-01  5.33669740e-01
  5.36342825e-01  5.37589646e-01  5.37858801e-01  5.41889780e-01
  5.44499483e-01  5.48524741e-01  5.50357070e-01  5.52381507e-01
  5.55432605e-01  5.57877901e-01  5.58319853e-01  5.62042811e-01
  5.65617974e-01  5.71945617e-01  5.75817512e-01  5.77859799e-01
  5.78510041e-01  5.90046064e-01  5.90352625e-01  6.00120682e-01
  6.03988060e-01  6.09743685e-01  6.09957101e-01  6.15076122e-01
  6.25692114e-01  6.26086699e-01  6.27832248e-01  6.31135218e-01
  6.34224595e-01  6.35995275e-01  6.37600943e-01  6.43178570e-01
  6.47360142e-01  6.48960124e-01  6.54751135e-01  6.56206325e-01
  6.57422555e-01  6.59268487e-01  6.61040518e-01  6.61229458e-01
  6.61682837e-01  6.63754617e-01  6.65460287e-01  6.81357906e-01
  6.95399360e-01  6.98863485e-01  7.08615571e-01  7.11143186e-01
  7.23386764e-01  7.31792121e-01  7.44148432e-01  7.51683522e-01
  7.53750989e-01  7.54238146e-01  7.55414068e-01  7.69112124e-01
  7.69602977e-01  7.88598070e-01  8.00260841e-01  8.06871157e-01
  8.19636488e-01  8.28641412e-01  8.40591190e-01  8.46361346e-01
  8.56895270e-01  8.58950560e-01  8.60702173e-01  8.61010864e-01
  8.66807600e-01  8.81225774e-01  8.95337520e-01  8.95766219e-01
  9.01771522e-01  9.01786151e-01  9.41604907e-01]

  warnings.warn(

2022-11-03 10:53:32,409:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.35287157e-02 -6.85901234e-02 -5.08076822e-02 -3.20296889e-02
 -3.12656639e-02 -2.81147354e-02 -2.80677514e-02 -2.67189788e-02
 -2.59400649e-02 -2.57540977e-02 -2.53481951e-02 -2.52558628e-02
 -2.46116671e-02 -2.33760931e-02 -2.26654445e-02 -2.19230234e-02
 -2.16514840e-02 -2.14127183e-02 -1.97744223e-02 -1.95698787e-02
 -1.84210238e-02 -1.76822442e-02 -1.73799137e-02 -1.70486990e-02
 -1.58725507e-02 -1.48681731e-02 -1.42119314e-02 -1.37880524e-02
 -1.27174698e-02 -1.21826836e-02 -1.13056133e-02 -1.12549173e-02
 -1.05795922e-02 -1.04095592e-02 -9.61452015e-03 -9.58609677e-03
 -9.27538698e-03 -9.26005402e-03 -8.77908779e-03 -8.61790613e-03
 -8.44787306e-03 -7.94728912e-03 -7.68553081e-03 -7.15606829e-03
 -7.14069034e-03 -7.05585333e-03 -6.80423901e-03 -6.32885356e-03
 -6.14624583e-03 -5.29324865e-03 -4.78092553e-03 -4.50208171e-03
 -4.49050691e-03 -4.37859427e-03 -4.22375285e-03 -3.73940321e-03
 -3.70605124e-03 -3.62702068e-03 -3.50505131e-03 -2.89776442e-03
 -2.85348932e-03 -2.83015056e-03 -2.79584077e-03 -2.59640208e-03
 -2.59396578e-03 -2.58970974e-03 -2.41894576e-03 -2.14891038e-03
 -2.03683946e-03 -1.88874259e-03 -1.88818641e-03 -1.64122330e-03
 -1.60490332e-03 -7.75539058e-04 -5.03255790e-04  1.71937006e-04
  1.30606622e-03  1.30815981e-03  1.32955346e-03  1.47014441e-03
  2.95982846e-03  3.23204588e-03  3.80898923e-03  4.15918812e-03
  4.75039719e-03  4.82786149e-03  4.91734701e-03  5.01722465e-03
  5.64233684e-03  5.77067534e-03  6.34915317e-03  7.38627694e-03
  7.52857685e-03  7.86489391e-03  8.09757011e-03  8.77836574e-03
  8.95016723e-03  9.49134862e-03  9.51129678e-03  9.79109257e-03
  1.00709703e-02  1.02103975e-02  1.13192806e-02  1.15464809e-02
  1.17088315e-02  1.27819409e-02  1.40053868e-02  1.41554070e-02
  1.43101034e-02  1.65332734e-02  1.68719288e-02  1.83055385e-02
  1.93445650e-02  1.94279916e-02  2.21199040e-02  2.25347565e-02
  2.35286994e-02  2.44034476e-02  2.63047040e-02  2.64673212e-02
  2.73175373e-02  2.92321763e-02  3.02249596e-02  3.06415812e-02
  3.10830366e-02  3.18494806e-02  3.18555279e-02  3.21515563e-02
  3.70377075e-02  3.90270543e-02  3.96677749e-02  4.10365273e-02
  4.26113387e-02  4.40118266e-02  4.42981993e-02  4.55718880e-02
  4.57372979e-02  4.58780386e-02  4.79804857e-02  4.80516354e-02
  4.87657675e-02  4.99425923e-02  5.08857260e-02  5.11249007e-02
  5.24999853e-02  5.27533686e-02  5.48754669e-02  5.56210764e-02
  5.57993241e-02  5.77323554e-02  6.04387129e-02  6.18613819e-02
  6.23160926e-02  6.29213252e-02  6.64124752e-02  6.65208905e-02
  6.83526883e-02  6.96455317e-02  7.05640107e-02  7.12072067e-02
  7.17494318e-02  7.28839501e-02  7.33698188e-02  7.53388199e-02
  7.57928647e-02  7.66942836e-02  7.67744145e-02  7.74909333e-02
  7.96200858e-02  8.06659657e-02  8.17780401e-02  8.56141109e-02
  8.90533214e-02  8.99845285e-02  9.48985404e-02  9.53525489e-02
  9.60475168e-02  9.64733126e-02  9.67551702e-02  9.83366803e-02
  1.01883817e-01  1.02272969e-01  1.02726065e-01  1.03344824e-01
  1.03586818e-01  1.05126091e-01  1.07392502e-01  1.09174818e-01
  1.09859797e-01  1.10625838e-01  1.10789112e-01  1.14540698e-01
  1.15151274e-01  1.15271507e-01  1.15417500e-01  1.16043405e-01
  1.16654579e-01  1.18142459e-01  1.23150540e-01  1.25284327e-01
  1.27460988e-01  1.27736599e-01  1.28717301e-01  1.30595875e-01
  1.31400277e-01  1.32575132e-01  1.34811405e-01  1.35477679e-01
  1.35594234e-01  1.39113345e-01  1.39337975e-01  1.40472008e-01
  1.41022062e-01  1.41741444e-01  1.43099909e-01  1.45558424e-01
  1.49446288e-01  1.49661968e-01  1.50136307e-01  1.51020618e-01
  1.51127917e-01  1.51439825e-01  1.53584541e-01  1.55273196e-01
  1.58457347e-01  1.59190650e-01  1.61248015e-01  1.61288235e-01
  1.62505770e-01  1.66533542e-01  1.67373415e-01  1.67571968e-01
  1.72305867e-01  1.73747989e-01  1.76417872e-01  1.77015538e-01
  1.77487259e-01  1.78364017e-01  1.80347601e-01  1.82114529e-01
  1.85194831e-01  1.86509839e-01  1.87877259e-01  1.88359312e-01
  1.90325594e-01  1.93888671e-01  1.94606500e-01  1.99627188e-01
  2.01595764e-01  2.05513147e-01  2.06137951e-01  2.07228022e-01
  2.11883919e-01  2.12979432e-01  2.15316586e-01  2.16479074e-01
  2.18575615e-01  2.19543834e-01  2.24022647e-01  2.25110688e-01
  2.28992594e-01  2.31311975e-01  2.33868172e-01  2.34135669e-01
  2.34235642e-01  2.36411420e-01  2.37147368e-01  2.38106192e-01
  2.39198229e-01  2.41867209e-01  2.42350161e-01  2.42934213e-01
  2.45156233e-01  2.45397786e-01  2.45641875e-01  2.48788323e-01
  2.56134843e-01  2.57012484e-01  2.61158365e-01  2.62394602e-01
  2.64441750e-01  2.66353999e-01  2.67287721e-01  2.68483502e-01
  2.70806451e-01  2.73069586e-01  2.74747807e-01  2.75957371e-01
  2.77967572e-01  2.80146140e-01  2.86429957e-01  2.86504171e-01
  2.87490935e-01  2.88739911e-01  2.90032902e-01  2.91006474e-01
  2.91931598e-01  2.92344850e-01  2.98520755e-01  3.00573301e-01
  3.01646585e-01  3.04383815e-01  3.04909086e-01  3.07090588e-01
  3.07385551e-01  3.08132416e-01  3.12501105e-01  3.16060183e-01
  3.17353377e-01  3.20358649e-01  3.21952051e-01  3.23308534e-01
  3.23896225e-01  3.29919578e-01  3.34070552e-01  3.35007404e-01
  3.38507368e-01  3.40388410e-01  3.40624481e-01  3.40901675e-01
  3.43368587e-01  3.44533858e-01  3.44932909e-01  3.47687007e-01
  3.47974241e-01  3.50285793e-01  3.50653456e-01  3.53555809e-01
  3.61498714e-01  3.66790730e-01  3.69676853e-01  3.70773023e-01
  3.70969019e-01  3.71263588e-01  3.74492552e-01  3.76950672e-01
  3.77529918e-01  3.80270380e-01  3.84024604e-01  3.84841954e-01
  3.87761146e-01  3.91383122e-01  3.92262182e-01  3.93525653e-01
  3.93897221e-01  3.94045545e-01  3.99856696e-01  4.00075161e-01
  4.02092631e-01  4.03346483e-01  4.04610276e-01  4.07876810e-01
  4.10158320e-01  4.16141216e-01  4.18354978e-01  4.20519285e-01
  4.22680110e-01  4.22743902e-01  4.25399306e-01  4.26623352e-01
  4.28869125e-01  4.32413659e-01  4.32901057e-01  4.33740810e-01
  4.42691530e-01  4.46254683e-01  4.46528996e-01  4.49884283e-01
  4.51971207e-01  4.56084636e-01  4.57930304e-01  4.58102912e-01
  4.64148805e-01  4.66390874e-01  4.67082031e-01  4.68188401e-01
  4.71072724e-01  4.72106314e-01  4.72843581e-01  4.73311495e-01
  4.74098983e-01  4.76542817e-01  4.76859520e-01  4.77595398e-01
  4.79500271e-01  4.79847741e-01  4.86444217e-01  4.89197472e-01
  4.99758492e-01  5.04583981e-01  5.04785987e-01  5.08214714e-01
  5.09404048e-01  5.09676647e-01  5.12671682e-01  5.15832931e-01
  5.21271204e-01  5.21643533e-01  5.31053731e-01  5.37339745e-01
  5.37429571e-01  5.40066525e-01  5.43129083e-01  5.43277837e-01
  5.47830842e-01  5.49324083e-01  5.51032178e-01  5.56602763e-01
  5.59400420e-01  5.67956248e-01  5.69231020e-01  5.72918201e-01
  5.77724093e-01  5.86178892e-01  5.87829407e-01  5.89350768e-01
  5.89600805e-01  5.90978030e-01  5.91815588e-01  5.92560657e-01
  5.94116929e-01  5.95230764e-01  5.98487287e-01  5.99777339e-01
  6.06919257e-01  6.08441737e-01  6.10222627e-01  6.12270521e-01
  6.16524588e-01  6.26978982e-01  6.32553095e-01  6.33509924e-01
  6.35533313e-01  6.37130648e-01  6.45001829e-01  6.48951579e-01
  6.52013126e-01  6.52735819e-01  6.52861092e-01  6.56032560e-01
  6.59376959e-01  6.59704741e-01  6.65953415e-01  6.67735348e-01
  6.69040127e-01  6.70556461e-01  6.70625458e-01  6.72226796e-01
  6.82593866e-01  6.83216576e-01  6.83511981e-01  6.86953943e-01
  6.89703031e-01  6.90656293e-01  6.91752226e-01  6.92191845e-01
  7.05057671e-01  7.09601221e-01  7.30551053e-01  7.44212489e-01
  7.56382837e-01  7.69617728e-01  7.78228800e-01  7.81634870e-01
  7.83659104e-01  7.84146610e-01  7.94855706e-01  8.02212437e-01
  8.13864814e-01  8.15846016e-01  8.16924727e-01  8.25769328e-01
  8.29846447e-01  8.44039946e-01  8.46842864e-01  8.66127480e-01
  8.68901899e-01  8.76824521e-01  8.90363119e-01  9.17593116e-01
  9.20718006e-01  9.22646528e-01  9.57168525e-01]

  warnings.warn(

2022-11-03 10:53:32,441:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.19546823e-02 -3.34896261e-02 -3.06190414e-02 -2.85295002e-02
 -2.40608007e-02 -2.23110183e-02 -2.16100234e-02 -2.02757654e-02
 -2.01386979e-02 -1.96964865e-02 -1.91216736e-02 -1.88056860e-02
 -1.86238668e-02 -1.78397176e-02 -1.72415147e-02 -1.69328059e-02
 -1.66226158e-02 -1.60222525e-02 -1.59852374e-02 -1.58636090e-02
 -1.46113346e-02 -1.45654257e-02 -1.43294774e-02 -1.41737995e-02
 -1.30351099e-02 -1.27043327e-02 -1.20416790e-02 -1.19510315e-02
 -1.04334993e-02 -9.05548110e-03 -8.24506258e-03 -8.18812664e-03
 -7.57916884e-03 -6.43566825e-03 -6.32900604e-03 -5.20909042e-03
 -4.77615265e-03 -3.09033789e-03 -2.72746084e-03 -2.38037843e-04
  2.40764190e-05  9.26436305e-04  1.09087907e-03  2.00503944e-03
  2.19307828e-03  2.50883627e-03  3.04948153e-03  3.12435763e-03
  3.25001523e-03  3.90750023e-03  3.99193784e-03  4.69001279e-03
  4.70036483e-03  5.45088691e-03  5.67729242e-03  5.86993009e-03
  6.80298080e-03  6.83183410e-03  7.11544820e-03  7.82655569e-03
  8.31885031e-03  8.53647989e-03  8.75750979e-03  8.96169509e-03
  9.11963253e-03  1.14133039e-02  1.15256018e-02  1.23167991e-02
  1.41658724e-02  1.69148180e-02  1.73577830e-02  1.79143527e-02
  1.79594371e-02  1.87413854e-02  1.96482971e-02  2.05241112e-02
  2.23762288e-02  2.23836883e-02  2.30975167e-02  2.37298385e-02
  2.41406798e-02  2.44260249e-02  2.49971404e-02  2.64220991e-02
  2.67807465e-02  2.72055942e-02  2.73592587e-02  2.76010685e-02
  2.89892474e-02  3.06084494e-02  3.16189649e-02  3.16483252e-02
  3.23023391e-02  3.37961558e-02  3.58034986e-02  3.68708552e-02
  3.71822205e-02  3.86543163e-02  3.88805848e-02  4.04515539e-02
  4.27379438e-02  4.29466327e-02  4.29726094e-02  4.32905811e-02
  4.48917421e-02  4.55864130e-02  4.58826913e-02  4.60210555e-02
  4.68245400e-02  4.73728522e-02  4.73915133e-02  4.75539205e-02
  4.88170388e-02  4.92725816e-02  5.06993287e-02  5.08364462e-02
  5.10758035e-02  5.15903049e-02  5.37744803e-02  5.46453387e-02
  5.63543238e-02  5.80761653e-02  6.00482794e-02  6.01994919e-02
  6.50276720e-02  6.56918497e-02  6.84398123e-02  6.97817990e-02
  7.00010965e-02  7.14302063e-02  7.27200683e-02  7.56449019e-02
  7.80387760e-02  7.93318455e-02  8.04042902e-02  8.07366648e-02
  8.09821110e-02  8.43490591e-02  8.45269798e-02  8.53711613e-02
  8.60158734e-02  8.64363993e-02  8.73214515e-02  8.91147349e-02
  8.93415083e-02  8.96057719e-02  8.96141352e-02  8.97011562e-02
  9.44970104e-02  9.52231099e-02  9.57309085e-02  9.58100761e-02
  9.62568445e-02  9.70505564e-02  9.70765439e-02  9.81966039e-02
  9.88707931e-02  9.93780825e-02  9.94049681e-02  9.96807357e-02
  1.00229552e-01  1.00894327e-01  1.01234894e-01  1.01266760e-01
  1.01811421e-01  1.04631869e-01  1.05198367e-01  1.06132016e-01
  1.06372728e-01  1.07599983e-01  1.07669571e-01  1.07976728e-01
  1.10180151e-01  1.13944532e-01  1.15703748e-01  1.15942440e-01
  1.16900786e-01  1.18165217e-01  1.23502946e-01  1.26630169e-01
  1.27585713e-01  1.28374162e-01  1.28465223e-01  1.28651081e-01
  1.29923117e-01  1.33774124e-01  1.34537910e-01  1.35509890e-01
  1.35851397e-01  1.35938046e-01  1.38433302e-01  1.41360360e-01
  1.43171542e-01  1.43301741e-01  1.43921309e-01  1.45027049e-01
  1.53127967e-01  1.53932329e-01  1.56575783e-01  1.56992079e-01
  1.59250953e-01  1.62013237e-01  1.62061128e-01  1.63364758e-01
  1.63538002e-01  1.63786658e-01  1.65261612e-01  1.65744191e-01
  1.69631930e-01  1.71123370e-01  1.71319713e-01  1.71706874e-01
  1.73332669e-01  1.73454239e-01  1.74070064e-01  1.74480669e-01
  1.76634332e-01  1.76837283e-01  1.76895736e-01  1.77264492e-01
  1.77925522e-01  1.79232198e-01  1.81477951e-01  1.83023910e-01
  1.85247277e-01  1.87096361e-01  1.87231002e-01  1.88855472e-01
  1.90968527e-01  1.91519980e-01  1.91555422e-01  1.93693021e-01
  1.98229248e-01  1.99953775e-01  2.01282518e-01  2.02176120e-01
  2.05283941e-01  2.08073187e-01  2.10632814e-01  2.11449597e-01
  2.13536583e-01  2.13555865e-01  2.14913021e-01  2.18211481e-01
  2.19469230e-01  2.19747385e-01  2.22712816e-01  2.23383949e-01
  2.25420468e-01  2.26179906e-01  2.28089109e-01  2.29339951e-01
  2.31124171e-01  2.32870032e-01  2.34877869e-01  2.34993833e-01
  2.35426709e-01  2.35562972e-01  2.39140523e-01  2.41526184e-01
  2.45026250e-01  2.45152615e-01  2.48047682e-01  2.48265068e-01
  2.50967736e-01  2.51039768e-01  2.54224764e-01  2.55236445e-01
  2.56378781e-01  2.58905574e-01  2.61265775e-01  2.61314363e-01
  2.63016020e-01  2.63614623e-01  2.65043358e-01  2.65984291e-01
  2.66862803e-01  2.70955628e-01  2.71783264e-01  2.72024565e-01
  2.77396326e-01  2.79974783e-01  2.84004606e-01  2.85573459e-01
  2.86086971e-01  2.87324117e-01  2.88102158e-01  2.89202737e-01
  2.90772675e-01  2.91208281e-01  2.91277976e-01  2.91988721e-01
  2.95279316e-01  2.95324749e-01  2.96483218e-01  2.98420294e-01
  2.98501734e-01  2.98729363e-01  2.99435930e-01  3.00275881e-01
  3.00447712e-01  3.00619988e-01  3.00951785e-01  3.04574368e-01
  3.07178412e-01  3.07672718e-01  3.10217276e-01  3.12514890e-01
  3.14269451e-01  3.19506902e-01  3.22799335e-01  3.24203522e-01
  3.24595153e-01  3.24815937e-01  3.25176769e-01  3.26021429e-01
  3.26256619e-01  3.28457630e-01  3.30483273e-01  3.38644620e-01
  3.39266818e-01  3.39536352e-01  3.41791204e-01  3.43774015e-01
  3.46188424e-01  3.50542324e-01  3.55725706e-01  3.58049176e-01
  3.62865313e-01  3.65281876e-01  3.67568977e-01  3.71041282e-01
  3.73519772e-01  3.74175774e-01  3.77712767e-01  3.78229220e-01
  3.79674834e-01  3.79802818e-01  3.82313782e-01  3.90919750e-01
  3.91465041e-01  3.91617701e-01  3.94119706e-01  3.94186908e-01
  3.94580162e-01  3.97262238e-01  3.98203964e-01  4.01971560e-01
  4.02909628e-01  4.11336982e-01  4.11914527e-01  4.12970078e-01
  4.13097999e-01  4.14286313e-01  4.14485856e-01  4.17093290e-01
  4.20556655e-01  4.22939372e-01  4.25689681e-01  4.27989790e-01
  4.32913319e-01  4.33146964e-01  4.35843610e-01  4.36109028e-01
  4.39341292e-01  4.39484386e-01  4.42010951e-01  4.42448578e-01
  4.43696372e-01  4.52454437e-01  4.54380190e-01  4.56684581e-01
  4.60177556e-01  4.63520519e-01  4.63732334e-01  4.66793615e-01
  4.73673860e-01  4.76734490e-01  4.77677122e-01  4.80884398e-01
  4.81901204e-01  4.85282550e-01  4.90361399e-01  4.91254118e-01
  4.91398996e-01  4.94053569e-01  4.94450186e-01  4.94590908e-01
  4.95950768e-01  4.96152798e-01  4.96238018e-01  4.97621502e-01
  5.00107382e-01  5.02286835e-01  5.05086121e-01  5.08643328e-01
  5.10189156e-01  5.11213350e-01  5.13186762e-01  5.20574276e-01
  5.33638210e-01  5.33904694e-01  5.37743220e-01  5.39429726e-01
  5.41347774e-01  5.43173991e-01  5.46720845e-01  5.47905271e-01
  5.53014171e-01  5.54975024e-01  5.55696817e-01  5.57170786e-01
  5.57717205e-01  5.58296354e-01  5.63279646e-01  5.64738883e-01
  5.65546326e-01  5.66294462e-01  5.67688244e-01  5.71988399e-01
  5.72647001e-01  5.75500734e-01  5.77176946e-01  5.83179136e-01
  5.85291065e-01  5.91813751e-01  5.91995281e-01  5.94206070e-01
  5.96466349e-01  5.97101853e-01  5.97838779e-01  6.06329729e-01
  6.08342735e-01  6.08922639e-01  6.09616547e-01  6.09752335e-01
  6.15906560e-01  6.21507819e-01  6.24540362e-01  6.35483539e-01
  6.35807832e-01  6.41737921e-01  6.56944262e-01  6.59007746e-01
  6.66399866e-01  6.68053764e-01  6.70274929e-01  6.70930384e-01
  6.71812880e-01  6.82061738e-01  6.97025729e-01  6.99497689e-01
  7.03194707e-01  7.05394586e-01  7.06002346e-01  7.11652806e-01
  7.14226076e-01  7.28213872e-01  7.39183733e-01  7.45064605e-01
  7.46716603e-01  7.48243842e-01  7.54835698e-01  7.55646381e-01
  7.62497343e-01  7.66033770e-01  7.68280412e-01  7.69590163e-01
  7.70572816e-01  7.91318149e-01  7.94025852e-01  7.95046301e-01
  7.96885522e-01  7.97302555e-01  8.01842998e-01  8.48245060e-01
  8.55411566e-01  8.56961458e-01  8.59104947e-01  8.72097704e-01
  9.00316396e-01  9.03244487e-01  9.59790759e-01  9.92758171e-01
  1.00223678e+00]

  warnings.warn(

2022-11-03 10:53:32,441:INFO:Calculating mean and std
2022-11-03 10:53:32,441:INFO:Creating metrics dataframe
2022-11-03 10:53:32,449:INFO:Uploading results into container
2022-11-03 10:53:32,457:INFO:Uploading model into container now
2022-11-03 10:53:32,457:INFO:master_model_container: 32
2022-11-03 10:53:32,457:INFO:display_container: 2
2022-11-03 10:53:32,457:INFO:GradientBoostingRegressor(random_state=4411)
2022-11-03 10:53:32,457:INFO:create_model() successfully completed......................................
2022-11-03 10:53:32,730:ERROR:create_model() for GradientBoostingRegressor(random_state=4411) raised an exception or returned all 0.0:
2022-11-03 10:53:32,730:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:53:32,730:INFO:Initializing Extreme Gradient Boosting
2022-11-03 10:53:32,730:INFO:Total runtime is 4.947011776765188 minutes
2022-11-03 10:53:32,730:INFO:SubProcess create_model() called ==================================
2022-11-03 10:53:32,730:INFO:Initializing create_model()
2022-11-03 10:53:32,730:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:53:32,730:INFO:Checking exceptions
2022-11-03 10:53:32,730:INFO:Importing libraries
2022-11-03 10:53:32,730:INFO:Copying training dataset
2022-11-03 10:53:32,746:INFO:Defining folds
2022-11-03 10:53:32,746:INFO:Declaring metric variables
2022-11-03 10:53:32,746:INFO:Importing untrained model
2022-11-03 10:53:32,746:INFO:Extreme Gradient Boosting Imported successfully
2022-11-03 10:53:32,746:INFO:Starting cross validation
2022-11-03 10:53:32,746:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:53:40,712:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.88988000e-01 -1.76576972e-01 -1.71571448e-01 -1.66697547e-01
 -1.33248359e-01 -1.00311220e-01 -9.74804536e-02 -9.48901176e-02
 -9.12350491e-02 -8.87042284e-02 -8.78828987e-02 -8.77174214e-02
 -7.27807283e-02 -7.23476484e-02 -6.85189888e-02 -6.61663562e-02
 -6.49424717e-02 -6.37840107e-02 -6.30119964e-02 -6.08711764e-02
 -5.69863580e-02 -5.60489334e-02 -5.54712154e-02 -5.54161072e-02
 -5.53485528e-02 -5.50363548e-02 -5.37240021e-02 -5.30433655e-02
 -5.21487407e-02 -4.73575965e-02 -4.16230150e-02 -3.84438746e-02
 -3.83744277e-02 -3.63756232e-02 -3.46212089e-02 -3.06432918e-02
 -2.99950726e-02 -2.66422369e-02 -2.66095400e-02 -2.29848009e-02
 -2.25852169e-02 -2.25018188e-02 -2.24423856e-02 -2.22119298e-02
 -2.17900183e-02 -2.11085416e-02 -2.02375799e-02 -1.98044218e-02
 -1.85180884e-02 -1.81676261e-02 -1.60597079e-02 -1.54845370e-02
 -1.47886695e-02 -1.41007965e-02 -1.37957148e-02 -1.36116054e-02
 -1.16248569e-02 -1.15826074e-02 -1.13252811e-02 -1.10612940e-02
 -9.91250761e-03 -9.72751062e-03 -8.21047276e-03 -8.11202172e-03
 -8.03146791e-03 -8.01900309e-03 -6.71380898e-03 -6.33452088e-03
 -5.36071649e-03 -5.34667494e-03 -5.05407201e-03 -4.44296980e-03
 -4.15786076e-03 -3.44156916e-03 -2.78524077e-03 -2.11609527e-03
 -1.43873820e-03 -1.37747999e-03 -1.11995789e-03 -4.77418303e-04
  4.36981791e-04  5.12878178e-04  8.21028138e-04  8.33104714e-04
  9.76170995e-04  1.07085670e-03  1.08434854e-03  1.51699269e-03
  2.70336028e-03  2.77858041e-03  3.03944433e-03  3.08622979e-03
  3.18189408e-03  3.28977453e-03  3.55147221e-03  3.59455752e-03
  3.87476757e-03  3.98058770e-03  4.11919039e-03  4.16616537e-03
  4.60662879e-03  5.01287589e-03  5.09506091e-03  6.05752179e-03
  6.07958715e-03  6.25154655e-03  6.69522723e-03  7.02030398e-03
  7.28497095e-03  8.02448485e-03  8.57318752e-03  9.32154991e-03
  9.41257831e-03  9.89381596e-03  9.99648124e-03  1.00518772e-02
  1.03772953e-02  1.04760230e-02  1.05178552e-02  1.11280913e-02
  1.14988368e-02  1.16849914e-02  1.48736555e-02  1.58962198e-02
  1.77481417e-02  1.80982128e-02  1.87523551e-02  1.93422772e-02
  1.93647612e-02  1.93809681e-02  1.98205244e-02  2.04116143e-02
  2.08658203e-02  2.10326519e-02  2.14316268e-02  2.41217464e-02
  2.53152661e-02  2.55239550e-02  2.67761331e-02  2.68519055e-02
  2.73139868e-02  2.76470240e-02  2.86543928e-02  3.12674604e-02
  3.14416699e-02  3.20742838e-02  3.22664194e-02  3.22677344e-02
  3.24297957e-02  3.35247628e-02  3.48715484e-02  3.52618098e-02
  3.57540920e-02  3.65446359e-02  3.70554700e-02  3.71068530e-02
  3.75638790e-02  3.97426561e-02  3.97645906e-02  4.06735912e-02
  4.10337038e-02  4.13098708e-02  4.44141403e-02  4.53335755e-02
  4.59356904e-02  4.65507396e-02  4.75028530e-02  4.90898862e-02
  4.91206758e-02  4.93863635e-02  4.95435782e-02  5.10387011e-02
  5.20932749e-02  5.25311045e-02  5.26433885e-02  5.44174165e-02
  5.47296554e-02  5.51134534e-02  5.58888502e-02  5.69142513e-02
  5.70354462e-02  5.87835461e-02  6.08757734e-02  6.15921952e-02
  6.39340952e-02  6.51827008e-02  6.75964281e-02  6.84829056e-02
  6.92695379e-02  7.13249967e-02  7.19300508e-02  7.26898164e-02
  7.53858238e-02  7.56519958e-02  7.61069208e-02  7.68628865e-02
  7.76281357e-02  7.80649260e-02  7.86592588e-02  7.94903412e-02
  7.97868595e-02  8.00074562e-02  8.07819143e-02  8.08644071e-02
  8.11987370e-02  8.12203810e-02  8.25183615e-02  8.27471241e-02
  8.29710439e-02  8.48010704e-02  8.54125395e-02  8.64737481e-02
  8.67063105e-02  8.86067450e-02  8.99238139e-02  9.04096961e-02
  9.37522203e-02  9.50096026e-02  9.56893191e-02  9.70747620e-02
  9.74152386e-02  9.79439169e-02  9.82738212e-02  9.89339128e-02
  9.93441492e-02  1.00768030e-01  1.06400184e-01  1.06881104e-01
  1.07083842e-01  1.09652348e-01  1.10151172e-01  1.11134060e-01
  1.13979146e-01  1.14622734e-01  1.15421042e-01  1.15499884e-01
  1.17582664e-01  1.19687356e-01  1.20385848e-01  1.20950229e-01
  1.22679055e-01  1.22894257e-01  1.22910790e-01  1.23758204e-01
  1.24100715e-01  1.24804184e-01  1.27230868e-01  1.27811521e-01
  1.31455258e-01  1.36664152e-01  1.38871059e-01  1.39528900e-01
  1.45144358e-01  1.49277821e-01  1.51582092e-01  1.59464508e-01
  1.63488075e-01  1.63507313e-01  1.66043177e-01  1.66634098e-01
  1.69121578e-01  1.77861840e-01  1.78514495e-01  1.78747967e-01
  1.78838059e-01  1.79252759e-01  1.81535542e-01  1.84221253e-01
  1.84766486e-01  1.87429503e-01  1.89068154e-01  1.89899564e-01
  1.91793948e-01  1.93487987e-01  1.94888741e-01  1.97166920e-01
  2.00299174e-01  2.02683464e-01  2.03623801e-01  2.06746161e-01
  2.10553110e-01  2.11372405e-01  2.13829160e-01  2.18258828e-01
  2.22528741e-01  2.23170862e-01  2.29995117e-01  2.34139740e-01
  2.37926915e-01  2.38230109e-01  2.39890531e-01  2.47314960e-01
  2.47650892e-01  2.47736320e-01  2.47965381e-01  2.48756602e-01
  2.48983815e-01  2.50943333e-01  2.51949698e-01  2.53100246e-01
  2.53437400e-01  2.53538638e-01  2.55282164e-01  2.57212311e-01
  2.59153575e-01  2.60608077e-01  2.63005853e-01  2.63328165e-01
  2.66103506e-01  2.68036634e-01  2.72541314e-01  2.74569064e-01
  2.74642080e-01  2.81251639e-01  2.82083869e-01  2.84624636e-01
  2.85110116e-01  2.86885649e-01  2.88564712e-01  2.89354742e-01
  2.90607423e-01  2.91190386e-01  2.91478217e-01  2.97034323e-01
  2.98803002e-01  2.99337655e-01  2.99771041e-01  3.02048385e-01
  3.03432912e-01  3.16955328e-01  3.18341792e-01  3.22736174e-01
  3.26494038e-01  3.30179304e-01  3.38613719e-01  3.42234313e-01
  3.46001685e-01  3.46417457e-01  3.48507494e-01  3.59373569e-01
  3.61131757e-01  3.65664363e-01  3.67879540e-01  3.69945794e-01
  3.73654038e-01  3.80191177e-01  3.81978840e-01  3.84584814e-01
  3.86430442e-01  3.87800097e-01  3.92290801e-01  3.98442626e-01
  3.98770303e-01  3.98871779e-01  4.01166439e-01  4.02691394e-01
  4.08387601e-01  4.09736753e-01  4.14884746e-01  4.16047662e-01
  4.17071313e-01  4.20369953e-01  4.24400508e-01  4.30193186e-01
  4.34159964e-01  4.41580951e-01  4.42710280e-01  4.54819858e-01
  4.58154440e-01  4.60000753e-01  4.61160362e-01  4.67168212e-01
  4.72265393e-01  4.73904043e-01  4.74355668e-01  4.74796623e-01
  4.77929235e-01  4.78448331e-01  4.89932328e-01  4.90556628e-01
  4.98081595e-01  5.11834621e-01  5.19866705e-01  5.20956933e-01
  5.26012540e-01  5.26103616e-01  5.29764414e-01  5.38170695e-01
  5.40493727e-01  5.41164875e-01  5.41309178e-01  5.42257190e-01
  5.43882489e-01  5.46441734e-01  5.53346515e-01  5.57508826e-01
  5.65432847e-01  5.66767573e-01  5.68976343e-01  5.72425961e-01
  5.72481990e-01  5.73576510e-01  5.73668182e-01  5.75030327e-01
  5.80136180e-01  5.81779420e-01  5.85253119e-01  5.86083174e-01
  5.92763782e-01  6.02474749e-01  6.07875288e-01  6.10379159e-01
  6.13651752e-01  6.14273012e-01  6.22463703e-01  6.25066996e-01
  6.28315270e-01  6.29028141e-01  6.30064785e-01  6.44558668e-01
  6.51838660e-01  6.54634356e-01  6.56761646e-01  6.69203222e-01
  6.78956509e-01  6.80767477e-01  6.81956470e-01  6.99888766e-01
  7.00951397e-01  7.01074123e-01  7.01669872e-01  7.08895326e-01
  7.11792886e-01  7.29475558e-01  7.42548227e-01  7.47837901e-01
  7.53527880e-01  7.54387140e-01  7.54457057e-01  7.59973764e-01
  7.64862061e-01  7.69461751e-01  7.69538164e-01  7.75240839e-01
  7.76298881e-01  7.77969778e-01  7.81998098e-01  7.84575045e-01
  7.85646319e-01  7.91790426e-01  7.92509079e-01  7.98959434e-01
  8.02150428e-01  8.09132576e-01  8.21642578e-01  8.25154126e-01
  8.29188287e-01  8.29841077e-01  8.30112040e-01  8.35459769e-01
  8.49293530e-01  8.51338148e-01  8.52296293e-01  8.63716662e-01
  8.77619445e-01  8.79638314e-01  8.84648025e-01  8.95167768e-01
  9.01441991e-01  9.01885092e-01  9.20217872e-01  9.27325785e-01
  9.29709911e-01  9.31311607e-01  9.37634706e-01  9.39463317e-01
  9.46382284e-01  9.47453320e-01  9.50279355e-01  9.68617737e-01
  9.69851851e-01  9.71685588e-01  9.82397914e-01  9.88648176e-01
  9.93091643e-01  1.00439823e+00  1.02544188e+00  1.03438663e+00
  1.04791522e+00  1.06993127e+00  1.08090532e+00  1.09394956e+00
  1.30071473e+00]

  warnings.warn(

2022-11-03 10:53:40,712:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.86453685e-01 -1.73096776e-01 -1.70364738e-01 -1.53137729e-01
 -1.34879738e-01 -1.27697080e-01 -1.17752932e-01 -1.13179646e-01
 -1.11451067e-01 -1.07123666e-01 -1.03784755e-01 -9.74390134e-02
 -9.63697881e-02 -9.44712758e-02 -9.24114808e-02 -8.90772939e-02
 -8.87283012e-02 -8.69307369e-02 -8.28613266e-02 -8.09697509e-02
 -7.54536986e-02 -7.43904337e-02 -6.93479776e-02 -6.69196397e-02
 -6.59680068e-02 -5.81041798e-02 -5.67717999e-02 -5.48091270e-02
 -4.95422482e-02 -4.76058535e-02 -4.69429083e-02 -4.27739583e-02
 -4.02819850e-02 -3.76246944e-02 -3.73845734e-02 -3.63582782e-02
 -3.58591937e-02 -3.58446985e-02 -3.49887870e-02 -3.39323655e-02
 -3.13994475e-02 -3.06807812e-02 -2.93162223e-02 -2.90437844e-02
 -2.65856218e-02 -2.34324113e-02 -2.12483853e-02 -1.97832901e-02
 -1.68113820e-02 -1.63946897e-02 -1.61098018e-02 -1.40685923e-02
 -1.39874304e-02 -1.36386929e-02 -1.31371543e-02 -1.24564767e-02
 -1.22633278e-02 -1.15682641e-02 -1.14018423e-02 -1.11328168e-02
 -9.38697904e-03 -9.33929067e-03 -9.06516984e-03 -8.70581716e-03
 -8.37686472e-03 -8.22954345e-03 -8.14496446e-03 -7.92899542e-03
 -7.06175808e-03 -6.10203436e-03 -5.59368310e-03 -4.80092503e-03
 -3.31174978e-03 -2.78409687e-03 -1.53743860e-03 -9.52619826e-04
 -1.53617788e-04  1.91755491e-04  2.93550576e-04  3.14745732e-04
  3.21921660e-04  4.41082404e-04  1.04170037e-03  2.03420664e-03
  2.61907233e-03  2.84279487e-03  3.02130287e-03  3.95367155e-03
  4.00436204e-03  4.04291786e-03  4.24462091e-03  4.41827346e-03
  5.14955493e-03  6.73335930e-03  8.44011363e-03  8.73423181e-03
  9.05303936e-03  9.24706366e-03  1.00003425e-02  1.00288372e-02
  1.08854594e-02  1.12582762e-02  1.18644107e-02  1.20447651e-02
  1.30470414e-02  1.30902482e-02  1.33340005e-02  1.37524605e-02
  1.41133741e-02  1.46123804e-02  1.50362095e-02  1.72973610e-02
  1.78084727e-02  1.79798920e-02  1.80777665e-02  1.90044343e-02
  1.97962187e-02  2.01439876e-02  2.02233549e-02  2.08824892e-02
  2.19561439e-02  2.20069196e-02  2.25758459e-02  2.28504911e-02
  2.32453980e-02  2.32644603e-02  2.38300525e-02  2.43505109e-02
  2.44696606e-02  2.52897702e-02  2.56992616e-02  2.57065091e-02
  2.75918152e-02  2.84989551e-02  2.93062013e-02  2.93869134e-02
  2.96056904e-02  3.33089642e-02  3.35991122e-02  3.37311849e-02
  3.58021110e-02  3.58250178e-02  3.70264538e-02  3.81138325e-02
  3.82859111e-02  3.83442603e-02  3.99648324e-02  4.01025452e-02
  4.11267243e-02  4.19925787e-02  4.27715890e-02  4.37901802e-02
  4.56172749e-02  4.59352843e-02  4.65608761e-02  4.76476513e-02
  4.89088967e-02  5.24465069e-02  5.58775924e-02  5.62446862e-02
  5.65985255e-02  5.66439033e-02  5.95830418e-02  6.03981763e-02
  6.14977814e-02  6.38643131e-02  6.52753413e-02  6.55632392e-02
  6.55933768e-02  6.61784112e-02  6.69808835e-02  6.75083399e-02
  6.77026808e-02  6.82540536e-02  6.91604689e-02  6.93055764e-02
  6.97167143e-02  6.98202327e-02  6.99652135e-02  7.16170296e-02
  7.49352798e-02  7.64284730e-02  7.80150965e-02  7.94065297e-02
  7.95396641e-02  8.04376379e-02  8.55889246e-02  8.72443244e-02
  8.72717872e-02  8.75875205e-02  8.86903927e-02  8.89147148e-02
  8.97983015e-02  9.03731063e-02  9.42283720e-02  9.59748030e-02
  1.01293914e-01  1.01782605e-01  1.02047071e-01  1.02952182e-01
  1.03680246e-01  1.04628295e-01  1.05318926e-01  1.05608255e-01
  1.06162369e-01  1.07248239e-01  1.07893780e-01  1.07980601e-01
  1.10404544e-01  1.10899337e-01  1.11493304e-01  1.11745670e-01
  1.13532983e-01  1.13682628e-01  1.16701432e-01  1.18287422e-01
  1.19431354e-01  1.19500682e-01  1.22298546e-01  1.22888863e-01
  1.23314299e-01  1.24278471e-01  1.24304928e-01  1.24681205e-01
  1.24708876e-01  1.26307234e-01  1.30344898e-01  1.33449167e-01
  1.34308398e-01  1.34706348e-01  1.38602763e-01  1.38838530e-01
  1.40621662e-01  1.41583055e-01  1.41853288e-01  1.43801451e-01
  1.44645363e-01  1.46246940e-01  1.47526518e-01  1.47689611e-01
  1.49574816e-01  1.50803566e-01  1.58673763e-01  1.60072401e-01
  1.60848781e-01  1.63328603e-01  1.64069697e-01  1.69095963e-01
  1.70979574e-01  1.72640786e-01  1.72812611e-01  1.74257874e-01
  1.78522006e-01  1.79353297e-01  1.79406345e-01  1.80521086e-01
  1.81370243e-01  1.83746740e-01  1.83921859e-01  1.84292838e-01
  1.86759666e-01  1.90160170e-01  1.91004738e-01  1.91621006e-01
  1.98322266e-01  1.98722005e-01  1.98932722e-01  2.13947669e-01
  2.15508714e-01  2.16462076e-01  2.16580987e-01  2.16822863e-01
  2.19965994e-01  2.23447368e-01  2.26552531e-01  2.31091082e-01
  2.31458470e-01  2.35710129e-01  2.35725656e-01  2.36000896e-01
  2.36792743e-01  2.41097778e-01  2.44808286e-01  2.47401282e-01
  2.48163968e-01  2.48218849e-01  2.49079734e-01  2.49319419e-01
  2.51073033e-01  2.53544748e-01  2.56197602e-01  2.57250607e-01
  2.59285063e-01  2.59951860e-01  2.65433103e-01  2.65753299e-01
  2.78273255e-01  2.82703310e-01  2.83160388e-01  2.84812540e-01
  2.86628813e-01  2.93294996e-01  2.94306904e-01  2.98800081e-01
  2.99133897e-01  2.99716204e-01  3.02498758e-01  3.05917829e-01
  3.09941560e-01  3.11807096e-01  3.15006763e-01  3.18243325e-01
  3.20044577e-01  3.20095599e-01  3.22216481e-01  3.22984248e-01
  3.24211925e-01  3.27579409e-01  3.32643688e-01  3.33808243e-01
  3.36334378e-01  3.39063853e-01  3.40915293e-01  3.49732608e-01
  3.50275934e-01  3.50951821e-01  3.51500899e-01  3.53998035e-01
  3.56575787e-01  3.59760791e-01  3.59774858e-01  3.60875100e-01
  3.65043640e-01  3.71433824e-01  3.75281483e-01  3.76624078e-01
  3.78666103e-01  3.79929394e-01  3.86434376e-01  3.92096847e-01
  4.01552856e-01  4.05242532e-01  4.06711698e-01  4.12208200e-01
  4.17631090e-01  4.28496748e-01  4.30998862e-01  4.36672896e-01
  4.37937230e-01  4.46527928e-01  4.47854161e-01  4.49018747e-01
  4.55168575e-01  4.59863126e-01  4.61048037e-01  4.64544028e-01
  4.69759583e-01  4.70164180e-01  4.72294897e-01  4.78735358e-01
  4.79523510e-01  4.80581462e-01  4.92026150e-01  4.94447619e-01
  4.99179393e-01  5.06341934e-01  5.11873424e-01  5.13845980e-01
  5.21168292e-01  5.29980361e-01  5.31557858e-01  5.32297432e-01
  5.33383548e-01  5.34012556e-01  5.40623069e-01  5.44184387e-01
  5.44437408e-01  5.44823468e-01  5.45937181e-01  5.47427595e-01
  5.47573030e-01  5.47689974e-01  5.48285007e-01  5.48574626e-01
  5.51823795e-01  5.52356899e-01  5.59807181e-01  5.61147153e-01
  5.69019437e-01  5.69690287e-01  5.70228994e-01  5.72965264e-01
  5.75336993e-01  5.78767955e-01  5.79954445e-01  5.83080351e-01
  5.85589826e-01  5.85799098e-01  5.90406477e-01  5.91715634e-01
  5.96756518e-01  6.00431502e-01  6.05502665e-01  6.11593008e-01
  6.16046667e-01  6.16380095e-01  6.17088020e-01  6.23115659e-01
  6.23994470e-01  6.26225531e-01  6.27010643e-01  6.40629649e-01
  6.46050394e-01  6.46373689e-01  6.48220062e-01  6.61384821e-01
  6.63450062e-01  6.65529490e-01  6.66393697e-01  6.69054270e-01
  6.74565554e-01  6.77335262e-01  6.80229127e-01  6.92379236e-01
  7.00016618e-01  7.00698495e-01  7.01117218e-01  7.07054198e-01
  7.07088768e-01  7.10525811e-01  7.10637093e-01  7.11534202e-01
  7.14519978e-01  7.14778841e-01  7.17383027e-01  7.18115330e-01
  7.22651064e-01  7.27274299e-01  7.35107064e-01  7.48562157e-01
  7.49339521e-01  7.53315151e-01  7.65057385e-01  7.72385299e-01
  7.83373237e-01  7.84648359e-01  7.94935226e-01  7.96584904e-01
  8.00218940e-01  8.05892766e-01  8.12892675e-01  8.16632152e-01
  8.17825198e-01  8.19005370e-01  8.22846293e-01  8.32589447e-01
  8.36668611e-01  8.37782025e-01  8.43545675e-01  8.50412488e-01
  8.53327930e-01  8.55889499e-01  8.56867135e-01  8.61585796e-01
  8.62462878e-01  8.71358693e-01  8.73015761e-01  8.80390227e-01
  8.97541165e-01  9.02768195e-01  9.02896821e-01  9.03707743e-01
  9.07400727e-01  9.15344834e-01  9.37188864e-01  9.37801480e-01
  9.45582449e-01  9.49658096e-01  9.52544034e-01  9.60369051e-01
  9.77999628e-01  9.95931089e-01  9.99918759e-01  1.00391054e+00
  1.00625145e+00  1.00774574e+00  1.04214227e+00  1.04966700e+00
  1.07402062e+00  1.09999812e+00  1.17238069e+00  1.19503880e+00
  1.20490408e+00]

  warnings.warn(

2022-11-03 10:53:40,744:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.41895109e-01 -1.84924543e-01 -1.69234917e-01 -1.52095780e-01
 -1.39159366e-01 -1.17166489e-01 -1.12446427e-01 -9.68548730e-02
 -9.56162289e-02 -8.91174898e-02 -8.87474343e-02 -8.72981623e-02
 -8.68616030e-02 -7.52801746e-02 -7.43265748e-02 -7.22901523e-02
 -7.05612451e-02 -6.62080869e-02 -6.37652054e-02 -6.18715659e-02
 -5.86094074e-02 -5.83499409e-02 -5.75146042e-02 -5.51406778e-02
 -4.15935330e-02 -4.10973653e-02 -3.97843644e-02 -3.81956026e-02
 -3.64732705e-02 -3.31856534e-02 -3.30816694e-02 -3.27145867e-02
 -3.02020386e-02 -2.95971371e-02 -2.83999033e-02 -2.81592719e-02
 -2.78230496e-02 -2.70580463e-02 -2.50487644e-02 -2.40450837e-02
 -2.37502772e-02 -2.32033003e-02 -2.30398066e-02 -1.93495490e-02
 -1.90155879e-02 -1.82725657e-02 -1.79988891e-02 -1.79326124e-02
 -1.63077433e-02 -1.49121685e-02 -1.48596307e-02 -1.46075655e-02
 -1.35354903e-02 -1.29093193e-02 -1.18355434e-02 -1.18200835e-02
 -1.15840016e-02 -1.12919779e-02 -1.07907001e-02 -1.02625042e-02
 -9.69701819e-03 -9.32975579e-03 -9.14930739e-03 -7.57125439e-03
 -6.04506861e-03 -5.58902975e-03 -5.51127596e-03 -5.35468617e-03
 -4.80791181e-03 -4.71944967e-03 -3.67058371e-03 -3.63022671e-03
 -3.06584616e-03 -2.32976954e-03 -2.29445775e-03 -2.01494782e-03
 -1.64155569e-03 -1.15075859e-03 -6.31478324e-04 -3.47551773e-04
  6.97321258e-04  1.28071452e-03  1.59866980e-03  1.81937579e-03
  1.87094673e-03  3.14652594e-03  3.71143338e-03  4.31198301e-03
  4.45832917e-03  5.11500333e-03  5.33441640e-03  5.52044529e-03
  6.76377723e-03  6.80156285e-03  6.97875535e-03  7.73029123e-03
  8.19672924e-03  8.79812893e-03  9.42897610e-03  1.04061225e-02
  1.04443012e-02  1.05312532e-02  1.25334458e-02  1.25546185e-02
  1.30938347e-02  1.37450211e-02  1.38094416e-02  1.43715879e-02
  1.52873211e-02  1.52963940e-02  1.66330189e-02  1.77642591e-02
  1.79964341e-02  2.10691467e-02  2.11322121e-02  2.13275682e-02
  2.15671137e-02  2.30114702e-02  2.48812195e-02  2.66633816e-02
  2.75769066e-02  2.78134122e-02  2.95330919e-02  2.95517333e-02
  3.01392544e-02  3.02259568e-02  3.08539160e-02  3.11707817e-02
  3.12981978e-02  3.24212611e-02  3.32395136e-02  3.36857848e-02
  3.69647332e-02  3.72687802e-02  3.82128134e-02  3.95741835e-02
  4.00258303e-02  4.02682126e-02  4.02763151e-02  4.04184051e-02
  4.09960859e-02  4.10050564e-02  4.16019298e-02  4.22075875e-02
  4.28715311e-02  4.31169569e-02  4.36053127e-02  4.44516987e-02
  4.50174920e-02  4.51642983e-02  4.54372019e-02  4.56403568e-02
  4.62008901e-02  4.69371155e-02  4.73684967e-02  4.77989800e-02
  4.79626358e-02  4.81504165e-02  4.81970869e-02  4.94711585e-02
  4.99657467e-02  5.00796251e-02  5.00889607e-02  5.11899404e-02
  5.20553142e-02  5.24525605e-02  5.29489703e-02  5.66002131e-02
  5.88133447e-02  6.04917854e-02  6.10810779e-02  6.14607148e-02
  6.21588007e-02  6.34770170e-02  6.36467561e-02  6.54774755e-02
  6.60039634e-02  6.73479140e-02  6.75433427e-02  6.80650100e-02
  6.87100217e-02  6.98543191e-02  7.02354610e-02  7.42738768e-02
  7.44630098e-02  7.55006224e-02  7.63542056e-02  7.77730495e-02
  7.78983310e-02  7.80146047e-02  7.96169639e-02  8.18162784e-02
  8.24158937e-02  8.32758546e-02  8.50055590e-02  8.51753354e-02
  8.59676152e-02  8.68623480e-02  8.69219750e-02  8.71748477e-02
  8.86263251e-02  8.92423019e-02  9.10445750e-02  9.14042592e-02
  9.14979652e-02  9.26011205e-02  9.41508114e-02  9.63175818e-02
  9.69574228e-02  9.79977250e-02  9.80244651e-02  9.92553681e-02
  9.99136791e-02  1.00107595e-01  1.00115225e-01  1.01928249e-01
  1.02095246e-01  1.03210464e-01  1.05491698e-01  1.05531700e-01
  1.05671652e-01  1.05970018e-01  1.07288465e-01  1.07889473e-01
  1.08647630e-01  1.09078504e-01  1.13893002e-01  1.16061471e-01
  1.17193423e-01  1.18653327e-01  1.19435638e-01  1.22814603e-01
  1.28521517e-01  1.28738865e-01  1.29091486e-01  1.30841196e-01
  1.31229430e-01  1.31416515e-01  1.31426200e-01  1.33706436e-01
  1.35238573e-01  1.37482807e-01  1.37780190e-01  1.38748616e-01
  1.42494813e-01  1.43991485e-01  1.46743491e-01  1.51072577e-01
  1.51325092e-01  1.52845994e-01  1.53690025e-01  1.55822888e-01
  1.56995073e-01  1.62324756e-01  1.63200021e-01  1.64817438e-01
  1.69821665e-01  1.71266168e-01  1.75477669e-01  1.81999639e-01
  1.84343025e-01  1.84725910e-01  1.85182884e-01  1.86856523e-01
  1.91094071e-01  1.91633850e-01  1.93259716e-01  1.98387012e-01
  1.98440731e-01  1.98998138e-01  1.99032798e-01  2.02778578e-01
  2.05812290e-01  2.07721844e-01  2.09029555e-01  2.12809861e-01
  2.15387270e-01  2.18506753e-01  2.19289973e-01  2.21733510e-01
  2.23064438e-01  2.23884031e-01  2.28067845e-01  2.28278488e-01
  2.32259810e-01  2.32473359e-01  2.35276505e-01  2.43555710e-01
  2.43825287e-01  2.45321989e-01  2.46243253e-01  2.46403739e-01
  2.46607289e-01  2.49063030e-01  2.49643937e-01  2.50218779e-01
  2.58230269e-01  2.63316721e-01  2.67446876e-01  2.67669380e-01
  2.69252717e-01  2.69437522e-01  2.75771379e-01  2.77218252e-01
  2.78891444e-01  2.81028777e-01  2.81992674e-01  2.82723397e-01
  2.85793006e-01  2.85982430e-01  2.87276715e-01  2.89792061e-01
  2.93984771e-01  2.94132769e-01  2.94885993e-01  2.95611680e-01
  2.96635002e-01  3.01366389e-01  3.04003865e-01  3.04199159e-01
  3.05053562e-01  3.05570662e-01  3.06540698e-01  3.12920660e-01
  3.15906733e-01  3.16133320e-01  3.18241537e-01  3.18361372e-01
  3.20286870e-01  3.22773814e-01  3.22838992e-01  3.23517412e-01
  3.26129615e-01  3.27038586e-01  3.28461617e-01  3.31065387e-01
  3.35680038e-01  3.39784831e-01  3.42427105e-01  3.43023181e-01
  3.47128689e-01  3.47336411e-01  3.49067599e-01  3.54072750e-01
  3.55037481e-01  3.55468929e-01  3.56541514e-01  3.64147395e-01
  3.66279185e-01  3.66721302e-01  3.68782490e-01  3.72552544e-01
  3.74357224e-01  3.77082169e-01  3.77752781e-01  3.82669330e-01
  3.83526921e-01  3.94822717e-01  3.98075372e-01  3.99902850e-01
  4.00670260e-01  4.00928944e-01  4.12159622e-01  4.21407521e-01
  4.23598796e-01  4.29632157e-01  4.33215529e-01  4.33418036e-01
  4.35926408e-01  4.37304914e-01  4.37891543e-01  4.39745247e-01
  4.42226529e-01  4.55056846e-01  4.55680400e-01  4.57219869e-01
  4.57437038e-01  4.57630843e-01  4.57975149e-01  4.66346264e-01
  4.70894605e-01  4.74826068e-01  4.76030380e-01  4.76115346e-01
  4.78041500e-01  4.81604755e-01  4.84125614e-01  4.89694864e-01
  4.93672490e-01  5.00812948e-01  5.01006424e-01  5.05005419e-01
  5.10730028e-01  5.17618954e-01  5.19548297e-01  5.20230651e-01
  5.21788359e-01  5.28762519e-01  5.31117558e-01  5.33417284e-01
  5.41244984e-01  5.44408798e-01  5.47284424e-01  5.53286910e-01
  5.54530859e-01  5.66101372e-01  5.68319201e-01  5.70966959e-01
  5.73012769e-01  5.76227069e-01  5.80382705e-01  5.82343817e-01
  5.85775018e-01  5.92020690e-01  5.94546020e-01  6.09632492e-01
  6.14179134e-01  6.24204397e-01  6.28046334e-01  6.32763565e-01
  6.33248508e-01  6.42009020e-01  6.43213332e-01  6.47288859e-01
  6.64706588e-01  6.64904773e-01  6.78667843e-01  6.79268718e-01
  6.85594082e-01  6.89810216e-01  6.93895698e-01  6.98149145e-01
  7.03693032e-01  7.07786143e-01  7.09365010e-01  7.09396183e-01
  7.10538447e-01  7.12831318e-01  7.15638399e-01  7.25106120e-01
  7.29028881e-01  7.38282204e-01  7.49211013e-01  7.51648247e-01
  7.54588425e-01  7.60355771e-01  7.63640523e-01  7.69767046e-01
  7.82856166e-01  7.83913314e-01  7.87509620e-01  7.89612174e-01
  7.91516721e-01  7.91543186e-01  7.98738241e-01  8.00110459e-01
  8.02125752e-01  8.18375051e-01  8.27289879e-01  8.29078257e-01
  8.30120862e-01  8.31301987e-01  8.43601286e-01  8.48055303e-01
  8.51467907e-01  8.59177291e-01  8.62041950e-01  8.83049905e-01
  8.86636436e-01  8.89469504e-01  8.94880772e-01  8.96323144e-01
  8.98369968e-01  9.04843390e-01  9.17979181e-01  9.20053422e-01
  9.24774528e-01  9.29971993e-01  9.31416333e-01  9.43963468e-01
  9.45582807e-01  9.56767380e-01  9.67124820e-01  9.68535483e-01
  9.74584401e-01  9.91963446e-01  9.93678212e-01  1.00310993e+00
  1.01374936e+00  1.02902067e+00  1.08749270e+00  1.11917388e+00
  1.22920334e+00]

  warnings.warn(

2022-11-03 10:53:40,839:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.41973788e-01 -2.25245759e-01 -1.71147808e-01 -1.70578837e-01
 -1.64338261e-01 -1.49234444e-01 -1.38623849e-01 -1.00532651e-01
 -9.92353186e-02 -9.22031924e-02 -8.42857957e-02 -8.39339197e-02
 -8.19465071e-02 -7.24167973e-02 -7.14769214e-02 -6.92429170e-02
 -6.77258447e-02 -6.70992807e-02 -6.20093532e-02 -5.18554188e-02
 -5.16554117e-02 -5.14005944e-02 -5.07839508e-02 -5.05513251e-02
 -4.96393107e-02 -4.90949936e-02 -4.65422869e-02 -4.38302532e-02
 -4.29598130e-02 -4.13159281e-02 -4.02998179e-02 -3.92829590e-02
 -3.82440723e-02 -3.79798934e-02 -3.71850803e-02 -3.62747237e-02
 -3.20775621e-02 -3.15095857e-02 -2.94189509e-02 -2.94121392e-02
 -2.55238768e-02 -2.41177827e-02 -2.36358587e-02 -2.27399711e-02
 -2.27163285e-02 -2.11510006e-02 -2.03364324e-02 -2.02136766e-02
 -1.98505130e-02 -1.98289007e-02 -1.96505710e-02 -1.95668433e-02
 -1.89616848e-02 -1.82630606e-02 -1.72297414e-02 -1.69449784e-02
 -1.58412848e-02 -1.53762642e-02 -1.49719929e-02 -1.46356039e-02
 -1.43518224e-02 -1.38967652e-02 -1.38207339e-02 -1.32986568e-02
 -1.27821760e-02 -1.27542894e-02 -1.26809487e-02 -1.13628628e-02
 -1.13615505e-02 -1.12961531e-02 -1.12810051e-02 -1.00410748e-02
 -9.42201074e-03 -8.73257499e-03 -6.46048039e-03 -6.11707242e-03
 -5.58580970e-03 -5.42859314e-03 -5.38851880e-03 -5.19318320e-03
 -5.01265517e-03 -4.87453165e-03 -4.67646448e-03 -4.14223224e-03
 -3.98880476e-03 -3.04654380e-03 -2.80883024e-03 -2.68783607e-03
 -1.58149132e-03 -5.47427771e-05  1.98937976e-03  2.41009379e-03
  2.66874046e-03  2.69236299e-03  3.57658602e-03  3.91911343e-03
  4.60094633e-03  4.85447049e-03  5.83110610e-03  6.28137868e-03
  6.29679579e-03  6.52102754e-03  7.30911503e-03  7.66280293e-03
  7.78172817e-03  8.54982808e-03  9.76838637e-03  1.05346488e-02
  1.08170575e-02  1.08690225e-02  1.10416366e-02  1.17050977e-02
  1.20356167e-02  1.29860258e-02  1.33730434e-02  1.34965843e-02
  1.37643069e-02  1.43995127e-02  1.43999858e-02  1.50223095e-02
  1.53861903e-02  1.55475065e-02  1.70799699e-02  1.72554869e-02
  1.74165927e-02  1.77407451e-02  1.85776539e-02  1.99982729e-02
  2.14665234e-02  2.22828761e-02  2.25562789e-02  2.29094308e-02
  2.32659895e-02  2.34326907e-02  2.36823037e-02  2.56462879e-02
  2.57361811e-02  2.72219535e-02  2.73259822e-02  2.76261419e-02
  2.78771874e-02  2.85609700e-02  2.86104176e-02  2.91221775e-02
  2.93399021e-02  2.94308756e-02  3.05430498e-02  3.13447416e-02
  3.22095640e-02  3.41257714e-02  3.60204279e-02  3.75348479e-02
  3.80380936e-02  3.86878625e-02  3.87291089e-02  4.12993915e-02
  4.43391949e-02  4.46925201e-02  4.48582582e-02  4.55251522e-02
  4.62989360e-02  4.64920290e-02  4.71798591e-02  4.78559583e-02
  4.80952971e-02  4.86529879e-02  4.89377715e-02  4.90690768e-02
  4.93126959e-02  5.10360189e-02  5.40926084e-02  5.43788187e-02
  5.46386950e-02  5.62430657e-02  5.62862232e-02  5.67830913e-02
  6.53382912e-02  6.73735440e-02  6.76043481e-02  6.77109808e-02
  6.79038242e-02  6.82959408e-02  6.84459507e-02  6.87450320e-02
  7.15199858e-02  7.23517016e-02  7.23847598e-02  7.35071078e-02
  7.47514367e-02  7.51089603e-02  7.57592767e-02  7.63804615e-02
  7.76705965e-02  7.83024654e-02  8.04317817e-02  8.08122754e-02
  8.24966058e-02  8.37066695e-02  8.54385421e-02  8.63107145e-02
  8.82293358e-02  8.94409046e-02  9.18638632e-02  9.18892100e-02
  9.33827683e-02  9.46689919e-02  9.54709873e-02  9.69256982e-02
  9.76359844e-02  9.92515311e-02  9.93361026e-02  9.97209027e-02
  1.00769475e-01  1.09126329e-01  1.12146243e-01  1.12172052e-01
  1.14088133e-01  1.20005205e-01  1.22942641e-01  1.28556103e-01
  1.30755723e-01  1.30777627e-01  1.31115869e-01  1.31625116e-01
  1.32181540e-01  1.37246698e-01  1.39333025e-01  1.40731528e-01
  1.42585531e-01  1.46657914e-01  1.48099989e-01  1.50808349e-01
  1.52844831e-01  1.55579180e-01  1.58437803e-01  1.59578323e-01
  1.61539033e-01  1.64109573e-01  1.64874345e-01  1.65081173e-01
  1.68927118e-01  1.69747770e-01  1.71648055e-01  1.75372660e-01
  1.76093206e-01  1.78247541e-01  1.79506570e-01  1.80047110e-01
  1.81565270e-01  1.82918698e-01  1.83167920e-01  1.88164279e-01
  1.90104634e-01  1.90704718e-01  1.94075152e-01  1.98596701e-01
  1.98861957e-01  1.99503630e-01  2.01494351e-01  2.08040476e-01
  2.08222330e-01  2.08775163e-01  2.14886233e-01  2.17597440e-01
  2.18288079e-01  2.20706865e-01  2.20924035e-01  2.24855915e-01
  2.28517652e-01  2.32082203e-01  2.34874755e-01  2.37819985e-01
  2.38642901e-01  2.38793448e-01  2.41592064e-01  2.42244139e-01
  2.48986438e-01  2.53255278e-01  2.55645335e-01  2.55687058e-01
  2.61584491e-01  2.63940126e-01  2.64561206e-01  2.67333597e-01
  2.67511159e-01  2.69870520e-01  2.73600966e-01  2.76433885e-01
  2.76601970e-01  2.78207421e-01  2.78888792e-01  2.79437929e-01
  2.84092724e-01  2.86420375e-01  2.92797863e-01  2.97692388e-01
  3.00473809e-01  3.04838330e-01  3.07363153e-01  3.11419696e-01
  3.14907968e-01  3.16538572e-01  3.18435550e-01  3.18688631e-01
  3.18859994e-01  3.19904178e-01  3.20374548e-01  3.20801467e-01
  3.21102411e-01  3.26018721e-01  3.27211708e-01  3.27376872e-01
  3.27900350e-01  3.28251839e-01  3.30395877e-01  3.33006054e-01
  3.34584177e-01  3.34797442e-01  3.42223942e-01  3.42257321e-01
  3.45185786e-01  3.47753018e-01  3.50810647e-01  3.52976918e-01
  3.53025079e-01  3.56778115e-01  3.61108094e-01  3.63512576e-01
  3.75503689e-01  3.76480043e-01  3.80918264e-01  3.84920746e-01
  3.93951088e-01  3.96159887e-01  3.97898138e-01  3.99320513e-01
  3.99783254e-01  4.00840521e-01  4.04219329e-01  4.04725820e-01
  4.06700432e-01  4.07201648e-01  4.08806086e-01  4.12332147e-01
  4.15697813e-01  4.16423202e-01  4.16782022e-01  4.16918099e-01
  4.17487383e-01  4.20083553e-01  4.21064973e-01  4.22919869e-01
  4.28492397e-01  4.30135459e-01  4.35010135e-01  4.38854039e-01
  4.42619890e-01  4.49184299e-01  4.54362959e-01  4.61690664e-01
  4.63580310e-01  4.64584053e-01  4.67140645e-01  4.72071588e-01
  4.74638611e-01  4.75268483e-01  4.75702941e-01  4.76358175e-01
  4.77597922e-01  4.79725450e-01  4.79903728e-01  4.80634749e-01
  4.81319427e-01  4.82000679e-01  4.86888677e-01  4.92011368e-01
  4.93035674e-01  4.94771183e-01  4.94853526e-01  4.97984380e-01
  4.99706537e-01  5.00448465e-01  5.06717503e-01  5.15146136e-01
  5.17995417e-01  5.19248605e-01  5.25510669e-01  5.27338564e-01
  5.28249741e-01  5.34191549e-01  5.46492994e-01  5.50576746e-01
  5.50675750e-01  5.55642307e-01  5.57106674e-01  5.57283819e-01
  5.59246361e-01  5.59755445e-01  5.60386956e-01  5.60590446e-01
  5.61064661e-01  5.62397003e-01  5.70858121e-01  5.71617663e-01
  5.74387431e-01  5.77444315e-01  5.82525074e-01  5.87343216e-01
  5.89296997e-01  5.90704083e-01  5.93228996e-01  5.97114742e-01
  6.07086241e-01  6.10936761e-01  6.13351762e-01  6.13442481e-01
  6.17560565e-01  6.31236851e-01  6.37468934e-01  6.37730420e-01
  6.40245020e-01  6.40672863e-01  6.44718170e-01  6.50839686e-01
  6.50933266e-01  6.53057754e-01  6.57566428e-01  6.59660339e-01
  6.63467228e-01  6.64031744e-01  6.68348253e-01  6.70492232e-01
  6.75002098e-01  6.80618286e-01  6.83645785e-01  6.88571632e-01
  7.02825248e-01  7.11556554e-01  7.11989045e-01  7.12997496e-01
  7.16381907e-01  7.27939904e-01  7.41924286e-01  7.43215024e-01
  7.54347682e-01  7.63199985e-01  7.66280830e-01  7.70555317e-01
  7.72489727e-01  7.88969398e-01  7.90347457e-01  7.92486668e-01
  7.92795002e-01  7.94337988e-01  8.11862230e-01  8.12181115e-01
  8.14708650e-01  8.21307361e-01  8.32112730e-01  8.34257603e-01
  8.35584998e-01  8.37072551e-01  8.40897560e-01  8.41805398e-01
  8.44504714e-01  8.51307452e-01  8.53451550e-01  8.74177516e-01
  8.76979172e-01  8.79003704e-01  8.80118966e-01  8.88440013e-01
  8.90400589e-01  8.91530871e-01  9.19967711e-01  9.22429681e-01
  9.22607720e-01  9.23510969e-01  9.26246285e-01  9.34806168e-01
  9.46619213e-01  9.48441029e-01  9.51679289e-01  9.74910378e-01
  9.79134679e-01  9.89768505e-01  1.03150630e+00  1.07669365e+00
  1.08738470e+00  1.10430527e+00  1.11612654e+00  1.15259910e+00
  1.16363847e+00]

  warnings.warn(

2022-11-03 10:53:40,876:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.95084566e-01 -2.22720012e-01 -2.10219353e-01 -1.68064684e-01
 -1.40978247e-01 -1.35118857e-01 -1.22781709e-01 -1.14110261e-01
 -9.96714979e-02 -9.01750922e-02 -9.00355279e-02 -8.48739669e-02
 -7.36881793e-02 -7.25435987e-02 -7.19607919e-02 -7.01783150e-02
 -6.61658570e-02 -6.54489025e-02 -6.37479350e-02 -6.34305254e-02
 -6.28660843e-02 -6.19453713e-02 -6.15718588e-02 -5.38895167e-02
 -5.15461601e-02 -4.93647717e-02 -4.78210859e-02 -4.77834940e-02
 -4.76124957e-02 -4.42681313e-02 -3.91983986e-02 -3.53984497e-02
 -3.53466347e-02 -3.48930024e-02 -3.39259617e-02 -3.20560373e-02
 -3.15805562e-02 -3.14315408e-02 -2.98430659e-02 -2.78125368e-02
 -2.72269882e-02 -2.68399306e-02 -2.37907767e-02 -2.35713907e-02
 -2.32333466e-02 -2.27130391e-02 -2.26760842e-02 -2.26065498e-02
 -2.11587567e-02 -1.92774776e-02 -1.87583473e-02 -1.86270140e-02
 -1.66827161e-02 -1.64522734e-02 -1.53535558e-02 -1.53452363e-02
 -1.52509641e-02 -1.52111612e-02 -1.46003254e-02 -1.41213844e-02
 -1.34355733e-02 -1.16329826e-02 -1.09902341e-02 -9.35806800e-03
 -9.05697048e-03 -8.12966190e-03 -7.76680512e-03 -6.31670747e-03
 -6.03842456e-03 -4.70764190e-03 -4.38144244e-03 -4.22046101e-03
 -4.18214547e-03 -3.54354456e-03 -3.46893584e-03 -3.39456159e-03
 -1.77374203e-03 -1.55013776e-03 -7.97002111e-04 -5.84949914e-04
 -1.15865842e-04  9.64030623e-05  1.58656877e-03  2.23660260e-03
  2.36805575e-03  2.79384083e-03  3.07764532e-03  3.57594108e-03
  3.90350749e-03  4.62144054e-03  5.17882593e-03  5.18555148e-03
  5.20122983e-03  5.42810932e-03  5.45106735e-03  5.55058848e-03
  5.91885857e-03  6.65124692e-03  8.09759181e-03  8.88915919e-03
  9.09756497e-03  9.12428927e-03  9.68559645e-03  1.03625208e-02
  1.08296517e-02  1.14716440e-02  1.18786348e-02  1.20897694e-02
  1.33839482e-02  1.38046145e-02  1.49467662e-02  1.59214083e-02
  1.60015281e-02  1.60422623e-02  1.72833763e-02  1.83607228e-02
  1.88027639e-02  1.90337915e-02  1.98984351e-02  2.07342766e-02
  2.07871124e-02  2.12172512e-02  2.37812214e-02  2.40948834e-02
  2.45370176e-02  2.60327198e-02  2.82912161e-02  2.89035663e-02
  2.95373574e-02  3.17183323e-02  3.30077372e-02  3.52715142e-02
  3.61486487e-02  3.62444632e-02  3.66115831e-02  3.69909965e-02
  3.98523249e-02  4.00840677e-02  4.09971252e-02  4.15377431e-02
  4.21562046e-02  4.23267409e-02  4.24138941e-02  4.26931195e-02
  4.27956544e-02  4.47021164e-02  4.52241935e-02  4.53367531e-02
  4.63337973e-02  4.78467382e-02  4.78881933e-02  4.79281209e-02
  4.80404384e-02  4.96297069e-02  5.12209646e-02  5.26820756e-02
  5.36739565e-02  5.44729643e-02  5.52095659e-02  5.53866215e-02
  5.54738231e-02  5.63959330e-02  5.98464459e-02  6.03373274e-02
  6.03810102e-02  6.15919568e-02  6.54562712e-02  6.62250295e-02
  6.63228184e-02  6.63601607e-02  6.93773329e-02  6.98578432e-02
  7.02572614e-02  7.23005757e-02  7.42991492e-02  7.43623748e-02
  7.44089782e-02  7.84011632e-02  7.90524185e-02  8.01025182e-02
  8.30010474e-02  8.31208825e-02  8.37562233e-02  8.41070637e-02
  8.46674219e-02  8.53133351e-02  8.61525610e-02  9.07649919e-02
  9.12202671e-02  9.12644193e-02  9.25805941e-02  9.58935916e-02
  9.59803537e-02  9.64763835e-02  9.65573192e-02  9.67813358e-02
  9.71470848e-02  9.78272334e-02  9.78443325e-02  9.97206941e-02
  9.97590199e-02  9.99357253e-02  1.01018675e-01  1.01853095e-01
  1.05510861e-01  1.05828665e-01  1.05887324e-01  1.06849603e-01
  1.07375108e-01  1.07506596e-01  1.08092874e-01  1.14726819e-01
  1.16110414e-01  1.17136046e-01  1.17465757e-01  1.17528059e-01
  1.17792949e-01  1.19642965e-01  1.20087378e-01  1.20175220e-01
  1.22747004e-01  1.26073197e-01  1.26196459e-01  1.26479670e-01
  1.27349228e-01  1.30259290e-01  1.30538002e-01  1.30566761e-01
  1.33530229e-01  1.35677397e-01  1.40977412e-01  1.41213775e-01
  1.42547786e-01  1.46602213e-01  1.47716105e-01  1.47731140e-01
  1.49172693e-01  1.49763256e-01  1.50882140e-01  1.51362419e-01
  1.54057220e-01  1.54906765e-01  1.55266285e-01  1.57790169e-01
  1.59033805e-01  1.59363836e-01  1.61573321e-01  1.61977932e-01
  1.63633794e-01  1.63945213e-01  1.65047675e-01  1.65231317e-01
  1.69828385e-01  1.69833243e-01  1.70661822e-01  1.71265945e-01
  1.72418147e-01  1.75854206e-01  1.76881284e-01  1.78521022e-01
  1.79371327e-01  1.80143565e-01  1.82376549e-01  1.87568203e-01
  1.93201408e-01  1.94330156e-01  1.94510520e-01  1.96059138e-01
  1.96210012e-01  1.97260752e-01  1.98657870e-01  2.01971084e-01
  2.03964621e-01  2.06384197e-01  2.11283013e-01  2.11969391e-01
  2.16770962e-01  2.17768475e-01  2.21780866e-01  2.22350240e-01
  2.27371544e-01  2.27416962e-01  2.34208047e-01  2.40384489e-01
  2.40597159e-01  2.42233366e-01  2.44178668e-01  2.44431466e-01
  2.45275304e-01  2.46697709e-01  2.47497544e-01  2.51689017e-01
  2.52945870e-01  2.56502539e-01  2.57224649e-01  2.63683587e-01
  2.63890058e-01  2.73062885e-01  2.73465842e-01  2.73706377e-01
  2.74938226e-01  2.76334077e-01  2.81843901e-01  2.86703587e-01
  2.92658478e-01  2.93379217e-01  2.93546230e-01  2.94711053e-01
  2.97143400e-01  2.98286259e-01  3.00283432e-01  3.01908135e-01
  3.03055435e-01  3.04984391e-01  3.05354774e-01  3.05982888e-01
  3.06451619e-01  3.11881185e-01  3.12261254e-01  3.16924632e-01
  3.20956916e-01  3.23742121e-01  3.25850129e-01  3.30783337e-01
  3.31037343e-01  3.33618969e-01  3.34462881e-01  3.35196137e-01
  3.35473359e-01  3.36986512e-01  3.47463787e-01  3.48681092e-01
  3.51924360e-01  3.52467865e-01  3.61150384e-01  3.61885399e-01
  3.68361235e-01  3.71482193e-01  3.79161954e-01  3.82898957e-01
  3.84243131e-01  3.84723991e-01  3.85042995e-01  3.91060829e-01
  3.97110313e-01  4.00540143e-01  4.03893411e-01  4.05357540e-01
  4.10596818e-01  4.11842823e-01  4.14356440e-01  4.21825141e-01
  4.24174249e-01  4.24215138e-01  4.26778674e-01  4.28900361e-01
  4.31063235e-01  4.31430370e-01  4.37533796e-01  4.41946059e-01
  4.42352712e-01  4.44458872e-01  4.46089447e-01  4.46889639e-01
  4.47340727e-01  4.57181484e-01  4.58682299e-01  4.62436765e-01
  4.65057969e-01  4.67148155e-01  4.67233092e-01  4.67679203e-01
  4.70180392e-01  4.71339673e-01  4.73809838e-01  4.80712861e-01
  4.80788082e-01  4.81099963e-01  4.84734565e-01  4.86196399e-01
  4.86465007e-01  4.86694962e-01  4.87827808e-01  4.89663661e-01
  4.96052027e-01  4.99889851e-01  5.03831744e-01  5.12130260e-01
  5.17803729e-01  5.21858573e-01  5.25244534e-01  5.26205301e-01
  5.26716173e-01  5.29528677e-01  5.29762030e-01  5.38364887e-01
  5.43735683e-01  5.44015408e-01  5.52537620e-01  5.53211331e-01
  5.63699007e-01  5.64568996e-01  5.74714303e-01  5.81970870e-01
  5.82607567e-01  5.84043086e-01  5.86769104e-01  5.87683976e-01
  5.90501785e-01  5.93561292e-01  5.98863661e-01  6.11400008e-01
  6.12642407e-01  6.14434659e-01  6.16573155e-01  6.19389415e-01
  6.29026711e-01  6.30799890e-01  6.48472130e-01  6.52650356e-01
  6.52904272e-01  6.58087075e-01  6.73011422e-01  6.74165964e-01
  6.79459095e-01  6.85044050e-01  6.98303699e-01  6.98591650e-01
  7.01662242e-01  7.06041634e-01  7.07049131e-01  7.08094656e-01
  7.10355580e-01  7.11607575e-01  7.20449626e-01  7.21149683e-01
  7.29558408e-01  7.29619741e-01  7.30809450e-01  7.36883223e-01
  7.43090212e-01  7.43565261e-01  7.47191012e-01  7.51465440e-01
  7.52537072e-01  7.53723800e-01  7.54047036e-01  7.57218599e-01
  7.59487510e-01  7.59535134e-01  7.63794899e-01  7.68709302e-01
  7.73430169e-01  7.74185896e-01  7.74496794e-01  7.75694609e-01
  7.76709676e-01  7.81377852e-01  7.83105016e-01  7.90015936e-01
  7.91190922e-01  7.98002660e-01  8.16698909e-01  8.17439198e-01
  8.25727701e-01  8.42972577e-01  8.43883932e-01  8.45175087e-01
  8.47205520e-01  8.49546492e-01  8.52590382e-01  8.59655201e-01
  8.78329456e-01  8.84617209e-01  8.87602210e-01  8.90186369e-01
  8.94403696e-01  8.94411445e-01  8.97849381e-01  8.98573220e-01
  9.05936956e-01  9.18871284e-01  9.29884374e-01  9.36429083e-01
  9.45474386e-01  9.63476717e-01  9.65864301e-01  9.67370749e-01
  9.84071493e-01  9.98739660e-01  1.00262344e+00  1.05349028e+00
  1.12655795e+00]

  warnings.warn(

2022-11-03 10:53:40,884:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.56566614e-01 -2.07179800e-01 -1.98163554e-01 -1.37839749e-01
 -1.34467050e-01 -1.30100816e-01 -1.22531272e-01 -1.19845070e-01
 -1.11705832e-01 -9.82576087e-02 -9.82046053e-02 -9.33876634e-02
 -8.06415156e-02 -7.14448020e-02 -6.19519502e-02 -6.01251200e-02
 -5.80015182e-02 -5.66299446e-02 -5.56493476e-02 -5.38609475e-02
 -5.38367182e-02 -5.25060371e-02 -5.22204600e-02 -5.18634059e-02
 -5.11744767e-02 -4.98848669e-02 -4.96909693e-02 -4.78630103e-02
 -4.19053398e-02 -3.88093740e-02 -3.86038646e-02 -3.79812792e-02
 -3.52906026e-02 -3.17711756e-02 -3.06360219e-02 -3.01964637e-02
 -2.97156349e-02 -2.97151897e-02 -2.90521551e-02 -2.88570598e-02
 -2.86555290e-02 -2.60661691e-02 -2.48382408e-02 -2.38315649e-02
 -2.35124920e-02 -2.34249625e-02 -2.33871937e-02 -2.29287464e-02
 -2.24495064e-02 -2.22833846e-02 -2.19130684e-02 -2.05643438e-02
 -2.04418171e-02 -1.93573534e-02 -1.93190016e-02 -1.88356750e-02
 -1.85400788e-02 -1.72443874e-02 -1.50606260e-02 -1.41809536e-02
 -1.34796556e-02 -1.33169936e-02 -1.32597229e-02 -1.28865847e-02
 -1.28365746e-02 -1.25225484e-02 -1.16770435e-02 -1.14092883e-02
 -1.10825533e-02 -1.10229729e-02 -1.09497709e-02 -1.07558090e-02
 -1.06071895e-02 -1.04095424e-02 -1.00676566e-02 -9.95105691e-03
 -9.68914758e-03 -9.41073988e-03 -9.26597696e-03 -8.19353759e-03
 -7.90518709e-03 -7.75906350e-03 -7.71209877e-03 -7.69792031e-03
 -7.49642169e-03 -7.38403946e-03 -6.55828370e-03 -6.55410904e-03
 -6.51059812e-03 -5.68331080e-03 -5.11208922e-03 -4.74520680e-03
 -3.75461881e-03 -3.48966499e-03 -2.60537094e-03 -2.56212917e-03
 -1.65214995e-03 -1.34961295e-03 -1.33513322e-03 -1.33251736e-03
 -6.80588710e-04 -3.12789663e-04 -2.53992650e-04  4.08992957e-04
  5.36728883e-04  9.94862756e-04  1.00441184e-03  1.02108996e-03
  1.88738387e-03  2.17171037e-03  3.37545900e-03  3.63165140e-03
  3.75191984e-03  4.45425091e-03  4.88654245e-03  5.33465575e-03
  6.23517251e-03  6.70930184e-03  6.81980280e-03  6.91853976e-03
  7.41656590e-03  7.74550252e-03  7.80896796e-03  7.82426633e-03
  8.09618644e-03  1.04294354e-02  1.13010686e-02  1.14168497e-02
  1.17378440e-02  1.17576187e-02  1.19067049e-02  1.21305138e-02
  1.21564129e-02  1.25529068e-02  1.33160343e-02  1.54312961e-02
  1.57411881e-02  1.57456100e-02  1.65593401e-02  1.71394087e-02
  1.74278785e-02  1.80701185e-02  1.97516810e-02  2.01771948e-02
  2.04469729e-02  2.09841337e-02  2.09980551e-02  2.11676881e-02
  2.12697480e-02  2.15300675e-02  2.16587968e-02  2.38223169e-02
  2.40587406e-02  2.57871710e-02  2.60460228e-02  2.65608318e-02
  2.66372617e-02  2.79476717e-02  2.85677705e-02  2.91320290e-02
  2.97538787e-02  3.07028070e-02  3.07899062e-02  3.11645288e-02
  3.21191289e-02  3.43185328e-02  3.52310166e-02  3.80763337e-02
  4.08696979e-02  4.13999036e-02  4.26187143e-02  4.26607393e-02
  4.35084142e-02  4.35245335e-02  4.38879319e-02  4.57859337e-02
  4.58240919e-02  4.60734852e-02  4.63890582e-02  4.72771674e-02
  4.76662628e-02  4.76951748e-02  4.82336953e-02  5.03552705e-02
  5.10503352e-02  5.22792861e-02  5.28888740e-02  5.31785302e-02
  5.33383600e-02  5.41674122e-02  5.62344268e-02  5.65802455e-02
  5.68060875e-02  5.85358813e-02  5.87911047e-02  6.05181046e-02
  6.12768196e-02  6.17620833e-02  6.28309771e-02  6.33767322e-02
  6.42738417e-02  6.49012178e-02  6.57305643e-02  6.83480948e-02
  6.89650774e-02  6.93119764e-02  6.98526949e-02  7.09186420e-02
  7.24277645e-02  7.42024928e-02  7.44356588e-02  7.47966394e-02
  7.74173513e-02  7.90500715e-02  8.04873109e-02  8.20644870e-02
  8.26965272e-02  8.45076218e-02  8.54169652e-02  8.88263956e-02
  8.93564969e-02  9.07182395e-02  9.09334123e-02  9.15871486e-02
  9.62799788e-02  9.79435369e-02  1.00840606e-01  1.01764873e-01
  1.01954110e-01  1.02508128e-01  1.04486249e-01  1.04884796e-01
  1.04951322e-01  1.05219200e-01  1.08077854e-01  1.10870980e-01
  1.12330645e-01  1.12809636e-01  1.12853378e-01  1.13055214e-01
  1.15982018e-01  1.19665772e-01  1.28387332e-01  1.31203964e-01
  1.33628920e-01  1.36285633e-01  1.37651950e-01  1.37730449e-01
  1.38285160e-01  1.38840258e-01  1.39765978e-01  1.42443731e-01
  1.43412799e-01  1.45460039e-01  1.47108197e-01  1.48174897e-01
  1.48948908e-01  1.50071338e-01  1.50698811e-01  1.52895123e-01
  1.55672878e-01  1.60673216e-01  1.63081363e-01  1.66876107e-01
  1.72071412e-01  1.72077313e-01  1.74108431e-01  1.75087824e-01
  1.78043589e-01  1.78167656e-01  1.79133415e-01  1.79653436e-01
  1.81667387e-01  1.83978736e-01  1.87586293e-01  1.88691571e-01
  1.91439912e-01  1.94029659e-01  1.94339305e-01  1.95432350e-01
  1.98772922e-01  2.01711670e-01  2.03195974e-01  2.03756541e-01
  2.07362175e-01  2.08334252e-01  2.08418861e-01  2.10226595e-01
  2.10666731e-01  2.12750375e-01  2.14333043e-01  2.15557203e-01
  2.17262462e-01  2.23923326e-01  2.24590585e-01  2.24607408e-01
  2.26469532e-01  2.27894187e-01  2.28011712e-01  2.30595633e-01
  2.32170895e-01  2.36970678e-01  2.37507939e-01  2.39895418e-01
  2.48918027e-01  2.49538258e-01  2.51432091e-01  2.54961610e-01
  2.58693874e-01  2.60001153e-01  2.60124564e-01  2.61321694e-01
  2.65893728e-01  2.68678278e-01  2.69297302e-01  2.73616940e-01
  2.79371351e-01  2.82019496e-01  2.84018040e-01  2.84832776e-01
  2.89822191e-01  2.99359173e-01  3.04373890e-01  3.08562875e-01
  3.10373038e-01  3.15157980e-01  3.18643272e-01  3.21014851e-01
  3.25721920e-01  3.33848059e-01  3.34948093e-01  3.40879709e-01
  3.43160599e-01  3.49374384e-01  3.50319058e-01  3.54426652e-01
  3.57010990e-01  3.60461742e-01  3.61119151e-01  3.61285686e-01
  3.64839166e-01  3.67884189e-01  3.71056080e-01  3.71216446e-01
  3.72642189e-01  3.74698579e-01  3.78793091e-01  3.91819537e-01
  3.93620133e-01  3.95452529e-01  4.06544089e-01  4.15574491e-01
  4.20042366e-01  4.20109361e-01  4.20283437e-01  4.21629488e-01
  4.27641004e-01  4.28935528e-01  4.33155388e-01  4.37786698e-01
  4.42353398e-01  4.56678987e-01  4.57930654e-01  4.61402953e-01
  4.61800277e-01  4.62671131e-01  4.64877754e-01  4.66059446e-01
  4.69048738e-01  4.69784379e-01  4.80762124e-01  4.81660128e-01
  4.83460486e-01  4.85270709e-01  4.86193001e-01  4.94590819e-01
  4.96525794e-01  5.03144741e-01  5.04883826e-01  5.06683946e-01
  5.09905517e-01  5.16957223e-01  5.17546892e-01  5.20104110e-01
  5.20899653e-01  5.22106707e-01  5.23587465e-01  5.25287867e-01
  5.27136564e-01  5.29184461e-01  5.30121505e-01  5.32238901e-01
  5.36581039e-01  5.37161410e-01  5.38079143e-01  5.38289547e-01
  5.40699124e-01  5.41190147e-01  5.44695616e-01  5.47882736e-01
  5.58409870e-01  5.61433434e-01  5.68440557e-01  5.68675578e-01
  5.72114944e-01  5.77164412e-01  5.78985333e-01  5.80170453e-01
  5.80443919e-01  5.81307530e-01  5.92890620e-01  5.99424064e-01
  5.99937379e-01  6.06357336e-01  6.19813919e-01  6.23165846e-01
  6.23933852e-01  6.33007348e-01  6.37767613e-01  6.41513586e-01
  6.43328786e-01  6.47514522e-01  6.56912148e-01  6.76473558e-01
  6.78513944e-01  6.83976412e-01  6.84392869e-01  6.85509861e-01
  6.86091304e-01  6.87031984e-01  6.90802395e-01  6.95231318e-01
  6.97933316e-01  7.02549458e-01  7.12017715e-01  7.25714445e-01
  7.25846589e-01  7.26079345e-01  7.26333022e-01  7.28317857e-01
  7.33809054e-01  7.34823108e-01  7.35501349e-01  7.42176175e-01
  7.58414865e-01  7.58991838e-01  7.70344496e-01  7.70396590e-01
  7.82727599e-01  7.97503948e-01  7.98701644e-01  8.07898045e-01
  8.20598781e-01  8.34532082e-01  8.37445319e-01  8.38098288e-01
  8.41799498e-01  8.42474818e-01  8.47977102e-01  8.50043952e-01
  8.50701809e-01  8.60179961e-01  8.60900998e-01  8.61274362e-01
  8.64525437e-01  8.67390871e-01  8.75807941e-01  8.80006850e-01
  8.80598903e-01  8.87559295e-01  8.89292955e-01  8.91448021e-01
  8.93147767e-01  8.99785817e-01  9.06727374e-01  9.09865558e-01
  9.22948360e-01  9.26188350e-01  9.37471449e-01  9.43370163e-01
  9.46514547e-01  9.51881826e-01  9.51942384e-01  9.56941247e-01
  9.59595740e-01  9.69426215e-01  9.75639880e-01  9.94754910e-01
  1.00220180e+00  1.01195014e+00  1.04780495e+00  1.07194519e+00
  1.12920272e+00]

  warnings.warn(

2022-11-03 10:53:40,892:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.84596950e-01 -1.50113225e-01 -1.48437068e-01 -1.26500160e-01
 -1.19564883e-01 -1.17652759e-01 -1.07794628e-01 -1.01190165e-01
 -9.85587463e-02 -9.78184640e-02 -9.33057144e-02 -9.26660597e-02
 -8.28177184e-02 -8.27626511e-02 -8.03758502e-02 -6.75747097e-02
 -6.23369701e-02 -6.11501671e-02 -6.09451123e-02 -5.99404201e-02
 -5.92531338e-02 -5.85211888e-02 -5.81970401e-02 -5.39000854e-02
 -5.32422438e-02 -5.07858656e-02 -4.89565209e-02 -4.39647920e-02
 -4.28638756e-02 -4.26469631e-02 -3.54373083e-02 -3.51514183e-02
 -3.41598094e-02 -2.96564903e-02 -2.88519822e-02 -2.86912676e-02
 -2.78933048e-02 -2.73485519e-02 -2.63339281e-02 -2.56755855e-02
 -2.33162493e-02 -2.31925454e-02 -2.30667684e-02 -2.29573939e-02
 -2.28762701e-02 -2.20874324e-02 -2.16704402e-02 -2.11460851e-02
 -2.08916403e-02 -2.06026621e-02 -1.80296861e-02 -1.77284982e-02
 -1.75085366e-02 -1.73368808e-02 -1.69532988e-02 -1.69188175e-02
 -1.63255241e-02 -1.53427320e-02 -1.40942764e-02 -1.34892138e-02
 -1.34561099e-02 -1.33144287e-02 -1.32154105e-02 -1.30029982e-02
 -1.28785931e-02 -1.28295226e-02 -1.14363786e-02 -1.03460364e-02
 -9.33039375e-03 -9.27753001e-03 -9.15019028e-03 -9.07322112e-03
 -8.34537297e-03 -8.19047354e-03 -7.78410863e-03 -7.64364190e-03
 -6.29885122e-03 -5.06414939e-03 -4.99200402e-03 -4.38276865e-03
 -3.85805732e-03 -3.70152411e-03 -3.57296783e-03 -3.56076984e-03
 -3.31441429e-03 -1.92342687e-03 -1.70620496e-03 -1.59868132e-03
 -5.73183643e-04 -1.35055991e-04 -1.13679998e-04 -1.03463448e-04
  7.90727499e-04  1.32049317e-03  1.37179310e-03  2.26426264e-03
  2.43103039e-03  2.52640294e-03  3.13735683e-03  3.94193456e-03
  4.06985031e-03  4.17888816e-03  4.58007958e-03  4.90860641e-03
  5.26698027e-03  5.33880387e-03  5.84071968e-03  5.88712562e-03
  5.95888961e-03  6.01223856e-03  7.09325727e-03  7.44968653e-03
  7.49732554e-03  7.83941988e-03  7.95066077e-03  8.52413476e-03
  8.66779126e-03  8.74630455e-03  8.94342177e-03  9.06297658e-03
  9.67188552e-03  9.85327363e-03  1.15690641e-02  1.19858216e-02
  1.35359932e-02  1.55571327e-02  1.55923609e-02  1.67386476e-02
  1.70190316e-02  1.70346219e-02  1.72964148e-02  1.96747016e-02
  1.98082328e-02  2.19446383e-02  2.26774197e-02  2.36040410e-02
  2.51496807e-02  2.58803163e-02  2.60395203e-02  2.69792564e-02
  2.88564842e-02  3.02185230e-02  3.06810848e-02  3.09037957e-02
  3.09574082e-02  3.14363576e-02  3.22183259e-02  3.30475755e-02
  3.31391431e-02  3.32752056e-02  3.45610902e-02  3.52991782e-02
  3.60538401e-02  3.74948829e-02  3.83877084e-02  3.84869576e-02
  3.88782062e-02  3.93956564e-02  4.44124900e-02  4.50998731e-02
  4.76120450e-02  4.80118096e-02  4.86349910e-02  4.88596000e-02
  4.96760421e-02  5.00549264e-02  5.01353480e-02  5.08801788e-02
  5.12157455e-02  5.16140237e-02  5.20817526e-02  5.51715530e-02
  5.58345020e-02  5.69274165e-02  5.70358001e-02  5.72575778e-02
  5.72641045e-02  5.78872524e-02  6.11576699e-02  6.12845905e-02
  6.44091368e-02  6.50883317e-02  6.57971501e-02  6.66741803e-02
  6.77394941e-02  6.81257397e-02  6.88865781e-02  6.94827214e-02
  6.97620139e-02  7.25895762e-02  7.26856515e-02  7.45604709e-02
  7.68432468e-02  7.72506371e-02  7.85692185e-02  7.88152814e-02
  8.08350295e-02  8.18019137e-02  8.25792477e-02  8.36050138e-02
  8.40308145e-02  8.60845596e-02  8.66730139e-02  9.04119462e-02
  9.15574729e-02  9.19825062e-02  9.51163396e-02  9.58179533e-02
  9.60052907e-02  9.82169211e-02  9.83359292e-02  1.00053430e-01
  1.01028964e-01  1.02832742e-01  1.07576124e-01  1.08752213e-01
  1.11101866e-01  1.13567546e-01  1.14183255e-01  1.14900000e-01
  1.16047740e-01  1.19598374e-01  1.20297328e-01  1.21209837e-01
  1.21560186e-01  1.23069949e-01  1.24759927e-01  1.26423776e-01
  1.28153339e-01  1.28709882e-01  1.29833058e-01  1.30497470e-01
  1.30845368e-01  1.31439641e-01  1.32387966e-01  1.32636532e-01
  1.32917672e-01  1.34712979e-01  1.36624575e-01  1.40899166e-01
  1.41095772e-01  1.42007902e-01  1.42401010e-01  1.45377159e-01
  1.49315104e-01  1.49948969e-01  1.50708273e-01  1.52162939e-01
  1.52578071e-01  1.53529420e-01  1.54135957e-01  1.55149564e-01
  1.55345857e-01  1.57006934e-01  1.57305837e-01  1.57678142e-01
  1.59034550e-01  1.61410853e-01  1.63389802e-01  1.64543167e-01
  1.65791705e-01  1.68426409e-01  1.72677264e-01  1.73199207e-01
  1.73284754e-01  1.80645987e-01  1.84121042e-01  1.86425969e-01
  1.87480032e-01  1.87570408e-01  1.88331902e-01  1.91237435e-01
  1.92452475e-01  1.92913309e-01  1.98370636e-01  1.99542090e-01
  2.04262689e-01  2.04846039e-01  2.06909180e-01  2.09957749e-01
  2.13585481e-01  2.14110777e-01  2.14911342e-01  2.16352805e-01
  2.17054114e-01  2.17851028e-01  2.19807521e-01  2.20017940e-01
  2.20458359e-01  2.21030757e-01  2.21518934e-01  2.22062737e-01
  2.23057106e-01  2.25538746e-01  2.25908771e-01  2.28863075e-01
  2.36389056e-01  2.41168573e-01  2.41319209e-01  2.46916801e-01
  2.48484194e-01  2.50746578e-01  2.52931714e-01  2.60135204e-01
  2.60507554e-01  2.60600448e-01  2.61890084e-01  2.62287766e-01
  2.62753665e-01  2.63166428e-01  2.64155537e-01  2.66948193e-01
  2.67891079e-01  2.69546658e-01  2.70520091e-01  2.74612159e-01
  2.75861114e-01  2.80162305e-01  2.80492485e-01  2.81017572e-01
  2.81830668e-01  2.85794139e-01  2.89749026e-01  2.92026848e-01
  2.93659687e-01  2.96574414e-01  3.01695675e-01  3.01974922e-01
  3.05244863e-01  3.05826277e-01  3.08218449e-01  3.09890538e-01
  3.14587414e-01  3.18334460e-01  3.18720728e-01  3.21595281e-01
  3.26887876e-01  3.27410519e-01  3.31204236e-01  3.31243396e-01
  3.38363975e-01  3.39660406e-01  3.40078115e-01  3.45520139e-01
  3.52585942e-01  3.55089754e-01  3.57846141e-01  3.59194487e-01
  3.65697861e-01  3.66565347e-01  3.70529950e-01  3.70956182e-01
  3.77587289e-01  3.83833647e-01  3.85141492e-01  3.86142969e-01
  3.87355238e-01  3.87499750e-01  3.87807399e-01  3.87930810e-01
  3.89786601e-01  3.90468061e-01  3.90597880e-01  4.01729167e-01
  4.02294666e-01  4.03880119e-01  4.25722182e-01  4.27691638e-01
  4.28451717e-01  4.31812853e-01  4.32710618e-01  4.33005631e-01
  4.33135450e-01  4.36322987e-01  4.37891036e-01  4.42698628e-01
  4.45937097e-01  4.49533671e-01  4.55638468e-01  4.58685458e-01
  4.62020755e-01  4.67208862e-01  4.72724140e-01  4.78861570e-01
  4.94805872e-01  4.95829076e-01  5.00999093e-01  5.11424839e-01
  5.12887776e-01  5.17646134e-01  5.27939975e-01  5.29011369e-01
  5.29348671e-01  5.30845106e-01  5.31414390e-01  5.37631392e-01
  5.40469706e-01  5.44225693e-01  5.51162541e-01  5.53619385e-01
  5.54508448e-01  5.58164835e-01  5.64480841e-01  5.67712426e-01
  5.71404278e-01  5.76947093e-01  5.85389078e-01  5.89714706e-01
  5.91506124e-01  5.98704636e-01  6.00006819e-01  6.00538731e-01
  6.05601370e-01  6.06310844e-01  6.11211658e-01  6.15898967e-01
  6.18774295e-01  6.23117626e-01  6.32032812e-01  6.36529267e-01
  6.38412774e-01  6.42508626e-01  6.47242188e-01  6.48869872e-01
  6.57061815e-01  6.61962092e-01  6.68561161e-01  6.69342458e-01
  6.73317492e-01  6.83143198e-01  6.84364200e-01  6.88818336e-01
  6.90525889e-01  6.94026053e-01  6.99620366e-01  7.01976418e-01
  7.04063296e-01  7.04696298e-01  7.09554970e-01  7.10817933e-01
  7.14818656e-01  7.36659467e-01  7.51582861e-01  7.56343842e-01
  7.58637726e-01  7.73740172e-01  7.77366161e-01  7.78537989e-01
  7.79905796e-01  7.82393098e-01  7.82910526e-01  7.84385145e-01
  7.85151064e-01  7.86863923e-01  7.91772723e-01  7.96798944e-01
  7.97674656e-01  7.99577951e-01  8.10629785e-01  8.22921216e-01
  8.30144346e-01  8.36133718e-01  8.37530732e-01  8.39829743e-01
  8.40277910e-01  8.43070805e-01  8.43509078e-01  8.45499575e-01
  8.47165585e-01  8.47257912e-01  8.51632297e-01  8.60467017e-01
  9.03596461e-01  9.07167614e-01  9.14014399e-01  9.15396929e-01
  9.18949366e-01  9.29701567e-01  9.37255263e-01  9.51666296e-01
  9.52892721e-01  9.53283668e-01  9.62491989e-01  9.67087507e-01
  9.96539116e-01  1.01586246e+00  1.02067721e+00  1.02359569e+00
  1.03528810e+00  1.13264537e+00  1.14117360e+00  1.22227943e+00]

  warnings.warn(

2022-11-03 10:53:40,900:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.98763841e-01 -2.90714324e-01 -2.69759476e-01 -2.24052891e-01
 -1.65509596e-01 -1.36681929e-01 -1.26608446e-01 -1.25213370e-01
 -1.22676261e-01 -1.15057625e-01 -1.09969132e-01 -1.08437143e-01
 -1.04442522e-01 -8.95800740e-02 -8.90918896e-02 -8.80338028e-02
 -8.12897459e-02 -8.11064914e-02 -7.98000470e-02 -6.72795177e-02
 -6.54995590e-02 -5.89233711e-02 -5.86758330e-02 -5.67793958e-02
 -5.55005036e-02 -5.30231446e-02 -4.88684885e-02 -4.80935760e-02
 -4.72949557e-02 -4.71937023e-02 -4.57636304e-02 -4.55584601e-02
 -4.49060611e-02 -4.46580052e-02 -4.39065322e-02 -4.18193266e-02
 -4.13691327e-02 -3.84303182e-02 -3.64188068e-02 -3.47786024e-02
 -3.29220071e-02 -3.24750617e-02 -3.23846489e-02 -3.16675939e-02
 -3.06465607e-02 -3.06334049e-02 -3.01144589e-02 -2.97432095e-02
 -2.83986069e-02 -2.77556051e-02 -2.64535882e-02 -2.48198155e-02
 -2.45577134e-02 -2.40914058e-02 -2.37036515e-02 -2.28682216e-02
 -2.27381643e-02 -2.27150228e-02 -2.17441004e-02 -2.04747748e-02
 -2.03960743e-02 -2.03445051e-02 -1.96970571e-02 -1.95303112e-02
 -1.82596892e-02 -1.81909464e-02 -1.75356995e-02 -1.66173093e-02
 -1.63799934e-02 -1.61768086e-02 -1.42427646e-02 -1.40009560e-02
 -1.35105308e-02 -1.27183236e-02 -1.23116625e-02 -1.22189680e-02
 -1.20145800e-02 -1.13218129e-02 -1.10914661e-02 -1.07784066e-02
 -9.61138681e-03 -8.65606591e-03 -8.39586183e-03 -7.89369550e-03
 -7.85444584e-03 -7.30034104e-03 -6.45147637e-03 -5.64067485e-03
 -5.60865225e-03 -5.11034252e-03 -4.99317981e-03 -4.89312736e-03
 -4.86995513e-03 -4.51084087e-03 -3.92677868e-03 -3.86579381e-03
 -3.29546258e-03 -3.12910415e-03 -2.30100891e-03 -2.14241119e-03
 -1.98694365e-03 -5.32775419e-04 -2.83320085e-04  1.05494761e-03
  1.15847064e-03  1.44220167e-03  2.08606618e-03  2.74058757e-03
  3.32875596e-03  4.40588640e-03  5.95173566e-03  6.25765882e-03
  7.50486972e-03  9.27697867e-03  9.28985141e-03  9.52418242e-03
  9.63051897e-03  1.08130015e-02  1.24721322e-02  1.25267915e-02
  1.27118351e-02  1.29820500e-02  1.33846812e-02  1.35983471e-02
  1.40400678e-02  1.51715111e-02  1.55606177e-02  1.56365484e-02
  1.59155112e-02  1.61039047e-02  1.63596720e-02  1.68201607e-02
  1.71510223e-02  1.76071618e-02  1.89512018e-02  1.99569054e-02
  2.04549171e-02  2.06624214e-02  2.08486002e-02  2.16418989e-02
  2.17527281e-02  2.49960925e-02  2.60942616e-02  2.61793733e-02
  2.74348743e-02  2.81485952e-02  2.95712277e-02  3.06304842e-02
  3.09979059e-02  3.10575329e-02  3.10727805e-02  3.10787261e-02
  3.14518735e-02  3.20532359e-02  3.23520675e-02  3.57350893e-02
  3.64804491e-02  3.68347801e-02  3.73787619e-02  3.75316441e-02
  3.87482047e-02  3.90611514e-02  3.94810699e-02  4.01968472e-02
  4.15538102e-02  4.16363813e-02  4.39808145e-02  4.47046831e-02
  4.77342345e-02  4.77780141e-02  4.95660231e-02  5.01549654e-02
  5.27901426e-02  5.31353466e-02  5.37669174e-02  6.39091879e-02
  6.41963109e-02  6.43570870e-02  6.68882802e-02  6.76299706e-02
  6.76851720e-02  6.77457452e-02  6.79777041e-02  7.13107958e-02
  7.31104538e-02  7.34027326e-02  7.36210197e-02  7.38995001e-02
  7.48819932e-02  7.55212083e-02  7.56976157e-02  7.63367862e-02
  7.68586919e-02  7.74826258e-02  7.77712017e-02  7.82752261e-02
  7.90327936e-02  8.21612552e-02  8.22828859e-02  8.24243799e-02
  8.27745870e-02  8.30710307e-02  8.65957513e-02  8.72031674e-02
  8.89452696e-02  8.99556652e-02  9.04253274e-02  9.30479169e-02
  9.51428860e-02  9.54506621e-02  9.61140394e-02  9.71962512e-02
  9.79444087e-02  9.80753675e-02  1.00670576e-01  1.04298435e-01
  1.04960233e-01  1.05518378e-01  1.05890907e-01  1.07745230e-01
  1.08373061e-01  1.11921579e-01  1.13497972e-01  1.15014434e-01
  1.15688235e-01  1.18905164e-01  1.19150288e-01  1.19915478e-01
  1.20247930e-01  1.20374106e-01  1.20967388e-01  1.21074080e-01
  1.23572953e-01  1.25663489e-01  1.26613751e-01  1.26659080e-01
  1.28121391e-01  1.28229707e-01  1.28687575e-01  1.29201308e-01
  1.29668027e-01  1.30415693e-01  1.30920708e-01  1.31735891e-01
  1.37411609e-01  1.38379633e-01  1.39939666e-01  1.44593880e-01
  1.46703929e-01  1.49342477e-01  1.51825786e-01  1.55416712e-01
  1.55881464e-01  1.57710120e-01  1.61773697e-01  1.62127897e-01
  1.63750559e-01  1.65415108e-01  1.66064173e-01  1.66960463e-01
  1.67553827e-01  1.71615466e-01  1.76287994e-01  1.77495256e-01
  1.80016994e-01  1.80294886e-01  1.82307258e-01  1.84309259e-01
  1.92981526e-01  1.94344908e-01  1.95512310e-01  1.95664153e-01
  1.97409421e-01  2.05989406e-01  2.09830716e-01  2.11165965e-01
  2.11582661e-01  2.16387093e-01  2.25832939e-01  2.28483662e-01
  2.31273398e-01  2.31589437e-01  2.31938884e-01  2.33265832e-01
  2.33725548e-01  2.33784884e-01  2.35723391e-01  2.41019413e-01
  2.41333202e-01  2.41344050e-01  2.47330070e-01  2.53781050e-01
  2.54704803e-01  2.56468832e-01  2.56723642e-01  2.57099241e-01
  2.57667810e-01  2.57733077e-01  2.64198333e-01  2.71821052e-01
  2.76276350e-01  2.77473092e-01  2.80505866e-01  2.81316578e-01
  2.84207374e-01  2.85779864e-01  2.86246926e-01  2.86834389e-01
  2.87166923e-01  2.88000911e-01  2.88480371e-01  2.90486187e-01
  2.91150033e-01  2.91540056e-01  2.93514699e-01  2.93748051e-01
  2.93979317e-01  2.99155891e-01  3.00006390e-01  3.01010668e-01
  3.04772228e-01  3.05230051e-01  3.05591375e-01  3.10174048e-01
  3.11382592e-01  3.11896384e-01  3.18203539e-01  3.21825564e-01
  3.27211648e-01  3.29305112e-01  3.29925448e-01  3.38118702e-01
  3.39014530e-01  3.40256691e-01  3.48566830e-01  3.61473918e-01
  3.62863094e-01  3.63266021e-01  3.63430589e-01  3.63484859e-01
  3.64643008e-01  3.64839554e-01  3.67033482e-01  3.71287078e-01
  3.73748124e-01  3.76516998e-01  3.85539889e-01  3.87160510e-01
  3.88672054e-01  3.89063060e-01  3.93109083e-01  3.93532634e-01
  3.96307439e-01  3.98179501e-01  4.00090635e-01  4.05469030e-01
  4.07781541e-01  4.10126805e-01  4.10604089e-01  4.13569421e-01
  4.13633853e-01  4.18264061e-01  4.23377097e-01  4.26158518e-01
  4.32257682e-01  4.35595065e-01  4.36917126e-01  4.37149376e-01
  4.40309674e-01  4.52273697e-01  4.52273905e-01  4.55131859e-01
  4.55422729e-01  4.56908584e-01  4.59715664e-01  4.64930803e-01
  4.69223559e-01  4.69980180e-01  4.70888048e-01  4.72497374e-01
  4.78130043e-01  4.78375286e-01  4.88725930e-01  4.91702229e-01
  4.92780924e-01  4.94435042e-01  4.96485502e-01  4.97213840e-01
  4.98168200e-01  4.99310941e-01  5.00545800e-01  5.04156590e-01
  5.05186558e-01  5.07494628e-01  5.09050727e-01  5.11500955e-01
  5.20495415e-01  5.34632087e-01  5.37113428e-01  5.45895755e-01
  5.47792554e-01  5.57766497e-01  5.58523536e-01  5.61342537e-01
  5.63271403e-01  5.63273728e-01  5.66591084e-01  5.68744957e-01
  5.69195032e-01  5.86880684e-01  5.87336957e-01  5.91659009e-01
  5.92867553e-01  6.02958560e-01  6.04746878e-01  6.07502639e-01
  6.13374591e-01  6.14679813e-01  6.18503869e-01  6.21231616e-01
  6.27912343e-01  6.29236758e-01  6.34207010e-01  6.34713531e-01
  6.38478577e-01  6.44968033e-01  6.55044794e-01  6.69304073e-01
  6.72214627e-01  6.74009919e-01  6.76102519e-01  6.83145940e-01
  6.84342265e-01  6.86518788e-01  6.88411534e-01  6.94471955e-01
  6.95986807e-01  6.98531091e-01  7.01915205e-01  7.06163883e-01
  7.14986980e-01  7.17928588e-01  7.19208539e-01  7.28306770e-01
  7.30556786e-01  7.33177185e-01  7.44441330e-01  7.53581345e-01
  7.56624103e-01  7.64567733e-01  7.73333251e-01  7.92081416e-01
  8.06750238e-01  8.11515808e-01  8.27719390e-01  8.36659253e-01
  8.44264388e-01  8.48490477e-01  8.49237621e-01  8.54081631e-01
  8.58248591e-01  8.67651761e-01  8.68138194e-01  8.68213534e-01
  8.79885852e-01  8.80910873e-01  8.81888211e-01  8.82496953e-01
  8.89384389e-01  8.96588087e-01  9.24295723e-01  9.45286214e-01
  9.47965920e-01  9.53814864e-01  9.66637015e-01  9.72676098e-01
  9.77679670e-01  9.87988174e-01  9.89203095e-01  9.89906013e-01
  9.90855753e-01  9.94652808e-01  1.03186882e+00  1.03222048e+00
  1.06228971e+00  1.07589531e+00  1.10924947e+00  1.11410129e+00
  1.13662183e+00  1.22550225e+00  1.29096687e+00]

  warnings.warn(

2022-11-03 10:53:45,766:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.84011251e-01 -1.37056112e-01 -1.29784152e-01 -1.24159880e-01
 -1.15808576e-01 -1.12471789e-01 -9.61473361e-02 -8.04146454e-02
 -7.71545768e-02 -7.62716904e-02 -7.43234605e-02 -7.27608278e-02
 -7.24735260e-02 -7.22261220e-02 -6.52391538e-02 -6.40200377e-02
 -6.13426641e-02 -6.07890561e-02 -5.96005805e-02 -5.79676032e-02
 -5.76933622e-02 -5.18640541e-02 -4.88285199e-02 -4.68401723e-02
 -4.67900895e-02 -4.55746241e-02 -4.41677347e-02 -3.74535024e-02
 -3.74254882e-02 -3.72528844e-02 -3.42237987e-02 -3.36366333e-02
 -3.27103771e-02 -3.18044610e-02 -3.03277448e-02 -2.92298794e-02
 -2.78726071e-02 -2.58593168e-02 -2.57755779e-02 -2.29811613e-02
 -2.27407962e-02 -2.21777204e-02 -2.16287393e-02 -2.06400305e-02
 -2.06309389e-02 -2.06225421e-02 -1.86345223e-02 -1.78237408e-02
 -1.66844595e-02 -1.59875304e-02 -1.56069035e-02 -1.50135141e-02
 -1.48441866e-02 -1.47204045e-02 -1.44358790e-02 -1.34916585e-02
 -1.28794899e-02 -1.23278340e-02 -1.18856058e-02 -1.16015896e-02
 -1.15090050e-02 -1.08986888e-02 -1.02366609e-02 -1.01892231e-02
 -9.47029609e-03 -6.86784321e-03 -6.22876734e-03 -5.87465009e-03
 -5.81104774e-03 -5.75711578e-03 -5.62097272e-03 -5.04825870e-03
 -5.00359852e-03 -4.68475511e-03 -4.51534381e-03 -4.49026609e-03
 -3.78838833e-03 -3.49802477e-03 -3.02166981e-03 -2.86157010e-03
 -2.71871546e-03 -2.37344438e-03 -1.43635960e-03 -1.11456669e-03
 -1.06571161e-03 -1.67980645e-04  3.32607306e-04  3.48608999e-04
  1.58877776e-03  1.76104286e-03  2.66485312e-03  3.11423372e-03
  3.18625779e-03  3.24819260e-03  3.65852285e-03  3.93943861e-03
  4.24008118e-03  4.45481064e-03  5.18837385e-03  5.54776238e-03
  6.90113287e-03  6.98070414e-03  7.16694538e-03  7.48402672e-03
  7.74225593e-03  9.38486494e-03  9.75865871e-03  9.76261776e-03
  9.93530732e-03  1.01917339e-02  1.04793217e-02  1.06148552e-02
  1.08640641e-02  1.10221701e-02  1.13289980e-02  1.22346347e-02
  1.25624267e-02  1.27624609e-02  1.28267482e-02  1.35456799e-02
  1.38312783e-02  1.38328327e-02  1.40466047e-02  1.41893355e-02
  1.47529645e-02  1.52389416e-02  1.55250533e-02  1.55704143e-02
  1.58856921e-02  1.60645936e-02  1.68423411e-02  1.78695824e-02
  1.90442149e-02  1.98696926e-02  2.04116628e-02  2.23296639e-02
  2.41227746e-02  2.41608303e-02  2.56690346e-02  2.58972514e-02
  2.62525640e-02  2.64673773e-02  2.66276486e-02  2.78883614e-02
  2.93640308e-02  3.16636898e-02  3.71480435e-02  3.88736166e-02
  3.95146534e-02  4.01795655e-02  4.38396409e-02  4.52319980e-02
  4.55563180e-02  4.79730219e-02  4.81329672e-02  4.87812385e-02
  5.01981638e-02  5.25298491e-02  5.32696210e-02  5.49021810e-02
  5.50218932e-02  5.51121123e-02  5.61426468e-02  5.62180430e-02
  5.82990013e-02  5.98003753e-02  5.99673726e-02  6.04922101e-02
  6.07161038e-02  6.21899851e-02  6.33757561e-02  6.49580956e-02
  6.61976263e-02  6.70423657e-02  6.70559630e-02  6.89794496e-02
  7.01518059e-02  7.07960352e-02  7.22432062e-02  7.22494498e-02
  7.28456080e-02  7.41794407e-02  7.52216801e-02  7.58370683e-02
  7.73669556e-02  7.73748085e-02  7.74084032e-02  7.83132315e-02
  7.84591436e-02  8.05021301e-02  8.13164562e-02  8.27183872e-02
  8.51926953e-02  8.58715847e-02  8.69774371e-02  8.86642858e-02
  8.87325704e-02  9.00587142e-02  9.24647450e-02  9.27215442e-02
  9.34376940e-02  9.65285152e-02  9.83176380e-02  9.93770361e-02
  9.96875539e-02  1.01243556e-01  1.03412710e-01  1.04241572e-01
  1.05375245e-01  1.05523556e-01  1.07754543e-01  1.08516142e-01
  1.16890028e-01  1.17342189e-01  1.18081324e-01  1.18875943e-01
  1.19871140e-01  1.23306885e-01  1.27075732e-01  1.28682837e-01
  1.29534796e-01  1.30535901e-01  1.33632079e-01  1.35491222e-01
  1.35929823e-01  1.37790158e-01  1.38672248e-01  1.42034322e-01
  1.42226666e-01  1.43004566e-01  1.44051567e-01  1.44101456e-01
  1.44140735e-01  1.46289364e-01  1.47279054e-01  1.50274411e-01
  1.55945331e-01  1.60824180e-01  1.61353394e-01  1.64215729e-01
  1.64695352e-01  1.65379003e-01  1.73456907e-01  1.78755924e-01
  1.79191634e-01  1.80194482e-01  1.83056727e-01  1.84659392e-01
  1.84697151e-01  1.87369332e-01  1.88167736e-01  1.88718513e-01
  1.89312190e-01  1.91149876e-01  1.91393554e-01  1.91774756e-01
  1.93486050e-01  1.94194853e-01  1.98921129e-01  1.99007943e-01
  2.00837299e-01  2.01108769e-01  2.03573361e-01  2.04907611e-01
  2.08125174e-01  2.10034162e-01  2.12549537e-01  2.12674245e-01
  2.13567898e-01  2.14347601e-01  2.17208907e-01  2.17387646e-01
  2.17805877e-01  2.19391048e-01  2.19494596e-01  2.19893694e-01
  2.22399637e-01  2.22655013e-01  2.25597858e-01  2.25808203e-01
  2.25928605e-01  2.26973563e-01  2.27038935e-01  2.29344502e-01
  2.29983568e-01  2.30893612e-01  2.33296663e-01  2.38153800e-01
  2.39185855e-01  2.41975576e-01  2.44221270e-01  2.51286864e-01
  2.52536923e-01  2.55368650e-01  2.56973505e-01  2.61678070e-01
  2.66531080e-01  2.69552886e-01  2.70513743e-01  2.71093011e-01
  2.71717042e-01  2.72884130e-01  2.73049057e-01  2.73737013e-01
  2.75952250e-01  2.76756525e-01  2.79204935e-01  2.81167597e-01
  2.83587903e-01  2.84129649e-01  2.88237393e-01  2.88249314e-01
  2.88450867e-01  2.88873911e-01  2.90869772e-01  2.91864991e-01
  2.93534219e-01  2.96550542e-01  2.99109578e-01  3.02722752e-01
  3.12822491e-01  3.13800395e-01  3.14090252e-01  3.15804303e-01
  3.16107005e-01  3.19074363e-01  3.20067793e-01  3.21104705e-01
  3.21686000e-01  3.22364837e-01  3.25516015e-01  3.27446282e-01
  3.28288883e-01  3.29345882e-01  3.30084622e-01  3.32378179e-01
  3.32783103e-01  3.37826788e-01  3.38329792e-01  3.43907416e-01
  3.47248375e-01  3.48164767e-01  3.48432004e-01  3.51038873e-01
  3.53131264e-01  3.54837924e-01  3.56726319e-01  3.61236393e-01
  3.67245197e-01  3.70661080e-01  3.70782793e-01  3.72471690e-01
  3.75625998e-01  3.76417160e-01  3.78878266e-01  3.79685134e-01
  3.83802772e-01  3.86510611e-01  3.87447596e-01  3.90372276e-01
  3.91449273e-01  3.92194450e-01  3.95962596e-01  3.96577269e-01
  4.07275468e-01  4.14531410e-01  4.18326408e-01  4.19481218e-01
  4.20429826e-01  4.20919776e-01  4.22915071e-01  4.34792161e-01
  4.36137497e-01  4.37608153e-01  4.38669056e-01  4.40831572e-01
  4.43264037e-01  4.49021548e-01  4.50726986e-01  4.51664686e-01
  4.56025183e-01  4.56093669e-01  4.57580805e-01  4.57998008e-01
  4.61113513e-01  4.61174399e-01  4.62462693e-01  4.68244731e-01
  4.73708063e-01  4.82774198e-01  4.87022489e-01  4.87691969e-01
  4.92188483e-01  4.92569625e-01  4.94714111e-01  5.01757920e-01
  5.05304277e-01  5.06556392e-01  5.09761393e-01  5.12350857e-01
  5.15960038e-01  5.16605198e-01  5.20949841e-01  5.22145391e-01
  5.33051074e-01  5.34902513e-01  5.37905514e-01  5.40188789e-01
  5.43208957e-01  5.45668066e-01  5.46170354e-01  5.46971738e-01
  5.67852914e-01  5.68838537e-01  5.71701586e-01  5.72998047e-01
  5.83813429e-01  5.92758775e-01  5.94814539e-01  5.97332358e-01
  5.98782659e-01  5.98944128e-01  6.03617311e-01  6.06318176e-01
  6.14148557e-01  6.16852164e-01  6.18995965e-01  6.28753245e-01
  6.32043421e-01  6.40636981e-01  6.41264141e-01  6.51915252e-01
  6.51931584e-01  6.52261555e-01  6.55422628e-01  6.63904548e-01
  6.65184140e-01  6.68884277e-01  6.81903899e-01  6.82681024e-01
  6.89596951e-01  6.92133129e-01  7.00668812e-01  7.02109337e-01
  7.03051567e-01  7.04225063e-01  7.10754275e-01  7.17288911e-01
  7.24033177e-01  7.26865649e-01  7.31927276e-01  7.32061684e-01
  7.35734046e-01  7.45156825e-01  7.56213307e-01  7.58200467e-01
  7.58434355e-01  7.63223350e-01  7.90070295e-01  7.90410161e-01
  7.92653322e-01  7.96850264e-01  7.96955287e-01  8.10569406e-01
  8.12744319e-01  8.29399645e-01  8.37955356e-01  8.39768648e-01
  8.44961643e-01  8.54547620e-01  8.76455069e-01  8.86109829e-01
  9.00673747e-01  9.12948668e-01  9.15201604e-01  9.15258765e-01
  9.24456835e-01  9.37663257e-01  9.43243086e-01  9.43432987e-01
  9.97354031e-01  1.00380456e+00  1.00798178e+00  1.01475859e+00
  1.01481164e+00  1.03764594e+00  1.06272495e+00  1.08376825e+00
  1.09094024e+00  1.11498392e+00  1.14774573e+00  1.15869725e+00
  1.33226919e+00]

  warnings.warn(

2022-11-03 10:53:45,766:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.36026183e-01 -2.19637409e-01 -1.66873649e-01 -1.12116069e-01
 -1.09182537e-01 -1.08353607e-01 -1.08169235e-01 -9.40750316e-02
 -9.35328230e-02 -8.99574161e-02 -8.45132172e-02 -8.42282102e-02
 -8.04673880e-02 -7.92534128e-02 -7.60652423e-02 -7.48545229e-02
 -7.36348629e-02 -6.88231066e-02 -6.85000420e-02 -6.43730983e-02
 -6.33585453e-02 -6.15996979e-02 -6.15544654e-02 -6.03072755e-02
 -5.55556938e-02 -5.21684289e-02 -5.15566953e-02 -5.11058643e-02
 -4.80719395e-02 -4.67905998e-02 -4.66571487e-02 -4.64898236e-02
 -4.39671390e-02 -4.03833315e-02 -3.97414528e-02 -3.86305042e-02
 -3.83247845e-02 -3.73425968e-02 -3.71063612e-02 -3.68237346e-02
 -3.58505584e-02 -3.51833180e-02 -3.51486020e-02 -3.48277688e-02
 -3.45818177e-02 -3.30849960e-02 -3.26746777e-02 -3.24327871e-02
 -3.19163986e-02 -2.91069783e-02 -2.90913414e-02 -2.90273521e-02
 -2.87252851e-02 -2.65466236e-02 -2.53090039e-02 -2.48910915e-02
 -2.42621154e-02 -2.39425041e-02 -2.36473288e-02 -2.22840514e-02
 -2.20073834e-02 -2.18070913e-02 -2.12209597e-02 -2.10432429e-02
 -2.07276344e-02 -2.04814356e-02 -1.88935194e-02 -1.87319815e-02
 -1.79071072e-02 -1.74779873e-02 -1.73534192e-02 -1.64573025e-02
 -1.60195567e-02 -1.58136021e-02 -1.51471458e-02 -1.48569578e-02
 -1.48326177e-02 -1.37043558e-02 -1.26672294e-02 -1.16240131e-02
 -1.13056339e-02 -9.92557034e-03 -9.91898775e-03 -9.48641077e-03
 -9.18301661e-03 -9.01393034e-03 -8.33589677e-03 -7.66948983e-03
 -7.56710116e-03 -7.27822399e-03 -6.75709546e-03 -6.42538769e-03
 -5.84245706e-03 -5.43457083e-03 -4.85426653e-03 -4.71939938e-03
 -3.31189181e-03 -2.73579103e-03 -2.08745664e-03 -4.25970939e-04
 -1.12029302e-04 -9.42722399e-05  1.41001865e-03  1.60253351e-03
  2.13898835e-03  2.37363228e-03  2.46548816e-03  2.69227312e-03
  3.00645665e-03  3.19603994e-03  3.94500839e-03  4.14295588e-03
  5.01785334e-03  5.20905014e-03  5.23985643e-03  5.63749112e-03
  6.01212494e-03  6.88254088e-03  7.23780645e-03  7.34447269e-03
  8.05998594e-03  1.07116848e-02  1.32548772e-02  1.34094954e-02
  1.35599216e-02  1.38691133e-02  1.41179133e-02  1.42039703e-02
  1.49430232e-02  1.50279487e-02  1.51417172e-02  1.55186718e-02
  1.55652771e-02  1.61169395e-02  1.62908994e-02  1.63642447e-02
  1.80076975e-02  1.80719811e-02  1.93824470e-02  1.94837786e-02
  1.95310060e-02  1.98605545e-02  2.00395118e-02  2.00644862e-02
  2.02576201e-02  2.03953087e-02  2.04535984e-02  2.14884002e-02
  2.17383299e-02  2.34444607e-02  2.64057424e-02  2.65171733e-02
  2.67953817e-02  2.91093495e-02  2.93703899e-02  3.06475554e-02
  3.14163491e-02  3.17741744e-02  3.20930704e-02  3.35443877e-02
  3.39203030e-02  3.39405537e-02  3.70635353e-02  3.87096293e-02
  3.89645174e-02  4.01714817e-02  4.09751162e-02  4.31087911e-02
  4.32030112e-02  4.34057191e-02  4.47027460e-02  4.65931259e-02
  4.67346273e-02  4.73834351e-02  4.79240641e-02  4.82076705e-02
  4.94688042e-02  4.96283993e-02  5.00345752e-02  5.02321832e-02
  5.10921068e-02  5.11919707e-02  5.17482609e-02  5.41515127e-02
  5.43709956e-02  5.51141910e-02  5.68009689e-02  5.86541854e-02
  5.88552840e-02  6.00646846e-02  6.06334209e-02  6.07689172e-02
  6.15610145e-02  6.29250631e-02  6.31531850e-02  6.52954951e-02
  6.66622519e-02  6.74592853e-02  6.93547726e-02  6.94029257e-02
  6.97953478e-02  7.04582632e-02  7.27434009e-02  7.71576762e-02
  7.92086795e-02  8.20871741e-02  8.23134705e-02  8.24544728e-02
  8.31472576e-02  8.39509442e-02  8.48160610e-02  8.54963362e-02
  9.18626338e-02  9.25198793e-02  9.67322290e-02  9.81768742e-02
  9.92927998e-02  9.98515338e-02  1.00814573e-01  1.02481358e-01
  1.02864750e-01  1.03304572e-01  1.06722705e-01  1.08364701e-01
  1.08560823e-01  1.08983621e-01  1.10005282e-01  1.10376798e-01
  1.12549700e-01  1.13152422e-01  1.13371037e-01  1.17283054e-01
  1.19771987e-01  1.20615073e-01  1.21545233e-01  1.25820622e-01
  1.27953246e-01  1.29525349e-01  1.30678356e-01  1.31442025e-01
  1.32860780e-01  1.33643612e-01  1.34055451e-01  1.35234728e-01
  1.38725579e-01  1.38881892e-01  1.39651954e-01  1.40665054e-01
  1.40669882e-01  1.42299309e-01  1.44316748e-01  1.54088780e-01
  1.55820355e-01  1.57848626e-01  1.58089638e-01  1.59223825e-01
  1.63076133e-01  1.63565531e-01  1.65712446e-01  1.66680485e-01
  1.70223355e-01  1.70640796e-01  1.71594918e-01  1.72262669e-01
  1.72776610e-01  1.74965352e-01  1.76104188e-01  1.76289216e-01
  1.78560048e-01  1.78712994e-01  1.85239434e-01  1.88519701e-01
  1.89211503e-01  1.89297274e-01  1.90603763e-01  1.90938175e-01
  1.92905009e-01  1.98277652e-01  2.00980768e-01  2.02321604e-01
  2.03648388e-01  2.07035109e-01  2.10311204e-01  2.11072162e-01
  2.13955477e-01  2.17972070e-01  2.22562268e-01  2.25540474e-01
  2.28907049e-01  2.29871079e-01  2.30946526e-01  2.37206250e-01
  2.37436011e-01  2.38830388e-01  2.41013765e-01  2.42140040e-01
  2.43745193e-01  2.45049179e-01  2.46198535e-01  2.46549562e-01
  2.49908164e-01  2.51287133e-01  2.52629787e-01  2.53502280e-01
  2.56325334e-01  2.57126570e-01  2.57726401e-01  2.59800315e-01
  2.60141701e-01  2.62987524e-01  2.63942838e-01  2.65099108e-01
  2.65153348e-01  2.73561686e-01  2.73773074e-01  2.75193810e-01
  2.76636600e-01  2.77346700e-01  2.78992593e-01  2.84118831e-01
  2.85889804e-01  2.86002964e-01  2.87979901e-01  2.92815894e-01
  2.94057906e-01  2.94580132e-01  2.95046598e-01  2.99130380e-01
  2.99499243e-01  3.00351351e-01  3.00523788e-01  3.01699966e-01
  3.02560568e-01  3.14442664e-01  3.15661639e-01  3.15988064e-01
  3.19239050e-01  3.22159410e-01  3.23458850e-01  3.28209996e-01
  3.28800946e-01  3.30873519e-01  3.36140662e-01  3.42292100e-01
  3.46037596e-01  3.48352969e-01  3.48763317e-01  3.66989315e-01
  3.67265135e-01  3.67597312e-01  3.68813068e-01  3.72555405e-01
  3.74299616e-01  3.74742240e-01  3.78156126e-01  3.78269941e-01
  3.79033983e-01  3.80372882e-01  3.82504880e-01  3.83738995e-01
  3.90466481e-01  3.93324703e-01  3.98565173e-01  3.99027288e-01
  3.99979860e-01  4.06946301e-01  4.09435540e-01  4.19599950e-01
  4.20444310e-01  4.22164440e-01  4.24076945e-01  4.35952395e-01
  4.56100821e-01  4.63201433e-01  4.63432193e-01  4.63843852e-01
  4.67513144e-01  4.68750417e-01  4.76299495e-01  4.80635166e-01
  4.85333711e-01  4.89496797e-01  4.93885875e-01  4.96386141e-01
  5.02173483e-01  5.10702848e-01  5.12908041e-01  5.25395870e-01
  5.28656304e-01  5.29756546e-01  5.31099737e-01  5.31400621e-01
  5.32789230e-01  5.40409446e-01  5.44128716e-01  5.44746339e-01
  5.45796037e-01  5.46830952e-01  5.55126846e-01  5.60335457e-01
  5.62580764e-01  5.65677583e-01  5.66531003e-01  5.73088944e-01
  5.79788744e-01  5.80224931e-01  5.84100068e-01  5.85676908e-01
  5.91856062e-01  5.92027128e-01  6.04083300e-01  6.08862281e-01
  6.09764159e-01  6.09937310e-01  6.11656427e-01  6.12513840e-01
  6.15471125e-01  6.16062045e-01  6.22393727e-01  6.22580588e-01
  6.27647758e-01  6.30867124e-01  6.32416666e-01  6.36854053e-01
  6.39861882e-01  6.42995715e-01  6.44136012e-01  6.54075503e-01
  6.58264935e-01  6.62465036e-01  6.65229917e-01  6.67300582e-01
  6.71651304e-01  6.75816178e-01  6.83765829e-01  6.88122451e-01
  6.89616561e-01  6.97307706e-01  7.07822204e-01  7.14580595e-01
  7.17953026e-01  7.18702674e-01  7.29950845e-01  7.31425405e-01
  7.34584749e-01  7.43041754e-01  7.45960176e-01  7.46595800e-01
  7.52317846e-01  7.52731085e-01  7.52916515e-01  7.60810435e-01
  7.69803405e-01  7.82024503e-01  7.84213781e-01  7.89027154e-01
  7.90164590e-01  8.06162834e-01  8.17565143e-01  8.29175770e-01
  8.36213350e-01  8.37445438e-01  8.42413485e-01  8.53647232e-01
  8.56510043e-01  8.61612976e-01  8.65988195e-01  8.71813595e-01
  9.01181221e-01  9.09650922e-01  9.10775721e-01  9.21060205e-01
  9.29016888e-01  9.33296084e-01  9.33359683e-01  9.39759910e-01
  9.39952314e-01  9.41558242e-01  9.43429351e-01  9.52499092e-01
  9.78034258e-01  9.78547335e-01  1.00049376e+00  1.00568235e+00
  1.00915027e+00  1.01271999e+00  1.01488066e+00  1.02683365e+00
  1.03584325e+00  1.05010855e+00  1.08406472e+00  1.11526811e+00
  1.11597419e+00]

  warnings.warn(

2022-11-03 10:53:45,766:INFO:Calculating mean and std
2022-11-03 10:53:45,766:INFO:Creating metrics dataframe
2022-11-03 10:53:45,781:INFO:Uploading results into container
2022-11-03 10:53:45,781:INFO:Uploading model into container now
2022-11-03 10:53:45,781:INFO:master_model_container: 33
2022-11-03 10:53:45,781:INFO:display_container: 2
2022-11-03 10:53:45,781:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, gamma=None,
             gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, n_estimators=100, n_jobs=-1,
             num_parallel_tree=None, predictor=None, random_state=4411,
             reg_alpha=None, reg_lambda=None, ...)
2022-11-03 10:53:45,781:INFO:create_model() successfully completed......................................
2022-11-03 10:53:46,052:WARNING:create_model() for XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, gamma=None,
             gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, n_estimators=100, n_jobs=-1,
             num_parallel_tree=None, predictor=None, random_state=4411,
             reg_alpha=None, reg_lambda=None, ...) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:53:46,052:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:53:46,052:INFO:Initializing create_model()
2022-11-03 10:53:46,052:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:53:46,052:INFO:Checking exceptions
2022-11-03 10:53:46,062:INFO:Importing libraries
2022-11-03 10:53:46,062:INFO:Copying training dataset
2022-11-03 10:53:46,078:INFO:Defining folds
2022-11-03 10:53:46,078:INFO:Declaring metric variables
2022-11-03 10:53:46,078:INFO:Importing untrained model
2022-11-03 10:53:46,078:INFO:Extreme Gradient Boosting Imported successfully
2022-11-03 10:53:46,078:INFO:Starting cross validation
2022-11-03 10:53:46,086:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:53:53,719:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.41973788e-01 -2.25245759e-01 -1.71147808e-01 -1.70578837e-01
 -1.64338261e-01 -1.49234444e-01 -1.38623849e-01 -1.00532651e-01
 -9.92353186e-02 -9.22031924e-02 -8.42857957e-02 -8.39339197e-02
 -8.19465071e-02 -7.24167973e-02 -7.14769214e-02 -6.92429170e-02
 -6.77258447e-02 -6.70992807e-02 -6.20093532e-02 -5.18554188e-02
 -5.16554117e-02 -5.14005944e-02 -5.07839508e-02 -5.05513251e-02
 -4.96393107e-02 -4.90949936e-02 -4.65422869e-02 -4.38302532e-02
 -4.29598130e-02 -4.13159281e-02 -4.02998179e-02 -3.92829590e-02
 -3.82440723e-02 -3.79798934e-02 -3.71850803e-02 -3.62747237e-02
 -3.20775621e-02 -3.15095857e-02 -2.94189509e-02 -2.94121392e-02
 -2.55238768e-02 -2.41177827e-02 -2.36358587e-02 -2.27399711e-02
 -2.27163285e-02 -2.11510006e-02 -2.03364324e-02 -2.02136766e-02
 -1.98505130e-02 -1.98289007e-02 -1.96505710e-02 -1.95668433e-02
 -1.89616848e-02 -1.82630606e-02 -1.72297414e-02 -1.69449784e-02
 -1.58412848e-02 -1.53762642e-02 -1.49719929e-02 -1.46356039e-02
 -1.43518224e-02 -1.38967652e-02 -1.38207339e-02 -1.32986568e-02
 -1.27821760e-02 -1.27542894e-02 -1.26809487e-02 -1.13628628e-02
 -1.13615505e-02 -1.12961531e-02 -1.12810051e-02 -1.00410748e-02
 -9.42201074e-03 -8.73257499e-03 -6.46048039e-03 -6.11707242e-03
 -5.58580970e-03 -5.42859314e-03 -5.38851880e-03 -5.19318320e-03
 -5.01265517e-03 -4.87453165e-03 -4.67646448e-03 -4.14223224e-03
 -3.98880476e-03 -3.04654380e-03 -2.80883024e-03 -2.68783607e-03
 -1.58149132e-03 -5.47427771e-05  1.98937976e-03  2.41009379e-03
  2.66874046e-03  2.69236299e-03  3.57658602e-03  3.91911343e-03
  4.60094633e-03  4.85447049e-03  5.83110610e-03  6.28137868e-03
  6.29679579e-03  6.52102754e-03  7.30911503e-03  7.66280293e-03
  7.78172817e-03  8.54982808e-03  9.76838637e-03  1.05346488e-02
  1.08170575e-02  1.08690225e-02  1.10416366e-02  1.17050977e-02
  1.20356167e-02  1.29860258e-02  1.33730434e-02  1.34965843e-02
  1.37643069e-02  1.43995127e-02  1.43999858e-02  1.50223095e-02
  1.53861903e-02  1.55475065e-02  1.70799699e-02  1.72554869e-02
  1.74165927e-02  1.77407451e-02  1.85776539e-02  1.99982729e-02
  2.14665234e-02  2.22828761e-02  2.25562789e-02  2.29094308e-02
  2.32659895e-02  2.34326907e-02  2.36823037e-02  2.56462879e-02
  2.57361811e-02  2.72219535e-02  2.73259822e-02  2.76261419e-02
  2.78771874e-02  2.85609700e-02  2.86104176e-02  2.91221775e-02
  2.93399021e-02  2.94308756e-02  3.05430498e-02  3.13447416e-02
  3.22095640e-02  3.41257714e-02  3.60204279e-02  3.75348479e-02
  3.80380936e-02  3.86878625e-02  3.87291089e-02  4.12993915e-02
  4.43391949e-02  4.46925201e-02  4.48582582e-02  4.55251522e-02
  4.62989360e-02  4.64920290e-02  4.71798591e-02  4.78559583e-02
  4.80952971e-02  4.86529879e-02  4.89377715e-02  4.90690768e-02
  4.93126959e-02  5.10360189e-02  5.40926084e-02  5.43788187e-02
  5.46386950e-02  5.62430657e-02  5.62862232e-02  5.67830913e-02
  6.53382912e-02  6.73735440e-02  6.76043481e-02  6.77109808e-02
  6.79038242e-02  6.82959408e-02  6.84459507e-02  6.87450320e-02
  7.15199858e-02  7.23517016e-02  7.23847598e-02  7.35071078e-02
  7.47514367e-02  7.51089603e-02  7.57592767e-02  7.63804615e-02
  7.76705965e-02  7.83024654e-02  8.04317817e-02  8.08122754e-02
  8.24966058e-02  8.37066695e-02  8.54385421e-02  8.63107145e-02
  8.82293358e-02  8.94409046e-02  9.18638632e-02  9.18892100e-02
  9.33827683e-02  9.46689919e-02  9.54709873e-02  9.69256982e-02
  9.76359844e-02  9.92515311e-02  9.93361026e-02  9.97209027e-02
  1.00769475e-01  1.09126329e-01  1.12146243e-01  1.12172052e-01
  1.14088133e-01  1.20005205e-01  1.22942641e-01  1.28556103e-01
  1.30755723e-01  1.30777627e-01  1.31115869e-01  1.31625116e-01
  1.32181540e-01  1.37246698e-01  1.39333025e-01  1.40731528e-01
  1.42585531e-01  1.46657914e-01  1.48099989e-01  1.50808349e-01
  1.52844831e-01  1.55579180e-01  1.58437803e-01  1.59578323e-01
  1.61539033e-01  1.64109573e-01  1.64874345e-01  1.65081173e-01
  1.68927118e-01  1.69747770e-01  1.71648055e-01  1.75372660e-01
  1.76093206e-01  1.78247541e-01  1.79506570e-01  1.80047110e-01
  1.81565270e-01  1.82918698e-01  1.83167920e-01  1.88164279e-01
  1.90104634e-01  1.90704718e-01  1.94075152e-01  1.98596701e-01
  1.98861957e-01  1.99503630e-01  2.01494351e-01  2.08040476e-01
  2.08222330e-01  2.08775163e-01  2.14886233e-01  2.17597440e-01
  2.18288079e-01  2.20706865e-01  2.20924035e-01  2.24855915e-01
  2.28517652e-01  2.32082203e-01  2.34874755e-01  2.37819985e-01
  2.38642901e-01  2.38793448e-01  2.41592064e-01  2.42244139e-01
  2.48986438e-01  2.53255278e-01  2.55645335e-01  2.55687058e-01
  2.61584491e-01  2.63940126e-01  2.64561206e-01  2.67333597e-01
  2.67511159e-01  2.69870520e-01  2.73600966e-01  2.76433885e-01
  2.76601970e-01  2.78207421e-01  2.78888792e-01  2.79437929e-01
  2.84092724e-01  2.86420375e-01  2.92797863e-01  2.97692388e-01
  3.00473809e-01  3.04838330e-01  3.07363153e-01  3.11419696e-01
  3.14907968e-01  3.16538572e-01  3.18435550e-01  3.18688631e-01
  3.18859994e-01  3.19904178e-01  3.20374548e-01  3.20801467e-01
  3.21102411e-01  3.26018721e-01  3.27211708e-01  3.27376872e-01
  3.27900350e-01  3.28251839e-01  3.30395877e-01  3.33006054e-01
  3.34584177e-01  3.34797442e-01  3.42223942e-01  3.42257321e-01
  3.45185786e-01  3.47753018e-01  3.50810647e-01  3.52976918e-01
  3.53025079e-01  3.56778115e-01  3.61108094e-01  3.63512576e-01
  3.75503689e-01  3.76480043e-01  3.80918264e-01  3.84920746e-01
  3.93951088e-01  3.96159887e-01  3.97898138e-01  3.99320513e-01
  3.99783254e-01  4.00840521e-01  4.04219329e-01  4.04725820e-01
  4.06700432e-01  4.07201648e-01  4.08806086e-01  4.12332147e-01
  4.15697813e-01  4.16423202e-01  4.16782022e-01  4.16918099e-01
  4.17487383e-01  4.20083553e-01  4.21064973e-01  4.22919869e-01
  4.28492397e-01  4.30135459e-01  4.35010135e-01  4.38854039e-01
  4.42619890e-01  4.49184299e-01  4.54362959e-01  4.61690664e-01
  4.63580310e-01  4.64584053e-01  4.67140645e-01  4.72071588e-01
  4.74638611e-01  4.75268483e-01  4.75702941e-01  4.76358175e-01
  4.77597922e-01  4.79725450e-01  4.79903728e-01  4.80634749e-01
  4.81319427e-01  4.82000679e-01  4.86888677e-01  4.92011368e-01
  4.93035674e-01  4.94771183e-01  4.94853526e-01  4.97984380e-01
  4.99706537e-01  5.00448465e-01  5.06717503e-01  5.15146136e-01
  5.17995417e-01  5.19248605e-01  5.25510669e-01  5.27338564e-01
  5.28249741e-01  5.34191549e-01  5.46492994e-01  5.50576746e-01
  5.50675750e-01  5.55642307e-01  5.57106674e-01  5.57283819e-01
  5.59246361e-01  5.59755445e-01  5.60386956e-01  5.60590446e-01
  5.61064661e-01  5.62397003e-01  5.70858121e-01  5.71617663e-01
  5.74387431e-01  5.77444315e-01  5.82525074e-01  5.87343216e-01
  5.89296997e-01  5.90704083e-01  5.93228996e-01  5.97114742e-01
  6.07086241e-01  6.10936761e-01  6.13351762e-01  6.13442481e-01
  6.17560565e-01  6.31236851e-01  6.37468934e-01  6.37730420e-01
  6.40245020e-01  6.40672863e-01  6.44718170e-01  6.50839686e-01
  6.50933266e-01  6.53057754e-01  6.57566428e-01  6.59660339e-01
  6.63467228e-01  6.64031744e-01  6.68348253e-01  6.70492232e-01
  6.75002098e-01  6.80618286e-01  6.83645785e-01  6.88571632e-01
  7.02825248e-01  7.11556554e-01  7.11989045e-01  7.12997496e-01
  7.16381907e-01  7.27939904e-01  7.41924286e-01  7.43215024e-01
  7.54347682e-01  7.63199985e-01  7.66280830e-01  7.70555317e-01
  7.72489727e-01  7.88969398e-01  7.90347457e-01  7.92486668e-01
  7.92795002e-01  7.94337988e-01  8.11862230e-01  8.12181115e-01
  8.14708650e-01  8.21307361e-01  8.32112730e-01  8.34257603e-01
  8.35584998e-01  8.37072551e-01  8.40897560e-01  8.41805398e-01
  8.44504714e-01  8.51307452e-01  8.53451550e-01  8.74177516e-01
  8.76979172e-01  8.79003704e-01  8.80118966e-01  8.88440013e-01
  8.90400589e-01  8.91530871e-01  9.19967711e-01  9.22429681e-01
  9.22607720e-01  9.23510969e-01  9.26246285e-01  9.34806168e-01
  9.46619213e-01  9.48441029e-01  9.51679289e-01  9.74910378e-01
  9.79134679e-01  9.89768505e-01  1.03150630e+00  1.07669365e+00
  1.08738470e+00  1.10430527e+00  1.11612654e+00  1.15259910e+00
  1.16363847e+00]

  warnings.warn(

2022-11-03 10:53:53,833:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.95084566e-01 -2.22720012e-01 -2.10219353e-01 -1.68064684e-01
 -1.40978247e-01 -1.35118857e-01 -1.22781709e-01 -1.14110261e-01
 -9.96714979e-02 -9.01750922e-02 -9.00355279e-02 -8.48739669e-02
 -7.36881793e-02 -7.25435987e-02 -7.19607919e-02 -7.01783150e-02
 -6.61658570e-02 -6.54489025e-02 -6.37479350e-02 -6.34305254e-02
 -6.28660843e-02 -6.19453713e-02 -6.15718588e-02 -5.38895167e-02
 -5.15461601e-02 -4.93647717e-02 -4.78210859e-02 -4.77834940e-02
 -4.76124957e-02 -4.42681313e-02 -3.91983986e-02 -3.53984497e-02
 -3.53466347e-02 -3.48930024e-02 -3.39259617e-02 -3.20560373e-02
 -3.15805562e-02 -3.14315408e-02 -2.98430659e-02 -2.78125368e-02
 -2.72269882e-02 -2.68399306e-02 -2.37907767e-02 -2.35713907e-02
 -2.32333466e-02 -2.27130391e-02 -2.26760842e-02 -2.26065498e-02
 -2.11587567e-02 -1.92774776e-02 -1.87583473e-02 -1.86270140e-02
 -1.66827161e-02 -1.64522734e-02 -1.53535558e-02 -1.53452363e-02
 -1.52509641e-02 -1.52111612e-02 -1.46003254e-02 -1.41213844e-02
 -1.34355733e-02 -1.16329826e-02 -1.09902341e-02 -9.35806800e-03
 -9.05697048e-03 -8.12966190e-03 -7.76680512e-03 -6.31670747e-03
 -6.03842456e-03 -4.70764190e-03 -4.38144244e-03 -4.22046101e-03
 -4.18214547e-03 -3.54354456e-03 -3.46893584e-03 -3.39456159e-03
 -1.77374203e-03 -1.55013776e-03 -7.97002111e-04 -5.84949914e-04
 -1.15865842e-04  9.64030623e-05  1.58656877e-03  2.23660260e-03
  2.36805575e-03  2.79384083e-03  3.07764532e-03  3.57594108e-03
  3.90350749e-03  4.62144054e-03  5.17882593e-03  5.18555148e-03
  5.20122983e-03  5.42810932e-03  5.45106735e-03  5.55058848e-03
  5.91885857e-03  6.65124692e-03  8.09759181e-03  8.88915919e-03
  9.09756497e-03  9.12428927e-03  9.68559645e-03  1.03625208e-02
  1.08296517e-02  1.14716440e-02  1.18786348e-02  1.20897694e-02
  1.33839482e-02  1.38046145e-02  1.49467662e-02  1.59214083e-02
  1.60015281e-02  1.60422623e-02  1.72833763e-02  1.83607228e-02
  1.88027639e-02  1.90337915e-02  1.98984351e-02  2.07342766e-02
  2.07871124e-02  2.12172512e-02  2.37812214e-02  2.40948834e-02
  2.45370176e-02  2.60327198e-02  2.82912161e-02  2.89035663e-02
  2.95373574e-02  3.17183323e-02  3.30077372e-02  3.52715142e-02
  3.61486487e-02  3.62444632e-02  3.66115831e-02  3.69909965e-02
  3.98523249e-02  4.00840677e-02  4.09971252e-02  4.15377431e-02
  4.21562046e-02  4.23267409e-02  4.24138941e-02  4.26931195e-02
  4.27956544e-02  4.47021164e-02  4.52241935e-02  4.53367531e-02
  4.63337973e-02  4.78467382e-02  4.78881933e-02  4.79281209e-02
  4.80404384e-02  4.96297069e-02  5.12209646e-02  5.26820756e-02
  5.36739565e-02  5.44729643e-02  5.52095659e-02  5.53866215e-02
  5.54738231e-02  5.63959330e-02  5.98464459e-02  6.03373274e-02
  6.03810102e-02  6.15919568e-02  6.54562712e-02  6.62250295e-02
  6.63228184e-02  6.63601607e-02  6.93773329e-02  6.98578432e-02
  7.02572614e-02  7.23005757e-02  7.42991492e-02  7.43623748e-02
  7.44089782e-02  7.84011632e-02  7.90524185e-02  8.01025182e-02
  8.30010474e-02  8.31208825e-02  8.37562233e-02  8.41070637e-02
  8.46674219e-02  8.53133351e-02  8.61525610e-02  9.07649919e-02
  9.12202671e-02  9.12644193e-02  9.25805941e-02  9.58935916e-02
  9.59803537e-02  9.64763835e-02  9.65573192e-02  9.67813358e-02
  9.71470848e-02  9.78272334e-02  9.78443325e-02  9.97206941e-02
  9.97590199e-02  9.99357253e-02  1.01018675e-01  1.01853095e-01
  1.05510861e-01  1.05828665e-01  1.05887324e-01  1.06849603e-01
  1.07375108e-01  1.07506596e-01  1.08092874e-01  1.14726819e-01
  1.16110414e-01  1.17136046e-01  1.17465757e-01  1.17528059e-01
  1.17792949e-01  1.19642965e-01  1.20087378e-01  1.20175220e-01
  1.22747004e-01  1.26073197e-01  1.26196459e-01  1.26479670e-01
  1.27349228e-01  1.30259290e-01  1.30538002e-01  1.30566761e-01
  1.33530229e-01  1.35677397e-01  1.40977412e-01  1.41213775e-01
  1.42547786e-01  1.46602213e-01  1.47716105e-01  1.47731140e-01
  1.49172693e-01  1.49763256e-01  1.50882140e-01  1.51362419e-01
  1.54057220e-01  1.54906765e-01  1.55266285e-01  1.57790169e-01
  1.59033805e-01  1.59363836e-01  1.61573321e-01  1.61977932e-01
  1.63633794e-01  1.63945213e-01  1.65047675e-01  1.65231317e-01
  1.69828385e-01  1.69833243e-01  1.70661822e-01  1.71265945e-01
  1.72418147e-01  1.75854206e-01  1.76881284e-01  1.78521022e-01
  1.79371327e-01  1.80143565e-01  1.82376549e-01  1.87568203e-01
  1.93201408e-01  1.94330156e-01  1.94510520e-01  1.96059138e-01
  1.96210012e-01  1.97260752e-01  1.98657870e-01  2.01971084e-01
  2.03964621e-01  2.06384197e-01  2.11283013e-01  2.11969391e-01
  2.16770962e-01  2.17768475e-01  2.21780866e-01  2.22350240e-01
  2.27371544e-01  2.27416962e-01  2.34208047e-01  2.40384489e-01
  2.40597159e-01  2.42233366e-01  2.44178668e-01  2.44431466e-01
  2.45275304e-01  2.46697709e-01  2.47497544e-01  2.51689017e-01
  2.52945870e-01  2.56502539e-01  2.57224649e-01  2.63683587e-01
  2.63890058e-01  2.73062885e-01  2.73465842e-01  2.73706377e-01
  2.74938226e-01  2.76334077e-01  2.81843901e-01  2.86703587e-01
  2.92658478e-01  2.93379217e-01  2.93546230e-01  2.94711053e-01
  2.97143400e-01  2.98286259e-01  3.00283432e-01  3.01908135e-01
  3.03055435e-01  3.04984391e-01  3.05354774e-01  3.05982888e-01
  3.06451619e-01  3.11881185e-01  3.12261254e-01  3.16924632e-01
  3.20956916e-01  3.23742121e-01  3.25850129e-01  3.30783337e-01
  3.31037343e-01  3.33618969e-01  3.34462881e-01  3.35196137e-01
  3.35473359e-01  3.36986512e-01  3.47463787e-01  3.48681092e-01
  3.51924360e-01  3.52467865e-01  3.61150384e-01  3.61885399e-01
  3.68361235e-01  3.71482193e-01  3.79161954e-01  3.82898957e-01
  3.84243131e-01  3.84723991e-01  3.85042995e-01  3.91060829e-01
  3.97110313e-01  4.00540143e-01  4.03893411e-01  4.05357540e-01
  4.10596818e-01  4.11842823e-01  4.14356440e-01  4.21825141e-01
  4.24174249e-01  4.24215138e-01  4.26778674e-01  4.28900361e-01
  4.31063235e-01  4.31430370e-01  4.37533796e-01  4.41946059e-01
  4.42352712e-01  4.44458872e-01  4.46089447e-01  4.46889639e-01
  4.47340727e-01  4.57181484e-01  4.58682299e-01  4.62436765e-01
  4.65057969e-01  4.67148155e-01  4.67233092e-01  4.67679203e-01
  4.70180392e-01  4.71339673e-01  4.73809838e-01  4.80712861e-01
  4.80788082e-01  4.81099963e-01  4.84734565e-01  4.86196399e-01
  4.86465007e-01  4.86694962e-01  4.87827808e-01  4.89663661e-01
  4.96052027e-01  4.99889851e-01  5.03831744e-01  5.12130260e-01
  5.17803729e-01  5.21858573e-01  5.25244534e-01  5.26205301e-01
  5.26716173e-01  5.29528677e-01  5.29762030e-01  5.38364887e-01
  5.43735683e-01  5.44015408e-01  5.52537620e-01  5.53211331e-01
  5.63699007e-01  5.64568996e-01  5.74714303e-01  5.81970870e-01
  5.82607567e-01  5.84043086e-01  5.86769104e-01  5.87683976e-01
  5.90501785e-01  5.93561292e-01  5.98863661e-01  6.11400008e-01
  6.12642407e-01  6.14434659e-01  6.16573155e-01  6.19389415e-01
  6.29026711e-01  6.30799890e-01  6.48472130e-01  6.52650356e-01
  6.52904272e-01  6.58087075e-01  6.73011422e-01  6.74165964e-01
  6.79459095e-01  6.85044050e-01  6.98303699e-01  6.98591650e-01
  7.01662242e-01  7.06041634e-01  7.07049131e-01  7.08094656e-01
  7.10355580e-01  7.11607575e-01  7.20449626e-01  7.21149683e-01
  7.29558408e-01  7.29619741e-01  7.30809450e-01  7.36883223e-01
  7.43090212e-01  7.43565261e-01  7.47191012e-01  7.51465440e-01
  7.52537072e-01  7.53723800e-01  7.54047036e-01  7.57218599e-01
  7.59487510e-01  7.59535134e-01  7.63794899e-01  7.68709302e-01
  7.73430169e-01  7.74185896e-01  7.74496794e-01  7.75694609e-01
  7.76709676e-01  7.81377852e-01  7.83105016e-01  7.90015936e-01
  7.91190922e-01  7.98002660e-01  8.16698909e-01  8.17439198e-01
  8.25727701e-01  8.42972577e-01  8.43883932e-01  8.45175087e-01
  8.47205520e-01  8.49546492e-01  8.52590382e-01  8.59655201e-01
  8.78329456e-01  8.84617209e-01  8.87602210e-01  8.90186369e-01
  8.94403696e-01  8.94411445e-01  8.97849381e-01  8.98573220e-01
  9.05936956e-01  9.18871284e-01  9.29884374e-01  9.36429083e-01
  9.45474386e-01  9.63476717e-01  9.65864301e-01  9.67370749e-01
  9.84071493e-01  9.98739660e-01  1.00262344e+00  1.05349028e+00
  1.12655795e+00]

  warnings.warn(

2022-11-03 10:53:53,903:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.56566614e-01 -2.07179800e-01 -1.98163554e-01 -1.37839749e-01
 -1.34467050e-01 -1.30100816e-01 -1.22531272e-01 -1.19845070e-01
 -1.11705832e-01 -9.82576087e-02 -9.82046053e-02 -9.33876634e-02
 -8.06415156e-02 -7.14448020e-02 -6.19519502e-02 -6.01251200e-02
 -5.80015182e-02 -5.66299446e-02 -5.56493476e-02 -5.38609475e-02
 -5.38367182e-02 -5.25060371e-02 -5.22204600e-02 -5.18634059e-02
 -5.11744767e-02 -4.98848669e-02 -4.96909693e-02 -4.78630103e-02
 -4.19053398e-02 -3.88093740e-02 -3.86038646e-02 -3.79812792e-02
 -3.52906026e-02 -3.17711756e-02 -3.06360219e-02 -3.01964637e-02
 -2.97156349e-02 -2.97151897e-02 -2.90521551e-02 -2.88570598e-02
 -2.86555290e-02 -2.60661691e-02 -2.48382408e-02 -2.38315649e-02
 -2.35124920e-02 -2.34249625e-02 -2.33871937e-02 -2.29287464e-02
 -2.24495064e-02 -2.22833846e-02 -2.19130684e-02 -2.05643438e-02
 -2.04418171e-02 -1.93573534e-02 -1.93190016e-02 -1.88356750e-02
 -1.85400788e-02 -1.72443874e-02 -1.50606260e-02 -1.41809536e-02
 -1.34796556e-02 -1.33169936e-02 -1.32597229e-02 -1.28865847e-02
 -1.28365746e-02 -1.25225484e-02 -1.16770435e-02 -1.14092883e-02
 -1.10825533e-02 -1.10229729e-02 -1.09497709e-02 -1.07558090e-02
 -1.06071895e-02 -1.04095424e-02 -1.00676566e-02 -9.95105691e-03
 -9.68914758e-03 -9.41073988e-03 -9.26597696e-03 -8.19353759e-03
 -7.90518709e-03 -7.75906350e-03 -7.71209877e-03 -7.69792031e-03
 -7.49642169e-03 -7.38403946e-03 -6.55828370e-03 -6.55410904e-03
 -6.51059812e-03 -5.68331080e-03 -5.11208922e-03 -4.74520680e-03
 -3.75461881e-03 -3.48966499e-03 -2.60537094e-03 -2.56212917e-03
 -1.65214995e-03 -1.34961295e-03 -1.33513322e-03 -1.33251736e-03
 -6.80588710e-04 -3.12789663e-04 -2.53992650e-04  4.08992957e-04
  5.36728883e-04  9.94862756e-04  1.00441184e-03  1.02108996e-03
  1.88738387e-03  2.17171037e-03  3.37545900e-03  3.63165140e-03
  3.75191984e-03  4.45425091e-03  4.88654245e-03  5.33465575e-03
  6.23517251e-03  6.70930184e-03  6.81980280e-03  6.91853976e-03
  7.41656590e-03  7.74550252e-03  7.80896796e-03  7.82426633e-03
  8.09618644e-03  1.04294354e-02  1.13010686e-02  1.14168497e-02
  1.17378440e-02  1.17576187e-02  1.19067049e-02  1.21305138e-02
  1.21564129e-02  1.25529068e-02  1.33160343e-02  1.54312961e-02
  1.57411881e-02  1.57456100e-02  1.65593401e-02  1.71394087e-02
  1.74278785e-02  1.80701185e-02  1.97516810e-02  2.01771948e-02
  2.04469729e-02  2.09841337e-02  2.09980551e-02  2.11676881e-02
  2.12697480e-02  2.15300675e-02  2.16587968e-02  2.38223169e-02
  2.40587406e-02  2.57871710e-02  2.60460228e-02  2.65608318e-02
  2.66372617e-02  2.79476717e-02  2.85677705e-02  2.91320290e-02
  2.97538787e-02  3.07028070e-02  3.07899062e-02  3.11645288e-02
  3.21191289e-02  3.43185328e-02  3.52310166e-02  3.80763337e-02
  4.08696979e-02  4.13999036e-02  4.26187143e-02  4.26607393e-02
  4.35084142e-02  4.35245335e-02  4.38879319e-02  4.57859337e-02
  4.58240919e-02  4.60734852e-02  4.63890582e-02  4.72771674e-02
  4.76662628e-02  4.76951748e-02  4.82336953e-02  5.03552705e-02
  5.10503352e-02  5.22792861e-02  5.28888740e-02  5.31785302e-02
  5.33383600e-02  5.41674122e-02  5.62344268e-02  5.65802455e-02
  5.68060875e-02  5.85358813e-02  5.87911047e-02  6.05181046e-02
  6.12768196e-02  6.17620833e-02  6.28309771e-02  6.33767322e-02
  6.42738417e-02  6.49012178e-02  6.57305643e-02  6.83480948e-02
  6.89650774e-02  6.93119764e-02  6.98526949e-02  7.09186420e-02
  7.24277645e-02  7.42024928e-02  7.44356588e-02  7.47966394e-02
  7.74173513e-02  7.90500715e-02  8.04873109e-02  8.20644870e-02
  8.26965272e-02  8.45076218e-02  8.54169652e-02  8.88263956e-02
  8.93564969e-02  9.07182395e-02  9.09334123e-02  9.15871486e-02
  9.62799788e-02  9.79435369e-02  1.00840606e-01  1.01764873e-01
  1.01954110e-01  1.02508128e-01  1.04486249e-01  1.04884796e-01
  1.04951322e-01  1.05219200e-01  1.08077854e-01  1.10870980e-01
  1.12330645e-01  1.12809636e-01  1.12853378e-01  1.13055214e-01
  1.15982018e-01  1.19665772e-01  1.28387332e-01  1.31203964e-01
  1.33628920e-01  1.36285633e-01  1.37651950e-01  1.37730449e-01
  1.38285160e-01  1.38840258e-01  1.39765978e-01  1.42443731e-01
  1.43412799e-01  1.45460039e-01  1.47108197e-01  1.48174897e-01
  1.48948908e-01  1.50071338e-01  1.50698811e-01  1.52895123e-01
  1.55672878e-01  1.60673216e-01  1.63081363e-01  1.66876107e-01
  1.72071412e-01  1.72077313e-01  1.74108431e-01  1.75087824e-01
  1.78043589e-01  1.78167656e-01  1.79133415e-01  1.79653436e-01
  1.81667387e-01  1.83978736e-01  1.87586293e-01  1.88691571e-01
  1.91439912e-01  1.94029659e-01  1.94339305e-01  1.95432350e-01
  1.98772922e-01  2.01711670e-01  2.03195974e-01  2.03756541e-01
  2.07362175e-01  2.08334252e-01  2.08418861e-01  2.10226595e-01
  2.10666731e-01  2.12750375e-01  2.14333043e-01  2.15557203e-01
  2.17262462e-01  2.23923326e-01  2.24590585e-01  2.24607408e-01
  2.26469532e-01  2.27894187e-01  2.28011712e-01  2.30595633e-01
  2.32170895e-01  2.36970678e-01  2.37507939e-01  2.39895418e-01
  2.48918027e-01  2.49538258e-01  2.51432091e-01  2.54961610e-01
  2.58693874e-01  2.60001153e-01  2.60124564e-01  2.61321694e-01
  2.65893728e-01  2.68678278e-01  2.69297302e-01  2.73616940e-01
  2.79371351e-01  2.82019496e-01  2.84018040e-01  2.84832776e-01
  2.89822191e-01  2.99359173e-01  3.04373890e-01  3.08562875e-01
  3.10373038e-01  3.15157980e-01  3.18643272e-01  3.21014851e-01
  3.25721920e-01  3.33848059e-01  3.34948093e-01  3.40879709e-01
  3.43160599e-01  3.49374384e-01  3.50319058e-01  3.54426652e-01
  3.57010990e-01  3.60461742e-01  3.61119151e-01  3.61285686e-01
  3.64839166e-01  3.67884189e-01  3.71056080e-01  3.71216446e-01
  3.72642189e-01  3.74698579e-01  3.78793091e-01  3.91819537e-01
  3.93620133e-01  3.95452529e-01  4.06544089e-01  4.15574491e-01
  4.20042366e-01  4.20109361e-01  4.20283437e-01  4.21629488e-01
  4.27641004e-01  4.28935528e-01  4.33155388e-01  4.37786698e-01
  4.42353398e-01  4.56678987e-01  4.57930654e-01  4.61402953e-01
  4.61800277e-01  4.62671131e-01  4.64877754e-01  4.66059446e-01
  4.69048738e-01  4.69784379e-01  4.80762124e-01  4.81660128e-01
  4.83460486e-01  4.85270709e-01  4.86193001e-01  4.94590819e-01
  4.96525794e-01  5.03144741e-01  5.04883826e-01  5.06683946e-01
  5.09905517e-01  5.16957223e-01  5.17546892e-01  5.20104110e-01
  5.20899653e-01  5.22106707e-01  5.23587465e-01  5.25287867e-01
  5.27136564e-01  5.29184461e-01  5.30121505e-01  5.32238901e-01
  5.36581039e-01  5.37161410e-01  5.38079143e-01  5.38289547e-01
  5.40699124e-01  5.41190147e-01  5.44695616e-01  5.47882736e-01
  5.58409870e-01  5.61433434e-01  5.68440557e-01  5.68675578e-01
  5.72114944e-01  5.77164412e-01  5.78985333e-01  5.80170453e-01
  5.80443919e-01  5.81307530e-01  5.92890620e-01  5.99424064e-01
  5.99937379e-01  6.06357336e-01  6.19813919e-01  6.23165846e-01
  6.23933852e-01  6.33007348e-01  6.37767613e-01  6.41513586e-01
  6.43328786e-01  6.47514522e-01  6.56912148e-01  6.76473558e-01
  6.78513944e-01  6.83976412e-01  6.84392869e-01  6.85509861e-01
  6.86091304e-01  6.87031984e-01  6.90802395e-01  6.95231318e-01
  6.97933316e-01  7.02549458e-01  7.12017715e-01  7.25714445e-01
  7.25846589e-01  7.26079345e-01  7.26333022e-01  7.28317857e-01
  7.33809054e-01  7.34823108e-01  7.35501349e-01  7.42176175e-01
  7.58414865e-01  7.58991838e-01  7.70344496e-01  7.70396590e-01
  7.82727599e-01  7.97503948e-01  7.98701644e-01  8.07898045e-01
  8.20598781e-01  8.34532082e-01  8.37445319e-01  8.38098288e-01
  8.41799498e-01  8.42474818e-01  8.47977102e-01  8.50043952e-01
  8.50701809e-01  8.60179961e-01  8.60900998e-01  8.61274362e-01
  8.64525437e-01  8.67390871e-01  8.75807941e-01  8.80006850e-01
  8.80598903e-01  8.87559295e-01  8.89292955e-01  8.91448021e-01
  8.93147767e-01  8.99785817e-01  9.06727374e-01  9.09865558e-01
  9.22948360e-01  9.26188350e-01  9.37471449e-01  9.43370163e-01
  9.46514547e-01  9.51881826e-01  9.51942384e-01  9.56941247e-01
  9.59595740e-01  9.69426215e-01  9.75639880e-01  9.94754910e-01
  1.00220180e+00  1.01195014e+00  1.04780495e+00  1.07194519e+00
  1.12920272e+00]

  warnings.warn(

2022-11-03 10:53:53,962:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.86453685e-01 -1.73096776e-01 -1.70364738e-01 -1.53137729e-01
 -1.34879738e-01 -1.27697080e-01 -1.17752932e-01 -1.13179646e-01
 -1.11451067e-01 -1.07123666e-01 -1.03784755e-01 -9.74390134e-02
 -9.63697881e-02 -9.44712758e-02 -9.24114808e-02 -8.90772939e-02
 -8.87283012e-02 -8.69307369e-02 -8.28613266e-02 -8.09697509e-02
 -7.54536986e-02 -7.43904337e-02 -6.93479776e-02 -6.69196397e-02
 -6.59680068e-02 -5.81041798e-02 -5.67717999e-02 -5.48091270e-02
 -4.95422482e-02 -4.76058535e-02 -4.69429083e-02 -4.27739583e-02
 -4.02819850e-02 -3.76246944e-02 -3.73845734e-02 -3.63582782e-02
 -3.58591937e-02 -3.58446985e-02 -3.49887870e-02 -3.39323655e-02
 -3.13994475e-02 -3.06807812e-02 -2.93162223e-02 -2.90437844e-02
 -2.65856218e-02 -2.34324113e-02 -2.12483853e-02 -1.97832901e-02
 -1.68113820e-02 -1.63946897e-02 -1.61098018e-02 -1.40685923e-02
 -1.39874304e-02 -1.36386929e-02 -1.31371543e-02 -1.24564767e-02
 -1.22633278e-02 -1.15682641e-02 -1.14018423e-02 -1.11328168e-02
 -9.38697904e-03 -9.33929067e-03 -9.06516984e-03 -8.70581716e-03
 -8.37686472e-03 -8.22954345e-03 -8.14496446e-03 -7.92899542e-03
 -7.06175808e-03 -6.10203436e-03 -5.59368310e-03 -4.80092503e-03
 -3.31174978e-03 -2.78409687e-03 -1.53743860e-03 -9.52619826e-04
 -1.53617788e-04  1.91755491e-04  2.93550576e-04  3.14745732e-04
  3.21921660e-04  4.41082404e-04  1.04170037e-03  2.03420664e-03
  2.61907233e-03  2.84279487e-03  3.02130287e-03  3.95367155e-03
  4.00436204e-03  4.04291786e-03  4.24462091e-03  4.41827346e-03
  5.14955493e-03  6.73335930e-03  8.44011363e-03  8.73423181e-03
  9.05303936e-03  9.24706366e-03  1.00003425e-02  1.00288372e-02
  1.08854594e-02  1.12582762e-02  1.18644107e-02  1.20447651e-02
  1.30470414e-02  1.30902482e-02  1.33340005e-02  1.37524605e-02
  1.41133741e-02  1.46123804e-02  1.50362095e-02  1.72973610e-02
  1.78084727e-02  1.79798920e-02  1.80777665e-02  1.90044343e-02
  1.97962187e-02  2.01439876e-02  2.02233549e-02  2.08824892e-02
  2.19561439e-02  2.20069196e-02  2.25758459e-02  2.28504911e-02
  2.32453980e-02  2.32644603e-02  2.38300525e-02  2.43505109e-02
  2.44696606e-02  2.52897702e-02  2.56992616e-02  2.57065091e-02
  2.75918152e-02  2.84989551e-02  2.93062013e-02  2.93869134e-02
  2.96056904e-02  3.33089642e-02  3.35991122e-02  3.37311849e-02
  3.58021110e-02  3.58250178e-02  3.70264538e-02  3.81138325e-02
  3.82859111e-02  3.83442603e-02  3.99648324e-02  4.01025452e-02
  4.11267243e-02  4.19925787e-02  4.27715890e-02  4.37901802e-02
  4.56172749e-02  4.59352843e-02  4.65608761e-02  4.76476513e-02
  4.89088967e-02  5.24465069e-02  5.58775924e-02  5.62446862e-02
  5.65985255e-02  5.66439033e-02  5.95830418e-02  6.03981763e-02
  6.14977814e-02  6.38643131e-02  6.52753413e-02  6.55632392e-02
  6.55933768e-02  6.61784112e-02  6.69808835e-02  6.75083399e-02
  6.77026808e-02  6.82540536e-02  6.91604689e-02  6.93055764e-02
  6.97167143e-02  6.98202327e-02  6.99652135e-02  7.16170296e-02
  7.49352798e-02  7.64284730e-02  7.80150965e-02  7.94065297e-02
  7.95396641e-02  8.04376379e-02  8.55889246e-02  8.72443244e-02
  8.72717872e-02  8.75875205e-02  8.86903927e-02  8.89147148e-02
  8.97983015e-02  9.03731063e-02  9.42283720e-02  9.59748030e-02
  1.01293914e-01  1.01782605e-01  1.02047071e-01  1.02952182e-01
  1.03680246e-01  1.04628295e-01  1.05318926e-01  1.05608255e-01
  1.06162369e-01  1.07248239e-01  1.07893780e-01  1.07980601e-01
  1.10404544e-01  1.10899337e-01  1.11493304e-01  1.11745670e-01
  1.13532983e-01  1.13682628e-01  1.16701432e-01  1.18287422e-01
  1.19431354e-01  1.19500682e-01  1.22298546e-01  1.22888863e-01
  1.23314299e-01  1.24278471e-01  1.24304928e-01  1.24681205e-01
  1.24708876e-01  1.26307234e-01  1.30344898e-01  1.33449167e-01
  1.34308398e-01  1.34706348e-01  1.38602763e-01  1.38838530e-01
  1.40621662e-01  1.41583055e-01  1.41853288e-01  1.43801451e-01
  1.44645363e-01  1.46246940e-01  1.47526518e-01  1.47689611e-01
  1.49574816e-01  1.50803566e-01  1.58673763e-01  1.60072401e-01
  1.60848781e-01  1.63328603e-01  1.64069697e-01  1.69095963e-01
  1.70979574e-01  1.72640786e-01  1.72812611e-01  1.74257874e-01
  1.78522006e-01  1.79353297e-01  1.79406345e-01  1.80521086e-01
  1.81370243e-01  1.83746740e-01  1.83921859e-01  1.84292838e-01
  1.86759666e-01  1.90160170e-01  1.91004738e-01  1.91621006e-01
  1.98322266e-01  1.98722005e-01  1.98932722e-01  2.13947669e-01
  2.15508714e-01  2.16462076e-01  2.16580987e-01  2.16822863e-01
  2.19965994e-01  2.23447368e-01  2.26552531e-01  2.31091082e-01
  2.31458470e-01  2.35710129e-01  2.35725656e-01  2.36000896e-01
  2.36792743e-01  2.41097778e-01  2.44808286e-01  2.47401282e-01
  2.48163968e-01  2.48218849e-01  2.49079734e-01  2.49319419e-01
  2.51073033e-01  2.53544748e-01  2.56197602e-01  2.57250607e-01
  2.59285063e-01  2.59951860e-01  2.65433103e-01  2.65753299e-01
  2.78273255e-01  2.82703310e-01  2.83160388e-01  2.84812540e-01
  2.86628813e-01  2.93294996e-01  2.94306904e-01  2.98800081e-01
  2.99133897e-01  2.99716204e-01  3.02498758e-01  3.05917829e-01
  3.09941560e-01  3.11807096e-01  3.15006763e-01  3.18243325e-01
  3.20044577e-01  3.20095599e-01  3.22216481e-01  3.22984248e-01
  3.24211925e-01  3.27579409e-01  3.32643688e-01  3.33808243e-01
  3.36334378e-01  3.39063853e-01  3.40915293e-01  3.49732608e-01
  3.50275934e-01  3.50951821e-01  3.51500899e-01  3.53998035e-01
  3.56575787e-01  3.59760791e-01  3.59774858e-01  3.60875100e-01
  3.65043640e-01  3.71433824e-01  3.75281483e-01  3.76624078e-01
  3.78666103e-01  3.79929394e-01  3.86434376e-01  3.92096847e-01
  4.01552856e-01  4.05242532e-01  4.06711698e-01  4.12208200e-01
  4.17631090e-01  4.28496748e-01  4.30998862e-01  4.36672896e-01
  4.37937230e-01  4.46527928e-01  4.47854161e-01  4.49018747e-01
  4.55168575e-01  4.59863126e-01  4.61048037e-01  4.64544028e-01
  4.69759583e-01  4.70164180e-01  4.72294897e-01  4.78735358e-01
  4.79523510e-01  4.80581462e-01  4.92026150e-01  4.94447619e-01
  4.99179393e-01  5.06341934e-01  5.11873424e-01  5.13845980e-01
  5.21168292e-01  5.29980361e-01  5.31557858e-01  5.32297432e-01
  5.33383548e-01  5.34012556e-01  5.40623069e-01  5.44184387e-01
  5.44437408e-01  5.44823468e-01  5.45937181e-01  5.47427595e-01
  5.47573030e-01  5.47689974e-01  5.48285007e-01  5.48574626e-01
  5.51823795e-01  5.52356899e-01  5.59807181e-01  5.61147153e-01
  5.69019437e-01  5.69690287e-01  5.70228994e-01  5.72965264e-01
  5.75336993e-01  5.78767955e-01  5.79954445e-01  5.83080351e-01
  5.85589826e-01  5.85799098e-01  5.90406477e-01  5.91715634e-01
  5.96756518e-01  6.00431502e-01  6.05502665e-01  6.11593008e-01
  6.16046667e-01  6.16380095e-01  6.17088020e-01  6.23115659e-01
  6.23994470e-01  6.26225531e-01  6.27010643e-01  6.40629649e-01
  6.46050394e-01  6.46373689e-01  6.48220062e-01  6.61384821e-01
  6.63450062e-01  6.65529490e-01  6.66393697e-01  6.69054270e-01
  6.74565554e-01  6.77335262e-01  6.80229127e-01  6.92379236e-01
  7.00016618e-01  7.00698495e-01  7.01117218e-01  7.07054198e-01
  7.07088768e-01  7.10525811e-01  7.10637093e-01  7.11534202e-01
  7.14519978e-01  7.14778841e-01  7.17383027e-01  7.18115330e-01
  7.22651064e-01  7.27274299e-01  7.35107064e-01  7.48562157e-01
  7.49339521e-01  7.53315151e-01  7.65057385e-01  7.72385299e-01
  7.83373237e-01  7.84648359e-01  7.94935226e-01  7.96584904e-01
  8.00218940e-01  8.05892766e-01  8.12892675e-01  8.16632152e-01
  8.17825198e-01  8.19005370e-01  8.22846293e-01  8.32589447e-01
  8.36668611e-01  8.37782025e-01  8.43545675e-01  8.50412488e-01
  8.53327930e-01  8.55889499e-01  8.56867135e-01  8.61585796e-01
  8.62462878e-01  8.71358693e-01  8.73015761e-01  8.80390227e-01
  8.97541165e-01  9.02768195e-01  9.02896821e-01  9.03707743e-01
  9.07400727e-01  9.15344834e-01  9.37188864e-01  9.37801480e-01
  9.45582449e-01  9.49658096e-01  9.52544034e-01  9.60369051e-01
  9.77999628e-01  9.95931089e-01  9.99918759e-01  1.00391054e+00
  1.00625145e+00  1.00774574e+00  1.04214227e+00  1.04966700e+00
  1.07402062e+00  1.09999812e+00  1.17238069e+00  1.19503880e+00
  1.20490408e+00]

  warnings.warn(

2022-11-03 10:53:54,016:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.88988000e-01 -1.76576972e-01 -1.71571448e-01 -1.66697547e-01
 -1.33248359e-01 -1.00311220e-01 -9.74804536e-02 -9.48901176e-02
 -9.12350491e-02 -8.87042284e-02 -8.78828987e-02 -8.77174214e-02
 -7.27807283e-02 -7.23476484e-02 -6.85189888e-02 -6.61663562e-02
 -6.49424717e-02 -6.37840107e-02 -6.30119964e-02 -6.08711764e-02
 -5.69863580e-02 -5.60489334e-02 -5.54712154e-02 -5.54161072e-02
 -5.53485528e-02 -5.50363548e-02 -5.37240021e-02 -5.30433655e-02
 -5.21487407e-02 -4.73575965e-02 -4.16230150e-02 -3.84438746e-02
 -3.83744277e-02 -3.63756232e-02 -3.46212089e-02 -3.06432918e-02
 -2.99950726e-02 -2.66422369e-02 -2.66095400e-02 -2.29848009e-02
 -2.25852169e-02 -2.25018188e-02 -2.24423856e-02 -2.22119298e-02
 -2.17900183e-02 -2.11085416e-02 -2.02375799e-02 -1.98044218e-02
 -1.85180884e-02 -1.81676261e-02 -1.60597079e-02 -1.54845370e-02
 -1.47886695e-02 -1.41007965e-02 -1.37957148e-02 -1.36116054e-02
 -1.16248569e-02 -1.15826074e-02 -1.13252811e-02 -1.10612940e-02
 -9.91250761e-03 -9.72751062e-03 -8.21047276e-03 -8.11202172e-03
 -8.03146791e-03 -8.01900309e-03 -6.71380898e-03 -6.33452088e-03
 -5.36071649e-03 -5.34667494e-03 -5.05407201e-03 -4.44296980e-03
 -4.15786076e-03 -3.44156916e-03 -2.78524077e-03 -2.11609527e-03
 -1.43873820e-03 -1.37747999e-03 -1.11995789e-03 -4.77418303e-04
  4.36981791e-04  5.12878178e-04  8.21028138e-04  8.33104714e-04
  9.76170995e-04  1.07085670e-03  1.08434854e-03  1.51699269e-03
  2.70336028e-03  2.77858041e-03  3.03944433e-03  3.08622979e-03
  3.18189408e-03  3.28977453e-03  3.55147221e-03  3.59455752e-03
  3.87476757e-03  3.98058770e-03  4.11919039e-03  4.16616537e-03
  4.60662879e-03  5.01287589e-03  5.09506091e-03  6.05752179e-03
  6.07958715e-03  6.25154655e-03  6.69522723e-03  7.02030398e-03
  7.28497095e-03  8.02448485e-03  8.57318752e-03  9.32154991e-03
  9.41257831e-03  9.89381596e-03  9.99648124e-03  1.00518772e-02
  1.03772953e-02  1.04760230e-02  1.05178552e-02  1.11280913e-02
  1.14988368e-02  1.16849914e-02  1.48736555e-02  1.58962198e-02
  1.77481417e-02  1.80982128e-02  1.87523551e-02  1.93422772e-02
  1.93647612e-02  1.93809681e-02  1.98205244e-02  2.04116143e-02
  2.08658203e-02  2.10326519e-02  2.14316268e-02  2.41217464e-02
  2.53152661e-02  2.55239550e-02  2.67761331e-02  2.68519055e-02
  2.73139868e-02  2.76470240e-02  2.86543928e-02  3.12674604e-02
  3.14416699e-02  3.20742838e-02  3.22664194e-02  3.22677344e-02
  3.24297957e-02  3.35247628e-02  3.48715484e-02  3.52618098e-02
  3.57540920e-02  3.65446359e-02  3.70554700e-02  3.71068530e-02
  3.75638790e-02  3.97426561e-02  3.97645906e-02  4.06735912e-02
  4.10337038e-02  4.13098708e-02  4.44141403e-02  4.53335755e-02
  4.59356904e-02  4.65507396e-02  4.75028530e-02  4.90898862e-02
  4.91206758e-02  4.93863635e-02  4.95435782e-02  5.10387011e-02
  5.20932749e-02  5.25311045e-02  5.26433885e-02  5.44174165e-02
  5.47296554e-02  5.51134534e-02  5.58888502e-02  5.69142513e-02
  5.70354462e-02  5.87835461e-02  6.08757734e-02  6.15921952e-02
  6.39340952e-02  6.51827008e-02  6.75964281e-02  6.84829056e-02
  6.92695379e-02  7.13249967e-02  7.19300508e-02  7.26898164e-02
  7.53858238e-02  7.56519958e-02  7.61069208e-02  7.68628865e-02
  7.76281357e-02  7.80649260e-02  7.86592588e-02  7.94903412e-02
  7.97868595e-02  8.00074562e-02  8.07819143e-02  8.08644071e-02
  8.11987370e-02  8.12203810e-02  8.25183615e-02  8.27471241e-02
  8.29710439e-02  8.48010704e-02  8.54125395e-02  8.64737481e-02
  8.67063105e-02  8.86067450e-02  8.99238139e-02  9.04096961e-02
  9.37522203e-02  9.50096026e-02  9.56893191e-02  9.70747620e-02
  9.74152386e-02  9.79439169e-02  9.82738212e-02  9.89339128e-02
  9.93441492e-02  1.00768030e-01  1.06400184e-01  1.06881104e-01
  1.07083842e-01  1.09652348e-01  1.10151172e-01  1.11134060e-01
  1.13979146e-01  1.14622734e-01  1.15421042e-01  1.15499884e-01
  1.17582664e-01  1.19687356e-01  1.20385848e-01  1.20950229e-01
  1.22679055e-01  1.22894257e-01  1.22910790e-01  1.23758204e-01
  1.24100715e-01  1.24804184e-01  1.27230868e-01  1.27811521e-01
  1.31455258e-01  1.36664152e-01  1.38871059e-01  1.39528900e-01
  1.45144358e-01  1.49277821e-01  1.51582092e-01  1.59464508e-01
  1.63488075e-01  1.63507313e-01  1.66043177e-01  1.66634098e-01
  1.69121578e-01  1.77861840e-01  1.78514495e-01  1.78747967e-01
  1.78838059e-01  1.79252759e-01  1.81535542e-01  1.84221253e-01
  1.84766486e-01  1.87429503e-01  1.89068154e-01  1.89899564e-01
  1.91793948e-01  1.93487987e-01  1.94888741e-01  1.97166920e-01
  2.00299174e-01  2.02683464e-01  2.03623801e-01  2.06746161e-01
  2.10553110e-01  2.11372405e-01  2.13829160e-01  2.18258828e-01
  2.22528741e-01  2.23170862e-01  2.29995117e-01  2.34139740e-01
  2.37926915e-01  2.38230109e-01  2.39890531e-01  2.47314960e-01
  2.47650892e-01  2.47736320e-01  2.47965381e-01  2.48756602e-01
  2.48983815e-01  2.50943333e-01  2.51949698e-01  2.53100246e-01
  2.53437400e-01  2.53538638e-01  2.55282164e-01  2.57212311e-01
  2.59153575e-01  2.60608077e-01  2.63005853e-01  2.63328165e-01
  2.66103506e-01  2.68036634e-01  2.72541314e-01  2.74569064e-01
  2.74642080e-01  2.81251639e-01  2.82083869e-01  2.84624636e-01
  2.85110116e-01  2.86885649e-01  2.88564712e-01  2.89354742e-01
  2.90607423e-01  2.91190386e-01  2.91478217e-01  2.97034323e-01
  2.98803002e-01  2.99337655e-01  2.99771041e-01  3.02048385e-01
  3.03432912e-01  3.16955328e-01  3.18341792e-01  3.22736174e-01
  3.26494038e-01  3.30179304e-01  3.38613719e-01  3.42234313e-01
  3.46001685e-01  3.46417457e-01  3.48507494e-01  3.59373569e-01
  3.61131757e-01  3.65664363e-01  3.67879540e-01  3.69945794e-01
  3.73654038e-01  3.80191177e-01  3.81978840e-01  3.84584814e-01
  3.86430442e-01  3.87800097e-01  3.92290801e-01  3.98442626e-01
  3.98770303e-01  3.98871779e-01  4.01166439e-01  4.02691394e-01
  4.08387601e-01  4.09736753e-01  4.14884746e-01  4.16047662e-01
  4.17071313e-01  4.20369953e-01  4.24400508e-01  4.30193186e-01
  4.34159964e-01  4.41580951e-01  4.42710280e-01  4.54819858e-01
  4.58154440e-01  4.60000753e-01  4.61160362e-01  4.67168212e-01
  4.72265393e-01  4.73904043e-01  4.74355668e-01  4.74796623e-01
  4.77929235e-01  4.78448331e-01  4.89932328e-01  4.90556628e-01
  4.98081595e-01  5.11834621e-01  5.19866705e-01  5.20956933e-01
  5.26012540e-01  5.26103616e-01  5.29764414e-01  5.38170695e-01
  5.40493727e-01  5.41164875e-01  5.41309178e-01  5.42257190e-01
  5.43882489e-01  5.46441734e-01  5.53346515e-01  5.57508826e-01
  5.65432847e-01  5.66767573e-01  5.68976343e-01  5.72425961e-01
  5.72481990e-01  5.73576510e-01  5.73668182e-01  5.75030327e-01
  5.80136180e-01  5.81779420e-01  5.85253119e-01  5.86083174e-01
  5.92763782e-01  6.02474749e-01  6.07875288e-01  6.10379159e-01
  6.13651752e-01  6.14273012e-01  6.22463703e-01  6.25066996e-01
  6.28315270e-01  6.29028141e-01  6.30064785e-01  6.44558668e-01
  6.51838660e-01  6.54634356e-01  6.56761646e-01  6.69203222e-01
  6.78956509e-01  6.80767477e-01  6.81956470e-01  6.99888766e-01
  7.00951397e-01  7.01074123e-01  7.01669872e-01  7.08895326e-01
  7.11792886e-01  7.29475558e-01  7.42548227e-01  7.47837901e-01
  7.53527880e-01  7.54387140e-01  7.54457057e-01  7.59973764e-01
  7.64862061e-01  7.69461751e-01  7.69538164e-01  7.75240839e-01
  7.76298881e-01  7.77969778e-01  7.81998098e-01  7.84575045e-01
  7.85646319e-01  7.91790426e-01  7.92509079e-01  7.98959434e-01
  8.02150428e-01  8.09132576e-01  8.21642578e-01  8.25154126e-01
  8.29188287e-01  8.29841077e-01  8.30112040e-01  8.35459769e-01
  8.49293530e-01  8.51338148e-01  8.52296293e-01  8.63716662e-01
  8.77619445e-01  8.79638314e-01  8.84648025e-01  8.95167768e-01
  9.01441991e-01  9.01885092e-01  9.20217872e-01  9.27325785e-01
  9.29709911e-01  9.31311607e-01  9.37634706e-01  9.39463317e-01
  9.46382284e-01  9.47453320e-01  9.50279355e-01  9.68617737e-01
  9.69851851e-01  9.71685588e-01  9.82397914e-01  9.88648176e-01
  9.93091643e-01  1.00439823e+00  1.02544188e+00  1.03438663e+00
  1.04791522e+00  1.06993127e+00  1.08090532e+00  1.09394956e+00
  1.30071473e+00]

  warnings.warn(

2022-11-03 10:53:54,157:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.41895109e-01 -1.84924543e-01 -1.69234917e-01 -1.52095780e-01
 -1.39159366e-01 -1.17166489e-01 -1.12446427e-01 -9.68548730e-02
 -9.56162289e-02 -8.91174898e-02 -8.87474343e-02 -8.72981623e-02
 -8.68616030e-02 -7.52801746e-02 -7.43265748e-02 -7.22901523e-02
 -7.05612451e-02 -6.62080869e-02 -6.37652054e-02 -6.18715659e-02
 -5.86094074e-02 -5.83499409e-02 -5.75146042e-02 -5.51406778e-02
 -4.15935330e-02 -4.10973653e-02 -3.97843644e-02 -3.81956026e-02
 -3.64732705e-02 -3.31856534e-02 -3.30816694e-02 -3.27145867e-02
 -3.02020386e-02 -2.95971371e-02 -2.83999033e-02 -2.81592719e-02
 -2.78230496e-02 -2.70580463e-02 -2.50487644e-02 -2.40450837e-02
 -2.37502772e-02 -2.32033003e-02 -2.30398066e-02 -1.93495490e-02
 -1.90155879e-02 -1.82725657e-02 -1.79988891e-02 -1.79326124e-02
 -1.63077433e-02 -1.49121685e-02 -1.48596307e-02 -1.46075655e-02
 -1.35354903e-02 -1.29093193e-02 -1.18355434e-02 -1.18200835e-02
 -1.15840016e-02 -1.12919779e-02 -1.07907001e-02 -1.02625042e-02
 -9.69701819e-03 -9.32975579e-03 -9.14930739e-03 -7.57125439e-03
 -6.04506861e-03 -5.58902975e-03 -5.51127596e-03 -5.35468617e-03
 -4.80791181e-03 -4.71944967e-03 -3.67058371e-03 -3.63022671e-03
 -3.06584616e-03 -2.32976954e-03 -2.29445775e-03 -2.01494782e-03
 -1.64155569e-03 -1.15075859e-03 -6.31478324e-04 -3.47551773e-04
  6.97321258e-04  1.28071452e-03  1.59866980e-03  1.81937579e-03
  1.87094673e-03  3.14652594e-03  3.71143338e-03  4.31198301e-03
  4.45832917e-03  5.11500333e-03  5.33441640e-03  5.52044529e-03
  6.76377723e-03  6.80156285e-03  6.97875535e-03  7.73029123e-03
  8.19672924e-03  8.79812893e-03  9.42897610e-03  1.04061225e-02
  1.04443012e-02  1.05312532e-02  1.25334458e-02  1.25546185e-02
  1.30938347e-02  1.37450211e-02  1.38094416e-02  1.43715879e-02
  1.52873211e-02  1.52963940e-02  1.66330189e-02  1.77642591e-02
  1.79964341e-02  2.10691467e-02  2.11322121e-02  2.13275682e-02
  2.15671137e-02  2.30114702e-02  2.48812195e-02  2.66633816e-02
  2.75769066e-02  2.78134122e-02  2.95330919e-02  2.95517333e-02
  3.01392544e-02  3.02259568e-02  3.08539160e-02  3.11707817e-02
  3.12981978e-02  3.24212611e-02  3.32395136e-02  3.36857848e-02
  3.69647332e-02  3.72687802e-02  3.82128134e-02  3.95741835e-02
  4.00258303e-02  4.02682126e-02  4.02763151e-02  4.04184051e-02
  4.09960859e-02  4.10050564e-02  4.16019298e-02  4.22075875e-02
  4.28715311e-02  4.31169569e-02  4.36053127e-02  4.44516987e-02
  4.50174920e-02  4.51642983e-02  4.54372019e-02  4.56403568e-02
  4.62008901e-02  4.69371155e-02  4.73684967e-02  4.77989800e-02
  4.79626358e-02  4.81504165e-02  4.81970869e-02  4.94711585e-02
  4.99657467e-02  5.00796251e-02  5.00889607e-02  5.11899404e-02
  5.20553142e-02  5.24525605e-02  5.29489703e-02  5.66002131e-02
  5.88133447e-02  6.04917854e-02  6.10810779e-02  6.14607148e-02
  6.21588007e-02  6.34770170e-02  6.36467561e-02  6.54774755e-02
  6.60039634e-02  6.73479140e-02  6.75433427e-02  6.80650100e-02
  6.87100217e-02  6.98543191e-02  7.02354610e-02  7.42738768e-02
  7.44630098e-02  7.55006224e-02  7.63542056e-02  7.77730495e-02
  7.78983310e-02  7.80146047e-02  7.96169639e-02  8.18162784e-02
  8.24158937e-02  8.32758546e-02  8.50055590e-02  8.51753354e-02
  8.59676152e-02  8.68623480e-02  8.69219750e-02  8.71748477e-02
  8.86263251e-02  8.92423019e-02  9.10445750e-02  9.14042592e-02
  9.14979652e-02  9.26011205e-02  9.41508114e-02  9.63175818e-02
  9.69574228e-02  9.79977250e-02  9.80244651e-02  9.92553681e-02
  9.99136791e-02  1.00107595e-01  1.00115225e-01  1.01928249e-01
  1.02095246e-01  1.03210464e-01  1.05491698e-01  1.05531700e-01
  1.05671652e-01  1.05970018e-01  1.07288465e-01  1.07889473e-01
  1.08647630e-01  1.09078504e-01  1.13893002e-01  1.16061471e-01
  1.17193423e-01  1.18653327e-01  1.19435638e-01  1.22814603e-01
  1.28521517e-01  1.28738865e-01  1.29091486e-01  1.30841196e-01
  1.31229430e-01  1.31416515e-01  1.31426200e-01  1.33706436e-01
  1.35238573e-01  1.37482807e-01  1.37780190e-01  1.38748616e-01
  1.42494813e-01  1.43991485e-01  1.46743491e-01  1.51072577e-01
  1.51325092e-01  1.52845994e-01  1.53690025e-01  1.55822888e-01
  1.56995073e-01  1.62324756e-01  1.63200021e-01  1.64817438e-01
  1.69821665e-01  1.71266168e-01  1.75477669e-01  1.81999639e-01
  1.84343025e-01  1.84725910e-01  1.85182884e-01  1.86856523e-01
  1.91094071e-01  1.91633850e-01  1.93259716e-01  1.98387012e-01
  1.98440731e-01  1.98998138e-01  1.99032798e-01  2.02778578e-01
  2.05812290e-01  2.07721844e-01  2.09029555e-01  2.12809861e-01
  2.15387270e-01  2.18506753e-01  2.19289973e-01  2.21733510e-01
  2.23064438e-01  2.23884031e-01  2.28067845e-01  2.28278488e-01
  2.32259810e-01  2.32473359e-01  2.35276505e-01  2.43555710e-01
  2.43825287e-01  2.45321989e-01  2.46243253e-01  2.46403739e-01
  2.46607289e-01  2.49063030e-01  2.49643937e-01  2.50218779e-01
  2.58230269e-01  2.63316721e-01  2.67446876e-01  2.67669380e-01
  2.69252717e-01  2.69437522e-01  2.75771379e-01  2.77218252e-01
  2.78891444e-01  2.81028777e-01  2.81992674e-01  2.82723397e-01
  2.85793006e-01  2.85982430e-01  2.87276715e-01  2.89792061e-01
  2.93984771e-01  2.94132769e-01  2.94885993e-01  2.95611680e-01
  2.96635002e-01  3.01366389e-01  3.04003865e-01  3.04199159e-01
  3.05053562e-01  3.05570662e-01  3.06540698e-01  3.12920660e-01
  3.15906733e-01  3.16133320e-01  3.18241537e-01  3.18361372e-01
  3.20286870e-01  3.22773814e-01  3.22838992e-01  3.23517412e-01
  3.26129615e-01  3.27038586e-01  3.28461617e-01  3.31065387e-01
  3.35680038e-01  3.39784831e-01  3.42427105e-01  3.43023181e-01
  3.47128689e-01  3.47336411e-01  3.49067599e-01  3.54072750e-01
  3.55037481e-01  3.55468929e-01  3.56541514e-01  3.64147395e-01
  3.66279185e-01  3.66721302e-01  3.68782490e-01  3.72552544e-01
  3.74357224e-01  3.77082169e-01  3.77752781e-01  3.82669330e-01
  3.83526921e-01  3.94822717e-01  3.98075372e-01  3.99902850e-01
  4.00670260e-01  4.00928944e-01  4.12159622e-01  4.21407521e-01
  4.23598796e-01  4.29632157e-01  4.33215529e-01  4.33418036e-01
  4.35926408e-01  4.37304914e-01  4.37891543e-01  4.39745247e-01
  4.42226529e-01  4.55056846e-01  4.55680400e-01  4.57219869e-01
  4.57437038e-01  4.57630843e-01  4.57975149e-01  4.66346264e-01
  4.70894605e-01  4.74826068e-01  4.76030380e-01  4.76115346e-01
  4.78041500e-01  4.81604755e-01  4.84125614e-01  4.89694864e-01
  4.93672490e-01  5.00812948e-01  5.01006424e-01  5.05005419e-01
  5.10730028e-01  5.17618954e-01  5.19548297e-01  5.20230651e-01
  5.21788359e-01  5.28762519e-01  5.31117558e-01  5.33417284e-01
  5.41244984e-01  5.44408798e-01  5.47284424e-01  5.53286910e-01
  5.54530859e-01  5.66101372e-01  5.68319201e-01  5.70966959e-01
  5.73012769e-01  5.76227069e-01  5.80382705e-01  5.82343817e-01
  5.85775018e-01  5.92020690e-01  5.94546020e-01  6.09632492e-01
  6.14179134e-01  6.24204397e-01  6.28046334e-01  6.32763565e-01
  6.33248508e-01  6.42009020e-01  6.43213332e-01  6.47288859e-01
  6.64706588e-01  6.64904773e-01  6.78667843e-01  6.79268718e-01
  6.85594082e-01  6.89810216e-01  6.93895698e-01  6.98149145e-01
  7.03693032e-01  7.07786143e-01  7.09365010e-01  7.09396183e-01
  7.10538447e-01  7.12831318e-01  7.15638399e-01  7.25106120e-01
  7.29028881e-01  7.38282204e-01  7.49211013e-01  7.51648247e-01
  7.54588425e-01  7.60355771e-01  7.63640523e-01  7.69767046e-01
  7.82856166e-01  7.83913314e-01  7.87509620e-01  7.89612174e-01
  7.91516721e-01  7.91543186e-01  7.98738241e-01  8.00110459e-01
  8.02125752e-01  8.18375051e-01  8.27289879e-01  8.29078257e-01
  8.30120862e-01  8.31301987e-01  8.43601286e-01  8.48055303e-01
  8.51467907e-01  8.59177291e-01  8.62041950e-01  8.83049905e-01
  8.86636436e-01  8.89469504e-01  8.94880772e-01  8.96323144e-01
  8.98369968e-01  9.04843390e-01  9.17979181e-01  9.20053422e-01
  9.24774528e-01  9.29971993e-01  9.31416333e-01  9.43963468e-01
  9.45582807e-01  9.56767380e-01  9.67124820e-01  9.68535483e-01
  9.74584401e-01  9.91963446e-01  9.93678212e-01  1.00310993e+00
  1.01374936e+00  1.02902067e+00  1.08749270e+00  1.11917388e+00
  1.22920334e+00]

  warnings.warn(

2022-11-03 10:53:54,189:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.84596950e-01 -1.50113225e-01 -1.48437068e-01 -1.26500160e-01
 -1.19564883e-01 -1.17652759e-01 -1.07794628e-01 -1.01190165e-01
 -9.85587463e-02 -9.78184640e-02 -9.33057144e-02 -9.26660597e-02
 -8.28177184e-02 -8.27626511e-02 -8.03758502e-02 -6.75747097e-02
 -6.23369701e-02 -6.11501671e-02 -6.09451123e-02 -5.99404201e-02
 -5.92531338e-02 -5.85211888e-02 -5.81970401e-02 -5.39000854e-02
 -5.32422438e-02 -5.07858656e-02 -4.89565209e-02 -4.39647920e-02
 -4.28638756e-02 -4.26469631e-02 -3.54373083e-02 -3.51514183e-02
 -3.41598094e-02 -2.96564903e-02 -2.88519822e-02 -2.86912676e-02
 -2.78933048e-02 -2.73485519e-02 -2.63339281e-02 -2.56755855e-02
 -2.33162493e-02 -2.31925454e-02 -2.30667684e-02 -2.29573939e-02
 -2.28762701e-02 -2.20874324e-02 -2.16704402e-02 -2.11460851e-02
 -2.08916403e-02 -2.06026621e-02 -1.80296861e-02 -1.77284982e-02
 -1.75085366e-02 -1.73368808e-02 -1.69532988e-02 -1.69188175e-02
 -1.63255241e-02 -1.53427320e-02 -1.40942764e-02 -1.34892138e-02
 -1.34561099e-02 -1.33144287e-02 -1.32154105e-02 -1.30029982e-02
 -1.28785931e-02 -1.28295226e-02 -1.14363786e-02 -1.03460364e-02
 -9.33039375e-03 -9.27753001e-03 -9.15019028e-03 -9.07322112e-03
 -8.34537297e-03 -8.19047354e-03 -7.78410863e-03 -7.64364190e-03
 -6.29885122e-03 -5.06414939e-03 -4.99200402e-03 -4.38276865e-03
 -3.85805732e-03 -3.70152411e-03 -3.57296783e-03 -3.56076984e-03
 -3.31441429e-03 -1.92342687e-03 -1.70620496e-03 -1.59868132e-03
 -5.73183643e-04 -1.35055991e-04 -1.13679998e-04 -1.03463448e-04
  7.90727499e-04  1.32049317e-03  1.37179310e-03  2.26426264e-03
  2.43103039e-03  2.52640294e-03  3.13735683e-03  3.94193456e-03
  4.06985031e-03  4.17888816e-03  4.58007958e-03  4.90860641e-03
  5.26698027e-03  5.33880387e-03  5.84071968e-03  5.88712562e-03
  5.95888961e-03  6.01223856e-03  7.09325727e-03  7.44968653e-03
  7.49732554e-03  7.83941988e-03  7.95066077e-03  8.52413476e-03
  8.66779126e-03  8.74630455e-03  8.94342177e-03  9.06297658e-03
  9.67188552e-03  9.85327363e-03  1.15690641e-02  1.19858216e-02
  1.35359932e-02  1.55571327e-02  1.55923609e-02  1.67386476e-02
  1.70190316e-02  1.70346219e-02  1.72964148e-02  1.96747016e-02
  1.98082328e-02  2.19446383e-02  2.26774197e-02  2.36040410e-02
  2.51496807e-02  2.58803163e-02  2.60395203e-02  2.69792564e-02
  2.88564842e-02  3.02185230e-02  3.06810848e-02  3.09037957e-02
  3.09574082e-02  3.14363576e-02  3.22183259e-02  3.30475755e-02
  3.31391431e-02  3.32752056e-02  3.45610902e-02  3.52991782e-02
  3.60538401e-02  3.74948829e-02  3.83877084e-02  3.84869576e-02
  3.88782062e-02  3.93956564e-02  4.44124900e-02  4.50998731e-02
  4.76120450e-02  4.80118096e-02  4.86349910e-02  4.88596000e-02
  4.96760421e-02  5.00549264e-02  5.01353480e-02  5.08801788e-02
  5.12157455e-02  5.16140237e-02  5.20817526e-02  5.51715530e-02
  5.58345020e-02  5.69274165e-02  5.70358001e-02  5.72575778e-02
  5.72641045e-02  5.78872524e-02  6.11576699e-02  6.12845905e-02
  6.44091368e-02  6.50883317e-02  6.57971501e-02  6.66741803e-02
  6.77394941e-02  6.81257397e-02  6.88865781e-02  6.94827214e-02
  6.97620139e-02  7.25895762e-02  7.26856515e-02  7.45604709e-02
  7.68432468e-02  7.72506371e-02  7.85692185e-02  7.88152814e-02
  8.08350295e-02  8.18019137e-02  8.25792477e-02  8.36050138e-02
  8.40308145e-02  8.60845596e-02  8.66730139e-02  9.04119462e-02
  9.15574729e-02  9.19825062e-02  9.51163396e-02  9.58179533e-02
  9.60052907e-02  9.82169211e-02  9.83359292e-02  1.00053430e-01
  1.01028964e-01  1.02832742e-01  1.07576124e-01  1.08752213e-01
  1.11101866e-01  1.13567546e-01  1.14183255e-01  1.14900000e-01
  1.16047740e-01  1.19598374e-01  1.20297328e-01  1.21209837e-01
  1.21560186e-01  1.23069949e-01  1.24759927e-01  1.26423776e-01
  1.28153339e-01  1.28709882e-01  1.29833058e-01  1.30497470e-01
  1.30845368e-01  1.31439641e-01  1.32387966e-01  1.32636532e-01
  1.32917672e-01  1.34712979e-01  1.36624575e-01  1.40899166e-01
  1.41095772e-01  1.42007902e-01  1.42401010e-01  1.45377159e-01
  1.49315104e-01  1.49948969e-01  1.50708273e-01  1.52162939e-01
  1.52578071e-01  1.53529420e-01  1.54135957e-01  1.55149564e-01
  1.55345857e-01  1.57006934e-01  1.57305837e-01  1.57678142e-01
  1.59034550e-01  1.61410853e-01  1.63389802e-01  1.64543167e-01
  1.65791705e-01  1.68426409e-01  1.72677264e-01  1.73199207e-01
  1.73284754e-01  1.80645987e-01  1.84121042e-01  1.86425969e-01
  1.87480032e-01  1.87570408e-01  1.88331902e-01  1.91237435e-01
  1.92452475e-01  1.92913309e-01  1.98370636e-01  1.99542090e-01
  2.04262689e-01  2.04846039e-01  2.06909180e-01  2.09957749e-01
  2.13585481e-01  2.14110777e-01  2.14911342e-01  2.16352805e-01
  2.17054114e-01  2.17851028e-01  2.19807521e-01  2.20017940e-01
  2.20458359e-01  2.21030757e-01  2.21518934e-01  2.22062737e-01
  2.23057106e-01  2.25538746e-01  2.25908771e-01  2.28863075e-01
  2.36389056e-01  2.41168573e-01  2.41319209e-01  2.46916801e-01
  2.48484194e-01  2.50746578e-01  2.52931714e-01  2.60135204e-01
  2.60507554e-01  2.60600448e-01  2.61890084e-01  2.62287766e-01
  2.62753665e-01  2.63166428e-01  2.64155537e-01  2.66948193e-01
  2.67891079e-01  2.69546658e-01  2.70520091e-01  2.74612159e-01
  2.75861114e-01  2.80162305e-01  2.80492485e-01  2.81017572e-01
  2.81830668e-01  2.85794139e-01  2.89749026e-01  2.92026848e-01
  2.93659687e-01  2.96574414e-01  3.01695675e-01  3.01974922e-01
  3.05244863e-01  3.05826277e-01  3.08218449e-01  3.09890538e-01
  3.14587414e-01  3.18334460e-01  3.18720728e-01  3.21595281e-01
  3.26887876e-01  3.27410519e-01  3.31204236e-01  3.31243396e-01
  3.38363975e-01  3.39660406e-01  3.40078115e-01  3.45520139e-01
  3.52585942e-01  3.55089754e-01  3.57846141e-01  3.59194487e-01
  3.65697861e-01  3.66565347e-01  3.70529950e-01  3.70956182e-01
  3.77587289e-01  3.83833647e-01  3.85141492e-01  3.86142969e-01
  3.87355238e-01  3.87499750e-01  3.87807399e-01  3.87930810e-01
  3.89786601e-01  3.90468061e-01  3.90597880e-01  4.01729167e-01
  4.02294666e-01  4.03880119e-01  4.25722182e-01  4.27691638e-01
  4.28451717e-01  4.31812853e-01  4.32710618e-01  4.33005631e-01
  4.33135450e-01  4.36322987e-01  4.37891036e-01  4.42698628e-01
  4.45937097e-01  4.49533671e-01  4.55638468e-01  4.58685458e-01
  4.62020755e-01  4.67208862e-01  4.72724140e-01  4.78861570e-01
  4.94805872e-01  4.95829076e-01  5.00999093e-01  5.11424839e-01
  5.12887776e-01  5.17646134e-01  5.27939975e-01  5.29011369e-01
  5.29348671e-01  5.30845106e-01  5.31414390e-01  5.37631392e-01
  5.40469706e-01  5.44225693e-01  5.51162541e-01  5.53619385e-01
  5.54508448e-01  5.58164835e-01  5.64480841e-01  5.67712426e-01
  5.71404278e-01  5.76947093e-01  5.85389078e-01  5.89714706e-01
  5.91506124e-01  5.98704636e-01  6.00006819e-01  6.00538731e-01
  6.05601370e-01  6.06310844e-01  6.11211658e-01  6.15898967e-01
  6.18774295e-01  6.23117626e-01  6.32032812e-01  6.36529267e-01
  6.38412774e-01  6.42508626e-01  6.47242188e-01  6.48869872e-01
  6.57061815e-01  6.61962092e-01  6.68561161e-01  6.69342458e-01
  6.73317492e-01  6.83143198e-01  6.84364200e-01  6.88818336e-01
  6.90525889e-01  6.94026053e-01  6.99620366e-01  7.01976418e-01
  7.04063296e-01  7.04696298e-01  7.09554970e-01  7.10817933e-01
  7.14818656e-01  7.36659467e-01  7.51582861e-01  7.56343842e-01
  7.58637726e-01  7.73740172e-01  7.77366161e-01  7.78537989e-01
  7.79905796e-01  7.82393098e-01  7.82910526e-01  7.84385145e-01
  7.85151064e-01  7.86863923e-01  7.91772723e-01  7.96798944e-01
  7.97674656e-01  7.99577951e-01  8.10629785e-01  8.22921216e-01
  8.30144346e-01  8.36133718e-01  8.37530732e-01  8.39829743e-01
  8.40277910e-01  8.43070805e-01  8.43509078e-01  8.45499575e-01
  8.47165585e-01  8.47257912e-01  8.51632297e-01  8.60467017e-01
  9.03596461e-01  9.07167614e-01  9.14014399e-01  9.15396929e-01
  9.18949366e-01  9.29701567e-01  9.37255263e-01  9.51666296e-01
  9.52892721e-01  9.53283668e-01  9.62491989e-01  9.67087507e-01
  9.96539116e-01  1.01586246e+00  1.02067721e+00  1.02359569e+00
  1.03528810e+00  1.13264537e+00  1.14117360e+00  1.22227943e+00]

  warnings.warn(

2022-11-03 10:53:54,255:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.98763841e-01 -2.90714324e-01 -2.69759476e-01 -2.24052891e-01
 -1.65509596e-01 -1.36681929e-01 -1.26608446e-01 -1.25213370e-01
 -1.22676261e-01 -1.15057625e-01 -1.09969132e-01 -1.08437143e-01
 -1.04442522e-01 -8.95800740e-02 -8.90918896e-02 -8.80338028e-02
 -8.12897459e-02 -8.11064914e-02 -7.98000470e-02 -6.72795177e-02
 -6.54995590e-02 -5.89233711e-02 -5.86758330e-02 -5.67793958e-02
 -5.55005036e-02 -5.30231446e-02 -4.88684885e-02 -4.80935760e-02
 -4.72949557e-02 -4.71937023e-02 -4.57636304e-02 -4.55584601e-02
 -4.49060611e-02 -4.46580052e-02 -4.39065322e-02 -4.18193266e-02
 -4.13691327e-02 -3.84303182e-02 -3.64188068e-02 -3.47786024e-02
 -3.29220071e-02 -3.24750617e-02 -3.23846489e-02 -3.16675939e-02
 -3.06465607e-02 -3.06334049e-02 -3.01144589e-02 -2.97432095e-02
 -2.83986069e-02 -2.77556051e-02 -2.64535882e-02 -2.48198155e-02
 -2.45577134e-02 -2.40914058e-02 -2.37036515e-02 -2.28682216e-02
 -2.27381643e-02 -2.27150228e-02 -2.17441004e-02 -2.04747748e-02
 -2.03960743e-02 -2.03445051e-02 -1.96970571e-02 -1.95303112e-02
 -1.82596892e-02 -1.81909464e-02 -1.75356995e-02 -1.66173093e-02
 -1.63799934e-02 -1.61768086e-02 -1.42427646e-02 -1.40009560e-02
 -1.35105308e-02 -1.27183236e-02 -1.23116625e-02 -1.22189680e-02
 -1.20145800e-02 -1.13218129e-02 -1.10914661e-02 -1.07784066e-02
 -9.61138681e-03 -8.65606591e-03 -8.39586183e-03 -7.89369550e-03
 -7.85444584e-03 -7.30034104e-03 -6.45147637e-03 -5.64067485e-03
 -5.60865225e-03 -5.11034252e-03 -4.99317981e-03 -4.89312736e-03
 -4.86995513e-03 -4.51084087e-03 -3.92677868e-03 -3.86579381e-03
 -3.29546258e-03 -3.12910415e-03 -2.30100891e-03 -2.14241119e-03
 -1.98694365e-03 -5.32775419e-04 -2.83320085e-04  1.05494761e-03
  1.15847064e-03  1.44220167e-03  2.08606618e-03  2.74058757e-03
  3.32875596e-03  4.40588640e-03  5.95173566e-03  6.25765882e-03
  7.50486972e-03  9.27697867e-03  9.28985141e-03  9.52418242e-03
  9.63051897e-03  1.08130015e-02  1.24721322e-02  1.25267915e-02
  1.27118351e-02  1.29820500e-02  1.33846812e-02  1.35983471e-02
  1.40400678e-02  1.51715111e-02  1.55606177e-02  1.56365484e-02
  1.59155112e-02  1.61039047e-02  1.63596720e-02  1.68201607e-02
  1.71510223e-02  1.76071618e-02  1.89512018e-02  1.99569054e-02
  2.04549171e-02  2.06624214e-02  2.08486002e-02  2.16418989e-02
  2.17527281e-02  2.49960925e-02  2.60942616e-02  2.61793733e-02
  2.74348743e-02  2.81485952e-02  2.95712277e-02  3.06304842e-02
  3.09979059e-02  3.10575329e-02  3.10727805e-02  3.10787261e-02
  3.14518735e-02  3.20532359e-02  3.23520675e-02  3.57350893e-02
  3.64804491e-02  3.68347801e-02  3.73787619e-02  3.75316441e-02
  3.87482047e-02  3.90611514e-02  3.94810699e-02  4.01968472e-02
  4.15538102e-02  4.16363813e-02  4.39808145e-02  4.47046831e-02
  4.77342345e-02  4.77780141e-02  4.95660231e-02  5.01549654e-02
  5.27901426e-02  5.31353466e-02  5.37669174e-02  6.39091879e-02
  6.41963109e-02  6.43570870e-02  6.68882802e-02  6.76299706e-02
  6.76851720e-02  6.77457452e-02  6.79777041e-02  7.13107958e-02
  7.31104538e-02  7.34027326e-02  7.36210197e-02  7.38995001e-02
  7.48819932e-02  7.55212083e-02  7.56976157e-02  7.63367862e-02
  7.68586919e-02  7.74826258e-02  7.77712017e-02  7.82752261e-02
  7.90327936e-02  8.21612552e-02  8.22828859e-02  8.24243799e-02
  8.27745870e-02  8.30710307e-02  8.65957513e-02  8.72031674e-02
  8.89452696e-02  8.99556652e-02  9.04253274e-02  9.30479169e-02
  9.51428860e-02  9.54506621e-02  9.61140394e-02  9.71962512e-02
  9.79444087e-02  9.80753675e-02  1.00670576e-01  1.04298435e-01
  1.04960233e-01  1.05518378e-01  1.05890907e-01  1.07745230e-01
  1.08373061e-01  1.11921579e-01  1.13497972e-01  1.15014434e-01
  1.15688235e-01  1.18905164e-01  1.19150288e-01  1.19915478e-01
  1.20247930e-01  1.20374106e-01  1.20967388e-01  1.21074080e-01
  1.23572953e-01  1.25663489e-01  1.26613751e-01  1.26659080e-01
  1.28121391e-01  1.28229707e-01  1.28687575e-01  1.29201308e-01
  1.29668027e-01  1.30415693e-01  1.30920708e-01  1.31735891e-01
  1.37411609e-01  1.38379633e-01  1.39939666e-01  1.44593880e-01
  1.46703929e-01  1.49342477e-01  1.51825786e-01  1.55416712e-01
  1.55881464e-01  1.57710120e-01  1.61773697e-01  1.62127897e-01
  1.63750559e-01  1.65415108e-01  1.66064173e-01  1.66960463e-01
  1.67553827e-01  1.71615466e-01  1.76287994e-01  1.77495256e-01
  1.80016994e-01  1.80294886e-01  1.82307258e-01  1.84309259e-01
  1.92981526e-01  1.94344908e-01  1.95512310e-01  1.95664153e-01
  1.97409421e-01  2.05989406e-01  2.09830716e-01  2.11165965e-01
  2.11582661e-01  2.16387093e-01  2.25832939e-01  2.28483662e-01
  2.31273398e-01  2.31589437e-01  2.31938884e-01  2.33265832e-01
  2.33725548e-01  2.33784884e-01  2.35723391e-01  2.41019413e-01
  2.41333202e-01  2.41344050e-01  2.47330070e-01  2.53781050e-01
  2.54704803e-01  2.56468832e-01  2.56723642e-01  2.57099241e-01
  2.57667810e-01  2.57733077e-01  2.64198333e-01  2.71821052e-01
  2.76276350e-01  2.77473092e-01  2.80505866e-01  2.81316578e-01
  2.84207374e-01  2.85779864e-01  2.86246926e-01  2.86834389e-01
  2.87166923e-01  2.88000911e-01  2.88480371e-01  2.90486187e-01
  2.91150033e-01  2.91540056e-01  2.93514699e-01  2.93748051e-01
  2.93979317e-01  2.99155891e-01  3.00006390e-01  3.01010668e-01
  3.04772228e-01  3.05230051e-01  3.05591375e-01  3.10174048e-01
  3.11382592e-01  3.11896384e-01  3.18203539e-01  3.21825564e-01
  3.27211648e-01  3.29305112e-01  3.29925448e-01  3.38118702e-01
  3.39014530e-01  3.40256691e-01  3.48566830e-01  3.61473918e-01
  3.62863094e-01  3.63266021e-01  3.63430589e-01  3.63484859e-01
  3.64643008e-01  3.64839554e-01  3.67033482e-01  3.71287078e-01
  3.73748124e-01  3.76516998e-01  3.85539889e-01  3.87160510e-01
  3.88672054e-01  3.89063060e-01  3.93109083e-01  3.93532634e-01
  3.96307439e-01  3.98179501e-01  4.00090635e-01  4.05469030e-01
  4.07781541e-01  4.10126805e-01  4.10604089e-01  4.13569421e-01
  4.13633853e-01  4.18264061e-01  4.23377097e-01  4.26158518e-01
  4.32257682e-01  4.35595065e-01  4.36917126e-01  4.37149376e-01
  4.40309674e-01  4.52273697e-01  4.52273905e-01  4.55131859e-01
  4.55422729e-01  4.56908584e-01  4.59715664e-01  4.64930803e-01
  4.69223559e-01  4.69980180e-01  4.70888048e-01  4.72497374e-01
  4.78130043e-01  4.78375286e-01  4.88725930e-01  4.91702229e-01
  4.92780924e-01  4.94435042e-01  4.96485502e-01  4.97213840e-01
  4.98168200e-01  4.99310941e-01  5.00545800e-01  5.04156590e-01
  5.05186558e-01  5.07494628e-01  5.09050727e-01  5.11500955e-01
  5.20495415e-01  5.34632087e-01  5.37113428e-01  5.45895755e-01
  5.47792554e-01  5.57766497e-01  5.58523536e-01  5.61342537e-01
  5.63271403e-01  5.63273728e-01  5.66591084e-01  5.68744957e-01
  5.69195032e-01  5.86880684e-01  5.87336957e-01  5.91659009e-01
  5.92867553e-01  6.02958560e-01  6.04746878e-01  6.07502639e-01
  6.13374591e-01  6.14679813e-01  6.18503869e-01  6.21231616e-01
  6.27912343e-01  6.29236758e-01  6.34207010e-01  6.34713531e-01
  6.38478577e-01  6.44968033e-01  6.55044794e-01  6.69304073e-01
  6.72214627e-01  6.74009919e-01  6.76102519e-01  6.83145940e-01
  6.84342265e-01  6.86518788e-01  6.88411534e-01  6.94471955e-01
  6.95986807e-01  6.98531091e-01  7.01915205e-01  7.06163883e-01
  7.14986980e-01  7.17928588e-01  7.19208539e-01  7.28306770e-01
  7.30556786e-01  7.33177185e-01  7.44441330e-01  7.53581345e-01
  7.56624103e-01  7.64567733e-01  7.73333251e-01  7.92081416e-01
  8.06750238e-01  8.11515808e-01  8.27719390e-01  8.36659253e-01
  8.44264388e-01  8.48490477e-01  8.49237621e-01  8.54081631e-01
  8.58248591e-01  8.67651761e-01  8.68138194e-01  8.68213534e-01
  8.79885852e-01  8.80910873e-01  8.81888211e-01  8.82496953e-01
  8.89384389e-01  8.96588087e-01  9.24295723e-01  9.45286214e-01
  9.47965920e-01  9.53814864e-01  9.66637015e-01  9.72676098e-01
  9.77679670e-01  9.87988174e-01  9.89203095e-01  9.89906013e-01
  9.90855753e-01  9.94652808e-01  1.03186882e+00  1.03222048e+00
  1.06228971e+00  1.07589531e+00  1.10924947e+00  1.11410129e+00
  1.13662183e+00  1.22550225e+00  1.29096687e+00]

  warnings.warn(

2022-11-03 10:53:59,228:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.36026183e-01 -2.19637409e-01 -1.66873649e-01 -1.12116069e-01
 -1.09182537e-01 -1.08353607e-01 -1.08169235e-01 -9.40750316e-02
 -9.35328230e-02 -8.99574161e-02 -8.45132172e-02 -8.42282102e-02
 -8.04673880e-02 -7.92534128e-02 -7.60652423e-02 -7.48545229e-02
 -7.36348629e-02 -6.88231066e-02 -6.85000420e-02 -6.43730983e-02
 -6.33585453e-02 -6.15996979e-02 -6.15544654e-02 -6.03072755e-02
 -5.55556938e-02 -5.21684289e-02 -5.15566953e-02 -5.11058643e-02
 -4.80719395e-02 -4.67905998e-02 -4.66571487e-02 -4.64898236e-02
 -4.39671390e-02 -4.03833315e-02 -3.97414528e-02 -3.86305042e-02
 -3.83247845e-02 -3.73425968e-02 -3.71063612e-02 -3.68237346e-02
 -3.58505584e-02 -3.51833180e-02 -3.51486020e-02 -3.48277688e-02
 -3.45818177e-02 -3.30849960e-02 -3.26746777e-02 -3.24327871e-02
 -3.19163986e-02 -2.91069783e-02 -2.90913414e-02 -2.90273521e-02
 -2.87252851e-02 -2.65466236e-02 -2.53090039e-02 -2.48910915e-02
 -2.42621154e-02 -2.39425041e-02 -2.36473288e-02 -2.22840514e-02
 -2.20073834e-02 -2.18070913e-02 -2.12209597e-02 -2.10432429e-02
 -2.07276344e-02 -2.04814356e-02 -1.88935194e-02 -1.87319815e-02
 -1.79071072e-02 -1.74779873e-02 -1.73534192e-02 -1.64573025e-02
 -1.60195567e-02 -1.58136021e-02 -1.51471458e-02 -1.48569578e-02
 -1.48326177e-02 -1.37043558e-02 -1.26672294e-02 -1.16240131e-02
 -1.13056339e-02 -9.92557034e-03 -9.91898775e-03 -9.48641077e-03
 -9.18301661e-03 -9.01393034e-03 -8.33589677e-03 -7.66948983e-03
 -7.56710116e-03 -7.27822399e-03 -6.75709546e-03 -6.42538769e-03
 -5.84245706e-03 -5.43457083e-03 -4.85426653e-03 -4.71939938e-03
 -3.31189181e-03 -2.73579103e-03 -2.08745664e-03 -4.25970939e-04
 -1.12029302e-04 -9.42722399e-05  1.41001865e-03  1.60253351e-03
  2.13898835e-03  2.37363228e-03  2.46548816e-03  2.69227312e-03
  3.00645665e-03  3.19603994e-03  3.94500839e-03  4.14295588e-03
  5.01785334e-03  5.20905014e-03  5.23985643e-03  5.63749112e-03
  6.01212494e-03  6.88254088e-03  7.23780645e-03  7.34447269e-03
  8.05998594e-03  1.07116848e-02  1.32548772e-02  1.34094954e-02
  1.35599216e-02  1.38691133e-02  1.41179133e-02  1.42039703e-02
  1.49430232e-02  1.50279487e-02  1.51417172e-02  1.55186718e-02
  1.55652771e-02  1.61169395e-02  1.62908994e-02  1.63642447e-02
  1.80076975e-02  1.80719811e-02  1.93824470e-02  1.94837786e-02
  1.95310060e-02  1.98605545e-02  2.00395118e-02  2.00644862e-02
  2.02576201e-02  2.03953087e-02  2.04535984e-02  2.14884002e-02
  2.17383299e-02  2.34444607e-02  2.64057424e-02  2.65171733e-02
  2.67953817e-02  2.91093495e-02  2.93703899e-02  3.06475554e-02
  3.14163491e-02  3.17741744e-02  3.20930704e-02  3.35443877e-02
  3.39203030e-02  3.39405537e-02  3.70635353e-02  3.87096293e-02
  3.89645174e-02  4.01714817e-02  4.09751162e-02  4.31087911e-02
  4.32030112e-02  4.34057191e-02  4.47027460e-02  4.65931259e-02
  4.67346273e-02  4.73834351e-02  4.79240641e-02  4.82076705e-02
  4.94688042e-02  4.96283993e-02  5.00345752e-02  5.02321832e-02
  5.10921068e-02  5.11919707e-02  5.17482609e-02  5.41515127e-02
  5.43709956e-02  5.51141910e-02  5.68009689e-02  5.86541854e-02
  5.88552840e-02  6.00646846e-02  6.06334209e-02  6.07689172e-02
  6.15610145e-02  6.29250631e-02  6.31531850e-02  6.52954951e-02
  6.66622519e-02  6.74592853e-02  6.93547726e-02  6.94029257e-02
  6.97953478e-02  7.04582632e-02  7.27434009e-02  7.71576762e-02
  7.92086795e-02  8.20871741e-02  8.23134705e-02  8.24544728e-02
  8.31472576e-02  8.39509442e-02  8.48160610e-02  8.54963362e-02
  9.18626338e-02  9.25198793e-02  9.67322290e-02  9.81768742e-02
  9.92927998e-02  9.98515338e-02  1.00814573e-01  1.02481358e-01
  1.02864750e-01  1.03304572e-01  1.06722705e-01  1.08364701e-01
  1.08560823e-01  1.08983621e-01  1.10005282e-01  1.10376798e-01
  1.12549700e-01  1.13152422e-01  1.13371037e-01  1.17283054e-01
  1.19771987e-01  1.20615073e-01  1.21545233e-01  1.25820622e-01
  1.27953246e-01  1.29525349e-01  1.30678356e-01  1.31442025e-01
  1.32860780e-01  1.33643612e-01  1.34055451e-01  1.35234728e-01
  1.38725579e-01  1.38881892e-01  1.39651954e-01  1.40665054e-01
  1.40669882e-01  1.42299309e-01  1.44316748e-01  1.54088780e-01
  1.55820355e-01  1.57848626e-01  1.58089638e-01  1.59223825e-01
  1.63076133e-01  1.63565531e-01  1.65712446e-01  1.66680485e-01
  1.70223355e-01  1.70640796e-01  1.71594918e-01  1.72262669e-01
  1.72776610e-01  1.74965352e-01  1.76104188e-01  1.76289216e-01
  1.78560048e-01  1.78712994e-01  1.85239434e-01  1.88519701e-01
  1.89211503e-01  1.89297274e-01  1.90603763e-01  1.90938175e-01
  1.92905009e-01  1.98277652e-01  2.00980768e-01  2.02321604e-01
  2.03648388e-01  2.07035109e-01  2.10311204e-01  2.11072162e-01
  2.13955477e-01  2.17972070e-01  2.22562268e-01  2.25540474e-01
  2.28907049e-01  2.29871079e-01  2.30946526e-01  2.37206250e-01
  2.37436011e-01  2.38830388e-01  2.41013765e-01  2.42140040e-01
  2.43745193e-01  2.45049179e-01  2.46198535e-01  2.46549562e-01
  2.49908164e-01  2.51287133e-01  2.52629787e-01  2.53502280e-01
  2.56325334e-01  2.57126570e-01  2.57726401e-01  2.59800315e-01
  2.60141701e-01  2.62987524e-01  2.63942838e-01  2.65099108e-01
  2.65153348e-01  2.73561686e-01  2.73773074e-01  2.75193810e-01
  2.76636600e-01  2.77346700e-01  2.78992593e-01  2.84118831e-01
  2.85889804e-01  2.86002964e-01  2.87979901e-01  2.92815894e-01
  2.94057906e-01  2.94580132e-01  2.95046598e-01  2.99130380e-01
  2.99499243e-01  3.00351351e-01  3.00523788e-01  3.01699966e-01
  3.02560568e-01  3.14442664e-01  3.15661639e-01  3.15988064e-01
  3.19239050e-01  3.22159410e-01  3.23458850e-01  3.28209996e-01
  3.28800946e-01  3.30873519e-01  3.36140662e-01  3.42292100e-01
  3.46037596e-01  3.48352969e-01  3.48763317e-01  3.66989315e-01
  3.67265135e-01  3.67597312e-01  3.68813068e-01  3.72555405e-01
  3.74299616e-01  3.74742240e-01  3.78156126e-01  3.78269941e-01
  3.79033983e-01  3.80372882e-01  3.82504880e-01  3.83738995e-01
  3.90466481e-01  3.93324703e-01  3.98565173e-01  3.99027288e-01
  3.99979860e-01  4.06946301e-01  4.09435540e-01  4.19599950e-01
  4.20444310e-01  4.22164440e-01  4.24076945e-01  4.35952395e-01
  4.56100821e-01  4.63201433e-01  4.63432193e-01  4.63843852e-01
  4.67513144e-01  4.68750417e-01  4.76299495e-01  4.80635166e-01
  4.85333711e-01  4.89496797e-01  4.93885875e-01  4.96386141e-01
  5.02173483e-01  5.10702848e-01  5.12908041e-01  5.25395870e-01
  5.28656304e-01  5.29756546e-01  5.31099737e-01  5.31400621e-01
  5.32789230e-01  5.40409446e-01  5.44128716e-01  5.44746339e-01
  5.45796037e-01  5.46830952e-01  5.55126846e-01  5.60335457e-01
  5.62580764e-01  5.65677583e-01  5.66531003e-01  5.73088944e-01
  5.79788744e-01  5.80224931e-01  5.84100068e-01  5.85676908e-01
  5.91856062e-01  5.92027128e-01  6.04083300e-01  6.08862281e-01
  6.09764159e-01  6.09937310e-01  6.11656427e-01  6.12513840e-01
  6.15471125e-01  6.16062045e-01  6.22393727e-01  6.22580588e-01
  6.27647758e-01  6.30867124e-01  6.32416666e-01  6.36854053e-01
  6.39861882e-01  6.42995715e-01  6.44136012e-01  6.54075503e-01
  6.58264935e-01  6.62465036e-01  6.65229917e-01  6.67300582e-01
  6.71651304e-01  6.75816178e-01  6.83765829e-01  6.88122451e-01
  6.89616561e-01  6.97307706e-01  7.07822204e-01  7.14580595e-01
  7.17953026e-01  7.18702674e-01  7.29950845e-01  7.31425405e-01
  7.34584749e-01  7.43041754e-01  7.45960176e-01  7.46595800e-01
  7.52317846e-01  7.52731085e-01  7.52916515e-01  7.60810435e-01
  7.69803405e-01  7.82024503e-01  7.84213781e-01  7.89027154e-01
  7.90164590e-01  8.06162834e-01  8.17565143e-01  8.29175770e-01
  8.36213350e-01  8.37445438e-01  8.42413485e-01  8.53647232e-01
  8.56510043e-01  8.61612976e-01  8.65988195e-01  8.71813595e-01
  9.01181221e-01  9.09650922e-01  9.10775721e-01  9.21060205e-01
  9.29016888e-01  9.33296084e-01  9.33359683e-01  9.39759910e-01
  9.39952314e-01  9.41558242e-01  9.43429351e-01  9.52499092e-01
  9.78034258e-01  9.78547335e-01  1.00049376e+00  1.00568235e+00
  1.00915027e+00  1.01271999e+00  1.01488066e+00  1.02683365e+00
  1.03584325e+00  1.05010855e+00  1.08406472e+00  1.11526811e+00
  1.11597419e+00]

  warnings.warn(

2022-11-03 10:53:59,245:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.84011251e-01 -1.37056112e-01 -1.29784152e-01 -1.24159880e-01
 -1.15808576e-01 -1.12471789e-01 -9.61473361e-02 -8.04146454e-02
 -7.71545768e-02 -7.62716904e-02 -7.43234605e-02 -7.27608278e-02
 -7.24735260e-02 -7.22261220e-02 -6.52391538e-02 -6.40200377e-02
 -6.13426641e-02 -6.07890561e-02 -5.96005805e-02 -5.79676032e-02
 -5.76933622e-02 -5.18640541e-02 -4.88285199e-02 -4.68401723e-02
 -4.67900895e-02 -4.55746241e-02 -4.41677347e-02 -3.74535024e-02
 -3.74254882e-02 -3.72528844e-02 -3.42237987e-02 -3.36366333e-02
 -3.27103771e-02 -3.18044610e-02 -3.03277448e-02 -2.92298794e-02
 -2.78726071e-02 -2.58593168e-02 -2.57755779e-02 -2.29811613e-02
 -2.27407962e-02 -2.21777204e-02 -2.16287393e-02 -2.06400305e-02
 -2.06309389e-02 -2.06225421e-02 -1.86345223e-02 -1.78237408e-02
 -1.66844595e-02 -1.59875304e-02 -1.56069035e-02 -1.50135141e-02
 -1.48441866e-02 -1.47204045e-02 -1.44358790e-02 -1.34916585e-02
 -1.28794899e-02 -1.23278340e-02 -1.18856058e-02 -1.16015896e-02
 -1.15090050e-02 -1.08986888e-02 -1.02366609e-02 -1.01892231e-02
 -9.47029609e-03 -6.86784321e-03 -6.22876734e-03 -5.87465009e-03
 -5.81104774e-03 -5.75711578e-03 -5.62097272e-03 -5.04825870e-03
 -5.00359852e-03 -4.68475511e-03 -4.51534381e-03 -4.49026609e-03
 -3.78838833e-03 -3.49802477e-03 -3.02166981e-03 -2.86157010e-03
 -2.71871546e-03 -2.37344438e-03 -1.43635960e-03 -1.11456669e-03
 -1.06571161e-03 -1.67980645e-04  3.32607306e-04  3.48608999e-04
  1.58877776e-03  1.76104286e-03  2.66485312e-03  3.11423372e-03
  3.18625779e-03  3.24819260e-03  3.65852285e-03  3.93943861e-03
  4.24008118e-03  4.45481064e-03  5.18837385e-03  5.54776238e-03
  6.90113287e-03  6.98070414e-03  7.16694538e-03  7.48402672e-03
  7.74225593e-03  9.38486494e-03  9.75865871e-03  9.76261776e-03
  9.93530732e-03  1.01917339e-02  1.04793217e-02  1.06148552e-02
  1.08640641e-02  1.10221701e-02  1.13289980e-02  1.22346347e-02
  1.25624267e-02  1.27624609e-02  1.28267482e-02  1.35456799e-02
  1.38312783e-02  1.38328327e-02  1.40466047e-02  1.41893355e-02
  1.47529645e-02  1.52389416e-02  1.55250533e-02  1.55704143e-02
  1.58856921e-02  1.60645936e-02  1.68423411e-02  1.78695824e-02
  1.90442149e-02  1.98696926e-02  2.04116628e-02  2.23296639e-02
  2.41227746e-02  2.41608303e-02  2.56690346e-02  2.58972514e-02
  2.62525640e-02  2.64673773e-02  2.66276486e-02  2.78883614e-02
  2.93640308e-02  3.16636898e-02  3.71480435e-02  3.88736166e-02
  3.95146534e-02  4.01795655e-02  4.38396409e-02  4.52319980e-02
  4.55563180e-02  4.79730219e-02  4.81329672e-02  4.87812385e-02
  5.01981638e-02  5.25298491e-02  5.32696210e-02  5.49021810e-02
  5.50218932e-02  5.51121123e-02  5.61426468e-02  5.62180430e-02
  5.82990013e-02  5.98003753e-02  5.99673726e-02  6.04922101e-02
  6.07161038e-02  6.21899851e-02  6.33757561e-02  6.49580956e-02
  6.61976263e-02  6.70423657e-02  6.70559630e-02  6.89794496e-02
  7.01518059e-02  7.07960352e-02  7.22432062e-02  7.22494498e-02
  7.28456080e-02  7.41794407e-02  7.52216801e-02  7.58370683e-02
  7.73669556e-02  7.73748085e-02  7.74084032e-02  7.83132315e-02
  7.84591436e-02  8.05021301e-02  8.13164562e-02  8.27183872e-02
  8.51926953e-02  8.58715847e-02  8.69774371e-02  8.86642858e-02
  8.87325704e-02  9.00587142e-02  9.24647450e-02  9.27215442e-02
  9.34376940e-02  9.65285152e-02  9.83176380e-02  9.93770361e-02
  9.96875539e-02  1.01243556e-01  1.03412710e-01  1.04241572e-01
  1.05375245e-01  1.05523556e-01  1.07754543e-01  1.08516142e-01
  1.16890028e-01  1.17342189e-01  1.18081324e-01  1.18875943e-01
  1.19871140e-01  1.23306885e-01  1.27075732e-01  1.28682837e-01
  1.29534796e-01  1.30535901e-01  1.33632079e-01  1.35491222e-01
  1.35929823e-01  1.37790158e-01  1.38672248e-01  1.42034322e-01
  1.42226666e-01  1.43004566e-01  1.44051567e-01  1.44101456e-01
  1.44140735e-01  1.46289364e-01  1.47279054e-01  1.50274411e-01
  1.55945331e-01  1.60824180e-01  1.61353394e-01  1.64215729e-01
  1.64695352e-01  1.65379003e-01  1.73456907e-01  1.78755924e-01
  1.79191634e-01  1.80194482e-01  1.83056727e-01  1.84659392e-01
  1.84697151e-01  1.87369332e-01  1.88167736e-01  1.88718513e-01
  1.89312190e-01  1.91149876e-01  1.91393554e-01  1.91774756e-01
  1.93486050e-01  1.94194853e-01  1.98921129e-01  1.99007943e-01
  2.00837299e-01  2.01108769e-01  2.03573361e-01  2.04907611e-01
  2.08125174e-01  2.10034162e-01  2.12549537e-01  2.12674245e-01
  2.13567898e-01  2.14347601e-01  2.17208907e-01  2.17387646e-01
  2.17805877e-01  2.19391048e-01  2.19494596e-01  2.19893694e-01
  2.22399637e-01  2.22655013e-01  2.25597858e-01  2.25808203e-01
  2.25928605e-01  2.26973563e-01  2.27038935e-01  2.29344502e-01
  2.29983568e-01  2.30893612e-01  2.33296663e-01  2.38153800e-01
  2.39185855e-01  2.41975576e-01  2.44221270e-01  2.51286864e-01
  2.52536923e-01  2.55368650e-01  2.56973505e-01  2.61678070e-01
  2.66531080e-01  2.69552886e-01  2.70513743e-01  2.71093011e-01
  2.71717042e-01  2.72884130e-01  2.73049057e-01  2.73737013e-01
  2.75952250e-01  2.76756525e-01  2.79204935e-01  2.81167597e-01
  2.83587903e-01  2.84129649e-01  2.88237393e-01  2.88249314e-01
  2.88450867e-01  2.88873911e-01  2.90869772e-01  2.91864991e-01
  2.93534219e-01  2.96550542e-01  2.99109578e-01  3.02722752e-01
  3.12822491e-01  3.13800395e-01  3.14090252e-01  3.15804303e-01
  3.16107005e-01  3.19074363e-01  3.20067793e-01  3.21104705e-01
  3.21686000e-01  3.22364837e-01  3.25516015e-01  3.27446282e-01
  3.28288883e-01  3.29345882e-01  3.30084622e-01  3.32378179e-01
  3.32783103e-01  3.37826788e-01  3.38329792e-01  3.43907416e-01
  3.47248375e-01  3.48164767e-01  3.48432004e-01  3.51038873e-01
  3.53131264e-01  3.54837924e-01  3.56726319e-01  3.61236393e-01
  3.67245197e-01  3.70661080e-01  3.70782793e-01  3.72471690e-01
  3.75625998e-01  3.76417160e-01  3.78878266e-01  3.79685134e-01
  3.83802772e-01  3.86510611e-01  3.87447596e-01  3.90372276e-01
  3.91449273e-01  3.92194450e-01  3.95962596e-01  3.96577269e-01
  4.07275468e-01  4.14531410e-01  4.18326408e-01  4.19481218e-01
  4.20429826e-01  4.20919776e-01  4.22915071e-01  4.34792161e-01
  4.36137497e-01  4.37608153e-01  4.38669056e-01  4.40831572e-01
  4.43264037e-01  4.49021548e-01  4.50726986e-01  4.51664686e-01
  4.56025183e-01  4.56093669e-01  4.57580805e-01  4.57998008e-01
  4.61113513e-01  4.61174399e-01  4.62462693e-01  4.68244731e-01
  4.73708063e-01  4.82774198e-01  4.87022489e-01  4.87691969e-01
  4.92188483e-01  4.92569625e-01  4.94714111e-01  5.01757920e-01
  5.05304277e-01  5.06556392e-01  5.09761393e-01  5.12350857e-01
  5.15960038e-01  5.16605198e-01  5.20949841e-01  5.22145391e-01
  5.33051074e-01  5.34902513e-01  5.37905514e-01  5.40188789e-01
  5.43208957e-01  5.45668066e-01  5.46170354e-01  5.46971738e-01
  5.67852914e-01  5.68838537e-01  5.71701586e-01  5.72998047e-01
  5.83813429e-01  5.92758775e-01  5.94814539e-01  5.97332358e-01
  5.98782659e-01  5.98944128e-01  6.03617311e-01  6.06318176e-01
  6.14148557e-01  6.16852164e-01  6.18995965e-01  6.28753245e-01
  6.32043421e-01  6.40636981e-01  6.41264141e-01  6.51915252e-01
  6.51931584e-01  6.52261555e-01  6.55422628e-01  6.63904548e-01
  6.65184140e-01  6.68884277e-01  6.81903899e-01  6.82681024e-01
  6.89596951e-01  6.92133129e-01  7.00668812e-01  7.02109337e-01
  7.03051567e-01  7.04225063e-01  7.10754275e-01  7.17288911e-01
  7.24033177e-01  7.26865649e-01  7.31927276e-01  7.32061684e-01
  7.35734046e-01  7.45156825e-01  7.56213307e-01  7.58200467e-01
  7.58434355e-01  7.63223350e-01  7.90070295e-01  7.90410161e-01
  7.92653322e-01  7.96850264e-01  7.96955287e-01  8.10569406e-01
  8.12744319e-01  8.29399645e-01  8.37955356e-01  8.39768648e-01
  8.44961643e-01  8.54547620e-01  8.76455069e-01  8.86109829e-01
  9.00673747e-01  9.12948668e-01  9.15201604e-01  9.15258765e-01
  9.24456835e-01  9.37663257e-01  9.43243086e-01  9.43432987e-01
  9.97354031e-01  1.00380456e+00  1.00798178e+00  1.01475859e+00
  1.01481164e+00  1.03764594e+00  1.06272495e+00  1.08376825e+00
  1.09094024e+00  1.11498392e+00  1.14774573e+00  1.15869725e+00
  1.33226919e+00]

  warnings.warn(

2022-11-03 10:53:59,245:INFO:Calculating mean and std
2022-11-03 10:53:59,245:INFO:Creating metrics dataframe
2022-11-03 10:53:59,253:INFO:Uploading results into container
2022-11-03 10:53:59,261:INFO:Uploading model into container now
2022-11-03 10:53:59,261:INFO:master_model_container: 34
2022-11-03 10:53:59,261:INFO:display_container: 2
2022-11-03 10:53:59,261:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, gamma=None,
             gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, n_estimators=100, n_jobs=-1,
             num_parallel_tree=None, predictor=None, random_state=4411,
             reg_alpha=None, reg_lambda=None, ...)
2022-11-03 10:53:59,261:INFO:create_model() successfully completed......................................
2022-11-03 10:53:59,526:ERROR:create_model() for XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, gamma=None,
             gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, n_estimators=100, n_jobs=-1,
             num_parallel_tree=None, predictor=None, random_state=4411,
             reg_alpha=None, reg_lambda=None, ...) raised an exception or returned all 0.0:
2022-11-03 10:53:59,526:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:53:59,526:INFO:Initializing Light Gradient Boosting Machine
2022-11-03 10:53:59,526:INFO:Total runtime is 5.393604481220246 minutes
2022-11-03 10:53:59,526:INFO:SubProcess create_model() called ==================================
2022-11-03 10:53:59,526:INFO:Initializing create_model()
2022-11-03 10:53:59,526:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:53:59,526:INFO:Checking exceptions
2022-11-03 10:53:59,526:INFO:Importing libraries
2022-11-03 10:53:59,526:INFO:Copying training dataset
2022-11-03 10:53:59,541:INFO:Defining folds
2022-11-03 10:53:59,541:INFO:Declaring metric variables
2022-11-03 10:53:59,541:INFO:Importing untrained model
2022-11-03 10:53:59,541:INFO:Light Gradient Boosting Machine Imported successfully
2022-11-03 10:53:59,541:INFO:Starting cross validation
2022-11-03 10:53:59,556:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:54:04,104:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.04768192e-02 -5.79044319e-02 -4.75116932e-02 -4.31571291e-02
 -3.99256214e-02 -3.98561420e-02 -3.78398797e-02 -3.03339319e-02
 -2.63634907e-02 -2.12452538e-02 -2.10097220e-02 -1.97649597e-02
 -1.97548511e-02 -1.72600352e-02 -1.60952468e-02 -1.51978840e-02
 -1.45241521e-02 -1.41527963e-02 -1.20559792e-02 -1.20247965e-02
 -1.15955326e-02 -1.14501223e-02 -1.14131644e-02 -1.11385930e-02
 -1.10705641e-02 -1.05155238e-02 -9.72250279e-03 -9.57851846e-03
 -9.51984592e-03 -9.15956081e-03 -8.28529321e-03 -7.83537851e-03
 -7.67430093e-03 -7.54262804e-03 -7.50057673e-03 -7.36702659e-03
 -6.61003773e-03 -6.47899684e-03 -6.12750597e-03 -6.01889635e-03
 -5.96044818e-03 -5.81426991e-03 -5.22733203e-03 -4.77624725e-03
 -2.65947825e-03 -2.53148054e-03 -2.13601602e-03 -1.96663277e-03
 -1.59957541e-03 -1.09122623e-03 -6.55259016e-04 -9.42419942e-05
  8.99666916e-04  1.04007671e-03  1.28689130e-03  1.82297044e-03
  2.00480777e-03  2.46101033e-03  2.55562561e-03  3.04697693e-03
  3.30665107e-03  3.77787830e-03  4.49913646e-03  4.64755782e-03
  4.77200330e-03  4.78464387e-03  5.04327015e-03  5.36007974e-03
  6.05090290e-03  6.15760110e-03  7.17006206e-03  7.58326651e-03
  8.43909972e-03  9.38694716e-03  9.69841714e-03  1.02184193e-02
  1.02281340e-02  1.09570771e-02  1.18851373e-02  1.19190140e-02
  1.19981024e-02  1.28469516e-02  1.30206556e-02  1.34166926e-02
  1.38627512e-02  1.42481468e-02  1.43512526e-02  1.51368901e-02
  1.53230940e-02  1.53354016e-02  1.72058277e-02  1.80551971e-02
  1.85531063e-02  1.96678103e-02  2.06621720e-02  2.12918336e-02
  2.22240390e-02  2.33391155e-02  2.34089964e-02  2.46312288e-02
  2.47820612e-02  2.51103686e-02  2.62066851e-02  2.70501126e-02
  2.71705587e-02  2.75320463e-02  2.75666451e-02  2.78998117e-02
  2.91881076e-02  2.94823631e-02  2.97919343e-02  2.97963676e-02
  2.99002909e-02  2.99483817e-02  3.02055429e-02  3.12186813e-02
  3.14974148e-02  3.29014616e-02  3.41964207e-02  3.44561326e-02
  3.50012372e-02  3.60698953e-02  3.72814287e-02  3.90665156e-02
  3.92151798e-02  3.92703629e-02  4.00109787e-02  4.01827887e-02
  4.04574012e-02  4.05976150e-02  4.08016374e-02  4.10691561e-02
  4.11924570e-02  4.12526747e-02  4.21754976e-02  4.38244097e-02
  4.41992119e-02  4.53397100e-02  4.55028045e-02  4.78651639e-02
  4.89525482e-02  4.92722916e-02  5.08931931e-02  5.09952634e-02
  5.13479388e-02  5.26822272e-02  5.51502361e-02  5.55929312e-02
  5.66743342e-02  5.80450895e-02  5.96134704e-02  6.13662995e-02
  6.20562060e-02  6.27063192e-02  6.33000416e-02  6.33390666e-02
  6.49876063e-02  6.58085682e-02  6.69364043e-02  6.83895267e-02
  6.91918427e-02  7.05344074e-02  7.14145924e-02  7.21648151e-02
  7.22496941e-02  7.37472587e-02  7.38425785e-02  7.43777780e-02
  7.49775544e-02  7.57726210e-02  7.63876148e-02  7.67072871e-02
  7.70477084e-02  7.76635370e-02  7.83875774e-02  7.87823820e-02
  7.95628316e-02  8.12565546e-02  8.14484114e-02  8.17275567e-02
  8.34643932e-02  8.40306194e-02  8.67978746e-02  8.68465454e-02
  8.76782791e-02  8.79169430e-02  8.87129445e-02  8.87488462e-02
  8.88704690e-02  9.02539264e-02  9.08778885e-02  9.28650709e-02
  9.29425888e-02  9.35340422e-02  9.47965100e-02  9.48412805e-02
  9.51887038e-02  9.60912055e-02  9.75754089e-02  9.82913414e-02
  9.85290066e-02  9.98947600e-02  1.00224837e-01  1.01748296e-01
  1.01755213e-01  1.02223215e-01  1.03732665e-01  1.04161150e-01
  1.04726175e-01  1.06295794e-01  1.06672324e-01  1.08598631e-01
  1.08904453e-01  1.10470913e-01  1.14006045e-01  1.14300129e-01
  1.15515870e-01  1.16714992e-01  1.18170319e-01  1.21021822e-01
  1.21456021e-01  1.21962246e-01  1.22698834e-01  1.23296989e-01
  1.23978216e-01  1.26292868e-01  1.26817425e-01  1.28024918e-01
  1.28264812e-01  1.32237854e-01  1.34154593e-01  1.34474883e-01
  1.34682923e-01  1.35380490e-01  1.35407792e-01  1.35938376e-01
  1.36172123e-01  1.36569746e-01  1.40112527e-01  1.40523794e-01
  1.40642312e-01  1.41321002e-01  1.42917429e-01  1.45140693e-01
  1.49775150e-01  1.51248441e-01  1.51428140e-01  1.53347165e-01
  1.55111672e-01  1.58514087e-01  1.60773566e-01  1.61301835e-01
  1.61368963e-01  1.62226873e-01  1.62969794e-01  1.64977166e-01
  1.65322643e-01  1.65890528e-01  1.66978707e-01  1.68092521e-01
  1.70480746e-01  1.71906321e-01  1.72244955e-01  1.73456525e-01
  1.74922367e-01  1.77333236e-01  1.77896185e-01  1.79128034e-01
  1.81339489e-01  1.81563685e-01  1.82382160e-01  1.83281646e-01
  1.84155278e-01  1.86763575e-01  1.87846621e-01  1.96373629e-01
  1.99011974e-01  1.99748621e-01  2.03030700e-01  2.05540666e-01
  2.06316694e-01  2.06859386e-01  2.09145480e-01  2.11308985e-01
  2.11956885e-01  2.15041700e-01  2.15296782e-01  2.17871096e-01
  2.20456592e-01  2.22381650e-01  2.22638227e-01  2.23965544e-01
  2.24252770e-01  2.24992651e-01  2.27077197e-01  2.27946334e-01
  2.28605181e-01  2.29011158e-01  2.29727863e-01  2.29751493e-01
  2.30838628e-01  2.33894335e-01  2.35886504e-01  2.42164163e-01
  2.50647008e-01  2.51076694e-01  2.51703319e-01  2.53380816e-01
  2.53385639e-01  2.54216839e-01  2.55681008e-01  2.55980050e-01
  2.60029160e-01  2.65400338e-01  2.68060405e-01  2.68940295e-01
  2.74083771e-01  2.75972883e-01  2.76396802e-01  2.77039884e-01
  2.77898018e-01  2.78522573e-01  2.85083637e-01  2.85268982e-01
  2.86449216e-01  2.91232898e-01  2.91272057e-01  2.92371936e-01
  2.92945926e-01  2.94717129e-01  2.94828440e-01  2.94885713e-01
  2.95817106e-01  2.99084838e-01  3.04544451e-01  3.04677379e-01
  3.04749643e-01  3.06897141e-01  3.09576710e-01  3.13937590e-01
  3.19534818e-01  3.20010013e-01  3.20125276e-01  3.24531559e-01
  3.34611548e-01  3.38626095e-01  3.41177426e-01  3.58457689e-01
  3.60287207e-01  3.62986329e-01  3.65599415e-01  3.65789680e-01
  3.73236860e-01  3.75963401e-01  3.79788927e-01  3.80086075e-01
  3.81922458e-01  3.82938289e-01  3.83891699e-01  3.86291826e-01
  3.87137180e-01  3.89358589e-01  3.94008749e-01  3.98981140e-01
  4.00401301e-01  4.00754226e-01  4.00877754e-01  4.04135688e-01
  4.05782631e-01  4.06150303e-01  4.06384644e-01  4.10818987e-01
  4.11234106e-01  4.11992334e-01  4.20179357e-01  4.21574439e-01
  4.25123779e-01  4.25165834e-01  4.31937081e-01  4.34187907e-01
  4.37298363e-01  4.37446691e-01  4.49920997e-01  4.57695287e-01
  4.59734023e-01  4.59930675e-01  4.68183594e-01  4.73973978e-01
  4.74129440e-01  4.75037755e-01  4.78933850e-01  4.81216331e-01
  4.83268407e-01  4.87964757e-01  4.91171310e-01  4.95825777e-01
  4.97397644e-01  5.03987680e-01  5.08226781e-01  5.11756433e-01
  5.16190684e-01  5.16986885e-01  5.18857294e-01  5.21839051e-01
  5.24598800e-01  5.28491661e-01  5.29123451e-01  5.35489950e-01
  5.38140387e-01  5.40616335e-01  5.48510374e-01  5.49281796e-01
  5.52244589e-01  5.55215421e-01  5.61364635e-01  5.61911510e-01
  5.63565301e-01  5.65054628e-01  5.76336339e-01  5.82209082e-01
  5.89477858e-01  5.95132124e-01  5.96744012e-01  6.02224761e-01
  6.10724971e-01  6.13324052e-01  6.19256325e-01  6.25564352e-01
  6.25636542e-01  6.28567896e-01  6.28719910e-01  6.29849867e-01
  6.30542617e-01  6.35420003e-01  6.38755062e-01  6.51434551e-01
  6.52013055e-01  6.54153347e-01  6.57211414e-01  6.66865510e-01
  6.67481417e-01  6.67710487e-01  6.77770655e-01  6.88005678e-01
  6.89979371e-01  6.90422465e-01  6.90873787e-01  6.93207470e-01
  7.00196772e-01  7.03025443e-01  7.04780236e-01  7.05118602e-01
  7.06071643e-01  7.15720808e-01  7.17693876e-01  7.18158154e-01
  7.25750796e-01  7.30025759e-01  7.39030995e-01  7.40887508e-01
  7.48605730e-01  7.59144861e-01  7.61768642e-01  7.62523303e-01
  7.75697514e-01  7.76145948e-01  7.77531739e-01  7.86758319e-01
  7.89665244e-01  8.02438845e-01  8.02720656e-01  8.04691220e-01
  8.11582456e-01  8.33641400e-01  8.37561898e-01  8.39264145e-01
  8.49778928e-01  8.51278267e-01  8.59628819e-01  8.60252227e-01
  8.60765443e-01  8.61592337e-01  8.68374093e-01  8.81511709e-01
  9.04984424e-01  9.13866481e-01  9.31802735e-01  9.40311487e-01
  9.46876729e-01  9.60840689e-01  9.76855400e-01  1.01247695e+00
  1.04763167e+00]

  warnings.warn(

2022-11-03 10:54:04,120:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.25448534e-01 -1.02178988e-01 -9.25320985e-02 -8.23837777e-02
 -3.46948307e-02 -3.19310139e-02 -2.06193416e-02 -2.04698806e-02
 -1.70856290e-02 -1.70657183e-02 -1.64704926e-02 -1.51453411e-02
 -1.50641249e-02 -1.40654283e-02 -1.39537638e-02 -1.17957284e-02
 -1.15875295e-02 -9.79913033e-03 -8.51589777e-03 -8.22607581e-03
 -7.87795152e-03 -6.35794668e-03 -6.21775187e-03 -6.06110361e-03
 -6.01092001e-03 -5.78832924e-03 -5.49206111e-03 -5.30373418e-03
 -5.08866225e-03 -4.46835529e-03 -4.37678164e-03 -4.24783829e-03
 -4.10444158e-03 -3.96377283e-03 -3.40502420e-03 -2.92953798e-03
 -2.58998180e-03 -2.15467858e-03 -1.74114534e-03 -1.66459828e-03
 -1.52062834e-03 -8.35103764e-04 -5.09090989e-04  1.57517910e-04
  1.97593663e-04  6.96476156e-04  1.07659411e-03  1.49065573e-03
  1.59044196e-03  2.57161084e-03  3.05876624e-03  3.08512402e-03
  3.50587770e-03  3.59346348e-03  4.16654894e-03  4.38587775e-03
  4.46349151e-03  4.64460181e-03  5.11070485e-03  5.15074618e-03
  5.22960565e-03  5.62194462e-03  6.02739571e-03  7.70767860e-03
  8.03615756e-03  8.43531231e-03  8.67281232e-03  8.68141339e-03
  8.72498280e-03  8.80141686e-03  8.96849144e-03  9.23106967e-03
  9.47280218e-03  9.47799022e-03  1.05058172e-02  1.07102164e-02
  1.08973279e-02  1.10390685e-02  1.10983534e-02  1.17489794e-02
  1.20790463e-02  1.25941382e-02  1.27972822e-02  1.28195102e-02
  1.32620043e-02  1.37912799e-02  1.45450803e-02  1.47749638e-02
  1.51931035e-02  1.62164034e-02  1.65966209e-02  1.88378152e-02
  1.92988693e-02  1.95898620e-02  1.95922341e-02  1.99027176e-02
  2.08637921e-02  2.22196824e-02  2.24577972e-02  2.26464172e-02
  2.42031790e-02  2.48388079e-02  2.49723523e-02  2.53774588e-02
  2.72078909e-02  2.76031595e-02  2.77379416e-02  2.78393153e-02
  2.85072065e-02  2.95824349e-02  3.05089946e-02  3.06568157e-02
  3.07734782e-02  3.17014771e-02  3.19519657e-02  3.27470012e-02
  3.49211271e-02  3.58826468e-02  3.64065211e-02  3.65432152e-02
  3.70892607e-02  3.84076443e-02  3.84182807e-02  3.89361560e-02
  3.95124895e-02  3.96518401e-02  3.98551701e-02  4.03081564e-02
  4.29107548e-02  4.36601697e-02  4.47898772e-02  4.50125172e-02
  4.51691072e-02  4.59630267e-02  4.62559400e-02  4.64936322e-02
  4.68418545e-02  4.84790593e-02  4.85232214e-02  4.92200367e-02
  4.99330072e-02  5.11667441e-02  5.14836259e-02  5.23308229e-02
  5.23396847e-02  5.27455331e-02  5.32486458e-02  5.37787790e-02
  5.38432521e-02  5.50448254e-02  5.58062915e-02  5.71858185e-02
  5.75100070e-02  5.77206974e-02  5.85838468e-02  5.89893145e-02
  5.90090932e-02  6.12037733e-02  6.18841742e-02  6.26022269e-02
  6.35589913e-02  6.50768605e-02  6.60516245e-02  6.64712974e-02
  6.97239794e-02  7.01588392e-02  7.03796708e-02  7.05887741e-02
  7.15375881e-02  7.18705127e-02  7.31638217e-02  7.33957563e-02
  7.35288501e-02  7.35926776e-02  7.43377285e-02  7.61374757e-02
  7.63802411e-02  7.65394262e-02  7.69117236e-02  7.74994172e-02
  7.81937895e-02  7.98830392e-02  8.22022422e-02  8.25189075e-02
  8.41065684e-02  8.45840197e-02  8.50698517e-02  8.75078235e-02
  9.02642255e-02  9.10786522e-02  9.15700489e-02  9.30729209e-02
  9.33981332e-02  9.37490886e-02  9.55885527e-02  9.58740112e-02
  9.71870836e-02  9.71917394e-02  9.81838696e-02  9.81873238e-02
  1.00215549e-01  1.00228095e-01  1.02827477e-01  1.03383531e-01
  1.04055952e-01  1.04480659e-01  1.04931043e-01  1.05181650e-01
  1.06739983e-01  1.08786026e-01  1.09199063e-01  1.09951824e-01
  1.10003062e-01  1.10332451e-01  1.11051893e-01  1.12500796e-01
  1.12797856e-01  1.12858381e-01  1.13324407e-01  1.13775308e-01
  1.14636448e-01  1.15093624e-01  1.15261642e-01  1.19045862e-01
  1.19124618e-01  1.19571365e-01  1.22054949e-01  1.22717872e-01
  1.23347876e-01  1.25227654e-01  1.32826824e-01  1.33932582e-01
  1.34088450e-01  1.36218899e-01  1.36838319e-01  1.36943526e-01
  1.38154819e-01  1.39363742e-01  1.41649121e-01  1.43602657e-01
  1.44835542e-01  1.45122861e-01  1.47936994e-01  1.51732184e-01
  1.51812271e-01  1.53473688e-01  1.53689699e-01  1.55238164e-01
  1.56513873e-01  1.58865950e-01  1.59724374e-01  1.60544781e-01
  1.61325346e-01  1.62092638e-01  1.63106096e-01  1.64142346e-01
  1.64318619e-01  1.67761526e-01  1.67820979e-01  1.67995802e-01
  1.70343888e-01  1.71625718e-01  1.71834007e-01  1.78458542e-01
  1.78579744e-01  1.81697766e-01  1.83425521e-01  1.87175281e-01
  1.89556948e-01  1.92585093e-01  1.93088879e-01  1.94810623e-01
  1.95775370e-01  1.98860241e-01  2.00347080e-01  2.02567393e-01
  2.02576526e-01  2.02613706e-01  2.02840664e-01  2.11291463e-01
  2.12700005e-01  2.14859571e-01  2.17917515e-01  2.26028401e-01
  2.28278711e-01  2.29048451e-01  2.34052918e-01  2.35420929e-01
  2.35830938e-01  2.36838617e-01  2.39682331e-01  2.39869557e-01
  2.42134397e-01  2.42613492e-01  2.45392810e-01  2.45979902e-01
  2.47184906e-01  2.47404802e-01  2.49416221e-01  2.53993719e-01
  2.54265578e-01  2.55411645e-01  2.55841371e-01  2.56010460e-01
  2.56987260e-01  2.59797809e-01  2.65496152e-01  2.65744891e-01
  2.66694451e-01  2.70171772e-01  2.71910689e-01  2.72040314e-01
  2.79432279e-01  2.81262775e-01  2.82256179e-01  2.82826846e-01
  2.84478693e-01  2.88651205e-01  2.89927871e-01  2.90846647e-01
  2.91149368e-01  2.92561920e-01  2.96228285e-01  3.00361434e-01
  3.04663123e-01  3.05204649e-01  3.13798477e-01  3.13983903e-01
  3.15672252e-01  3.16385017e-01  3.18987057e-01  3.20675775e-01
  3.21711755e-01  3.25417380e-01  3.25508336e-01  3.30800675e-01
  3.33479268e-01  3.37567156e-01  3.38564472e-01  3.45023744e-01
  3.45059254e-01  3.48704899e-01  3.49757302e-01  3.64458662e-01
  3.68644287e-01  3.81178509e-01  3.82349751e-01  3.83159790e-01
  3.84699660e-01  3.89045596e-01  3.90236277e-01  3.91099642e-01
  3.92193730e-01  3.92455435e-01  3.95612394e-01  3.99326304e-01
  3.99529250e-01  4.01120667e-01  4.01353425e-01  4.07888338e-01
  4.08627073e-01  4.08635701e-01  4.09172142e-01  4.10603766e-01
  4.12372009e-01  4.17219010e-01  4.18010423e-01  4.22087940e-01
  4.25169880e-01  4.26139881e-01  4.33267595e-01  4.36306948e-01
  4.38316043e-01  4.44702809e-01  4.45629862e-01  4.51024505e-01
  4.51417035e-01  4.52010683e-01  4.52464586e-01  4.59618126e-01
  4.65189463e-01  4.65256266e-01  4.66794891e-01  4.67138422e-01
  4.77411303e-01  4.79197376e-01  4.80459549e-01  4.81106261e-01
  4.81301500e-01  4.90569370e-01  4.92827346e-01  4.96236146e-01
  4.99937540e-01  5.05123034e-01  5.09412061e-01  5.12078873e-01
  5.15774851e-01  5.20481093e-01  5.31430130e-01  5.36340402e-01
  5.36895675e-01  5.38290538e-01  5.40426650e-01  5.48976984e-01
  5.51233957e-01  5.58745018e-01  5.65243017e-01  5.70466008e-01
  5.82937947e-01  5.85795112e-01  5.86884186e-01  5.96342619e-01
  5.96656328e-01  5.97661480e-01  5.99712870e-01  6.01246945e-01
  6.03937517e-01  6.04750282e-01  6.05212860e-01  6.11313143e-01
  6.18688055e-01  6.19360428e-01  6.19947347e-01  6.26643769e-01
  6.28245826e-01  6.31202244e-01  6.35305557e-01  6.43276547e-01
  6.43895644e-01  6.50189663e-01  6.50737366e-01  6.51376881e-01
  6.58078282e-01  6.62276316e-01  6.71339924e-01  6.79469075e-01
  6.86559639e-01  6.86667641e-01  6.95079916e-01  7.03293264e-01
  7.06046412e-01  7.06906170e-01  7.07259931e-01  7.28302270e-01
  7.28481439e-01  7.35731405e-01  7.36067011e-01  7.39231662e-01
  7.46945759e-01  7.49041147e-01  7.50579407e-01  7.56053915e-01
  7.56995972e-01  7.57573836e-01  7.58247321e-01  7.64000162e-01
  7.64608714e-01  7.70988871e-01  7.92081693e-01  7.95599251e-01
  8.01426942e-01  8.04381148e-01  8.07615983e-01  8.09711833e-01
  8.10733970e-01  8.12469034e-01  8.28401569e-01  8.30046717e-01
  8.41044092e-01  8.51026940e-01  8.60890959e-01  8.67025959e-01
  8.75699907e-01  8.79263775e-01  8.86748652e-01  8.89815577e-01
  8.90894284e-01  8.95872222e-01  9.00339052e-01  9.06263102e-01
  9.15307687e-01  9.28503278e-01  9.29036418e-01  9.30632274e-01
  9.38592202e-01  9.52819301e-01  9.52940055e-01  9.55193773e-01
  9.60050111e-01  9.68704803e-01  9.69505388e-01  9.93472929e-01]

  warnings.warn(

2022-11-03 10:54:04,136:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.21486673e-02 -7.36277409e-02 -5.36416713e-02 -4.82933355e-02
 -4.51977521e-02 -4.29231607e-02 -3.88686782e-02 -3.78852491e-02
 -3.55550976e-02 -3.47342645e-02 -3.06634403e-02 -3.06111585e-02
 -2.99536981e-02 -2.95909709e-02 -2.89644713e-02 -2.85591656e-02
 -2.17643879e-02 -2.12008792e-02 -1.97913791e-02 -1.94539080e-02
 -1.91360648e-02 -1.85383019e-02 -1.85044853e-02 -1.84831182e-02
 -1.83154724e-02 -1.75277269e-02 -1.74607278e-02 -1.33123333e-02
 -1.27466374e-02 -1.23719959e-02 -1.23344661e-02 -1.19495003e-02
 -1.12039410e-02 -1.06433128e-02 -1.04107776e-02 -1.01559761e-02
 -9.97262342e-03 -8.88146795e-03 -8.63646599e-03 -8.27003805e-03
 -7.85277160e-03 -6.55142273e-03 -6.53183749e-03 -6.50177263e-03
 -6.17863718e-03 -5.37588563e-03 -5.32580585e-03 -5.08205879e-03
 -4.73646015e-03 -4.66502082e-03 -4.14242217e-03 -3.95028202e-03
 -3.58004913e-03 -3.19880275e-03 -5.36825534e-04 -5.33362663e-04
 -4.60414502e-05  2.76679257e-04  7.31666733e-04  7.94907195e-04
  8.99885491e-04  9.52746757e-04  1.00899710e-03  1.18974841e-03
  1.29339042e-03  1.57533335e-03  2.08315995e-03  2.08861571e-03
  2.31661507e-03  2.54678028e-03  2.57949585e-03  3.39644977e-03
  3.56636202e-03  3.88099651e-03  4.73325208e-03  5.60088192e-03
  6.42850036e-03  6.68462866e-03  7.10696173e-03  7.14593540e-03
  8.06334145e-03  8.21852913e-03  8.38065519e-03  8.92109584e-03
  9.17059375e-03  9.49716181e-03  9.50645386e-03  1.08526116e-02
  1.13900063e-02  1.17560709e-02  1.29034882e-02  1.43129021e-02
  1.49374551e-02  1.58751873e-02  1.63058205e-02  1.63737817e-02
  1.65772564e-02  1.68738339e-02  1.70090401e-02  1.71462430e-02
  1.76603951e-02  1.77536912e-02  1.79746477e-02  1.81513881e-02
  1.81892013e-02  1.91068747e-02  1.91939099e-02  1.96643857e-02
  1.97253917e-02  2.00089382e-02  2.01198662e-02  2.01808541e-02
  2.06528686e-02  2.07230211e-02  2.11905239e-02  2.18418300e-02
  2.22319795e-02  2.57411055e-02  2.79676271e-02  2.81651103e-02
  2.85345588e-02  2.88736289e-02  2.92650206e-02  3.06184681e-02
  3.15295901e-02  3.17603772e-02  3.24339100e-02  3.35040531e-02
  3.37034527e-02  3.47809218e-02  3.66067021e-02  4.09421111e-02
  4.21211613e-02  4.26402078e-02  4.34422908e-02  4.35069415e-02
  4.36884336e-02  4.72907029e-02  4.73765825e-02  4.79629952e-02
  4.94205616e-02  5.02026854e-02  5.20101095e-02  5.32945163e-02
  5.34902514e-02  5.35253196e-02  5.53871949e-02  5.54719218e-02
  5.57303627e-02  5.68248221e-02  5.72778847e-02  5.81034940e-02
  5.94651560e-02  5.97501850e-02  6.13865688e-02  6.14876318e-02
  6.21895814e-02  6.27345888e-02  6.28975479e-02  6.33454165e-02
  6.39502828e-02  6.79164885e-02  6.81523753e-02  6.83437424e-02
  6.91020512e-02  6.97324852e-02  7.08668706e-02  7.11601462e-02
  7.40526119e-02  7.45247400e-02  7.54521172e-02  7.65670081e-02
  7.66970621e-02  7.68467825e-02  7.69121627e-02  7.75733320e-02
  7.75900290e-02  7.82182773e-02  7.88649702e-02  8.14487202e-02
  8.17375228e-02  8.24134280e-02  8.65426170e-02  8.77213126e-02
  8.95544937e-02  9.00124593e-02  9.04907655e-02  9.14799945e-02
  9.40468965e-02  9.50350616e-02  9.52542518e-02  9.60490388e-02
  9.67992792e-02  9.68422885e-02  9.71240855e-02  9.79459239e-02
  1.01816888e-01  1.02753439e-01  1.03953378e-01  1.03992387e-01
  1.05256761e-01  1.06783309e-01  1.08063608e-01  1.09409435e-01
  1.09511552e-01  1.09697669e-01  1.11177740e-01  1.12428069e-01
  1.14499437e-01  1.16088654e-01  1.18108423e-01  1.18948883e-01
  1.20937388e-01  1.21085378e-01  1.21852850e-01  1.26591735e-01
  1.26952647e-01  1.28921747e-01  1.29033984e-01  1.30056855e-01
  1.31367549e-01  1.31580726e-01  1.32716329e-01  1.33404158e-01
  1.33545207e-01  1.34695903e-01  1.34863330e-01  1.35535968e-01
  1.35740625e-01  1.37624583e-01  1.39550909e-01  1.40397771e-01
  1.43484486e-01  1.44021788e-01  1.45644817e-01  1.45817348e-01
  1.46564687e-01  1.48131277e-01  1.48371011e-01  1.49160962e-01
  1.49267506e-01  1.51436311e-01  1.51862032e-01  1.53937631e-01
  1.54084173e-01  1.54293699e-01  1.55406611e-01  1.60717167e-01
  1.60792717e-01  1.62163318e-01  1.62617525e-01  1.66289669e-01
  1.71033651e-01  1.72527647e-01  1.75978468e-01  1.80002760e-01
  1.81092915e-01  1.85737247e-01  1.86944336e-01  1.86961315e-01
  1.88970490e-01  1.88987995e-01  1.95629523e-01  1.97866725e-01
  1.98344465e-01  2.01044654e-01  2.01461415e-01  2.01621485e-01
  2.02120196e-01  2.05516638e-01  2.06405545e-01  2.07138091e-01
  2.08350743e-01  2.08362421e-01  2.12891193e-01  2.12978850e-01
  2.13157621e-01  2.14148367e-01  2.14925793e-01  2.15353011e-01
  2.16391742e-01  2.16875744e-01  2.21279515e-01  2.21666230e-01
  2.25981446e-01  2.26248675e-01  2.28021083e-01  2.29449618e-01
  2.31518528e-01  2.32258711e-01  2.33609970e-01  2.34954077e-01
  2.34991035e-01  2.36776211e-01  2.37383608e-01  2.38032759e-01
  2.38874340e-01  2.41625990e-01  2.45573287e-01  2.47677504e-01
  2.48183703e-01  2.48440554e-01  2.57962889e-01  2.59031238e-01
  2.63352578e-01  2.63506823e-01  2.65509583e-01  2.66496317e-01
  2.68022201e-01  2.70132524e-01  2.70274227e-01  2.74601317e-01
  2.75049971e-01  2.75667413e-01  2.77215599e-01  2.79263701e-01
  2.80308863e-01  2.80716019e-01  2.81586174e-01  2.84170746e-01
  2.84522237e-01  2.84645777e-01  2.86446321e-01  2.86892311e-01
  2.87579469e-01  2.92943022e-01  3.00458205e-01  3.05397210e-01
  3.08689272e-01  3.11428264e-01  3.14286567e-01  3.14890473e-01
  3.15627323e-01  3.16295551e-01  3.19867276e-01  3.20425095e-01
  3.20482367e-01  3.21718687e-01  3.26871209e-01  3.29286574e-01
  3.36038080e-01  3.37230029e-01  3.37339149e-01  3.37616059e-01
  3.43884809e-01  3.50841243e-01  3.55775104e-01  3.56948859e-01
  3.62218243e-01  3.65298842e-01  3.71072507e-01  3.71585743e-01
  3.72448485e-01  3.77669054e-01  3.79587738e-01  3.82568242e-01
  3.83686497e-01  3.88321850e-01  3.94875714e-01  4.00505339e-01
  4.00532560e-01  4.01917635e-01  4.04752134e-01  4.05392384e-01
  4.05452834e-01  4.08123826e-01  4.10726530e-01  4.12400975e-01
  4.19458897e-01  4.19878766e-01  4.19964620e-01  4.20345598e-01
  4.24172572e-01  4.25397776e-01  4.29789949e-01  4.31523323e-01
  4.31560039e-01  4.36004598e-01  4.40089651e-01  4.40140018e-01
  4.41525515e-01  4.42272252e-01  4.45637810e-01  4.54051783e-01
  4.54558206e-01  4.58280551e-01  4.63827457e-01  4.68110763e-01
  4.78056935e-01  4.78669485e-01  4.79439588e-01  4.79774417e-01
  4.84001897e-01  4.84524982e-01  4.90860283e-01  4.96854361e-01
  5.07644633e-01  5.09095450e-01  5.11101588e-01  5.13282838e-01
  5.16472144e-01  5.17595120e-01  5.17913274e-01  5.18470308e-01
  5.23360963e-01  5.25910517e-01  5.31004303e-01  5.34381117e-01
  5.44171717e-01  5.48434330e-01  5.55739068e-01  5.65235012e-01
  5.75493513e-01  5.76582922e-01  5.77397356e-01  5.92649335e-01
  6.02531936e-01  6.13275355e-01  6.13414913e-01  6.15400721e-01
  6.15957464e-01  6.17513458e-01  6.25628210e-01  6.25755335e-01
  6.29047373e-01  6.42394971e-01  6.43960493e-01  6.48516772e-01
  6.50224104e-01  6.59337533e-01  6.62004139e-01  6.66887369e-01
  6.67943133e-01  6.69770413e-01  6.75534697e-01  6.85169286e-01
  6.87844407e-01  6.93140291e-01  6.97350785e-01  6.99032538e-01
  7.00576957e-01  7.07728399e-01  7.13760876e-01  7.14468876e-01
  7.18193929e-01  7.21655303e-01  7.22389149e-01  7.23465932e-01
  7.24552214e-01  7.25376894e-01  7.25634395e-01  7.30893493e-01
  7.31407104e-01  7.33336049e-01  7.38250689e-01  7.42230588e-01
  7.43294457e-01  7.46149770e-01  7.48267382e-01  7.64612898e-01
  7.65895580e-01  7.66108154e-01  7.69646989e-01  7.76375052e-01
  7.77403348e-01  7.91732625e-01  7.95522866e-01  8.13098565e-01
  8.21791366e-01  8.27170640e-01  8.27244854e-01  8.35979535e-01
  8.40584718e-01  8.41108407e-01  8.47545770e-01  8.56127603e-01
  8.56750944e-01  8.68110692e-01  8.72219820e-01  8.75531609e-01
  8.94180258e-01  9.05345374e-01  9.12084648e-01  9.27930163e-01
  9.33396302e-01  9.49048233e-01  9.49249629e-01  9.79701771e-01
  9.80238568e-01  9.85902368e-01  9.98245757e-01  1.00611979e+00
  1.02604243e+00]

  warnings.warn(

2022-11-03 10:54:04,151:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.47027486e-02 -4.22752996e-02 -3.61680716e-02 -3.02899701e-02
 -2.97144171e-02 -2.81489756e-02 -2.75392445e-02 -2.53182195e-02
 -2.41280808e-02 -2.22889370e-02 -1.96484401e-02 -1.84623127e-02
 -1.82296409e-02 -1.81639767e-02 -1.69342759e-02 -1.68315228e-02
 -1.55243607e-02 -1.40508281e-02 -1.18812644e-02 -1.09861960e-02
 -1.04077178e-02 -9.60024081e-03 -9.56620376e-03 -9.24818550e-03
 -8.83044485e-03 -8.74136205e-03 -7.95818722e-03 -7.19552416e-03
 -6.61123450e-03 -6.60045604e-03 -5.99499052e-03 -5.85577032e-03
 -5.70664799e-03 -5.58754871e-03 -4.42240485e-03 -4.29777560e-03
 -3.92961421e-03 -3.25248859e-03 -3.10003609e-03 -2.49252849e-03
 -1.94531942e-03 -1.77226611e-03 -7.55857550e-04  6.80615835e-04
  1.31755579e-03  1.37304714e-03  1.45577414e-03  1.57999021e-03
  1.69082828e-03  1.95119804e-03  2.07507777e-03  2.16214198e-03
  2.63054041e-03  3.00923187e-03  3.34253623e-03  3.34554812e-03
  3.42153989e-03  3.80223328e-03  4.21182930e-03  4.32072313e-03
  4.63183230e-03  4.84834690e-03  5.13621002e-03  5.18774444e-03
  5.22924411e-03  5.23664804e-03  5.33869758e-03  5.45164380e-03
  5.59928374e-03  5.64759180e-03  5.98414552e-03  6.03364522e-03
  6.35973612e-03  6.62557448e-03  6.67997683e-03  7.18511249e-03
  7.31714516e-03  7.97055869e-03  8.05987745e-03  8.10137986e-03
  8.15415038e-03  8.39687161e-03  8.50118363e-03  8.95878850e-03
  9.63894048e-03  1.05330067e-02  1.07127264e-02  1.16549997e-02
  1.21136864e-02  1.27599164e-02  1.29592388e-02  1.42877249e-02
  1.45590515e-02  1.46845145e-02  1.55911513e-02  1.57890561e-02
  1.58374273e-02  1.62344854e-02  1.64606502e-02  1.64981777e-02
  1.66265255e-02  1.80831480e-02  1.86030421e-02  1.97804267e-02
  2.15162714e-02  2.20443816e-02  2.21148786e-02  2.30175041e-02
  2.35043811e-02  2.38534295e-02  2.43084630e-02  2.44747897e-02
  2.52105497e-02  2.54491211e-02  2.57872215e-02  2.62250509e-02
  2.66245322e-02  2.73663120e-02  2.76517513e-02  2.79096779e-02
  2.80141052e-02  2.92585181e-02  2.92652671e-02  3.08953400e-02
  3.14786825e-02  3.20922593e-02  3.23063469e-02  3.32753148e-02
  3.36545859e-02  3.36774832e-02  3.43241503e-02  3.49939643e-02
  3.55653687e-02  3.56366505e-02  3.74934305e-02  3.84498878e-02
  3.88368797e-02  3.88940013e-02  3.90824919e-02  4.00094129e-02
  4.00151349e-02  4.12224041e-02  4.14684728e-02  4.24956954e-02
  4.56067045e-02  4.64680394e-02  4.68525127e-02  4.81432226e-02
  4.83191856e-02  4.84966726e-02  4.86955695e-02  4.99513797e-02
  4.99754816e-02  5.02669426e-02  5.15723710e-02  5.18012375e-02
  5.27113498e-02  5.33151869e-02  5.46389816e-02  5.46594728e-02
  5.61547342e-02  5.76990409e-02  5.80659470e-02  5.83022486e-02
  5.83701789e-02  5.98009506e-02  6.07375945e-02  6.08834883e-02
  6.10314186e-02  6.20163578e-02  6.76483641e-02  6.83733999e-02
  6.87584496e-02  6.87869289e-02  6.88487882e-02  7.29254099e-02
  7.39451446e-02  7.44817898e-02  7.51570568e-02  7.68203324e-02
  7.93712418e-02  7.95147872e-02  8.01583890e-02  8.06114576e-02
  8.36334014e-02  8.39745566e-02  8.40895988e-02  8.58359320e-02
  8.99724560e-02  9.12548831e-02  9.31060922e-02  9.39972709e-02
  9.48257809e-02  9.87199634e-02  1.00759892e-01  1.01853964e-01
  1.06664046e-01  1.07208628e-01  1.07884957e-01  1.08113776e-01
  1.08239273e-01  1.14423760e-01  1.15117624e-01  1.16663718e-01
  1.17221574e-01  1.17621245e-01  1.19436867e-01  1.21329392e-01
  1.25696121e-01  1.25734036e-01  1.27022049e-01  1.28919125e-01
  1.30551414e-01  1.31628824e-01  1.32194676e-01  1.34207578e-01
  1.35119624e-01  1.35993640e-01  1.36032922e-01  1.36498353e-01
  1.36982409e-01  1.37195902e-01  1.37543937e-01  1.39033686e-01
  1.41383290e-01  1.42441920e-01  1.44155955e-01  1.47947688e-01
  1.49048120e-01  1.49572605e-01  1.50365029e-01  1.54354941e-01
  1.54765316e-01  1.59257852e-01  1.60356713e-01  1.61883391e-01
  1.62115799e-01  1.63476640e-01  1.65082566e-01  1.67097556e-01
  1.67569838e-01  1.70602582e-01  1.71622995e-01  1.75575323e-01
  1.76014997e-01  1.78616888e-01  1.79888700e-01  1.82744191e-01
  1.83549901e-01  1.84046116e-01  1.84308117e-01  1.84340002e-01
  1.86748475e-01  1.87498389e-01  1.90562087e-01  1.91453228e-01
  1.91470481e-01  1.93044709e-01  1.95769179e-01  1.96883627e-01
  1.99475427e-01  2.00108568e-01  2.01649453e-01  2.02559669e-01
  2.04563732e-01  2.06003620e-01  2.07785933e-01  2.08557279e-01
  2.11589546e-01  2.12237747e-01  2.12813378e-01  2.13120396e-01
  2.13805497e-01  2.14353354e-01  2.17636084e-01  2.21285035e-01
  2.21625875e-01  2.22624408e-01  2.26501482e-01  2.29555951e-01
  2.30279541e-01  2.32910587e-01  2.33355020e-01  2.33601087e-01
  2.37193454e-01  2.37868931e-01  2.39267729e-01  2.42675741e-01
  2.46105121e-01  2.46952691e-01  2.49945778e-01  2.52332929e-01
  2.53000271e-01  2.56144072e-01  2.57321820e-01  2.65086870e-01
  2.65259263e-01  2.65565123e-01  2.65749673e-01  2.73945338e-01
  2.75956900e-01  2.76189295e-01  2.81033655e-01  2.81953433e-01
  2.83424676e-01  2.88647985e-01  2.89191317e-01  2.89894196e-01
  2.92014788e-01  2.93099705e-01  2.95967402e-01  2.97042912e-01
  3.02349097e-01  3.05772907e-01  3.06690784e-01  3.08062695e-01
  3.17357667e-01  3.19467149e-01  3.28488046e-01  3.33390096e-01
  3.33587736e-01  3.35021541e-01  3.35130973e-01  3.35620707e-01
  3.36803114e-01  3.37213266e-01  3.37361358e-01  3.37799071e-01
  3.41294763e-01  3.50328982e-01  3.51082542e-01  3.51681684e-01
  3.52494855e-01  3.61899978e-01  3.63804282e-01  3.72389644e-01
  3.77348604e-01  3.79287566e-01  3.80006047e-01  3.84087656e-01
  3.84457672e-01  3.87088234e-01  3.91032709e-01  3.91396299e-01
  3.93087313e-01  3.95372124e-01  3.97331003e-01  3.97512996e-01
  4.02473031e-01  4.03564059e-01  4.06742263e-01  4.12767428e-01
  4.12819334e-01  4.13314048e-01  4.14745026e-01  4.14884112e-01
  4.15169939e-01  4.15389159e-01  4.18471521e-01  4.18728799e-01
  4.22198790e-01  4.25068977e-01  4.29100882e-01  4.32391480e-01
  4.33056513e-01  4.33820709e-01  4.35083419e-01  4.36856084e-01
  4.37311994e-01  4.41516778e-01  4.45815750e-01  4.45844707e-01
  4.48486952e-01  4.48816015e-01  4.49117956e-01  4.51591319e-01
  4.54103690e-01  4.54429324e-01  4.54708180e-01  4.56792834e-01
  4.56904838e-01  4.58221358e-01  4.65693030e-01  4.67294552e-01
  4.71168749e-01  4.74016862e-01  4.81938740e-01  4.82048752e-01
  4.84146631e-01  4.86445547e-01  4.86521066e-01  4.88505184e-01
  4.92789010e-01  4.99242790e-01  5.00190303e-01  5.02582745e-01
  5.08291297e-01  5.14797366e-01  5.16541330e-01  5.30355513e-01
  5.32100223e-01  5.32137961e-01  5.33699893e-01  5.34139890e-01
  5.34959780e-01  5.36711487e-01  5.46672643e-01  5.48802205e-01
  5.51876063e-01  5.55074695e-01  5.59837421e-01  5.64584436e-01
  5.68139879e-01  5.69636166e-01  5.80020717e-01  5.81105002e-01
  5.86320865e-01  5.88567854e-01  5.94600564e-01  5.96343343e-01
  5.97123027e-01  6.00102318e-01  6.09183761e-01  6.19338481e-01
  6.23831154e-01  6.41302141e-01  6.48361266e-01  6.49423200e-01
  6.50151489e-01  6.51130726e-01  6.58861936e-01  6.61042312e-01
  6.61272426e-01  6.63803618e-01  6.65821641e-01  6.67372101e-01
  6.69176860e-01  6.74792123e-01  6.78382535e-01  6.81468968e-01
  6.91392051e-01  6.91663417e-01  6.93287771e-01  6.93947884e-01
  6.95641153e-01  6.97552205e-01  6.98472553e-01  7.07499815e-01
  7.12115072e-01  7.26736712e-01  7.30880351e-01  7.32059818e-01
  7.35728362e-01  7.37532527e-01  7.40489123e-01  7.44486459e-01
  7.46457636e-01  7.47666182e-01  7.58960927e-01  7.59520174e-01
  7.60502647e-01  7.61121471e-01  7.64375888e-01  7.64920459e-01
  7.77912975e-01  7.80398029e-01  7.80730076e-01  7.94963545e-01
  7.95372019e-01  8.09669122e-01  8.16851178e-01  8.23993874e-01
  8.24001933e-01  8.54430889e-01  8.60561224e-01  8.83635513e-01
  9.05503981e-01  9.09263192e-01  9.10627202e-01  9.11018160e-01
  9.13460599e-01  9.14103521e-01  9.18424056e-01  9.34722634e-01
  9.39537049e-01  9.46679299e-01  9.57852941e-01  1.03336527e+00
  1.03772716e+00  1.04491827e+00  1.05376189e+00]

  warnings.warn(

2022-11-03 10:54:04,151:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.61105161e-02 -6.26147069e-02 -5.41747777e-02 -4.77955368e-02
 -4.44655496e-02 -3.40772119e-02 -3.33726185e-02 -3.25075431e-02
 -2.55950254e-02 -2.55062411e-02 -2.48281243e-02 -2.23959380e-02
 -1.90894398e-02 -1.90718607e-02 -1.86322501e-02 -1.84067744e-02
 -1.59421358e-02 -1.50820934e-02 -1.40147534e-02 -1.39154536e-02
 -1.35506398e-02 -1.31074099e-02 -1.29520880e-02 -1.29298329e-02
 -1.22615372e-02 -1.20368415e-02 -1.09537397e-02 -1.04852130e-02
 -8.84023427e-03 -8.82039463e-03 -8.76326433e-03 -8.17455474e-03
 -7.88739620e-03 -7.32502497e-03 -7.15969283e-03 -6.21751430e-03
 -5.92587284e-03 -5.61431907e-03 -5.34815534e-03 -4.69926241e-03
 -3.83142515e-03 -3.72810180e-03 -3.69842548e-03 -3.67327049e-03
 -3.58471018e-03 -2.63649042e-03 -2.61330922e-03 -2.56285569e-03
 -1.78724772e-03 -1.69519517e-03 -1.28642058e-03 -1.26294428e-03
 -1.20545606e-03 -1.08973243e-03 -6.75251987e-04 -3.73453895e-04
  1.01143806e-04  1.09350719e-03  1.32707323e-03  1.59314541e-03
  2.28120818e-03  2.47266511e-03  3.54055799e-03  3.86881847e-03
  3.90173181e-03  4.37908298e-03  4.83536679e-03  5.69869597e-03
  5.75068190e-03  6.11781139e-03  6.91952194e-03  7.96110184e-03
  8.74300311e-03  8.86594870e-03  9.27696158e-03  9.37633562e-03
  9.67087843e-03  9.88106150e-03  9.98695415e-03  1.05069418e-02
  1.11223305e-02  1.11499375e-02  1.13053846e-02  1.21936930e-02
  1.24294155e-02  1.34509264e-02  1.35700197e-02  1.37093692e-02
  1.39799734e-02  1.46113773e-02  1.47832085e-02  1.51986774e-02
  1.59243409e-02  1.66043061e-02  1.69451087e-02  1.71181286e-02
  1.77092170e-02  1.81348040e-02  1.87153635e-02  1.87748806e-02
  2.00477565e-02  2.07076348e-02  2.15664459e-02  2.16312346e-02
  2.18148829e-02  2.23617508e-02  2.27041619e-02  2.42124131e-02
  2.42628960e-02  2.58390657e-02  2.68831658e-02  2.76743587e-02
  2.80061560e-02  2.87791008e-02  2.92883729e-02  2.93299236e-02
  2.93602008e-02  2.95350770e-02  2.99680939e-02  3.00063231e-02
  3.01651129e-02  3.02726518e-02  3.13533579e-02  3.16007933e-02
  3.18430336e-02  3.21113434e-02  3.35993541e-02  3.36716120e-02
  3.37452645e-02  3.41709008e-02  3.47910502e-02  3.48341553e-02
  3.55603272e-02  3.57878706e-02  3.58642882e-02  3.70065825e-02
  3.95685931e-02  4.02331459e-02  4.03077356e-02  4.03453377e-02
  4.36240806e-02  4.39217156e-02  4.55346064e-02  4.55734318e-02
  4.64819433e-02  4.64940080e-02  4.68073555e-02  4.69335137e-02
  4.81368041e-02  4.83314506e-02  4.92537062e-02  4.93477758e-02
  5.09295393e-02  5.13400273e-02  5.19040178e-02  5.39607501e-02
  5.44789034e-02  5.45509206e-02  5.65047884e-02  5.69882545e-02
  5.83979687e-02  6.06847694e-02  6.26195170e-02  6.28133072e-02
  6.36195603e-02  6.48037528e-02  6.66254939e-02  6.68883846e-02
  6.95570493e-02  6.96237132e-02  6.96643795e-02  7.00714172e-02
  7.29999730e-02  7.38179285e-02  7.55276943e-02  7.75031419e-02
  7.94823499e-02  7.96612082e-02  8.16142828e-02  8.40194413e-02
  8.54936616e-02  8.61108583e-02  8.77249146e-02  9.12020118e-02
  9.16616428e-02  9.18912140e-02  9.26524174e-02  9.35084919e-02
  9.38780765e-02  9.53561656e-02  9.56774996e-02  9.60435242e-02
  9.61580335e-02  9.67761090e-02  9.71731851e-02  9.93438315e-02
  9.96767642e-02  1.00695528e-01  1.03412774e-01  1.03976134e-01
  1.04054172e-01  1.09990584e-01  1.10335174e-01  1.10631164e-01
  1.12766021e-01  1.12847643e-01  1.14390940e-01  1.15858876e-01
  1.16746145e-01  1.16774635e-01  1.18901220e-01  1.20607161e-01
  1.21940550e-01  1.22857179e-01  1.24384880e-01  1.26359700e-01
  1.26594011e-01  1.29128697e-01  1.30139484e-01  1.32947371e-01
  1.33790782e-01  1.34521740e-01  1.37487691e-01  1.40254650e-01
  1.41147020e-01  1.43270030e-01  1.44233380e-01  1.47080049e-01
  1.48232819e-01  1.48584165e-01  1.50667212e-01  1.52968436e-01
  1.53670891e-01  1.55596291e-01  1.57400951e-01  1.58927228e-01
  1.59669312e-01  1.64438452e-01  1.65113643e-01  1.69459593e-01
  1.69664093e-01  1.70697729e-01  1.71511352e-01  1.72997215e-01
  1.74077879e-01  1.75784036e-01  1.76179362e-01  1.76781903e-01
  1.76889202e-01  1.80455465e-01  1.81752906e-01  1.88480207e-01
  1.88755876e-01  1.89455748e-01  1.90445806e-01  1.90640961e-01
  1.91472827e-01  1.92970575e-01  1.94443319e-01  1.95571325e-01
  1.99208591e-01  2.03685144e-01  2.03818747e-01  2.03902590e-01
  2.05803279e-01  2.06976329e-01  2.10380157e-01  2.10782329e-01
  2.11468994e-01  2.15082527e-01  2.15922709e-01  2.19658825e-01
  2.20040152e-01  2.22652637e-01  2.23983893e-01  2.25370273e-01
  2.26031652e-01  2.27841506e-01  2.29988128e-01  2.32227114e-01
  2.34452616e-01  2.34676289e-01  2.37355601e-01  2.37829781e-01
  2.39080064e-01  2.40804771e-01  2.44646089e-01  2.44892599e-01
  2.44895982e-01  2.47845957e-01  2.50475612e-01  2.52824188e-01
  2.56998062e-01  2.57719701e-01  2.62281999e-01  2.66423325e-01
  2.67536759e-01  2.68745430e-01  2.76482204e-01  2.81486608e-01
  2.83009944e-01  2.83517600e-01  2.83685294e-01  2.88436191e-01
  2.90970164e-01  2.91396468e-01  2.92513770e-01  2.92614530e-01
  2.94238331e-01  2.95867077e-01  2.99178814e-01  2.99309832e-01
  3.00991242e-01  3.01102220e-01  3.01595391e-01  3.05400355e-01
  3.11583945e-01  3.13349072e-01  3.13483247e-01  3.16363156e-01
  3.17079614e-01  3.21841120e-01  3.25826053e-01  3.26679705e-01
  3.29833780e-01  3.31165428e-01  3.36333430e-01  3.39644225e-01
  3.40583000e-01  3.42615049e-01  3.44412005e-01  3.49006361e-01
  3.53423149e-01  3.54548817e-01  3.57631147e-01  3.58531145e-01
  3.67534357e-01  3.68833145e-01  3.69380698e-01  3.70266085e-01
  3.70656922e-01  3.71223289e-01  3.71516131e-01  3.73976216e-01
  3.83176703e-01  3.83542861e-01  3.88329968e-01  3.90596418e-01
  3.91336174e-01  3.92300423e-01  3.98067445e-01  4.01537357e-01
  4.01756770e-01  4.07086959e-01  4.07663481e-01  4.11963942e-01
  4.20619846e-01  4.21666508e-01  4.22200320e-01  4.24485479e-01
  4.29517981e-01  4.29975709e-01  4.33851873e-01  4.34395298e-01
  4.38996935e-01  4.41020649e-01  4.41229063e-01  4.43225445e-01
  4.45243553e-01  4.45630599e-01  4.52613790e-01  4.55679867e-01
  4.56288856e-01  4.56682500e-01  4.57786089e-01  4.62067865e-01
  4.66458818e-01  4.66481830e-01  4.67463145e-01  4.69032857e-01
  4.72309816e-01  4.75010243e-01  4.84027053e-01  4.85895330e-01
  4.89044764e-01  4.94985744e-01  4.97491388e-01  4.99227754e-01
  5.04674279e-01  5.06493757e-01  5.08918096e-01  5.11060047e-01
  5.12399920e-01  5.23940753e-01  5.27562343e-01  5.31892320e-01
  5.32435964e-01  5.36907850e-01  5.43377718e-01  5.51034088e-01
  5.52590620e-01  5.55475254e-01  5.56895043e-01  5.57355662e-01
  5.58625879e-01  5.58721536e-01  5.61349785e-01  5.61981448e-01
  5.64275159e-01  5.69009428e-01  5.69777057e-01  5.71560548e-01
  5.73796434e-01  5.75086044e-01  5.84526209e-01  5.90201810e-01
  5.90758257e-01  5.94376315e-01  5.99329112e-01  6.07092310e-01
  6.07211174e-01  6.13355461e-01  6.13608827e-01  6.13683843e-01
  6.14825234e-01  6.17913331e-01  6.18355756e-01  6.18548603e-01
  6.20656198e-01  6.31409308e-01  6.34298876e-01  6.40546955e-01
  6.41209404e-01  6.43194586e-01  6.48270140e-01  6.56712207e-01
  6.63859282e-01  6.64650065e-01  6.66443434e-01  6.73233322e-01
  6.74148151e-01  6.76711767e-01  6.81845039e-01  6.83437815e-01
  6.84888168e-01  6.87050847e-01  6.91381574e-01  6.92118523e-01
  7.05537156e-01  7.07206694e-01  7.11434344e-01  7.13868872e-01
  7.15329210e-01  7.18538291e-01  7.19923678e-01  7.22777277e-01
  7.29161943e-01  7.30275422e-01  7.34940363e-01  7.39810786e-01
  7.52990880e-01  7.74310966e-01  7.92625340e-01  7.94287144e-01
  8.00192125e-01  8.10394943e-01  8.13664170e-01  8.41849498e-01
  8.53190256e-01  8.53512266e-01  8.62972110e-01  8.71334799e-01
  8.74561863e-01  8.76733001e-01  8.82172387e-01  8.85422483e-01
  8.94623912e-01  8.94910763e-01  9.04932608e-01  9.12911629e-01
  9.16327757e-01  9.25409668e-01  9.31265165e-01  9.39995567e-01
  9.48626063e-01  9.49701268e-01  9.63704938e-01  9.69264459e-01
  9.77584684e-01  1.00180940e+00  1.01200194e+00  1.02074726e+00
  1.02214296e+00]

  warnings.warn(

2022-11-03 10:54:04,167:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-5.53333484e-02 -4.34169458e-02 -3.49539427e-02 -3.18987117e-02
 -3.10251592e-02 -3.03999302e-02 -2.85122435e-02 -2.67543941e-02
 -2.42584923e-02 -2.39770952e-02 -2.34871870e-02 -2.26474251e-02
 -2.13403678e-02 -2.08215346e-02 -2.06236459e-02 -1.90294675e-02
 -1.69299364e-02 -1.67532396e-02 -1.64294679e-02 -1.62455245e-02
 -1.59990787e-02 -1.56315966e-02 -1.47528501e-02 -1.37174475e-02
 -1.35371332e-02 -1.35167338e-02 -1.23529583e-02 -1.16136685e-02
 -1.09704462e-02 -1.08133965e-02 -1.03776406e-02 -1.02696337e-02
 -1.01201530e-02 -9.84591356e-03 -9.75115730e-03 -9.14951088e-03
 -8.60822385e-03 -8.53947260e-03 -8.36303619e-03 -8.31357148e-03
 -7.38732146e-03 -7.32047027e-03 -6.34688558e-03 -5.86499200e-03
 -5.28954679e-03 -5.27030061e-03 -4.99914446e-03 -4.91544154e-03
 -4.37052864e-03 -4.09641078e-03 -3.74618600e-03 -3.45337587e-03
 -3.39304909e-03 -3.33930760e-03 -2.60412994e-03 -2.47271262e-03
 -2.25416016e-03 -2.08803119e-03 -1.69326267e-03 -1.65719672e-03
 -1.52432397e-03 -7.68099174e-04 -4.78729819e-04  7.74301819e-05
  1.05275967e-04  4.59628221e-04  6.44595967e-04  9.65362718e-04
  1.05211864e-03  1.11094926e-03  1.22675080e-03  1.27453885e-03
  1.64465618e-03  2.08480184e-03  2.16913953e-03  2.79145478e-03
  2.84740997e-03  2.86874252e-03  3.52229244e-03  5.09628446e-03
  5.24930609e-03  5.56052503e-03  5.73918488e-03  5.99826487e-03
  6.13646290e-03  6.23990206e-03  6.27708719e-03  7.02054756e-03
  7.26948105e-03  7.60267158e-03  7.69797672e-03  7.70870704e-03
  7.90169171e-03  8.18862007e-03  8.45597606e-03  8.83571926e-03
  8.92200756e-03  9.87087219e-03  1.01954098e-02  1.03919109e-02
  1.12289546e-02  1.12405890e-02  1.34531661e-02  1.41440142e-02
  1.49745759e-02  1.51402008e-02  1.53005520e-02  1.56939756e-02
  1.65651952e-02  1.71153340e-02  1.79384038e-02  1.85070021e-02
  1.87247227e-02  1.90608157e-02  2.01383728e-02  2.01707563e-02
  2.04365921e-02  2.14625394e-02  2.15861298e-02  2.23952231e-02
  2.29003275e-02  2.29476980e-02  2.53029570e-02  2.54049597e-02
  2.55545024e-02  2.57423250e-02  2.58258496e-02  2.64270888e-02
  2.71465149e-02  2.76285891e-02  2.85833010e-02  2.90494723e-02
  2.93798966e-02  2.98884459e-02  3.11890446e-02  3.19416957e-02
  3.20713984e-02  3.25616407e-02  3.36893177e-02  3.44830563e-02
  3.52672560e-02  3.53505532e-02  3.77549668e-02  3.84486448e-02
  3.88519050e-02  3.99592911e-02  4.12663494e-02  4.30788798e-02
  4.50259414e-02  4.57686097e-02  4.62805843e-02  4.71721134e-02
  4.79607660e-02  4.94060757e-02  4.97705943e-02  5.15835010e-02
  5.18704379e-02  5.26185390e-02  5.29434298e-02  5.33336709e-02
  5.34510934e-02  5.38754786e-02  5.61873408e-02  5.70429906e-02
  5.73258659e-02  5.81009320e-02  5.83418986e-02  5.91477971e-02
  5.95395360e-02  6.04759956e-02  6.07929846e-02  6.17700224e-02
  6.31700328e-02  6.48241427e-02  6.48573767e-02  6.49640885e-02
  6.54031663e-02  6.57744656e-02  6.61870695e-02  6.62545673e-02
  6.64991603e-02  6.73438304e-02  6.77294949e-02  6.92585984e-02
  7.00609906e-02  7.07646230e-02  7.21314039e-02  7.25339581e-02
  7.25941102e-02  7.27106111e-02  7.38403428e-02  7.44321282e-02
  7.58090293e-02  7.58712009e-02  7.72552356e-02  7.75647168e-02
  7.82912665e-02  7.88840029e-02  8.25273722e-02  8.36392477e-02
  8.82111969e-02  8.94671525e-02  9.04208517e-02  9.28112642e-02
  9.49171319e-02  9.60657410e-02  9.61771958e-02  9.76547541e-02
  9.80383967e-02  9.86703637e-02  9.96680235e-02  9.98281278e-02
  1.00113959e-01  1.02707287e-01  1.03751833e-01  1.03802468e-01
  1.06037048e-01  1.06195389e-01  1.06432552e-01  1.06685825e-01
  1.08436963e-01  1.08922908e-01  1.09986049e-01  1.12489294e-01
  1.12941215e-01  1.13548004e-01  1.14607623e-01  1.14980915e-01
  1.15015087e-01  1.16731519e-01  1.19315261e-01  1.20521936e-01
  1.25158827e-01  1.29919752e-01  1.30387161e-01  1.30534165e-01
  1.31294755e-01  1.31472728e-01  1.31721980e-01  1.31762131e-01
  1.31860701e-01  1.32974025e-01  1.34526990e-01  1.36546222e-01
  1.39887047e-01  1.41041952e-01  1.44189277e-01  1.44336733e-01
  1.44957197e-01  1.52272905e-01  1.55937732e-01  1.56680355e-01
  1.60374869e-01  1.61883732e-01  1.61991564e-01  1.64282504e-01
  1.71002210e-01  1.71789028e-01  1.73473610e-01  1.73563045e-01
  1.77488875e-01  1.81913338e-01  1.82407399e-01  1.83317812e-01
  1.83660139e-01  1.85084559e-01  1.85690229e-01  1.87447382e-01
  1.88373650e-01  1.88897914e-01  1.89587228e-01  1.91325423e-01
  1.95779582e-01  1.96632311e-01  1.98798764e-01  2.01602021e-01
  2.02854818e-01  2.08207440e-01  2.08708330e-01  2.08755200e-01
  2.08921228e-01  2.10435159e-01  2.15227723e-01  2.16407741e-01
  2.17395543e-01  2.19476300e-01  2.19904701e-01  2.20188487e-01
  2.29256197e-01  2.29963546e-01  2.32376516e-01  2.34853726e-01
  2.36360213e-01  2.37491541e-01  2.38633514e-01  2.45660462e-01
  2.47098246e-01  2.47498623e-01  2.49931486e-01  2.52863331e-01
  2.57396391e-01  2.61577338e-01  2.62401682e-01  2.66298041e-01
  2.67671115e-01  2.68518051e-01  2.68639222e-01  2.68894502e-01
  2.70114391e-01  2.72342973e-01  2.76156921e-01  2.78191179e-01
  2.83545290e-01  2.83614007e-01  2.88148032e-01  2.91006339e-01
  2.93779516e-01  2.93954712e-01  2.94617426e-01  2.98541367e-01
  3.01696426e-01  3.03776527e-01  3.08062289e-01  3.08750327e-01
  3.11806779e-01  3.12473936e-01  3.15537826e-01  3.16721167e-01
  3.16939957e-01  3.19020819e-01  3.20722328e-01  3.21268430e-01
  3.21525855e-01  3.23326858e-01  3.27301966e-01  3.28130932e-01
  3.31908108e-01  3.34426467e-01  3.38523851e-01  3.42900162e-01
  3.44564211e-01  3.45952753e-01  3.48812214e-01  3.49632951e-01
  3.50876639e-01  3.57517413e-01  3.59438101e-01  3.64671155e-01
  3.65112981e-01  3.67420056e-01  3.67521879e-01  3.70188777e-01
  3.71452460e-01  3.80129626e-01  3.80468784e-01  3.81526703e-01
  3.84136640e-01  3.85177286e-01  3.86356510e-01  3.88206369e-01
  3.89858761e-01  3.90039593e-01  3.93107392e-01  3.98383717e-01
  4.01015749e-01  4.06316453e-01  4.06374741e-01  4.08556239e-01
  4.27145230e-01  4.32326014e-01  4.35355135e-01  4.36273845e-01
  4.40852232e-01  4.42418776e-01  4.45159079e-01  4.56229571e-01
  4.56428722e-01  4.58010342e-01  4.58483892e-01  4.61269522e-01
  4.61313893e-01  4.62561333e-01  4.63344179e-01  4.65712185e-01
  4.69367383e-01  4.78327307e-01  4.81050049e-01  4.82117539e-01
  4.82752003e-01  4.90591834e-01  4.91285275e-01  4.91444661e-01
  4.95689251e-01  5.01822301e-01  5.03505626e-01  5.05309451e-01
  5.08509787e-01  5.21739139e-01  5.31810034e-01  5.33558228e-01
  5.35056411e-01  5.37817624e-01  5.41974714e-01  5.43382342e-01
  5.62052806e-01  5.67730871e-01  5.68802954e-01  5.69606947e-01
  5.69888982e-01  5.85094112e-01  5.88573747e-01  5.89444828e-01
  5.90948671e-01  5.97495172e-01  5.97595993e-01  5.98273465e-01
  5.98908695e-01  6.02676711e-01  6.03053598e-01  6.04793755e-01
  6.06624667e-01  6.09555861e-01  6.10164472e-01  6.15523067e-01
  6.16527778e-01  6.17638994e-01  6.23143749e-01  6.24564402e-01
  6.25878571e-01  6.34664180e-01  6.52417060e-01  6.53100859e-01
  6.57623597e-01  6.60838693e-01  6.60916127e-01  6.64244336e-01
  6.64317137e-01  6.65183728e-01  6.65649047e-01  6.73311985e-01
  6.86318603e-01  6.93466365e-01  7.03451039e-01  7.09708294e-01
  7.11250431e-01  7.11866819e-01  7.17209582e-01  7.28350452e-01
  7.30252701e-01  7.34350160e-01  7.34684965e-01  7.40606138e-01
  7.43683546e-01  7.52236364e-01  7.54351189e-01  7.56007765e-01
  7.58502331e-01  7.66829956e-01  7.67313679e-01  7.69154524e-01
  7.69630915e-01  7.73562625e-01  7.79641016e-01  7.80645692e-01
  7.93163722e-01  8.10470666e-01  8.10585480e-01  8.12477709e-01
  8.15961785e-01  8.17737437e-01  8.20218900e-01  8.24291330e-01
  8.31846547e-01  8.38098340e-01  8.42988550e-01  8.53154029e-01
  8.58609291e-01  8.71972100e-01  8.74957036e-01  8.85197392e-01
  8.89206052e-01  8.94044583e-01  8.96567266e-01  8.97130939e-01
  8.99367846e-01  9.10079427e-01  9.18186551e-01  9.40687365e-01
  9.43059382e-01  9.44509275e-01  9.63035728e-01  9.76722132e-01]

  warnings.warn(

2022-11-03 10:54:04,278:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.18892453e-02 -4.56802354e-02 -4.03074329e-02 -3.05495444e-02
 -2.96149984e-02 -2.81429408e-02 -2.75463905e-02 -2.72756805e-02
 -2.71854297e-02 -2.47779670e-02 -2.35303851e-02 -2.02344298e-02
 -1.43076425e-02 -1.37384039e-02 -1.36103421e-02 -1.33933068e-02
 -1.30808101e-02 -1.29053816e-02 -1.17996041e-02 -1.16290668e-02
 -9.09885730e-03 -8.67523151e-03 -8.18665580e-03 -7.69547231e-03
 -7.55623486e-03 -6.33491816e-03 -6.05561819e-03 -5.87938193e-03
 -5.84027795e-03 -4.79385399e-03 -4.42143935e-03 -4.28854776e-03
 -3.83563191e-03 -2.98706231e-03 -2.12946147e-03 -1.78783763e-03
 -1.26831275e-03 -1.12667953e-03 -8.63319472e-04 -8.28045631e-04
 -8.20618896e-04 -3.10908300e-04 -6.67923862e-05  1.36266366e-04
  2.10753079e-04  1.09475338e-03  1.22210011e-03  1.56518781e-03
  1.77899581e-03  2.15889947e-03  2.61018289e-03  2.70926861e-03
  3.94571105e-03  4.43953608e-03  4.62908041e-03  4.78699851e-03
  5.38766575e-03  6.28469385e-03  6.34354696e-03  6.40128977e-03
  6.70339269e-03  6.75336404e-03  6.84901576e-03  7.68306574e-03
  8.19483872e-03  8.44144197e-03  8.62977040e-03  8.76535148e-03
  9.08129992e-03  9.67782252e-03  1.00304729e-02  1.01591857e-02
  1.03493467e-02  1.12322993e-02  1.19173445e-02  1.23163981e-02
  1.24987248e-02  1.47534335e-02  1.54318758e-02  1.57599525e-02
  1.64762981e-02  1.87599487e-02  1.88128336e-02  1.93254592e-02
  2.05593043e-02  2.05628374e-02  2.06432792e-02  2.11744962e-02
  2.19494570e-02  2.23388754e-02  2.24080495e-02  2.27256724e-02
  2.50356537e-02  2.50724892e-02  2.65958661e-02  2.68737492e-02
  2.69407157e-02  2.70018765e-02  2.71388052e-02  2.72758307e-02
  2.73548813e-02  2.82945050e-02  3.01924216e-02  3.02028237e-02
  3.10295048e-02  3.15364186e-02  3.17375745e-02  3.33453133e-02
  3.37836514e-02  3.40457953e-02  3.42158161e-02  3.44384587e-02
  3.50262987e-02  3.56912003e-02  3.67400547e-02  3.70699417e-02
  3.77890884e-02  3.86113626e-02  4.03527399e-02  4.13264565e-02
  4.16435011e-02  4.21164912e-02  4.22868709e-02  4.27782954e-02
  4.27837342e-02  4.35224858e-02  4.52909752e-02  4.59922132e-02
  4.67261542e-02  4.89240612e-02  4.98769312e-02  4.99567988e-02
  5.04468349e-02  5.06628820e-02  5.33431680e-02  5.40016570e-02
  5.66613423e-02  5.77528434e-02  5.90023308e-02  5.90694761e-02
  5.92113470e-02  5.94753120e-02  5.97329223e-02  5.99020528e-02
  6.02396309e-02  6.22570061e-02  6.28240598e-02  6.36432272e-02
  6.46526792e-02  6.64915829e-02  6.69400586e-02  6.69631698e-02
  6.70097240e-02  6.83933328e-02  6.91162863e-02  7.05718639e-02
  7.27645139e-02  7.50603217e-02  7.53483952e-02  7.67234185e-02
  7.76140018e-02  7.81429247e-02  7.97531246e-02  7.98122543e-02
  8.00608756e-02  8.00803665e-02  8.26844485e-02  8.41063300e-02
  8.56851054e-02  8.64520164e-02  8.69039247e-02  8.83437685e-02
  8.90757452e-02  8.97482065e-02  9.09712833e-02  9.14601037e-02
  9.21253428e-02  9.31694167e-02  9.45149448e-02  9.48049939e-02
  9.51449504e-02  9.62368085e-02  9.80298066e-02  9.87620388e-02
  9.99164864e-02  1.00623925e-01  1.02606903e-01  1.03176683e-01
  1.05300937e-01  1.05494510e-01  1.05906952e-01  1.05912391e-01
  1.06462048e-01  1.07300347e-01  1.07717960e-01  1.07841364e-01
  1.08109331e-01  1.10101188e-01  1.12174117e-01  1.12247762e-01
  1.16021889e-01  1.16063567e-01  1.17692835e-01  1.18331596e-01
  1.19762946e-01  1.20385523e-01  1.23027009e-01  1.25253394e-01
  1.25541074e-01  1.26337704e-01  1.27667085e-01  1.28581067e-01
  1.29374616e-01  1.29630014e-01  1.31762546e-01  1.31770103e-01
  1.33998558e-01  1.34250898e-01  1.35222858e-01  1.35435641e-01
  1.35569123e-01  1.36941283e-01  1.37603375e-01  1.37803494e-01
  1.38067715e-01  1.40411336e-01  1.41177889e-01  1.41645946e-01
  1.44357368e-01  1.45792386e-01  1.46396661e-01  1.47341186e-01
  1.47791253e-01  1.49100828e-01  1.50298456e-01  1.50397459e-01
  1.51072739e-01  1.55311492e-01  1.57902840e-01  1.59697562e-01
  1.59987997e-01  1.60517389e-01  1.64529166e-01  1.65651208e-01
  1.69126671e-01  1.70182354e-01  1.70454419e-01  1.70635583e-01
  1.74804657e-01  1.79272582e-01  1.79527249e-01  1.79879008e-01
  1.80242229e-01  1.84307012e-01  1.94738358e-01  1.95545811e-01
  1.95881001e-01  1.95904976e-01  1.96447232e-01  1.98136062e-01
  2.01205189e-01  2.07828054e-01  2.08469118e-01  2.08564619e-01
  2.08801838e-01  2.09006515e-01  2.09007161e-01  2.09343690e-01
  2.09518843e-01  2.09951146e-01  2.10197951e-01  2.15443995e-01
  2.17007814e-01  2.18047057e-01  2.18745752e-01  2.21108958e-01
  2.24077971e-01  2.24277603e-01  2.24511595e-01  2.27359869e-01
  2.27896658e-01  2.29441202e-01  2.29653340e-01  2.29812775e-01
  2.33990217e-01  2.35187250e-01  2.36163442e-01  2.37491114e-01
  2.38647902e-01  2.39041053e-01  2.39163992e-01  2.39755439e-01
  2.40150244e-01  2.41745179e-01  2.42820770e-01  2.44242660e-01
  2.44991181e-01  2.48095251e-01  2.48722977e-01  2.52366556e-01
  2.54352113e-01  2.55223533e-01  2.59386297e-01  2.61156204e-01
  2.64150738e-01  2.65374957e-01  2.66815777e-01  2.68835731e-01
  2.69180570e-01  2.71323974e-01  2.74958450e-01  2.76797720e-01
  2.78360512e-01  2.80802973e-01  2.82709091e-01  2.83051681e-01
  2.84972985e-01  2.87777166e-01  2.88209288e-01  2.91009550e-01
  2.93639234e-01  2.98728905e-01  3.00037133e-01  3.02825143e-01
  3.03111251e-01  3.15145134e-01  3.23096263e-01  3.24559306e-01
  3.24895793e-01  3.27550343e-01  3.30772692e-01  3.32075109e-01
  3.35919473e-01  3.37924891e-01  3.40523326e-01  3.41379310e-01
  3.43072788e-01  3.49236882e-01  3.53017021e-01  3.53712400e-01
  3.56922421e-01  3.57277232e-01  3.57375621e-01  3.58715828e-01
  3.60684159e-01  3.61722697e-01  3.63949956e-01  3.66307901e-01
  3.67331785e-01  3.71395467e-01  3.76035381e-01  3.76436300e-01
  3.85058716e-01  3.85247404e-01  3.88492664e-01  3.92455986e-01
  3.92508219e-01  3.93464194e-01  3.97502160e-01  4.03296227e-01
  4.04357893e-01  4.05257787e-01  4.05630573e-01  4.10454916e-01
  4.10753186e-01  4.13302873e-01  4.13629513e-01  4.16922614e-01
  4.17489505e-01  4.18396647e-01  4.19242119e-01  4.24810315e-01
  4.27226899e-01  4.27389246e-01  4.28048194e-01  4.28444810e-01
  4.31822036e-01  4.32723504e-01  4.35458486e-01  4.37609097e-01
  4.39372568e-01  4.44059796e-01  4.47758546e-01  4.49539857e-01
  4.58964695e-01  4.59512914e-01  4.63320098e-01  4.63735564e-01
  4.64018319e-01  4.64525696e-01  4.65019351e-01  4.66613408e-01
  4.72783208e-01  4.79096780e-01  4.83111429e-01  4.91553347e-01
  4.92902344e-01  5.00106153e-01  5.02849182e-01  5.03305609e-01
  5.10700332e-01  5.14095290e-01  5.14873418e-01  5.16428668e-01
  5.28138884e-01  5.29997048e-01  5.37828193e-01  5.38815715e-01
  5.44555085e-01  5.47238918e-01  5.48429427e-01  5.50400972e-01
  5.57802312e-01  5.60166706e-01  5.65067486e-01  5.71701610e-01
  5.71973845e-01  5.83703758e-01  5.86489544e-01  5.92257617e-01
  5.92599338e-01  5.93711174e-01  6.01022989e-01  6.02558439e-01
  6.14095797e-01  6.21302263e-01  6.25277421e-01  6.26385909e-01
  6.32316193e-01  6.34695259e-01  6.48535196e-01  6.50932235e-01
  6.52960710e-01  6.57144178e-01  6.57999684e-01  6.58753180e-01
  6.68783249e-01  6.69866899e-01  6.75258965e-01  6.78899744e-01
  6.80636243e-01  6.80970440e-01  6.88863469e-01  6.93254342e-01
  6.94081984e-01  6.94303683e-01  6.95498858e-01  7.01514851e-01
  7.08600291e-01  7.26148472e-01  7.27048770e-01  7.28456368e-01
  7.35815391e-01  7.40058179e-01  7.44014658e-01  7.45742088e-01
  7.48401526e-01  7.48688398e-01  7.66822793e-01  7.68067728e-01
  7.68284249e-01  7.70852701e-01  7.79960189e-01  7.91349980e-01
  7.95672759e-01  7.98576262e-01  8.01075937e-01  8.03271814e-01
  8.08051157e-01  8.08693785e-01  8.11229562e-01  8.13052956e-01
  8.15998763e-01  8.17895838e-01  8.33589423e-01  8.38653307e-01
  8.45171079e-01  8.47402184e-01  8.51404566e-01  8.53141889e-01
  8.56760677e-01  8.72735734e-01  8.75468452e-01  9.00514874e-01
  9.13808532e-01  9.23996142e-01  9.44440600e-01  9.49323609e-01
  9.58516789e-01  9.68701205e-01  9.86734956e-01  1.05364425e+00
  1.07894827e+00]

  warnings.warn(

2022-11-03 10:54:04,278:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.76593258e-01 -6.13587992e-02 -5.40963368e-02 -5.35885899e-02
 -5.19645222e-02 -4.10951417e-02 -3.66443058e-02 -3.10975680e-02
 -2.98076455e-02 -2.91771396e-02 -2.52844419e-02 -2.36898816e-02
 -2.20200962e-02 -2.13856162e-02 -1.97075314e-02 -1.96422728e-02
 -1.89320636e-02 -1.86213249e-02 -1.81504393e-02 -1.73230165e-02
 -1.72878778e-02 -1.53815251e-02 -1.48630904e-02 -1.45708070e-02
 -1.38926600e-02 -1.28397227e-02 -1.27757595e-02 -1.25216152e-02
 -1.21391699e-02 -1.10174005e-02 -1.03717773e-02 -9.98282783e-03
 -9.42110046e-03 -9.30996084e-03 -8.48231159e-03 -6.16141735e-03
 -5.62117077e-03 -5.61155816e-03 -5.49333742e-03 -5.20620718e-03
 -4.56923888e-03 -4.25104494e-03 -4.21888941e-03 -4.21877458e-03
 -3.63022622e-03 -3.57744039e-03 -3.31204721e-03 -3.27321597e-03
 -2.57946721e-03 -2.17036439e-03 -1.74847944e-03 -1.00217076e-03
 -9.79731518e-04 -9.18376872e-04 -5.87182525e-04 -4.00333371e-04
 -2.60952786e-04  2.85834691e-04  5.14649059e-04  2.89995844e-03
  3.53069808e-03  4.10441096e-03  4.88695026e-03  5.01583644e-03
  5.20867407e-03  5.37996037e-03  6.40010059e-03  6.40259488e-03
  6.92845331e-03  7.11007274e-03  7.51901611e-03  8.36330082e-03
  8.44087375e-03  8.75130965e-03  8.83224852e-03  8.91662066e-03
  9.23972002e-03  9.33089560e-03  9.59520056e-03  9.77494595e-03
  1.01059435e-02  1.06801621e-02  1.13906590e-02  1.14240677e-02
  1.17361796e-02  1.23945599e-02  1.25573539e-02  1.26056015e-02
  1.33666007e-02  1.48229267e-02  1.51479930e-02  1.56753777e-02
  1.70520618e-02  1.74440962e-02  1.74991998e-02  1.82351220e-02
  1.83645556e-02  1.83857171e-02  1.84752034e-02  1.86249466e-02
  1.90680050e-02  1.92903401e-02  1.94328417e-02  1.99391939e-02
  2.00740482e-02  2.02674352e-02  2.05028822e-02  2.08412097e-02
  2.16028115e-02  2.56834268e-02  2.60023641e-02  2.62795788e-02
  2.67326094e-02  2.70743052e-02  2.76828973e-02  2.82560087e-02
  2.86129137e-02  2.90115833e-02  2.91079951e-02  2.92256421e-02
  2.99660314e-02  3.04226657e-02  3.08559185e-02  3.09665987e-02
  3.13125536e-02  3.17275312e-02  3.34820398e-02  3.59162413e-02
  3.98294408e-02  4.00177602e-02  4.09312310e-02  4.20509927e-02
  4.21467572e-02  4.32742477e-02  4.48794716e-02  4.58859729e-02
  4.63226408e-02  4.74258905e-02  4.75889723e-02  4.83471600e-02
  4.83891399e-02  4.85329982e-02  4.92638757e-02  4.96494802e-02
  5.09377213e-02  5.15352948e-02  5.21267024e-02  5.40055005e-02
  5.41217654e-02  5.44473747e-02  5.49986674e-02  5.85970723e-02
  5.89973571e-02  6.01694002e-02  6.03636780e-02  6.18447992e-02
  6.28096314e-02  6.28812862e-02  6.33508317e-02  6.36582254e-02
  6.38634512e-02  6.42458303e-02  6.53618289e-02  6.54049741e-02
  6.60640247e-02  6.87527570e-02  6.94708994e-02  7.27468497e-02
  7.35999901e-02  7.37397992e-02  7.42859303e-02  7.49006244e-02
  7.56449843e-02  7.56724827e-02  7.71149516e-02  7.77281540e-02
  7.81907771e-02  7.94368677e-02  8.16703228e-02  8.26941117e-02
  8.57855032e-02  8.58187114e-02  8.63859839e-02  8.71300055e-02
  8.82236146e-02  8.96183764e-02  9.00019492e-02  9.01053345e-02
  9.11852731e-02  9.12096925e-02  9.15110732e-02  9.23044785e-02
  9.29615447e-02  9.34911577e-02  9.42163549e-02  9.45651750e-02
  9.53852726e-02  9.85733489e-02  1.00065747e-01  1.00224416e-01
  1.03490750e-01  1.03642426e-01  1.03730915e-01  1.05161312e-01
  1.05598508e-01  1.05777773e-01  1.09510586e-01  1.10574226e-01
  1.13978228e-01  1.14325472e-01  1.15099560e-01  1.15553732e-01
  1.15813921e-01  1.19713246e-01  1.19971281e-01  1.20317918e-01
  1.22782922e-01  1.24125717e-01  1.25887516e-01  1.26862598e-01
  1.26990521e-01  1.27842980e-01  1.28970396e-01  1.29680204e-01
  1.30882413e-01  1.34855540e-01  1.36215782e-01  1.36844539e-01
  1.36969740e-01  1.38276302e-01  1.39225468e-01  1.39875112e-01
  1.40173568e-01  1.41425447e-01  1.41907475e-01  1.41916792e-01
  1.45408313e-01  1.45969994e-01  1.46572825e-01  1.47860339e-01
  1.48494887e-01  1.48551651e-01  1.48576901e-01  1.50115921e-01
  1.53957001e-01  1.55034579e-01  1.56534291e-01  1.59606674e-01
  1.59814753e-01  1.61668704e-01  1.61969960e-01  1.62615958e-01
  1.64264120e-01  1.64379712e-01  1.64689457e-01  1.65080665e-01
  1.65823645e-01  1.69608665e-01  1.70296950e-01  1.71562073e-01
  1.73141955e-01  1.77018303e-01  1.77608089e-01  1.80349907e-01
  1.80771866e-01  1.83063716e-01  1.83260847e-01  1.84431680e-01
  1.84782064e-01  1.85104764e-01  1.90151399e-01  1.96833453e-01
  1.98002523e-01  2.01503350e-01  2.03259726e-01  2.05816957e-01
  2.10604541e-01  2.13249209e-01  2.13826199e-01  2.17968112e-01
  2.19242893e-01  2.23701899e-01  2.24202639e-01  2.25785571e-01
  2.26273306e-01  2.27436387e-01  2.29244071e-01  2.31726367e-01
  2.32890656e-01  2.34914361e-01  2.35431951e-01  2.36637908e-01
  2.39347392e-01  2.39959782e-01  2.40507817e-01  2.40582146e-01
  2.40967084e-01  2.41540358e-01  2.43204777e-01  2.46665424e-01
  2.49273204e-01  2.52823764e-01  2.54736345e-01  2.60565644e-01
  2.60707243e-01  2.64393817e-01  2.65200094e-01  2.65847903e-01
  2.70042676e-01  2.72439536e-01  2.72629696e-01  2.72853530e-01
  2.73832103e-01  2.76322758e-01  2.76634808e-01  2.80236031e-01
  2.82134434e-01  2.82979566e-01  2.83992986e-01  2.91779213e-01
  2.94408165e-01  2.97080380e-01  2.98827097e-01  3.04989074e-01
  3.05317230e-01  3.05758173e-01  3.07081094e-01  3.08310244e-01
  3.09617629e-01  3.15727385e-01  3.18563552e-01  3.20116940e-01
  3.21511440e-01  3.23814935e-01  3.29519731e-01  3.31411323e-01
  3.36219655e-01  3.38066757e-01  3.40759666e-01  3.42073729e-01
  3.44024331e-01  3.45294260e-01  3.47589664e-01  3.50120235e-01
  3.51325964e-01  3.51871835e-01  3.62146412e-01  3.63869612e-01
  3.64498430e-01  3.71586716e-01  3.73787514e-01  3.74171303e-01
  3.77319246e-01  3.79665490e-01  3.79863326e-01  3.84512485e-01
  3.85311011e-01  3.88640833e-01  3.99974354e-01  4.05457210e-01
  4.09609335e-01  4.12841756e-01  4.20023138e-01  4.35369603e-01
  4.38252191e-01  4.38614632e-01  4.39501268e-01  4.39608372e-01
  4.41458694e-01  4.43480754e-01  4.44335659e-01  4.48162269e-01
  4.48863855e-01  4.50278046e-01  4.51050881e-01  4.56087015e-01
  4.64520536e-01  4.69778232e-01  4.70986656e-01  4.76005963e-01
  4.78209569e-01  4.78382909e-01  4.80702558e-01  4.81755816e-01
  4.83908280e-01  4.90697353e-01  4.97458087e-01  5.02493716e-01
  5.06114390e-01  5.12562628e-01  5.12977043e-01  5.14134513e-01
  5.14290862e-01  5.16553032e-01  5.18770044e-01  5.20899322e-01
  5.20946304e-01  5.22165961e-01  5.24191191e-01  5.25955910e-01
  5.26001059e-01  5.36111548e-01  5.44265354e-01  5.45036253e-01
  5.45093018e-01  5.45111557e-01  5.50377800e-01  5.56884782e-01
  5.79964568e-01  5.85347087e-01  5.91572226e-01  5.93728626e-01
  5.98614422e-01  6.06402587e-01  6.10711030e-01  6.11374615e-01
  6.17193496e-01  6.17757685e-01  6.18096212e-01  6.19953687e-01
  6.21197582e-01  6.21714235e-01  6.30663672e-01  6.51172082e-01
  6.53158919e-01  6.57136119e-01  6.60496533e-01  6.63091701e-01
  6.65656291e-01  6.66031368e-01  6.68298046e-01  6.69341522e-01
  6.74548213e-01  6.76228306e-01  7.02510034e-01  7.04591302e-01
  7.11272135e-01  7.15812474e-01  7.20923598e-01  7.25944524e-01
  7.27402392e-01  7.32178477e-01  7.36579896e-01  7.40168003e-01
  7.41868730e-01  7.42458890e-01  7.42474904e-01  7.43340368e-01
  7.43608177e-01  7.55088838e-01  7.57881878e-01  7.59053600e-01
  7.63689873e-01  7.63790202e-01  7.72413509e-01  7.74069609e-01
  7.83584127e-01  7.86102759e-01  7.86536698e-01  7.95953836e-01
  8.04192484e-01  8.06473422e-01  8.12095759e-01  8.34326607e-01
  8.42258587e-01  8.43699786e-01  8.48692041e-01  8.51636858e-01
  8.56134429e-01  8.63163933e-01  8.63922911e-01  8.66991203e-01
  8.74591057e-01  8.93758462e-01  8.94584396e-01  8.95299528e-01
  8.95818498e-01  8.99428235e-01  9.02929665e-01  9.07223663e-01
  9.07756820e-01  9.07834265e-01  9.28713779e-01  9.37288617e-01
  9.52742377e-01  9.73976182e-01  9.75119723e-01  9.96464505e-01
  1.00029975e+00  1.01669692e+00  1.16297790e+00]

  warnings.warn(

2022-11-03 10:54:07,078:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.40090365e-02 -4.98985329e-02 -4.73172154e-02 -4.53116030e-02
 -4.44484462e-02 -4.20104887e-02 -4.15771019e-02 -3.96267906e-02
 -3.52227510e-02 -3.43542338e-02 -3.26493034e-02 -3.20317853e-02
 -2.72458256e-02 -2.48602644e-02 -2.46400522e-02 -2.32216575e-02
 -2.27100540e-02 -2.22612799e-02 -2.12695788e-02 -2.11292494e-02
 -2.01389164e-02 -1.93534115e-02 -1.91066956e-02 -1.83356948e-02
 -1.77296938e-02 -1.69091163e-02 -1.68300153e-02 -1.67772564e-02
 -1.65481649e-02 -1.43852208e-02 -1.40813152e-02 -1.28637102e-02
 -1.24657455e-02 -1.24248128e-02 -1.23967615e-02 -1.22537990e-02
 -1.10729946e-02 -1.08865900e-02 -1.08733121e-02 -1.08634088e-02
 -1.01772277e-02 -1.00237866e-02 -9.82405183e-03 -8.83060659e-03
 -8.78265936e-03 -8.59209628e-03 -7.95820930e-03 -7.89081282e-03
 -7.20213087e-03 -7.14061957e-03 -6.74110797e-03 -6.09283963e-03
 -5.74146853e-03 -5.34425869e-03 -5.30443916e-03 -4.72690184e-03
 -4.42731181e-03 -4.13730593e-03 -3.52208072e-03 -3.22157199e-03
 -3.17393736e-03 -2.68826131e-03 -2.63338130e-03 -2.60217356e-03
 -2.39370034e-03 -2.20391345e-03 -2.08565948e-03 -1.89284525e-03
 -1.77002346e-03 -1.76002638e-03 -1.18161485e-03 -1.05689667e-03
  3.78809697e-05  1.07911602e-04  3.44816624e-04  4.96365597e-04
  5.51867914e-04  6.28705146e-04  7.15434047e-04  9.76819673e-04
  1.02410148e-03  1.23534304e-03  1.75480029e-03  1.82982543e-03
  2.71399616e-03  3.37594582e-03  3.56875285e-03  4.04920486e-03
  4.09421175e-03  4.96252899e-03  5.17774692e-03  5.70365108e-03
  5.77452678e-03  6.28606267e-03  6.99976921e-03  7.15245745e-03
  8.32082630e-03  8.40013797e-03  9.64191883e-03  9.69871397e-03
  9.83304599e-03  1.05781563e-02  1.08552038e-02  1.13350702e-02
  1.18169298e-02  1.19564088e-02  1.24300713e-02  1.25658645e-02
  1.34401903e-02  1.34408114e-02  1.41610613e-02  1.45007697e-02
  1.54409729e-02  1.62506811e-02  1.63796979e-02  1.67184555e-02
  2.04710820e-02  2.22107693e-02  2.25769219e-02  2.26158377e-02
  2.30736139e-02  2.33074354e-02  2.35920374e-02  2.52384841e-02
  2.56201202e-02  2.61858048e-02  2.64360579e-02  2.81129410e-02
  3.03012231e-02  3.05650305e-02  3.10070195e-02  3.14490300e-02
  3.19703036e-02  3.20704134e-02  3.23129174e-02  3.26380477e-02
  3.37235642e-02  3.40568463e-02  3.53102274e-02  3.59003459e-02
  3.64329798e-02  3.70980142e-02  3.77498730e-02  3.89356531e-02
  3.95787565e-02  4.09170988e-02  4.24118809e-02  4.24257271e-02
  4.25755228e-02  4.29975876e-02  4.41886014e-02  4.75725181e-02
  4.85541393e-02  4.89693056e-02  4.96801543e-02  4.99273161e-02
  5.02381305e-02  5.07912657e-02  5.17838827e-02  5.21447128e-02
  5.28221401e-02  5.29013870e-02  5.37205225e-02  5.44316075e-02
  5.50006391e-02  5.79565977e-02  5.90986397e-02  5.92285108e-02
  5.99670081e-02  6.08586776e-02  6.16718114e-02  6.30352290e-02
  6.39540907e-02  6.40274257e-02  6.44593407e-02  6.54561156e-02
  6.57152378e-02  6.62228027e-02  6.77172531e-02  7.13233918e-02
  7.20859373e-02  7.26096785e-02  7.41700597e-02  7.53640166e-02
  7.63835341e-02  7.67157479e-02  7.71255219e-02  7.82320989e-02
  7.95377192e-02  8.06411841e-02  8.35583874e-02  8.38930782e-02
  8.72043527e-02  8.80664364e-02  8.83864693e-02  8.84150971e-02
  8.98670854e-02  9.22258055e-02  9.30299054e-02  9.39566774e-02
  9.62036777e-02  9.63258169e-02  9.64377308e-02  9.66947323e-02
  9.75999347e-02  9.92401667e-02  9.96707943e-02  9.98154971e-02
  1.00909531e-01  1.01505092e-01  1.02554626e-01  1.06015718e-01
  1.06341989e-01  1.06563845e-01  1.06695306e-01  1.07294002e-01
  1.07748220e-01  1.08761141e-01  1.09427977e-01  1.09854217e-01
  1.12023440e-01  1.13812571e-01  1.15519008e-01  1.18007728e-01
  1.18229733e-01  1.20593275e-01  1.23247797e-01  1.23835592e-01
  1.24042255e-01  1.26064673e-01  1.26631434e-01  1.29362833e-01
  1.30293244e-01  1.31184171e-01  1.33671595e-01  1.35204603e-01
  1.37157085e-01  1.37193363e-01  1.37742655e-01  1.40268647e-01
  1.42287403e-01  1.42410102e-01  1.44233200e-01  1.44593701e-01
  1.46116561e-01  1.49905178e-01  1.52220457e-01  1.52270151e-01
  1.53983105e-01  1.54596616e-01  1.55294736e-01  1.60227742e-01
  1.61002620e-01  1.61524305e-01  1.61651029e-01  1.61711426e-01
  1.63714092e-01  1.67193687e-01  1.67243572e-01  1.67495782e-01
  1.68145567e-01  1.71162332e-01  1.71678788e-01  1.73157932e-01
  1.73599437e-01  1.75312884e-01  1.77113365e-01  1.77325536e-01
  1.79997558e-01  1.80100770e-01  1.80193191e-01  1.83009924e-01
  1.84315818e-01  1.87612480e-01  1.88009712e-01  1.88637511e-01
  1.88779240e-01  1.89954066e-01  1.90009544e-01  1.94730310e-01
  1.95551793e-01  1.96027521e-01  1.98036376e-01  1.98891810e-01
  2.01127540e-01  2.01973160e-01  2.07942888e-01  2.07957979e-01
  2.08459732e-01  2.08775320e-01  2.09828127e-01  2.10726067e-01
  2.10938795e-01  2.13815868e-01  2.14768926e-01  2.18855718e-01
  2.21784487e-01  2.25227209e-01  2.26666719e-01  2.35656200e-01
  2.36757586e-01  2.38416545e-01  2.38833114e-01  2.42237164e-01
  2.49411045e-01  2.50361291e-01  2.50912084e-01  2.51598861e-01
  2.60289183e-01  2.61411216e-01  2.64426157e-01  2.65189612e-01
  2.67446902e-01  2.69765219e-01  2.70699064e-01  2.72440043e-01
  2.73033386e-01  2.78983833e-01  2.86178378e-01  2.86286662e-01
  2.87239692e-01  2.88824395e-01  2.92130152e-01  2.94258289e-01
  2.95273598e-01  2.95513859e-01  3.00763554e-01  3.00773018e-01
  3.01117269e-01  3.03373135e-01  3.04542775e-01  3.08769752e-01
  3.12512253e-01  3.14188365e-01  3.16884974e-01  3.17162745e-01
  3.24626842e-01  3.29573784e-01  3.30706740e-01  3.36502548e-01
  3.42771660e-01  3.46632812e-01  3.47528803e-01  3.49981163e-01
  3.53665564e-01  3.63641517e-01  3.65966288e-01  3.69496138e-01
  3.71640177e-01  3.74283176e-01  3.77561089e-01  3.77669474e-01
  3.79098788e-01  3.82058662e-01  3.84685203e-01  3.84738624e-01
  3.88351294e-01  3.93556155e-01  3.96488661e-01  3.99356327e-01
  4.00106923e-01  4.00448056e-01  4.04109104e-01  4.04660443e-01
  4.07478139e-01  4.07529584e-01  4.10812362e-01  4.12193659e-01
  4.16868692e-01  4.19608249e-01  4.21470584e-01  4.22230028e-01
  4.30211310e-01  4.35038118e-01  4.36293162e-01  4.37855664e-01
  4.38080600e-01  4.40469489e-01  4.42962238e-01  4.45078376e-01
  4.48955688e-01  4.49082344e-01  4.52238679e-01  4.52926422e-01
  4.57192683e-01  4.61140955e-01  4.61881690e-01  4.63763469e-01
  4.70317379e-01  4.86676582e-01  4.90654916e-01  4.91393695e-01
  4.91861700e-01  4.94827386e-01  4.98543233e-01  4.99084858e-01
  5.08433853e-01  5.17079236e-01  5.22573311e-01  5.23184403e-01
  5.23687415e-01  5.25506656e-01  5.28876434e-01  5.32208055e-01
  5.33946425e-01  5.37140825e-01  5.38051443e-01  5.54054989e-01
  5.55078312e-01  5.65144729e-01  5.74173198e-01  5.76245038e-01
  5.78166333e-01  5.80273452e-01  5.95715279e-01  5.97621383e-01
  5.99364873e-01  5.99371960e-01  6.00712496e-01  6.09268636e-01
  6.11412599e-01  6.14691025e-01  6.15999847e-01  6.19915302e-01
  6.26976437e-01  6.27586211e-01  6.28424942e-01  6.32804833e-01
  6.41856830e-01  6.41907539e-01  6.55538486e-01  6.57267373e-01
  6.67273851e-01  6.72383875e-01  6.75806366e-01  6.78716365e-01
  6.81576754e-01  6.83528633e-01  6.91594198e-01  6.92648847e-01
  6.96271736e-01  7.00431199e-01  7.05906166e-01  7.08985526e-01
  7.13270692e-01  7.14516066e-01  7.15700866e-01  7.17789293e-01
  7.21205708e-01  7.21838426e-01  7.34474233e-01  7.39367525e-01
  7.41033755e-01  7.43633138e-01  7.45950431e-01  7.59939582e-01
  7.63347036e-01  7.64207751e-01  7.65477258e-01  7.79170993e-01
  7.83550856e-01  7.87543315e-01  7.91009577e-01  7.93303420e-01
  7.94314272e-01  7.97311266e-01  7.97515598e-01  8.07842851e-01
  8.21555150e-01  8.23596820e-01  8.31274771e-01  8.33315674e-01
  8.35790085e-01  8.37705480e-01  8.47900308e-01  8.50377675e-01
  8.61313249e-01  8.63043859e-01  8.63980319e-01  8.82455802e-01
  8.82742360e-01  8.85305005e-01  8.86169566e-01  8.95960790e-01
  9.06169433e-01  9.12936031e-01  9.18577365e-01  9.25414697e-01
  9.37189904e-01  9.54922214e-01  9.64926171e-01  9.65232923e-01
  9.81147575e-01]

  warnings.warn(

2022-11-03 10:54:07,143:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.98817729e-02 -2.96232106e-02 -2.36506902e-02 -2.27284499e-02
 -2.25709060e-02 -1.92426506e-02 -1.74841763e-02 -1.70848298e-02
 -1.66896001e-02 -1.51058132e-02 -1.46219977e-02 -1.39875853e-02
 -1.23428595e-02 -1.18860757e-02 -1.04791475e-02 -1.00290219e-02
 -9.10880031e-03 -8.93925560e-03 -8.77294734e-03 -8.37033612e-03
 -8.04907430e-03 -7.83178410e-03 -7.59299706e-03 -7.25243115e-03
 -6.37361652e-03 -6.25785627e-03 -6.09820703e-03 -4.78150983e-03
 -4.11471527e-03 -3.53402366e-03 -2.79227846e-03 -2.76356319e-03
 -2.56337964e-03 -1.42724808e-03 -1.24200916e-03  5.56131822e-04
  8.67363342e-04  1.40777926e-03  1.56573283e-03  1.61247627e-03
  1.94976261e-03  2.02774619e-03  2.08247474e-03  2.36555396e-03
  2.89179728e-03  2.94014955e-03  3.28704253e-03  3.69547117e-03
  4.25108089e-03  4.33076504e-03  5.11340787e-03  5.24017297e-03
  5.36117951e-03  5.39664724e-03  5.88474949e-03  7.05331681e-03
  7.12256154e-03  7.15544144e-03  7.83925777e-03  8.38637803e-03
  9.26136239e-03  9.27201264e-03  9.41648259e-03  1.11199416e-02
  1.22386597e-02  1.24652406e-02  1.27455739e-02  1.35358595e-02
  1.37891392e-02  1.43415080e-02  1.48264223e-02  1.50144352e-02
  1.53419687e-02  1.62641515e-02  1.65301490e-02  1.69876327e-02
  1.74540268e-02  1.79146604e-02  1.79985650e-02  1.81686661e-02
  1.93940209e-02  1.94017778e-02  2.00449316e-02  2.04946484e-02
  2.07827981e-02  2.19805308e-02  2.22812656e-02  2.27175462e-02
  2.29343731e-02  2.39215394e-02  2.57744663e-02  2.58640490e-02
  2.65047774e-02  2.72741694e-02  2.76490125e-02  2.81092837e-02
  2.90849461e-02  3.05851241e-02  3.07130492e-02  3.08372756e-02
  3.10153938e-02  3.12224443e-02  3.20130493e-02  3.20168849e-02
  3.29650523e-02  3.33763880e-02  3.36426930e-02  3.36661671e-02
  3.42322449e-02  3.43211423e-02  3.45595937e-02  3.45956480e-02
  3.47631559e-02  3.47893285e-02  3.49904929e-02  3.68577732e-02
  3.73999192e-02  3.86859402e-02  4.28948912e-02  4.32308493e-02
  4.35987109e-02  4.38218544e-02  4.42035981e-02  4.48314042e-02
  4.61315035e-02  4.66778746e-02  4.81006259e-02  4.81665113e-02
  4.83068699e-02  4.87730647e-02  4.92255397e-02  4.99409395e-02
  5.04955067e-02  5.04968128e-02  5.26451582e-02  5.46358081e-02
  5.54826347e-02  5.58197197e-02  5.60132477e-02  5.60344180e-02
  5.82352341e-02  6.06157562e-02  6.08166918e-02  6.13912176e-02
  6.29432680e-02  6.29616667e-02  6.32695448e-02  6.33964524e-02
  6.39042831e-02  6.62155358e-02  6.67151107e-02  6.69612663e-02
  6.69698747e-02  6.78242397e-02  6.81602189e-02  6.85273494e-02
  6.89860496e-02  6.98447377e-02  6.99712036e-02  7.00589894e-02
  7.06130219e-02  7.11517872e-02  7.11695302e-02  7.24607232e-02
  7.64552855e-02  7.72232779e-02  7.79292325e-02  7.83748278e-02
  7.92218861e-02  8.02605220e-02  8.09169907e-02  8.25459224e-02
  8.27250836e-02  8.29912150e-02  8.40977684e-02  8.63040317e-02
  8.66672663e-02  8.72276257e-02  8.77189975e-02  8.85279733e-02
  8.94159843e-02  9.20992155e-02  9.39260890e-02  9.39882182e-02
  9.43378433e-02  9.48965795e-02  9.66818045e-02  9.73326182e-02
  9.85339290e-02  9.96688538e-02  9.97135454e-02  1.00838197e-01
  1.01233632e-01  1.02231397e-01  1.03032096e-01  1.04356025e-01
  1.04846242e-01  1.07961614e-01  1.08492163e-01  1.10884504e-01
  1.13197862e-01  1.17610109e-01  1.17746229e-01  1.17993503e-01
  1.18055914e-01  1.24286745e-01  1.27631514e-01  1.34966543e-01
  1.37386420e-01  1.38487453e-01  1.38623239e-01  1.42619798e-01
  1.43930733e-01  1.44964715e-01  1.47286569e-01  1.50005904e-01
  1.50843261e-01  1.53595795e-01  1.54482801e-01  1.56492879e-01
  1.56694241e-01  1.56853318e-01  1.58255831e-01  1.58577846e-01
  1.59462552e-01  1.62462037e-01  1.62495337e-01  1.63318435e-01
  1.67024097e-01  1.67747526e-01  1.68501183e-01  1.68587924e-01
  1.71721372e-01  1.72878411e-01  1.72980495e-01  1.74814826e-01
  1.76096244e-01  1.76223765e-01  1.80533648e-01  1.82273862e-01
  1.82437087e-01  1.85154908e-01  1.86497074e-01  1.91048038e-01
  1.93941639e-01  1.94978008e-01  1.95531669e-01  1.98692170e-01
  1.99207314e-01  2.00412285e-01  2.01325924e-01  2.01864242e-01
  2.02734554e-01  2.03246120e-01  2.03483566e-01  2.04229124e-01
  2.09347823e-01  2.11087129e-01  2.14116385e-01  2.15318296e-01
  2.15760102e-01  2.16119400e-01  2.18220946e-01  2.18337447e-01
  2.21223199e-01  2.21318537e-01  2.22145694e-01  2.22791176e-01
  2.24149374e-01  2.26123399e-01  2.31258154e-01  2.32174777e-01
  2.34130594e-01  2.35224793e-01  2.38326210e-01  2.38881524e-01
  2.41047380e-01  2.42122533e-01  2.42524165e-01  2.45707396e-01
  2.46389012e-01  2.46687156e-01  2.48338571e-01  2.49329917e-01
  2.51397265e-01  2.52116144e-01  2.54562864e-01  2.56698290e-01
  2.61863277e-01  2.62097808e-01  2.63250284e-01  2.63989430e-01
  2.64543636e-01  2.65703712e-01  2.66757223e-01  2.66773074e-01
  2.67454819e-01  2.69542512e-01  2.70350478e-01  2.71243822e-01
  2.73617065e-01  2.74794373e-01  2.77266410e-01  2.77495557e-01
  2.79112478e-01  2.80014617e-01  2.80853797e-01  2.83131474e-01
  2.84032278e-01  2.86704921e-01  2.90661620e-01  2.90859411e-01
  2.95773930e-01  2.96607328e-01  2.98332684e-01  2.99006450e-01
  2.99655648e-01  3.00003163e-01  3.01001463e-01  3.02122025e-01
  3.02390449e-01  3.03208229e-01  3.04626824e-01  3.06340142e-01
  3.06485765e-01  3.06954855e-01  3.09363619e-01  3.11217360e-01
  3.14427800e-01  3.16014516e-01  3.16633932e-01  3.16737386e-01
  3.16821089e-01  3.18015630e-01  3.26022158e-01  3.26432391e-01
  3.36409388e-01  3.36511752e-01  3.47032284e-01  3.51440892e-01
  3.53955052e-01  3.54485686e-01  3.59569139e-01  3.60243116e-01
  3.60886971e-01  3.61263205e-01  3.62589748e-01  3.68116787e-01
  3.70165510e-01  3.71472207e-01  3.71724244e-01  3.73980817e-01
  3.80798737e-01  3.82362927e-01  3.85578787e-01  3.88213356e-01
  3.92885446e-01  3.96411439e-01  3.96539156e-01  3.99355323e-01
  4.01133604e-01  4.15001012e-01  4.16465530e-01  4.19325915e-01
  4.23159669e-01  4.23759027e-01  4.29102483e-01  4.30501930e-01
  4.33121775e-01  4.33544583e-01  4.50354314e-01  4.50904672e-01
  4.54500447e-01  4.55453216e-01  4.55894779e-01  4.58326891e-01
  4.60194744e-01  4.60348682e-01  4.66795664e-01  4.68873303e-01
  4.69628899e-01  4.70314790e-01  4.78550563e-01  4.80096586e-01
  4.81745823e-01  4.82222941e-01  4.82909581e-01  4.85653301e-01
  4.90439546e-01  4.93767012e-01  4.94080919e-01  4.98407549e-01
  5.00234755e-01  5.01102899e-01  5.07891185e-01  5.08033656e-01
  5.12042301e-01  5.12278154e-01  5.14103674e-01  5.15886565e-01
  5.16353141e-01  5.25791839e-01  5.27046660e-01  5.27223641e-01
  5.28264622e-01  5.29959519e-01  5.31434158e-01  5.33162350e-01
  5.37241920e-01  5.39721439e-01  5.40797341e-01  5.41454622e-01
  5.47146316e-01  5.57403596e-01  5.60338824e-01  5.70421273e-01
  5.71136922e-01  5.74209108e-01  5.78143072e-01  5.78725481e-01
  5.85004527e-01  5.85510994e-01  5.85900914e-01  5.93475798e-01
  5.96495666e-01  5.99489007e-01  6.00796635e-01  6.08514624e-01
  6.10658785e-01  6.11550640e-01  6.15772957e-01  6.18805876e-01
  6.21699544e-01  6.22851522e-01  6.24369093e-01  6.33099098e-01
  6.33342149e-01  6.35114593e-01  6.41235187e-01  6.44393199e-01
  6.48810482e-01  6.49552184e-01  6.64833434e-01  6.65052034e-01
  6.69545216e-01  6.74197400e-01  6.85757679e-01  6.95725604e-01
  7.03214441e-01  7.05176757e-01  7.09982719e-01  7.13395687e-01
  7.23221428e-01  7.28243807e-01  7.29993766e-01  7.33795401e-01
  7.34729623e-01  7.44212750e-01  7.45487267e-01  7.50920931e-01
  7.53565978e-01  7.55695141e-01  7.63838918e-01  7.75411168e-01
  7.78672866e-01  7.80797173e-01  7.84999349e-01  7.93184373e-01
  7.94508765e-01  7.96393360e-01  7.99685781e-01  8.03832453e-01
  8.09045433e-01  8.12847378e-01  8.14381090e-01  8.22356307e-01
  8.38314911e-01  8.43651020e-01  8.44384777e-01  9.10520759e-01
  9.21532914e-01  9.30089110e-01  9.51775105e-01  9.55505712e-01
  9.90155057e-01  9.96968672e-01  1.00195694e+00  1.00349030e+00
  1.01571852e+00  1.05616024e+00  1.05733284e+00  1.05784113e+00]

  warnings.warn(

2022-11-03 10:54:07,143:INFO:Calculating mean and std
2022-11-03 10:54:07,143:INFO:Creating metrics dataframe
2022-11-03 10:54:07,158:INFO:Uploading results into container
2022-11-03 10:54:07,174:INFO:Uploading model into container now
2022-11-03 10:54:07,174:INFO:master_model_container: 35
2022-11-03 10:54:07,174:INFO:display_container: 2
2022-11-03 10:54:07,174:INFO:LGBMRegressor(random_state=4411)
2022-11-03 10:54:07,174:INFO:create_model() successfully completed......................................
2022-11-03 10:54:07,437:WARNING:create_model() for LGBMRegressor(random_state=4411) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:54:07,437:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:54:07,437:INFO:Initializing create_model()
2022-11-03 10:54:07,437:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:54:07,437:INFO:Checking exceptions
2022-11-03 10:54:07,437:INFO:Importing libraries
2022-11-03 10:54:07,437:INFO:Copying training dataset
2022-11-03 10:54:07,462:INFO:Defining folds
2022-11-03 10:54:07,462:INFO:Declaring metric variables
2022-11-03 10:54:07,462:INFO:Importing untrained model
2022-11-03 10:54:07,462:INFO:Light Gradient Boosting Machine Imported successfully
2022-11-03 10:54:07,462:INFO:Starting cross validation
2022-11-03 10:54:07,470:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:54:12,198:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.47027486e-02 -4.22752996e-02 -3.61680716e-02 -3.02899701e-02
 -2.97144171e-02 -2.81489756e-02 -2.75392445e-02 -2.53182195e-02
 -2.41280808e-02 -2.22889370e-02 -1.96484401e-02 -1.84623127e-02
 -1.82296409e-02 -1.81639767e-02 -1.69342759e-02 -1.68315228e-02
 -1.55243607e-02 -1.40508281e-02 -1.18812644e-02 -1.09861960e-02
 -1.04077178e-02 -9.60024081e-03 -9.56620376e-03 -9.24818550e-03
 -8.83044485e-03 -8.74136205e-03 -7.95818722e-03 -7.19552416e-03
 -6.61123450e-03 -6.60045604e-03 -5.99499052e-03 -5.85577032e-03
 -5.70664799e-03 -5.58754871e-03 -4.42240485e-03 -4.29777560e-03
 -3.92961421e-03 -3.25248859e-03 -3.10003609e-03 -2.49252849e-03
 -1.94531942e-03 -1.77226611e-03 -7.55857550e-04  6.80615835e-04
  1.31755579e-03  1.37304714e-03  1.45577414e-03  1.57999021e-03
  1.69082828e-03  1.95119804e-03  2.07507777e-03  2.16214198e-03
  2.63054041e-03  3.00923187e-03  3.34253623e-03  3.34554812e-03
  3.42153989e-03  3.80223328e-03  4.21182930e-03  4.32072313e-03
  4.63183230e-03  4.84834690e-03  5.13621002e-03  5.18774444e-03
  5.22924411e-03  5.23664804e-03  5.33869758e-03  5.45164380e-03
  5.59928374e-03  5.64759180e-03  5.98414552e-03  6.03364522e-03
  6.35973612e-03  6.62557448e-03  6.67997683e-03  7.18511249e-03
  7.31714516e-03  7.97055869e-03  8.05987745e-03  8.10137986e-03
  8.15415038e-03  8.39687161e-03  8.50118363e-03  8.95878850e-03
  9.63894048e-03  1.05330067e-02  1.07127264e-02  1.16549997e-02
  1.21136864e-02  1.27599164e-02  1.29592388e-02  1.42877249e-02
  1.45590515e-02  1.46845145e-02  1.55911513e-02  1.57890561e-02
  1.58374273e-02  1.62344854e-02  1.64606502e-02  1.64981777e-02
  1.66265255e-02  1.80831480e-02  1.86030421e-02  1.97804267e-02
  2.15162714e-02  2.20443816e-02  2.21148786e-02  2.30175041e-02
  2.35043811e-02  2.38534295e-02  2.43084630e-02  2.44747897e-02
  2.52105497e-02  2.54491211e-02  2.57872215e-02  2.62250509e-02
  2.66245322e-02  2.73663120e-02  2.76517513e-02  2.79096779e-02
  2.80141052e-02  2.92585181e-02  2.92652671e-02  3.08953400e-02
  3.14786825e-02  3.20922593e-02  3.23063469e-02  3.32753148e-02
  3.36545859e-02  3.36774832e-02  3.43241503e-02  3.49939643e-02
  3.55653687e-02  3.56366505e-02  3.74934305e-02  3.84498878e-02
  3.88368797e-02  3.88940013e-02  3.90824919e-02  4.00094129e-02
  4.00151349e-02  4.12224041e-02  4.14684728e-02  4.24956954e-02
  4.56067045e-02  4.64680394e-02  4.68525127e-02  4.81432226e-02
  4.83191856e-02  4.84966726e-02  4.86955695e-02  4.99513797e-02
  4.99754816e-02  5.02669426e-02  5.15723710e-02  5.18012375e-02
  5.27113498e-02  5.33151869e-02  5.46389816e-02  5.46594728e-02
  5.61547342e-02  5.76990409e-02  5.80659470e-02  5.83022486e-02
  5.83701789e-02  5.98009506e-02  6.07375945e-02  6.08834883e-02
  6.10314186e-02  6.20163578e-02  6.76483641e-02  6.83733999e-02
  6.87584496e-02  6.87869289e-02  6.88487882e-02  7.29254099e-02
  7.39451446e-02  7.44817898e-02  7.51570568e-02  7.68203324e-02
  7.93712418e-02  7.95147872e-02  8.01583890e-02  8.06114576e-02
  8.36334014e-02  8.39745566e-02  8.40895988e-02  8.58359320e-02
  8.99724560e-02  9.12548831e-02  9.31060922e-02  9.39972709e-02
  9.48257809e-02  9.87199634e-02  1.00759892e-01  1.01853964e-01
  1.06664046e-01  1.07208628e-01  1.07884957e-01  1.08113776e-01
  1.08239273e-01  1.14423760e-01  1.15117624e-01  1.16663718e-01
  1.17221574e-01  1.17621245e-01  1.19436867e-01  1.21329392e-01
  1.25696121e-01  1.25734036e-01  1.27022049e-01  1.28919125e-01
  1.30551414e-01  1.31628824e-01  1.32194676e-01  1.34207578e-01
  1.35119624e-01  1.35993640e-01  1.36032922e-01  1.36498353e-01
  1.36982409e-01  1.37195902e-01  1.37543937e-01  1.39033686e-01
  1.41383290e-01  1.42441920e-01  1.44155955e-01  1.47947688e-01
  1.49048120e-01  1.49572605e-01  1.50365029e-01  1.54354941e-01
  1.54765316e-01  1.59257852e-01  1.60356713e-01  1.61883391e-01
  1.62115799e-01  1.63476640e-01  1.65082566e-01  1.67097556e-01
  1.67569838e-01  1.70602582e-01  1.71622995e-01  1.75575323e-01
  1.76014997e-01  1.78616888e-01  1.79888700e-01  1.82744191e-01
  1.83549901e-01  1.84046116e-01  1.84308117e-01  1.84340002e-01
  1.86748475e-01  1.87498389e-01  1.90562087e-01  1.91453228e-01
  1.91470481e-01  1.93044709e-01  1.95769179e-01  1.96883627e-01
  1.99475427e-01  2.00108568e-01  2.01649453e-01  2.02559669e-01
  2.04563732e-01  2.06003620e-01  2.07785933e-01  2.08557279e-01
  2.11589546e-01  2.12237747e-01  2.12813378e-01  2.13120396e-01
  2.13805497e-01  2.14353354e-01  2.17636084e-01  2.21285035e-01
  2.21625875e-01  2.22624408e-01  2.26501482e-01  2.29555951e-01
  2.30279541e-01  2.32910587e-01  2.33355020e-01  2.33601087e-01
  2.37193454e-01  2.37868931e-01  2.39267729e-01  2.42675741e-01
  2.46105121e-01  2.46952691e-01  2.49945778e-01  2.52332929e-01
  2.53000271e-01  2.56144072e-01  2.57321820e-01  2.65086870e-01
  2.65259263e-01  2.65565123e-01  2.65749673e-01  2.73945338e-01
  2.75956900e-01  2.76189295e-01  2.81033655e-01  2.81953433e-01
  2.83424676e-01  2.88647985e-01  2.89191317e-01  2.89894196e-01
  2.92014788e-01  2.93099705e-01  2.95967402e-01  2.97042912e-01
  3.02349097e-01  3.05772907e-01  3.06690784e-01  3.08062695e-01
  3.17357667e-01  3.19467149e-01  3.28488046e-01  3.33390096e-01
  3.33587736e-01  3.35021541e-01  3.35130973e-01  3.35620707e-01
  3.36803114e-01  3.37213266e-01  3.37361358e-01  3.37799071e-01
  3.41294763e-01  3.50328982e-01  3.51082542e-01  3.51681684e-01
  3.52494855e-01  3.61899978e-01  3.63804282e-01  3.72389644e-01
  3.77348604e-01  3.79287566e-01  3.80006047e-01  3.84087656e-01
  3.84457672e-01  3.87088234e-01  3.91032709e-01  3.91396299e-01
  3.93087313e-01  3.95372124e-01  3.97331003e-01  3.97512996e-01
  4.02473031e-01  4.03564059e-01  4.06742263e-01  4.12767428e-01
  4.12819334e-01  4.13314048e-01  4.14745026e-01  4.14884112e-01
  4.15169939e-01  4.15389159e-01  4.18471521e-01  4.18728799e-01
  4.22198790e-01  4.25068977e-01  4.29100882e-01  4.32391480e-01
  4.33056513e-01  4.33820709e-01  4.35083419e-01  4.36856084e-01
  4.37311994e-01  4.41516778e-01  4.45815750e-01  4.45844707e-01
  4.48486952e-01  4.48816015e-01  4.49117956e-01  4.51591319e-01
  4.54103690e-01  4.54429324e-01  4.54708180e-01  4.56792834e-01
  4.56904838e-01  4.58221358e-01  4.65693030e-01  4.67294552e-01
  4.71168749e-01  4.74016862e-01  4.81938740e-01  4.82048752e-01
  4.84146631e-01  4.86445547e-01  4.86521066e-01  4.88505184e-01
  4.92789010e-01  4.99242790e-01  5.00190303e-01  5.02582745e-01
  5.08291297e-01  5.14797366e-01  5.16541330e-01  5.30355513e-01
  5.32100223e-01  5.32137961e-01  5.33699893e-01  5.34139890e-01
  5.34959780e-01  5.36711487e-01  5.46672643e-01  5.48802205e-01
  5.51876063e-01  5.55074695e-01  5.59837421e-01  5.64584436e-01
  5.68139879e-01  5.69636166e-01  5.80020717e-01  5.81105002e-01
  5.86320865e-01  5.88567854e-01  5.94600564e-01  5.96343343e-01
  5.97123027e-01  6.00102318e-01  6.09183761e-01  6.19338481e-01
  6.23831154e-01  6.41302141e-01  6.48361266e-01  6.49423200e-01
  6.50151489e-01  6.51130726e-01  6.58861936e-01  6.61042312e-01
  6.61272426e-01  6.63803618e-01  6.65821641e-01  6.67372101e-01
  6.69176860e-01  6.74792123e-01  6.78382535e-01  6.81468968e-01
  6.91392051e-01  6.91663417e-01  6.93287771e-01  6.93947884e-01
  6.95641153e-01  6.97552205e-01  6.98472553e-01  7.07499815e-01
  7.12115072e-01  7.26736712e-01  7.30880351e-01  7.32059818e-01
  7.35728362e-01  7.37532527e-01  7.40489123e-01  7.44486459e-01
  7.46457636e-01  7.47666182e-01  7.58960927e-01  7.59520174e-01
  7.60502647e-01  7.61121471e-01  7.64375888e-01  7.64920459e-01
  7.77912975e-01  7.80398029e-01  7.80730076e-01  7.94963545e-01
  7.95372019e-01  8.09669122e-01  8.16851178e-01  8.23993874e-01
  8.24001933e-01  8.54430889e-01  8.60561224e-01  8.83635513e-01
  9.05503981e-01  9.09263192e-01  9.10627202e-01  9.11018160e-01
  9.13460599e-01  9.14103521e-01  9.18424056e-01  9.34722634e-01
  9.39537049e-01  9.46679299e-01  9.57852941e-01  1.03336527e+00
  1.03772716e+00  1.04491827e+00  1.05376189e+00]

  warnings.warn(

2022-11-03 10:54:12,231:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.25448534e-01 -1.02178988e-01 -9.25320985e-02 -8.23837777e-02
 -3.46948307e-02 -3.19310139e-02 -2.06193416e-02 -2.04698806e-02
 -1.70856290e-02 -1.70657183e-02 -1.64704926e-02 -1.51453411e-02
 -1.50641249e-02 -1.40654283e-02 -1.39537638e-02 -1.17957284e-02
 -1.15875295e-02 -9.79913033e-03 -8.51589777e-03 -8.22607581e-03
 -7.87795152e-03 -6.35794668e-03 -6.21775187e-03 -6.06110361e-03
 -6.01092001e-03 -5.78832924e-03 -5.49206111e-03 -5.30373418e-03
 -5.08866225e-03 -4.46835529e-03 -4.37678164e-03 -4.24783829e-03
 -4.10444158e-03 -3.96377283e-03 -3.40502420e-03 -2.92953798e-03
 -2.58998180e-03 -2.15467858e-03 -1.74114534e-03 -1.66459828e-03
 -1.52062834e-03 -8.35103764e-04 -5.09090989e-04  1.57517910e-04
  1.97593663e-04  6.96476156e-04  1.07659411e-03  1.49065573e-03
  1.59044196e-03  2.57161084e-03  3.05876624e-03  3.08512402e-03
  3.50587770e-03  3.59346348e-03  4.16654894e-03  4.38587775e-03
  4.46349151e-03  4.64460181e-03  5.11070485e-03  5.15074618e-03
  5.22960565e-03  5.62194462e-03  6.02739571e-03  7.70767860e-03
  8.03615756e-03  8.43531231e-03  8.67281232e-03  8.68141339e-03
  8.72498280e-03  8.80141686e-03  8.96849144e-03  9.23106967e-03
  9.47280218e-03  9.47799022e-03  1.05058172e-02  1.07102164e-02
  1.08973279e-02  1.10390685e-02  1.10983534e-02  1.17489794e-02
  1.20790463e-02  1.25941382e-02  1.27972822e-02  1.28195102e-02
  1.32620043e-02  1.37912799e-02  1.45450803e-02  1.47749638e-02
  1.51931035e-02  1.62164034e-02  1.65966209e-02  1.88378152e-02
  1.92988693e-02  1.95898620e-02  1.95922341e-02  1.99027176e-02
  2.08637921e-02  2.22196824e-02  2.24577972e-02  2.26464172e-02
  2.42031790e-02  2.48388079e-02  2.49723523e-02  2.53774588e-02
  2.72078909e-02  2.76031595e-02  2.77379416e-02  2.78393153e-02
  2.85072065e-02  2.95824349e-02  3.05089946e-02  3.06568157e-02
  3.07734782e-02  3.17014771e-02  3.19519657e-02  3.27470012e-02
  3.49211271e-02  3.58826468e-02  3.64065211e-02  3.65432152e-02
  3.70892607e-02  3.84076443e-02  3.84182807e-02  3.89361560e-02
  3.95124895e-02  3.96518401e-02  3.98551701e-02  4.03081564e-02
  4.29107548e-02  4.36601697e-02  4.47898772e-02  4.50125172e-02
  4.51691072e-02  4.59630267e-02  4.62559400e-02  4.64936322e-02
  4.68418545e-02  4.84790593e-02  4.85232214e-02  4.92200367e-02
  4.99330072e-02  5.11667441e-02  5.14836259e-02  5.23308229e-02
  5.23396847e-02  5.27455331e-02  5.32486458e-02  5.37787790e-02
  5.38432521e-02  5.50448254e-02  5.58062915e-02  5.71858185e-02
  5.75100070e-02  5.77206974e-02  5.85838468e-02  5.89893145e-02
  5.90090932e-02  6.12037733e-02  6.18841742e-02  6.26022269e-02
  6.35589913e-02  6.50768605e-02  6.60516245e-02  6.64712974e-02
  6.97239794e-02  7.01588392e-02  7.03796708e-02  7.05887741e-02
  7.15375881e-02  7.18705127e-02  7.31638217e-02  7.33957563e-02
  7.35288501e-02  7.35926776e-02  7.43377285e-02  7.61374757e-02
  7.63802411e-02  7.65394262e-02  7.69117236e-02  7.74994172e-02
  7.81937895e-02  7.98830392e-02  8.22022422e-02  8.25189075e-02
  8.41065684e-02  8.45840197e-02  8.50698517e-02  8.75078235e-02
  9.02642255e-02  9.10786522e-02  9.15700489e-02  9.30729209e-02
  9.33981332e-02  9.37490886e-02  9.55885527e-02  9.58740112e-02
  9.71870836e-02  9.71917394e-02  9.81838696e-02  9.81873238e-02
  1.00215549e-01  1.00228095e-01  1.02827477e-01  1.03383531e-01
  1.04055952e-01  1.04480659e-01  1.04931043e-01  1.05181650e-01
  1.06739983e-01  1.08786026e-01  1.09199063e-01  1.09951824e-01
  1.10003062e-01  1.10332451e-01  1.11051893e-01  1.12500796e-01
  1.12797856e-01  1.12858381e-01  1.13324407e-01  1.13775308e-01
  1.14636448e-01  1.15093624e-01  1.15261642e-01  1.19045862e-01
  1.19124618e-01  1.19571365e-01  1.22054949e-01  1.22717872e-01
  1.23347876e-01  1.25227654e-01  1.32826824e-01  1.33932582e-01
  1.34088450e-01  1.36218899e-01  1.36838319e-01  1.36943526e-01
  1.38154819e-01  1.39363742e-01  1.41649121e-01  1.43602657e-01
  1.44835542e-01  1.45122861e-01  1.47936994e-01  1.51732184e-01
  1.51812271e-01  1.53473688e-01  1.53689699e-01  1.55238164e-01
  1.56513873e-01  1.58865950e-01  1.59724374e-01  1.60544781e-01
  1.61325346e-01  1.62092638e-01  1.63106096e-01  1.64142346e-01
  1.64318619e-01  1.67761526e-01  1.67820979e-01  1.67995802e-01
  1.70343888e-01  1.71625718e-01  1.71834007e-01  1.78458542e-01
  1.78579744e-01  1.81697766e-01  1.83425521e-01  1.87175281e-01
  1.89556948e-01  1.92585093e-01  1.93088879e-01  1.94810623e-01
  1.95775370e-01  1.98860241e-01  2.00347080e-01  2.02567393e-01
  2.02576526e-01  2.02613706e-01  2.02840664e-01  2.11291463e-01
  2.12700005e-01  2.14859571e-01  2.17917515e-01  2.26028401e-01
  2.28278711e-01  2.29048451e-01  2.34052918e-01  2.35420929e-01
  2.35830938e-01  2.36838617e-01  2.39682331e-01  2.39869557e-01
  2.42134397e-01  2.42613492e-01  2.45392810e-01  2.45979902e-01
  2.47184906e-01  2.47404802e-01  2.49416221e-01  2.53993719e-01
  2.54265578e-01  2.55411645e-01  2.55841371e-01  2.56010460e-01
  2.56987260e-01  2.59797809e-01  2.65496152e-01  2.65744891e-01
  2.66694451e-01  2.70171772e-01  2.71910689e-01  2.72040314e-01
  2.79432279e-01  2.81262775e-01  2.82256179e-01  2.82826846e-01
  2.84478693e-01  2.88651205e-01  2.89927871e-01  2.90846647e-01
  2.91149368e-01  2.92561920e-01  2.96228285e-01  3.00361434e-01
  3.04663123e-01  3.05204649e-01  3.13798477e-01  3.13983903e-01
  3.15672252e-01  3.16385017e-01  3.18987057e-01  3.20675775e-01
  3.21711755e-01  3.25417380e-01  3.25508336e-01  3.30800675e-01
  3.33479268e-01  3.37567156e-01  3.38564472e-01  3.45023744e-01
  3.45059254e-01  3.48704899e-01  3.49757302e-01  3.64458662e-01
  3.68644287e-01  3.81178509e-01  3.82349751e-01  3.83159790e-01
  3.84699660e-01  3.89045596e-01  3.90236277e-01  3.91099642e-01
  3.92193730e-01  3.92455435e-01  3.95612394e-01  3.99326304e-01
  3.99529250e-01  4.01120667e-01  4.01353425e-01  4.07888338e-01
  4.08627073e-01  4.08635701e-01  4.09172142e-01  4.10603766e-01
  4.12372009e-01  4.17219010e-01  4.18010423e-01  4.22087940e-01
  4.25169880e-01  4.26139881e-01  4.33267595e-01  4.36306948e-01
  4.38316043e-01  4.44702809e-01  4.45629862e-01  4.51024505e-01
  4.51417035e-01  4.52010683e-01  4.52464586e-01  4.59618126e-01
  4.65189463e-01  4.65256266e-01  4.66794891e-01  4.67138422e-01
  4.77411303e-01  4.79197376e-01  4.80459549e-01  4.81106261e-01
  4.81301500e-01  4.90569370e-01  4.92827346e-01  4.96236146e-01
  4.99937540e-01  5.05123034e-01  5.09412061e-01  5.12078873e-01
  5.15774851e-01  5.20481093e-01  5.31430130e-01  5.36340402e-01
  5.36895675e-01  5.38290538e-01  5.40426650e-01  5.48976984e-01
  5.51233957e-01  5.58745018e-01  5.65243017e-01  5.70466008e-01
  5.82937947e-01  5.85795112e-01  5.86884186e-01  5.96342619e-01
  5.96656328e-01  5.97661480e-01  5.99712870e-01  6.01246945e-01
  6.03937517e-01  6.04750282e-01  6.05212860e-01  6.11313143e-01
  6.18688055e-01  6.19360428e-01  6.19947347e-01  6.26643769e-01
  6.28245826e-01  6.31202244e-01  6.35305557e-01  6.43276547e-01
  6.43895644e-01  6.50189663e-01  6.50737366e-01  6.51376881e-01
  6.58078282e-01  6.62276316e-01  6.71339924e-01  6.79469075e-01
  6.86559639e-01  6.86667641e-01  6.95079916e-01  7.03293264e-01
  7.06046412e-01  7.06906170e-01  7.07259931e-01  7.28302270e-01
  7.28481439e-01  7.35731405e-01  7.36067011e-01  7.39231662e-01
  7.46945759e-01  7.49041147e-01  7.50579407e-01  7.56053915e-01
  7.56995972e-01  7.57573836e-01  7.58247321e-01  7.64000162e-01
  7.64608714e-01  7.70988871e-01  7.92081693e-01  7.95599251e-01
  8.01426942e-01  8.04381148e-01  8.07615983e-01  8.09711833e-01
  8.10733970e-01  8.12469034e-01  8.28401569e-01  8.30046717e-01
  8.41044092e-01  8.51026940e-01  8.60890959e-01  8.67025959e-01
  8.75699907e-01  8.79263775e-01  8.86748652e-01  8.89815577e-01
  8.90894284e-01  8.95872222e-01  9.00339052e-01  9.06263102e-01
  9.15307687e-01  9.28503278e-01  9.29036418e-01  9.30632274e-01
  9.38592202e-01  9.52819301e-01  9.52940055e-01  9.55193773e-01
  9.60050111e-01  9.68704803e-01  9.69505388e-01  9.93472929e-01]

  warnings.warn(

2022-11-03 10:54:12,305:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-5.53333484e-02 -4.34169458e-02 -3.49539427e-02 -3.18987117e-02
 -3.10251592e-02 -3.03999302e-02 -2.85122435e-02 -2.67543941e-02
 -2.42584923e-02 -2.39770952e-02 -2.34871870e-02 -2.26474251e-02
 -2.13403678e-02 -2.08215346e-02 -2.06236459e-02 -1.90294675e-02
 -1.69299364e-02 -1.67532396e-02 -1.64294679e-02 -1.62455245e-02
 -1.59990787e-02 -1.56315966e-02 -1.47528501e-02 -1.37174475e-02
 -1.35371332e-02 -1.35167338e-02 -1.23529583e-02 -1.16136685e-02
 -1.09704462e-02 -1.08133965e-02 -1.03776406e-02 -1.02696337e-02
 -1.01201530e-02 -9.84591356e-03 -9.75115730e-03 -9.14951088e-03
 -8.60822385e-03 -8.53947260e-03 -8.36303619e-03 -8.31357148e-03
 -7.38732146e-03 -7.32047027e-03 -6.34688558e-03 -5.86499200e-03
 -5.28954679e-03 -5.27030061e-03 -4.99914446e-03 -4.91544154e-03
 -4.37052864e-03 -4.09641078e-03 -3.74618600e-03 -3.45337587e-03
 -3.39304909e-03 -3.33930760e-03 -2.60412994e-03 -2.47271262e-03
 -2.25416016e-03 -2.08803119e-03 -1.69326267e-03 -1.65719672e-03
 -1.52432397e-03 -7.68099174e-04 -4.78729819e-04  7.74301819e-05
  1.05275967e-04  4.59628221e-04  6.44595967e-04  9.65362718e-04
  1.05211864e-03  1.11094926e-03  1.22675080e-03  1.27453885e-03
  1.64465618e-03  2.08480184e-03  2.16913953e-03  2.79145478e-03
  2.84740997e-03  2.86874252e-03  3.52229244e-03  5.09628446e-03
  5.24930609e-03  5.56052503e-03  5.73918488e-03  5.99826487e-03
  6.13646290e-03  6.23990206e-03  6.27708719e-03  7.02054756e-03
  7.26948105e-03  7.60267158e-03  7.69797672e-03  7.70870704e-03
  7.90169171e-03  8.18862007e-03  8.45597606e-03  8.83571926e-03
  8.92200756e-03  9.87087219e-03  1.01954098e-02  1.03919109e-02
  1.12289546e-02  1.12405890e-02  1.34531661e-02  1.41440142e-02
  1.49745759e-02  1.51402008e-02  1.53005520e-02  1.56939756e-02
  1.65651952e-02  1.71153340e-02  1.79384038e-02  1.85070021e-02
  1.87247227e-02  1.90608157e-02  2.01383728e-02  2.01707563e-02
  2.04365921e-02  2.14625394e-02  2.15861298e-02  2.23952231e-02
  2.29003275e-02  2.29476980e-02  2.53029570e-02  2.54049597e-02
  2.55545024e-02  2.57423250e-02  2.58258496e-02  2.64270888e-02
  2.71465149e-02  2.76285891e-02  2.85833010e-02  2.90494723e-02
  2.93798966e-02  2.98884459e-02  3.11890446e-02  3.19416957e-02
  3.20713984e-02  3.25616407e-02  3.36893177e-02  3.44830563e-02
  3.52672560e-02  3.53505532e-02  3.77549668e-02  3.84486448e-02
  3.88519050e-02  3.99592911e-02  4.12663494e-02  4.30788798e-02
  4.50259414e-02  4.57686097e-02  4.62805843e-02  4.71721134e-02
  4.79607660e-02  4.94060757e-02  4.97705943e-02  5.15835010e-02
  5.18704379e-02  5.26185390e-02  5.29434298e-02  5.33336709e-02
  5.34510934e-02  5.38754786e-02  5.61873408e-02  5.70429906e-02
  5.73258659e-02  5.81009320e-02  5.83418986e-02  5.91477971e-02
  5.95395360e-02  6.04759956e-02  6.07929846e-02  6.17700224e-02
  6.31700328e-02  6.48241427e-02  6.48573767e-02  6.49640885e-02
  6.54031663e-02  6.57744656e-02  6.61870695e-02  6.62545673e-02
  6.64991603e-02  6.73438304e-02  6.77294949e-02  6.92585984e-02
  7.00609906e-02  7.07646230e-02  7.21314039e-02  7.25339581e-02
  7.25941102e-02  7.27106111e-02  7.38403428e-02  7.44321282e-02
  7.58090293e-02  7.58712009e-02  7.72552356e-02  7.75647168e-02
  7.82912665e-02  7.88840029e-02  8.25273722e-02  8.36392477e-02
  8.82111969e-02  8.94671525e-02  9.04208517e-02  9.28112642e-02
  9.49171319e-02  9.60657410e-02  9.61771958e-02  9.76547541e-02
  9.80383967e-02  9.86703637e-02  9.96680235e-02  9.98281278e-02
  1.00113959e-01  1.02707287e-01  1.03751833e-01  1.03802468e-01
  1.06037048e-01  1.06195389e-01  1.06432552e-01  1.06685825e-01
  1.08436963e-01  1.08922908e-01  1.09986049e-01  1.12489294e-01
  1.12941215e-01  1.13548004e-01  1.14607623e-01  1.14980915e-01
  1.15015087e-01  1.16731519e-01  1.19315261e-01  1.20521936e-01
  1.25158827e-01  1.29919752e-01  1.30387161e-01  1.30534165e-01
  1.31294755e-01  1.31472728e-01  1.31721980e-01  1.31762131e-01
  1.31860701e-01  1.32974025e-01  1.34526990e-01  1.36546222e-01
  1.39887047e-01  1.41041952e-01  1.44189277e-01  1.44336733e-01
  1.44957197e-01  1.52272905e-01  1.55937732e-01  1.56680355e-01
  1.60374869e-01  1.61883732e-01  1.61991564e-01  1.64282504e-01
  1.71002210e-01  1.71789028e-01  1.73473610e-01  1.73563045e-01
  1.77488875e-01  1.81913338e-01  1.82407399e-01  1.83317812e-01
  1.83660139e-01  1.85084559e-01  1.85690229e-01  1.87447382e-01
  1.88373650e-01  1.88897914e-01  1.89587228e-01  1.91325423e-01
  1.95779582e-01  1.96632311e-01  1.98798764e-01  2.01602021e-01
  2.02854818e-01  2.08207440e-01  2.08708330e-01  2.08755200e-01
  2.08921228e-01  2.10435159e-01  2.15227723e-01  2.16407741e-01
  2.17395543e-01  2.19476300e-01  2.19904701e-01  2.20188487e-01
  2.29256197e-01  2.29963546e-01  2.32376516e-01  2.34853726e-01
  2.36360213e-01  2.37491541e-01  2.38633514e-01  2.45660462e-01
  2.47098246e-01  2.47498623e-01  2.49931486e-01  2.52863331e-01
  2.57396391e-01  2.61577338e-01  2.62401682e-01  2.66298041e-01
  2.67671115e-01  2.68518051e-01  2.68639222e-01  2.68894502e-01
  2.70114391e-01  2.72342973e-01  2.76156921e-01  2.78191179e-01
  2.83545290e-01  2.83614007e-01  2.88148032e-01  2.91006339e-01
  2.93779516e-01  2.93954712e-01  2.94617426e-01  2.98541367e-01
  3.01696426e-01  3.03776527e-01  3.08062289e-01  3.08750327e-01
  3.11806779e-01  3.12473936e-01  3.15537826e-01  3.16721167e-01
  3.16939957e-01  3.19020819e-01  3.20722328e-01  3.21268430e-01
  3.21525855e-01  3.23326858e-01  3.27301966e-01  3.28130932e-01
  3.31908108e-01  3.34426467e-01  3.38523851e-01  3.42900162e-01
  3.44564211e-01  3.45952753e-01  3.48812214e-01  3.49632951e-01
  3.50876639e-01  3.57517413e-01  3.59438101e-01  3.64671155e-01
  3.65112981e-01  3.67420056e-01  3.67521879e-01  3.70188777e-01
  3.71452460e-01  3.80129626e-01  3.80468784e-01  3.81526703e-01
  3.84136640e-01  3.85177286e-01  3.86356510e-01  3.88206369e-01
  3.89858761e-01  3.90039593e-01  3.93107392e-01  3.98383717e-01
  4.01015749e-01  4.06316453e-01  4.06374741e-01  4.08556239e-01
  4.27145230e-01  4.32326014e-01  4.35355135e-01  4.36273845e-01
  4.40852232e-01  4.42418776e-01  4.45159079e-01  4.56229571e-01
  4.56428722e-01  4.58010342e-01  4.58483892e-01  4.61269522e-01
  4.61313893e-01  4.62561333e-01  4.63344179e-01  4.65712185e-01
  4.69367383e-01  4.78327307e-01  4.81050049e-01  4.82117539e-01
  4.82752003e-01  4.90591834e-01  4.91285275e-01  4.91444661e-01
  4.95689251e-01  5.01822301e-01  5.03505626e-01  5.05309451e-01
  5.08509787e-01  5.21739139e-01  5.31810034e-01  5.33558228e-01
  5.35056411e-01  5.37817624e-01  5.41974714e-01  5.43382342e-01
  5.62052806e-01  5.67730871e-01  5.68802954e-01  5.69606947e-01
  5.69888982e-01  5.85094112e-01  5.88573747e-01  5.89444828e-01
  5.90948671e-01  5.97495172e-01  5.97595993e-01  5.98273465e-01
  5.98908695e-01  6.02676711e-01  6.03053598e-01  6.04793755e-01
  6.06624667e-01  6.09555861e-01  6.10164472e-01  6.15523067e-01
  6.16527778e-01  6.17638994e-01  6.23143749e-01  6.24564402e-01
  6.25878571e-01  6.34664180e-01  6.52417060e-01  6.53100859e-01
  6.57623597e-01  6.60838693e-01  6.60916127e-01  6.64244336e-01
  6.64317137e-01  6.65183728e-01  6.65649047e-01  6.73311985e-01
  6.86318603e-01  6.93466365e-01  7.03451039e-01  7.09708294e-01
  7.11250431e-01  7.11866819e-01  7.17209582e-01  7.28350452e-01
  7.30252701e-01  7.34350160e-01  7.34684965e-01  7.40606138e-01
  7.43683546e-01  7.52236364e-01  7.54351189e-01  7.56007765e-01
  7.58502331e-01  7.66829956e-01  7.67313679e-01  7.69154524e-01
  7.69630915e-01  7.73562625e-01  7.79641016e-01  7.80645692e-01
  7.93163722e-01  8.10470666e-01  8.10585480e-01  8.12477709e-01
  8.15961785e-01  8.17737437e-01  8.20218900e-01  8.24291330e-01
  8.31846547e-01  8.38098340e-01  8.42988550e-01  8.53154029e-01
  8.58609291e-01  8.71972100e-01  8.74957036e-01  8.85197392e-01
  8.89206052e-01  8.94044583e-01  8.96567266e-01  8.97130939e-01
  8.99367846e-01  9.10079427e-01  9.18186551e-01  9.40687365e-01
  9.43059382e-01  9.44509275e-01  9.63035728e-01  9.76722132e-01]

  warnings.warn(

2022-11-03 10:54:12,321:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.04768192e-02 -5.79044319e-02 -4.75116932e-02 -4.31571291e-02
 -3.99256214e-02 -3.98561420e-02 -3.78398797e-02 -3.03339319e-02
 -2.63634907e-02 -2.12452538e-02 -2.10097220e-02 -1.97649597e-02
 -1.97548511e-02 -1.72600352e-02 -1.60952468e-02 -1.51978840e-02
 -1.45241521e-02 -1.41527963e-02 -1.20559792e-02 -1.20247965e-02
 -1.15955326e-02 -1.14501223e-02 -1.14131644e-02 -1.11385930e-02
 -1.10705641e-02 -1.05155238e-02 -9.72250279e-03 -9.57851846e-03
 -9.51984592e-03 -9.15956081e-03 -8.28529321e-03 -7.83537851e-03
 -7.67430093e-03 -7.54262804e-03 -7.50057673e-03 -7.36702659e-03
 -6.61003773e-03 -6.47899684e-03 -6.12750597e-03 -6.01889635e-03
 -5.96044818e-03 -5.81426991e-03 -5.22733203e-03 -4.77624725e-03
 -2.65947825e-03 -2.53148054e-03 -2.13601602e-03 -1.96663277e-03
 -1.59957541e-03 -1.09122623e-03 -6.55259016e-04 -9.42419942e-05
  8.99666916e-04  1.04007671e-03  1.28689130e-03  1.82297044e-03
  2.00480777e-03  2.46101033e-03  2.55562561e-03  3.04697693e-03
  3.30665107e-03  3.77787830e-03  4.49913646e-03  4.64755782e-03
  4.77200330e-03  4.78464387e-03  5.04327015e-03  5.36007974e-03
  6.05090290e-03  6.15760110e-03  7.17006206e-03  7.58326651e-03
  8.43909972e-03  9.38694716e-03  9.69841714e-03  1.02184193e-02
  1.02281340e-02  1.09570771e-02  1.18851373e-02  1.19190140e-02
  1.19981024e-02  1.28469516e-02  1.30206556e-02  1.34166926e-02
  1.38627512e-02  1.42481468e-02  1.43512526e-02  1.51368901e-02
  1.53230940e-02  1.53354016e-02  1.72058277e-02  1.80551971e-02
  1.85531063e-02  1.96678103e-02  2.06621720e-02  2.12918336e-02
  2.22240390e-02  2.33391155e-02  2.34089964e-02  2.46312288e-02
  2.47820612e-02  2.51103686e-02  2.62066851e-02  2.70501126e-02
  2.71705587e-02  2.75320463e-02  2.75666451e-02  2.78998117e-02
  2.91881076e-02  2.94823631e-02  2.97919343e-02  2.97963676e-02
  2.99002909e-02  2.99483817e-02  3.02055429e-02  3.12186813e-02
  3.14974148e-02  3.29014616e-02  3.41964207e-02  3.44561326e-02
  3.50012372e-02  3.60698953e-02  3.72814287e-02  3.90665156e-02
  3.92151798e-02  3.92703629e-02  4.00109787e-02  4.01827887e-02
  4.04574012e-02  4.05976150e-02  4.08016374e-02  4.10691561e-02
  4.11924570e-02  4.12526747e-02  4.21754976e-02  4.38244097e-02
  4.41992119e-02  4.53397100e-02  4.55028045e-02  4.78651639e-02
  4.89525482e-02  4.92722916e-02  5.08931931e-02  5.09952634e-02
  5.13479388e-02  5.26822272e-02  5.51502361e-02  5.55929312e-02
  5.66743342e-02  5.80450895e-02  5.96134704e-02  6.13662995e-02
  6.20562060e-02  6.27063192e-02  6.33000416e-02  6.33390666e-02
  6.49876063e-02  6.58085682e-02  6.69364043e-02  6.83895267e-02
  6.91918427e-02  7.05344074e-02  7.14145924e-02  7.21648151e-02
  7.22496941e-02  7.37472587e-02  7.38425785e-02  7.43777780e-02
  7.49775544e-02  7.57726210e-02  7.63876148e-02  7.67072871e-02
  7.70477084e-02  7.76635370e-02  7.83875774e-02  7.87823820e-02
  7.95628316e-02  8.12565546e-02  8.14484114e-02  8.17275567e-02
  8.34643932e-02  8.40306194e-02  8.67978746e-02  8.68465454e-02
  8.76782791e-02  8.79169430e-02  8.87129445e-02  8.87488462e-02
  8.88704690e-02  9.02539264e-02  9.08778885e-02  9.28650709e-02
  9.29425888e-02  9.35340422e-02  9.47965100e-02  9.48412805e-02
  9.51887038e-02  9.60912055e-02  9.75754089e-02  9.82913414e-02
  9.85290066e-02  9.98947600e-02  1.00224837e-01  1.01748296e-01
  1.01755213e-01  1.02223215e-01  1.03732665e-01  1.04161150e-01
  1.04726175e-01  1.06295794e-01  1.06672324e-01  1.08598631e-01
  1.08904453e-01  1.10470913e-01  1.14006045e-01  1.14300129e-01
  1.15515870e-01  1.16714992e-01  1.18170319e-01  1.21021822e-01
  1.21456021e-01  1.21962246e-01  1.22698834e-01  1.23296989e-01
  1.23978216e-01  1.26292868e-01  1.26817425e-01  1.28024918e-01
  1.28264812e-01  1.32237854e-01  1.34154593e-01  1.34474883e-01
  1.34682923e-01  1.35380490e-01  1.35407792e-01  1.35938376e-01
  1.36172123e-01  1.36569746e-01  1.40112527e-01  1.40523794e-01
  1.40642312e-01  1.41321002e-01  1.42917429e-01  1.45140693e-01
  1.49775150e-01  1.51248441e-01  1.51428140e-01  1.53347165e-01
  1.55111672e-01  1.58514087e-01  1.60773566e-01  1.61301835e-01
  1.61368963e-01  1.62226873e-01  1.62969794e-01  1.64977166e-01
  1.65322643e-01  1.65890528e-01  1.66978707e-01  1.68092521e-01
  1.70480746e-01  1.71906321e-01  1.72244955e-01  1.73456525e-01
  1.74922367e-01  1.77333236e-01  1.77896185e-01  1.79128034e-01
  1.81339489e-01  1.81563685e-01  1.82382160e-01  1.83281646e-01
  1.84155278e-01  1.86763575e-01  1.87846621e-01  1.96373629e-01
  1.99011974e-01  1.99748621e-01  2.03030700e-01  2.05540666e-01
  2.06316694e-01  2.06859386e-01  2.09145480e-01  2.11308985e-01
  2.11956885e-01  2.15041700e-01  2.15296782e-01  2.17871096e-01
  2.20456592e-01  2.22381650e-01  2.22638227e-01  2.23965544e-01
  2.24252770e-01  2.24992651e-01  2.27077197e-01  2.27946334e-01
  2.28605181e-01  2.29011158e-01  2.29727863e-01  2.29751493e-01
  2.30838628e-01  2.33894335e-01  2.35886504e-01  2.42164163e-01
  2.50647008e-01  2.51076694e-01  2.51703319e-01  2.53380816e-01
  2.53385639e-01  2.54216839e-01  2.55681008e-01  2.55980050e-01
  2.60029160e-01  2.65400338e-01  2.68060405e-01  2.68940295e-01
  2.74083771e-01  2.75972883e-01  2.76396802e-01  2.77039884e-01
  2.77898018e-01  2.78522573e-01  2.85083637e-01  2.85268982e-01
  2.86449216e-01  2.91232898e-01  2.91272057e-01  2.92371936e-01
  2.92945926e-01  2.94717129e-01  2.94828440e-01  2.94885713e-01
  2.95817106e-01  2.99084838e-01  3.04544451e-01  3.04677379e-01
  3.04749643e-01  3.06897141e-01  3.09576710e-01  3.13937590e-01
  3.19534818e-01  3.20010013e-01  3.20125276e-01  3.24531559e-01
  3.34611548e-01  3.38626095e-01  3.41177426e-01  3.58457689e-01
  3.60287207e-01  3.62986329e-01  3.65599415e-01  3.65789680e-01
  3.73236860e-01  3.75963401e-01  3.79788927e-01  3.80086075e-01
  3.81922458e-01  3.82938289e-01  3.83891699e-01  3.86291826e-01
  3.87137180e-01  3.89358589e-01  3.94008749e-01  3.98981140e-01
  4.00401301e-01  4.00754226e-01  4.00877754e-01  4.04135688e-01
  4.05782631e-01  4.06150303e-01  4.06384644e-01  4.10818987e-01
  4.11234106e-01  4.11992334e-01  4.20179357e-01  4.21574439e-01
  4.25123779e-01  4.25165834e-01  4.31937081e-01  4.34187907e-01
  4.37298363e-01  4.37446691e-01  4.49920997e-01  4.57695287e-01
  4.59734023e-01  4.59930675e-01  4.68183594e-01  4.73973978e-01
  4.74129440e-01  4.75037755e-01  4.78933850e-01  4.81216331e-01
  4.83268407e-01  4.87964757e-01  4.91171310e-01  4.95825777e-01
  4.97397644e-01  5.03987680e-01  5.08226781e-01  5.11756433e-01
  5.16190684e-01  5.16986885e-01  5.18857294e-01  5.21839051e-01
  5.24598800e-01  5.28491661e-01  5.29123451e-01  5.35489950e-01
  5.38140387e-01  5.40616335e-01  5.48510374e-01  5.49281796e-01
  5.52244589e-01  5.55215421e-01  5.61364635e-01  5.61911510e-01
  5.63565301e-01  5.65054628e-01  5.76336339e-01  5.82209082e-01
  5.89477858e-01  5.95132124e-01  5.96744012e-01  6.02224761e-01
  6.10724971e-01  6.13324052e-01  6.19256325e-01  6.25564352e-01
  6.25636542e-01  6.28567896e-01  6.28719910e-01  6.29849867e-01
  6.30542617e-01  6.35420003e-01  6.38755062e-01  6.51434551e-01
  6.52013055e-01  6.54153347e-01  6.57211414e-01  6.66865510e-01
  6.67481417e-01  6.67710487e-01  6.77770655e-01  6.88005678e-01
  6.89979371e-01  6.90422465e-01  6.90873787e-01  6.93207470e-01
  7.00196772e-01  7.03025443e-01  7.04780236e-01  7.05118602e-01
  7.06071643e-01  7.15720808e-01  7.17693876e-01  7.18158154e-01
  7.25750796e-01  7.30025759e-01  7.39030995e-01  7.40887508e-01
  7.48605730e-01  7.59144861e-01  7.61768642e-01  7.62523303e-01
  7.75697514e-01  7.76145948e-01  7.77531739e-01  7.86758319e-01
  7.89665244e-01  8.02438845e-01  8.02720656e-01  8.04691220e-01
  8.11582456e-01  8.33641400e-01  8.37561898e-01  8.39264145e-01
  8.49778928e-01  8.51278267e-01  8.59628819e-01  8.60252227e-01
  8.60765443e-01  8.61592337e-01  8.68374093e-01  8.81511709e-01
  9.04984424e-01  9.13866481e-01  9.31802735e-01  9.40311487e-01
  9.46876729e-01  9.60840689e-01  9.76855400e-01  1.01247695e+00
  1.04763167e+00]

  warnings.warn(

2022-11-03 10:54:12,480:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.18892453e-02 -4.56802354e-02 -4.03074329e-02 -3.05495444e-02
 -2.96149984e-02 -2.81429408e-02 -2.75463905e-02 -2.72756805e-02
 -2.71854297e-02 -2.47779670e-02 -2.35303851e-02 -2.02344298e-02
 -1.43076425e-02 -1.37384039e-02 -1.36103421e-02 -1.33933068e-02
 -1.30808101e-02 -1.29053816e-02 -1.17996041e-02 -1.16290668e-02
 -9.09885730e-03 -8.67523151e-03 -8.18665580e-03 -7.69547231e-03
 -7.55623486e-03 -6.33491816e-03 -6.05561819e-03 -5.87938193e-03
 -5.84027795e-03 -4.79385399e-03 -4.42143935e-03 -4.28854776e-03
 -3.83563191e-03 -2.98706231e-03 -2.12946147e-03 -1.78783763e-03
 -1.26831275e-03 -1.12667953e-03 -8.63319472e-04 -8.28045631e-04
 -8.20618896e-04 -3.10908300e-04 -6.67923862e-05  1.36266366e-04
  2.10753079e-04  1.09475338e-03  1.22210011e-03  1.56518781e-03
  1.77899581e-03  2.15889947e-03  2.61018289e-03  2.70926861e-03
  3.94571105e-03  4.43953608e-03  4.62908041e-03  4.78699851e-03
  5.38766575e-03  6.28469385e-03  6.34354696e-03  6.40128977e-03
  6.70339269e-03  6.75336404e-03  6.84901576e-03  7.68306574e-03
  8.19483872e-03  8.44144197e-03  8.62977040e-03  8.76535148e-03
  9.08129992e-03  9.67782252e-03  1.00304729e-02  1.01591857e-02
  1.03493467e-02  1.12322993e-02  1.19173445e-02  1.23163981e-02
  1.24987248e-02  1.47534335e-02  1.54318758e-02  1.57599525e-02
  1.64762981e-02  1.87599487e-02  1.88128336e-02  1.93254592e-02
  2.05593043e-02  2.05628374e-02  2.06432792e-02  2.11744962e-02
  2.19494570e-02  2.23388754e-02  2.24080495e-02  2.27256724e-02
  2.50356537e-02  2.50724892e-02  2.65958661e-02  2.68737492e-02
  2.69407157e-02  2.70018765e-02  2.71388052e-02  2.72758307e-02
  2.73548813e-02  2.82945050e-02  3.01924216e-02  3.02028237e-02
  3.10295048e-02  3.15364186e-02  3.17375745e-02  3.33453133e-02
  3.37836514e-02  3.40457953e-02  3.42158161e-02  3.44384587e-02
  3.50262987e-02  3.56912003e-02  3.67400547e-02  3.70699417e-02
  3.77890884e-02  3.86113626e-02  4.03527399e-02  4.13264565e-02
  4.16435011e-02  4.21164912e-02  4.22868709e-02  4.27782954e-02
  4.27837342e-02  4.35224858e-02  4.52909752e-02  4.59922132e-02
  4.67261542e-02  4.89240612e-02  4.98769312e-02  4.99567988e-02
  5.04468349e-02  5.06628820e-02  5.33431680e-02  5.40016570e-02
  5.66613423e-02  5.77528434e-02  5.90023308e-02  5.90694761e-02
  5.92113470e-02  5.94753120e-02  5.97329223e-02  5.99020528e-02
  6.02396309e-02  6.22570061e-02  6.28240598e-02  6.36432272e-02
  6.46526792e-02  6.64915829e-02  6.69400586e-02  6.69631698e-02
  6.70097240e-02  6.83933328e-02  6.91162863e-02  7.05718639e-02
  7.27645139e-02  7.50603217e-02  7.53483952e-02  7.67234185e-02
  7.76140018e-02  7.81429247e-02  7.97531246e-02  7.98122543e-02
  8.00608756e-02  8.00803665e-02  8.26844485e-02  8.41063300e-02
  8.56851054e-02  8.64520164e-02  8.69039247e-02  8.83437685e-02
  8.90757452e-02  8.97482065e-02  9.09712833e-02  9.14601037e-02
  9.21253428e-02  9.31694167e-02  9.45149448e-02  9.48049939e-02
  9.51449504e-02  9.62368085e-02  9.80298066e-02  9.87620388e-02
  9.99164864e-02  1.00623925e-01  1.02606903e-01  1.03176683e-01
  1.05300937e-01  1.05494510e-01  1.05906952e-01  1.05912391e-01
  1.06462048e-01  1.07300347e-01  1.07717960e-01  1.07841364e-01
  1.08109331e-01  1.10101188e-01  1.12174117e-01  1.12247762e-01
  1.16021889e-01  1.16063567e-01  1.17692835e-01  1.18331596e-01
  1.19762946e-01  1.20385523e-01  1.23027009e-01  1.25253394e-01
  1.25541074e-01  1.26337704e-01  1.27667085e-01  1.28581067e-01
  1.29374616e-01  1.29630014e-01  1.31762546e-01  1.31770103e-01
  1.33998558e-01  1.34250898e-01  1.35222858e-01  1.35435641e-01
  1.35569123e-01  1.36941283e-01  1.37603375e-01  1.37803494e-01
  1.38067715e-01  1.40411336e-01  1.41177889e-01  1.41645946e-01
  1.44357368e-01  1.45792386e-01  1.46396661e-01  1.47341186e-01
  1.47791253e-01  1.49100828e-01  1.50298456e-01  1.50397459e-01
  1.51072739e-01  1.55311492e-01  1.57902840e-01  1.59697562e-01
  1.59987997e-01  1.60517389e-01  1.64529166e-01  1.65651208e-01
  1.69126671e-01  1.70182354e-01  1.70454419e-01  1.70635583e-01
  1.74804657e-01  1.79272582e-01  1.79527249e-01  1.79879008e-01
  1.80242229e-01  1.84307012e-01  1.94738358e-01  1.95545811e-01
  1.95881001e-01  1.95904976e-01  1.96447232e-01  1.98136062e-01
  2.01205189e-01  2.07828054e-01  2.08469118e-01  2.08564619e-01
  2.08801838e-01  2.09006515e-01  2.09007161e-01  2.09343690e-01
  2.09518843e-01  2.09951146e-01  2.10197951e-01  2.15443995e-01
  2.17007814e-01  2.18047057e-01  2.18745752e-01  2.21108958e-01
  2.24077971e-01  2.24277603e-01  2.24511595e-01  2.27359869e-01
  2.27896658e-01  2.29441202e-01  2.29653340e-01  2.29812775e-01
  2.33990217e-01  2.35187250e-01  2.36163442e-01  2.37491114e-01
  2.38647902e-01  2.39041053e-01  2.39163992e-01  2.39755439e-01
  2.40150244e-01  2.41745179e-01  2.42820770e-01  2.44242660e-01
  2.44991181e-01  2.48095251e-01  2.48722977e-01  2.52366556e-01
  2.54352113e-01  2.55223533e-01  2.59386297e-01  2.61156204e-01
  2.64150738e-01  2.65374957e-01  2.66815777e-01  2.68835731e-01
  2.69180570e-01  2.71323974e-01  2.74958450e-01  2.76797720e-01
  2.78360512e-01  2.80802973e-01  2.82709091e-01  2.83051681e-01
  2.84972985e-01  2.87777166e-01  2.88209288e-01  2.91009550e-01
  2.93639234e-01  2.98728905e-01  3.00037133e-01  3.02825143e-01
  3.03111251e-01  3.15145134e-01  3.23096263e-01  3.24559306e-01
  3.24895793e-01  3.27550343e-01  3.30772692e-01  3.32075109e-01
  3.35919473e-01  3.37924891e-01  3.40523326e-01  3.41379310e-01
  3.43072788e-01  3.49236882e-01  3.53017021e-01  3.53712400e-01
  3.56922421e-01  3.57277232e-01  3.57375621e-01  3.58715828e-01
  3.60684159e-01  3.61722697e-01  3.63949956e-01  3.66307901e-01
  3.67331785e-01  3.71395467e-01  3.76035381e-01  3.76436300e-01
  3.85058716e-01  3.85247404e-01  3.88492664e-01  3.92455986e-01
  3.92508219e-01  3.93464194e-01  3.97502160e-01  4.03296227e-01
  4.04357893e-01  4.05257787e-01  4.05630573e-01  4.10454916e-01
  4.10753186e-01  4.13302873e-01  4.13629513e-01  4.16922614e-01
  4.17489505e-01  4.18396647e-01  4.19242119e-01  4.24810315e-01
  4.27226899e-01  4.27389246e-01  4.28048194e-01  4.28444810e-01
  4.31822036e-01  4.32723504e-01  4.35458486e-01  4.37609097e-01
  4.39372568e-01  4.44059796e-01  4.47758546e-01  4.49539857e-01
  4.58964695e-01  4.59512914e-01  4.63320098e-01  4.63735564e-01
  4.64018319e-01  4.64525696e-01  4.65019351e-01  4.66613408e-01
  4.72783208e-01  4.79096780e-01  4.83111429e-01  4.91553347e-01
  4.92902344e-01  5.00106153e-01  5.02849182e-01  5.03305609e-01
  5.10700332e-01  5.14095290e-01  5.14873418e-01  5.16428668e-01
  5.28138884e-01  5.29997048e-01  5.37828193e-01  5.38815715e-01
  5.44555085e-01  5.47238918e-01  5.48429427e-01  5.50400972e-01
  5.57802312e-01  5.60166706e-01  5.65067486e-01  5.71701610e-01
  5.71973845e-01  5.83703758e-01  5.86489544e-01  5.92257617e-01
  5.92599338e-01  5.93711174e-01  6.01022989e-01  6.02558439e-01
  6.14095797e-01  6.21302263e-01  6.25277421e-01  6.26385909e-01
  6.32316193e-01  6.34695259e-01  6.48535196e-01  6.50932235e-01
  6.52960710e-01  6.57144178e-01  6.57999684e-01  6.58753180e-01
  6.68783249e-01  6.69866899e-01  6.75258965e-01  6.78899744e-01
  6.80636243e-01  6.80970440e-01  6.88863469e-01  6.93254342e-01
  6.94081984e-01  6.94303683e-01  6.95498858e-01  7.01514851e-01
  7.08600291e-01  7.26148472e-01  7.27048770e-01  7.28456368e-01
  7.35815391e-01  7.40058179e-01  7.44014658e-01  7.45742088e-01
  7.48401526e-01  7.48688398e-01  7.66822793e-01  7.68067728e-01
  7.68284249e-01  7.70852701e-01  7.79960189e-01  7.91349980e-01
  7.95672759e-01  7.98576262e-01  8.01075937e-01  8.03271814e-01
  8.08051157e-01  8.08693785e-01  8.11229562e-01  8.13052956e-01
  8.15998763e-01  8.17895838e-01  8.33589423e-01  8.38653307e-01
  8.45171079e-01  8.47402184e-01  8.51404566e-01  8.53141889e-01
  8.56760677e-01  8.72735734e-01  8.75468452e-01  9.00514874e-01
  9.13808532e-01  9.23996142e-01  9.44440600e-01  9.49323609e-01
  9.58516789e-01  9.68701205e-01  9.86734956e-01  1.05364425e+00
  1.07894827e+00]

  warnings.warn(

2022-11-03 10:54:12,488:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.21486673e-02 -7.36277409e-02 -5.36416713e-02 -4.82933355e-02
 -4.51977521e-02 -4.29231607e-02 -3.88686782e-02 -3.78852491e-02
 -3.55550976e-02 -3.47342645e-02 -3.06634403e-02 -3.06111585e-02
 -2.99536981e-02 -2.95909709e-02 -2.89644713e-02 -2.85591656e-02
 -2.17643879e-02 -2.12008792e-02 -1.97913791e-02 -1.94539080e-02
 -1.91360648e-02 -1.85383019e-02 -1.85044853e-02 -1.84831182e-02
 -1.83154724e-02 -1.75277269e-02 -1.74607278e-02 -1.33123333e-02
 -1.27466374e-02 -1.23719959e-02 -1.23344661e-02 -1.19495003e-02
 -1.12039410e-02 -1.06433128e-02 -1.04107776e-02 -1.01559761e-02
 -9.97262342e-03 -8.88146795e-03 -8.63646599e-03 -8.27003805e-03
 -7.85277160e-03 -6.55142273e-03 -6.53183749e-03 -6.50177263e-03
 -6.17863718e-03 -5.37588563e-03 -5.32580585e-03 -5.08205879e-03
 -4.73646015e-03 -4.66502082e-03 -4.14242217e-03 -3.95028202e-03
 -3.58004913e-03 -3.19880275e-03 -5.36825534e-04 -5.33362663e-04
 -4.60414502e-05  2.76679257e-04  7.31666733e-04  7.94907195e-04
  8.99885491e-04  9.52746757e-04  1.00899710e-03  1.18974841e-03
  1.29339042e-03  1.57533335e-03  2.08315995e-03  2.08861571e-03
  2.31661507e-03  2.54678028e-03  2.57949585e-03  3.39644977e-03
  3.56636202e-03  3.88099651e-03  4.73325208e-03  5.60088192e-03
  6.42850036e-03  6.68462866e-03  7.10696173e-03  7.14593540e-03
  8.06334145e-03  8.21852913e-03  8.38065519e-03  8.92109584e-03
  9.17059375e-03  9.49716181e-03  9.50645386e-03  1.08526116e-02
  1.13900063e-02  1.17560709e-02  1.29034882e-02  1.43129021e-02
  1.49374551e-02  1.58751873e-02  1.63058205e-02  1.63737817e-02
  1.65772564e-02  1.68738339e-02  1.70090401e-02  1.71462430e-02
  1.76603951e-02  1.77536912e-02  1.79746477e-02  1.81513881e-02
  1.81892013e-02  1.91068747e-02  1.91939099e-02  1.96643857e-02
  1.97253917e-02  2.00089382e-02  2.01198662e-02  2.01808541e-02
  2.06528686e-02  2.07230211e-02  2.11905239e-02  2.18418300e-02
  2.22319795e-02  2.57411055e-02  2.79676271e-02  2.81651103e-02
  2.85345588e-02  2.88736289e-02  2.92650206e-02  3.06184681e-02
  3.15295901e-02  3.17603772e-02  3.24339100e-02  3.35040531e-02
  3.37034527e-02  3.47809218e-02  3.66067021e-02  4.09421111e-02
  4.21211613e-02  4.26402078e-02  4.34422908e-02  4.35069415e-02
  4.36884336e-02  4.72907029e-02  4.73765825e-02  4.79629952e-02
  4.94205616e-02  5.02026854e-02  5.20101095e-02  5.32945163e-02
  5.34902514e-02  5.35253196e-02  5.53871949e-02  5.54719218e-02
  5.57303627e-02  5.68248221e-02  5.72778847e-02  5.81034940e-02
  5.94651560e-02  5.97501850e-02  6.13865688e-02  6.14876318e-02
  6.21895814e-02  6.27345888e-02  6.28975479e-02  6.33454165e-02
  6.39502828e-02  6.79164885e-02  6.81523753e-02  6.83437424e-02
  6.91020512e-02  6.97324852e-02  7.08668706e-02  7.11601462e-02
  7.40526119e-02  7.45247400e-02  7.54521172e-02  7.65670081e-02
  7.66970621e-02  7.68467825e-02  7.69121627e-02  7.75733320e-02
  7.75900290e-02  7.82182773e-02  7.88649702e-02  8.14487202e-02
  8.17375228e-02  8.24134280e-02  8.65426170e-02  8.77213126e-02
  8.95544937e-02  9.00124593e-02  9.04907655e-02  9.14799945e-02
  9.40468965e-02  9.50350616e-02  9.52542518e-02  9.60490388e-02
  9.67992792e-02  9.68422885e-02  9.71240855e-02  9.79459239e-02
  1.01816888e-01  1.02753439e-01  1.03953378e-01  1.03992387e-01
  1.05256761e-01  1.06783309e-01  1.08063608e-01  1.09409435e-01
  1.09511552e-01  1.09697669e-01  1.11177740e-01  1.12428069e-01
  1.14499437e-01  1.16088654e-01  1.18108423e-01  1.18948883e-01
  1.20937388e-01  1.21085378e-01  1.21852850e-01  1.26591735e-01
  1.26952647e-01  1.28921747e-01  1.29033984e-01  1.30056855e-01
  1.31367549e-01  1.31580726e-01  1.32716329e-01  1.33404158e-01
  1.33545207e-01  1.34695903e-01  1.34863330e-01  1.35535968e-01
  1.35740625e-01  1.37624583e-01  1.39550909e-01  1.40397771e-01
  1.43484486e-01  1.44021788e-01  1.45644817e-01  1.45817348e-01
  1.46564687e-01  1.48131277e-01  1.48371011e-01  1.49160962e-01
  1.49267506e-01  1.51436311e-01  1.51862032e-01  1.53937631e-01
  1.54084173e-01  1.54293699e-01  1.55406611e-01  1.60717167e-01
  1.60792717e-01  1.62163318e-01  1.62617525e-01  1.66289669e-01
  1.71033651e-01  1.72527647e-01  1.75978468e-01  1.80002760e-01
  1.81092915e-01  1.85737247e-01  1.86944336e-01  1.86961315e-01
  1.88970490e-01  1.88987995e-01  1.95629523e-01  1.97866725e-01
  1.98344465e-01  2.01044654e-01  2.01461415e-01  2.01621485e-01
  2.02120196e-01  2.05516638e-01  2.06405545e-01  2.07138091e-01
  2.08350743e-01  2.08362421e-01  2.12891193e-01  2.12978850e-01
  2.13157621e-01  2.14148367e-01  2.14925793e-01  2.15353011e-01
  2.16391742e-01  2.16875744e-01  2.21279515e-01  2.21666230e-01
  2.25981446e-01  2.26248675e-01  2.28021083e-01  2.29449618e-01
  2.31518528e-01  2.32258711e-01  2.33609970e-01  2.34954077e-01
  2.34991035e-01  2.36776211e-01  2.37383608e-01  2.38032759e-01
  2.38874340e-01  2.41625990e-01  2.45573287e-01  2.47677504e-01
  2.48183703e-01  2.48440554e-01  2.57962889e-01  2.59031238e-01
  2.63352578e-01  2.63506823e-01  2.65509583e-01  2.66496317e-01
  2.68022201e-01  2.70132524e-01  2.70274227e-01  2.74601317e-01
  2.75049971e-01  2.75667413e-01  2.77215599e-01  2.79263701e-01
  2.80308863e-01  2.80716019e-01  2.81586174e-01  2.84170746e-01
  2.84522237e-01  2.84645777e-01  2.86446321e-01  2.86892311e-01
  2.87579469e-01  2.92943022e-01  3.00458205e-01  3.05397210e-01
  3.08689272e-01  3.11428264e-01  3.14286567e-01  3.14890473e-01
  3.15627323e-01  3.16295551e-01  3.19867276e-01  3.20425095e-01
  3.20482367e-01  3.21718687e-01  3.26871209e-01  3.29286574e-01
  3.36038080e-01  3.37230029e-01  3.37339149e-01  3.37616059e-01
  3.43884809e-01  3.50841243e-01  3.55775104e-01  3.56948859e-01
  3.62218243e-01  3.65298842e-01  3.71072507e-01  3.71585743e-01
  3.72448485e-01  3.77669054e-01  3.79587738e-01  3.82568242e-01
  3.83686497e-01  3.88321850e-01  3.94875714e-01  4.00505339e-01
  4.00532560e-01  4.01917635e-01  4.04752134e-01  4.05392384e-01
  4.05452834e-01  4.08123826e-01  4.10726530e-01  4.12400975e-01
  4.19458897e-01  4.19878766e-01  4.19964620e-01  4.20345598e-01
  4.24172572e-01  4.25397776e-01  4.29789949e-01  4.31523323e-01
  4.31560039e-01  4.36004598e-01  4.40089651e-01  4.40140018e-01
  4.41525515e-01  4.42272252e-01  4.45637810e-01  4.54051783e-01
  4.54558206e-01  4.58280551e-01  4.63827457e-01  4.68110763e-01
  4.78056935e-01  4.78669485e-01  4.79439588e-01  4.79774417e-01
  4.84001897e-01  4.84524982e-01  4.90860283e-01  4.96854361e-01
  5.07644633e-01  5.09095450e-01  5.11101588e-01  5.13282838e-01
  5.16472144e-01  5.17595120e-01  5.17913274e-01  5.18470308e-01
  5.23360963e-01  5.25910517e-01  5.31004303e-01  5.34381117e-01
  5.44171717e-01  5.48434330e-01  5.55739068e-01  5.65235012e-01
  5.75493513e-01  5.76582922e-01  5.77397356e-01  5.92649335e-01
  6.02531936e-01  6.13275355e-01  6.13414913e-01  6.15400721e-01
  6.15957464e-01  6.17513458e-01  6.25628210e-01  6.25755335e-01
  6.29047373e-01  6.42394971e-01  6.43960493e-01  6.48516772e-01
  6.50224104e-01  6.59337533e-01  6.62004139e-01  6.66887369e-01
  6.67943133e-01  6.69770413e-01  6.75534697e-01  6.85169286e-01
  6.87844407e-01  6.93140291e-01  6.97350785e-01  6.99032538e-01
  7.00576957e-01  7.07728399e-01  7.13760876e-01  7.14468876e-01
  7.18193929e-01  7.21655303e-01  7.22389149e-01  7.23465932e-01
  7.24552214e-01  7.25376894e-01  7.25634395e-01  7.30893493e-01
  7.31407104e-01  7.33336049e-01  7.38250689e-01  7.42230588e-01
  7.43294457e-01  7.46149770e-01  7.48267382e-01  7.64612898e-01
  7.65895580e-01  7.66108154e-01  7.69646989e-01  7.76375052e-01
  7.77403348e-01  7.91732625e-01  7.95522866e-01  8.13098565e-01
  8.21791366e-01  8.27170640e-01  8.27244854e-01  8.35979535e-01
  8.40584718e-01  8.41108407e-01  8.47545770e-01  8.56127603e-01
  8.56750944e-01  8.68110692e-01  8.72219820e-01  8.75531609e-01
  8.94180258e-01  9.05345374e-01  9.12084648e-01  9.27930163e-01
  9.33396302e-01  9.49048233e-01  9.49249629e-01  9.79701771e-01
  9.80238568e-01  9.85902368e-01  9.98245757e-01  1.00611979e+00
  1.02604243e+00]

  warnings.warn(

2022-11-03 10:54:12,569:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.76593258e-01 -6.13587992e-02 -5.40963368e-02 -5.35885899e-02
 -5.19645222e-02 -4.10951417e-02 -3.66443058e-02 -3.10975680e-02
 -2.98076455e-02 -2.91771396e-02 -2.52844419e-02 -2.36898816e-02
 -2.20200962e-02 -2.13856162e-02 -1.97075314e-02 -1.96422728e-02
 -1.89320636e-02 -1.86213249e-02 -1.81504393e-02 -1.73230165e-02
 -1.72878778e-02 -1.53815251e-02 -1.48630904e-02 -1.45708070e-02
 -1.38926600e-02 -1.28397227e-02 -1.27757595e-02 -1.25216152e-02
 -1.21391699e-02 -1.10174005e-02 -1.03717773e-02 -9.98282783e-03
 -9.42110046e-03 -9.30996084e-03 -8.48231159e-03 -6.16141735e-03
 -5.62117077e-03 -5.61155816e-03 -5.49333742e-03 -5.20620718e-03
 -4.56923888e-03 -4.25104494e-03 -4.21888941e-03 -4.21877458e-03
 -3.63022622e-03 -3.57744039e-03 -3.31204721e-03 -3.27321597e-03
 -2.57946721e-03 -2.17036439e-03 -1.74847944e-03 -1.00217076e-03
 -9.79731518e-04 -9.18376872e-04 -5.87182525e-04 -4.00333371e-04
 -2.60952786e-04  2.85834691e-04  5.14649059e-04  2.89995844e-03
  3.53069808e-03  4.10441096e-03  4.88695026e-03  5.01583644e-03
  5.20867407e-03  5.37996037e-03  6.40010059e-03  6.40259488e-03
  6.92845331e-03  7.11007274e-03  7.51901611e-03  8.36330082e-03
  8.44087375e-03  8.75130965e-03  8.83224852e-03  8.91662066e-03
  9.23972002e-03  9.33089560e-03  9.59520056e-03  9.77494595e-03
  1.01059435e-02  1.06801621e-02  1.13906590e-02  1.14240677e-02
  1.17361796e-02  1.23945599e-02  1.25573539e-02  1.26056015e-02
  1.33666007e-02  1.48229267e-02  1.51479930e-02  1.56753777e-02
  1.70520618e-02  1.74440962e-02  1.74991998e-02  1.82351220e-02
  1.83645556e-02  1.83857171e-02  1.84752034e-02  1.86249466e-02
  1.90680050e-02  1.92903401e-02  1.94328417e-02  1.99391939e-02
  2.00740482e-02  2.02674352e-02  2.05028822e-02  2.08412097e-02
  2.16028115e-02  2.56834268e-02  2.60023641e-02  2.62795788e-02
  2.67326094e-02  2.70743052e-02  2.76828973e-02  2.82560087e-02
  2.86129137e-02  2.90115833e-02  2.91079951e-02  2.92256421e-02
  2.99660314e-02  3.04226657e-02  3.08559185e-02  3.09665987e-02
  3.13125536e-02  3.17275312e-02  3.34820398e-02  3.59162413e-02
  3.98294408e-02  4.00177602e-02  4.09312310e-02  4.20509927e-02
  4.21467572e-02  4.32742477e-02  4.48794716e-02  4.58859729e-02
  4.63226408e-02  4.74258905e-02  4.75889723e-02  4.83471600e-02
  4.83891399e-02  4.85329982e-02  4.92638757e-02  4.96494802e-02
  5.09377213e-02  5.15352948e-02  5.21267024e-02  5.40055005e-02
  5.41217654e-02  5.44473747e-02  5.49986674e-02  5.85970723e-02
  5.89973571e-02  6.01694002e-02  6.03636780e-02  6.18447992e-02
  6.28096314e-02  6.28812862e-02  6.33508317e-02  6.36582254e-02
  6.38634512e-02  6.42458303e-02  6.53618289e-02  6.54049741e-02
  6.60640247e-02  6.87527570e-02  6.94708994e-02  7.27468497e-02
  7.35999901e-02  7.37397992e-02  7.42859303e-02  7.49006244e-02
  7.56449843e-02  7.56724827e-02  7.71149516e-02  7.77281540e-02
  7.81907771e-02  7.94368677e-02  8.16703228e-02  8.26941117e-02
  8.57855032e-02  8.58187114e-02  8.63859839e-02  8.71300055e-02
  8.82236146e-02  8.96183764e-02  9.00019492e-02  9.01053345e-02
  9.11852731e-02  9.12096925e-02  9.15110732e-02  9.23044785e-02
  9.29615447e-02  9.34911577e-02  9.42163549e-02  9.45651750e-02
  9.53852726e-02  9.85733489e-02  1.00065747e-01  1.00224416e-01
  1.03490750e-01  1.03642426e-01  1.03730915e-01  1.05161312e-01
  1.05598508e-01  1.05777773e-01  1.09510586e-01  1.10574226e-01
  1.13978228e-01  1.14325472e-01  1.15099560e-01  1.15553732e-01
  1.15813921e-01  1.19713246e-01  1.19971281e-01  1.20317918e-01
  1.22782922e-01  1.24125717e-01  1.25887516e-01  1.26862598e-01
  1.26990521e-01  1.27842980e-01  1.28970396e-01  1.29680204e-01
  1.30882413e-01  1.34855540e-01  1.36215782e-01  1.36844539e-01
  1.36969740e-01  1.38276302e-01  1.39225468e-01  1.39875112e-01
  1.40173568e-01  1.41425447e-01  1.41907475e-01  1.41916792e-01
  1.45408313e-01  1.45969994e-01  1.46572825e-01  1.47860339e-01
  1.48494887e-01  1.48551651e-01  1.48576901e-01  1.50115921e-01
  1.53957001e-01  1.55034579e-01  1.56534291e-01  1.59606674e-01
  1.59814753e-01  1.61668704e-01  1.61969960e-01  1.62615958e-01
  1.64264120e-01  1.64379712e-01  1.64689457e-01  1.65080665e-01
  1.65823645e-01  1.69608665e-01  1.70296950e-01  1.71562073e-01
  1.73141955e-01  1.77018303e-01  1.77608089e-01  1.80349907e-01
  1.80771866e-01  1.83063716e-01  1.83260847e-01  1.84431680e-01
  1.84782064e-01  1.85104764e-01  1.90151399e-01  1.96833453e-01
  1.98002523e-01  2.01503350e-01  2.03259726e-01  2.05816957e-01
  2.10604541e-01  2.13249209e-01  2.13826199e-01  2.17968112e-01
  2.19242893e-01  2.23701899e-01  2.24202639e-01  2.25785571e-01
  2.26273306e-01  2.27436387e-01  2.29244071e-01  2.31726367e-01
  2.32890656e-01  2.34914361e-01  2.35431951e-01  2.36637908e-01
  2.39347392e-01  2.39959782e-01  2.40507817e-01  2.40582146e-01
  2.40967084e-01  2.41540358e-01  2.43204777e-01  2.46665424e-01
  2.49273204e-01  2.52823764e-01  2.54736345e-01  2.60565644e-01
  2.60707243e-01  2.64393817e-01  2.65200094e-01  2.65847903e-01
  2.70042676e-01  2.72439536e-01  2.72629696e-01  2.72853530e-01
  2.73832103e-01  2.76322758e-01  2.76634808e-01  2.80236031e-01
  2.82134434e-01  2.82979566e-01  2.83992986e-01  2.91779213e-01
  2.94408165e-01  2.97080380e-01  2.98827097e-01  3.04989074e-01
  3.05317230e-01  3.05758173e-01  3.07081094e-01  3.08310244e-01
  3.09617629e-01  3.15727385e-01  3.18563552e-01  3.20116940e-01
  3.21511440e-01  3.23814935e-01  3.29519731e-01  3.31411323e-01
  3.36219655e-01  3.38066757e-01  3.40759666e-01  3.42073729e-01
  3.44024331e-01  3.45294260e-01  3.47589664e-01  3.50120235e-01
  3.51325964e-01  3.51871835e-01  3.62146412e-01  3.63869612e-01
  3.64498430e-01  3.71586716e-01  3.73787514e-01  3.74171303e-01
  3.77319246e-01  3.79665490e-01  3.79863326e-01  3.84512485e-01
  3.85311011e-01  3.88640833e-01  3.99974354e-01  4.05457210e-01
  4.09609335e-01  4.12841756e-01  4.20023138e-01  4.35369603e-01
  4.38252191e-01  4.38614632e-01  4.39501268e-01  4.39608372e-01
  4.41458694e-01  4.43480754e-01  4.44335659e-01  4.48162269e-01
  4.48863855e-01  4.50278046e-01  4.51050881e-01  4.56087015e-01
  4.64520536e-01  4.69778232e-01  4.70986656e-01  4.76005963e-01
  4.78209569e-01  4.78382909e-01  4.80702558e-01  4.81755816e-01
  4.83908280e-01  4.90697353e-01  4.97458087e-01  5.02493716e-01
  5.06114390e-01  5.12562628e-01  5.12977043e-01  5.14134513e-01
  5.14290862e-01  5.16553032e-01  5.18770044e-01  5.20899322e-01
  5.20946304e-01  5.22165961e-01  5.24191191e-01  5.25955910e-01
  5.26001059e-01  5.36111548e-01  5.44265354e-01  5.45036253e-01
  5.45093018e-01  5.45111557e-01  5.50377800e-01  5.56884782e-01
  5.79964568e-01  5.85347087e-01  5.91572226e-01  5.93728626e-01
  5.98614422e-01  6.06402587e-01  6.10711030e-01  6.11374615e-01
  6.17193496e-01  6.17757685e-01  6.18096212e-01  6.19953687e-01
  6.21197582e-01  6.21714235e-01  6.30663672e-01  6.51172082e-01
  6.53158919e-01  6.57136119e-01  6.60496533e-01  6.63091701e-01
  6.65656291e-01  6.66031368e-01  6.68298046e-01  6.69341522e-01
  6.74548213e-01  6.76228306e-01  7.02510034e-01  7.04591302e-01
  7.11272135e-01  7.15812474e-01  7.20923598e-01  7.25944524e-01
  7.27402392e-01  7.32178477e-01  7.36579896e-01  7.40168003e-01
  7.41868730e-01  7.42458890e-01  7.42474904e-01  7.43340368e-01
  7.43608177e-01  7.55088838e-01  7.57881878e-01  7.59053600e-01
  7.63689873e-01  7.63790202e-01  7.72413509e-01  7.74069609e-01
  7.83584127e-01  7.86102759e-01  7.86536698e-01  7.95953836e-01
  8.04192484e-01  8.06473422e-01  8.12095759e-01  8.34326607e-01
  8.42258587e-01  8.43699786e-01  8.48692041e-01  8.51636858e-01
  8.56134429e-01  8.63163933e-01  8.63922911e-01  8.66991203e-01
  8.74591057e-01  8.93758462e-01  8.94584396e-01  8.95299528e-01
  8.95818498e-01  8.99428235e-01  9.02929665e-01  9.07223663e-01
  9.07756820e-01  9.07834265e-01  9.28713779e-01  9.37288617e-01
  9.52742377e-01  9.73976182e-01  9.75119723e-01  9.96464505e-01
  1.00029975e+00  1.01669692e+00  1.16297790e+00]

  warnings.warn(

2022-11-03 10:54:15,182:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.98817729e-02 -2.96232106e-02 -2.36506902e-02 -2.27284499e-02
 -2.25709060e-02 -1.92426506e-02 -1.74841763e-02 -1.70848298e-02
 -1.66896001e-02 -1.51058132e-02 -1.46219977e-02 -1.39875853e-02
 -1.23428595e-02 -1.18860757e-02 -1.04791475e-02 -1.00290219e-02
 -9.10880031e-03 -8.93925560e-03 -8.77294734e-03 -8.37033612e-03
 -8.04907430e-03 -7.83178410e-03 -7.59299706e-03 -7.25243115e-03
 -6.37361652e-03 -6.25785627e-03 -6.09820703e-03 -4.78150983e-03
 -4.11471527e-03 -3.53402366e-03 -2.79227846e-03 -2.76356319e-03
 -2.56337964e-03 -1.42724808e-03 -1.24200916e-03  5.56131822e-04
  8.67363342e-04  1.40777926e-03  1.56573283e-03  1.61247627e-03
  1.94976261e-03  2.02774619e-03  2.08247474e-03  2.36555396e-03
  2.89179728e-03  2.94014955e-03  3.28704253e-03  3.69547117e-03
  4.25108089e-03  4.33076504e-03  5.11340787e-03  5.24017297e-03
  5.36117951e-03  5.39664724e-03  5.88474949e-03  7.05331681e-03
  7.12256154e-03  7.15544144e-03  7.83925777e-03  8.38637803e-03
  9.26136239e-03  9.27201264e-03  9.41648259e-03  1.11199416e-02
  1.22386597e-02  1.24652406e-02  1.27455739e-02  1.35358595e-02
  1.37891392e-02  1.43415080e-02  1.48264223e-02  1.50144352e-02
  1.53419687e-02  1.62641515e-02  1.65301490e-02  1.69876327e-02
  1.74540268e-02  1.79146604e-02  1.79985650e-02  1.81686661e-02
  1.93940209e-02  1.94017778e-02  2.00449316e-02  2.04946484e-02
  2.07827981e-02  2.19805308e-02  2.22812656e-02  2.27175462e-02
  2.29343731e-02  2.39215394e-02  2.57744663e-02  2.58640490e-02
  2.65047774e-02  2.72741694e-02  2.76490125e-02  2.81092837e-02
  2.90849461e-02  3.05851241e-02  3.07130492e-02  3.08372756e-02
  3.10153938e-02  3.12224443e-02  3.20130493e-02  3.20168849e-02
  3.29650523e-02  3.33763880e-02  3.36426930e-02  3.36661671e-02
  3.42322449e-02  3.43211423e-02  3.45595937e-02  3.45956480e-02
  3.47631559e-02  3.47893285e-02  3.49904929e-02  3.68577732e-02
  3.73999192e-02  3.86859402e-02  4.28948912e-02  4.32308493e-02
  4.35987109e-02  4.38218544e-02  4.42035981e-02  4.48314042e-02
  4.61315035e-02  4.66778746e-02  4.81006259e-02  4.81665113e-02
  4.83068699e-02  4.87730647e-02  4.92255397e-02  4.99409395e-02
  5.04955067e-02  5.04968128e-02  5.26451582e-02  5.46358081e-02
  5.54826347e-02  5.58197197e-02  5.60132477e-02  5.60344180e-02
  5.82352341e-02  6.06157562e-02  6.08166918e-02  6.13912176e-02
  6.29432680e-02  6.29616667e-02  6.32695448e-02  6.33964524e-02
  6.39042831e-02  6.62155358e-02  6.67151107e-02  6.69612663e-02
  6.69698747e-02  6.78242397e-02  6.81602189e-02  6.85273494e-02
  6.89860496e-02  6.98447377e-02  6.99712036e-02  7.00589894e-02
  7.06130219e-02  7.11517872e-02  7.11695302e-02  7.24607232e-02
  7.64552855e-02  7.72232779e-02  7.79292325e-02  7.83748278e-02
  7.92218861e-02  8.02605220e-02  8.09169907e-02  8.25459224e-02
  8.27250836e-02  8.29912150e-02  8.40977684e-02  8.63040317e-02
  8.66672663e-02  8.72276257e-02  8.77189975e-02  8.85279733e-02
  8.94159843e-02  9.20992155e-02  9.39260890e-02  9.39882182e-02
  9.43378433e-02  9.48965795e-02  9.66818045e-02  9.73326182e-02
  9.85339290e-02  9.96688538e-02  9.97135454e-02  1.00838197e-01
  1.01233632e-01  1.02231397e-01  1.03032096e-01  1.04356025e-01
  1.04846242e-01  1.07961614e-01  1.08492163e-01  1.10884504e-01
  1.13197862e-01  1.17610109e-01  1.17746229e-01  1.17993503e-01
  1.18055914e-01  1.24286745e-01  1.27631514e-01  1.34966543e-01
  1.37386420e-01  1.38487453e-01  1.38623239e-01  1.42619798e-01
  1.43930733e-01  1.44964715e-01  1.47286569e-01  1.50005904e-01
  1.50843261e-01  1.53595795e-01  1.54482801e-01  1.56492879e-01
  1.56694241e-01  1.56853318e-01  1.58255831e-01  1.58577846e-01
  1.59462552e-01  1.62462037e-01  1.62495337e-01  1.63318435e-01
  1.67024097e-01  1.67747526e-01  1.68501183e-01  1.68587924e-01
  1.71721372e-01  1.72878411e-01  1.72980495e-01  1.74814826e-01
  1.76096244e-01  1.76223765e-01  1.80533648e-01  1.82273862e-01
  1.82437087e-01  1.85154908e-01  1.86497074e-01  1.91048038e-01
  1.93941639e-01  1.94978008e-01  1.95531669e-01  1.98692170e-01
  1.99207314e-01  2.00412285e-01  2.01325924e-01  2.01864242e-01
  2.02734554e-01  2.03246120e-01  2.03483566e-01  2.04229124e-01
  2.09347823e-01  2.11087129e-01  2.14116385e-01  2.15318296e-01
  2.15760102e-01  2.16119400e-01  2.18220946e-01  2.18337447e-01
  2.21223199e-01  2.21318537e-01  2.22145694e-01  2.22791176e-01
  2.24149374e-01  2.26123399e-01  2.31258154e-01  2.32174777e-01
  2.34130594e-01  2.35224793e-01  2.38326210e-01  2.38881524e-01
  2.41047380e-01  2.42122533e-01  2.42524165e-01  2.45707396e-01
  2.46389012e-01  2.46687156e-01  2.48338571e-01  2.49329917e-01
  2.51397265e-01  2.52116144e-01  2.54562864e-01  2.56698290e-01
  2.61863277e-01  2.62097808e-01  2.63250284e-01  2.63989430e-01
  2.64543636e-01  2.65703712e-01  2.66757223e-01  2.66773074e-01
  2.67454819e-01  2.69542512e-01  2.70350478e-01  2.71243822e-01
  2.73617065e-01  2.74794373e-01  2.77266410e-01  2.77495557e-01
  2.79112478e-01  2.80014617e-01  2.80853797e-01  2.83131474e-01
  2.84032278e-01  2.86704921e-01  2.90661620e-01  2.90859411e-01
  2.95773930e-01  2.96607328e-01  2.98332684e-01  2.99006450e-01
  2.99655648e-01  3.00003163e-01  3.01001463e-01  3.02122025e-01
  3.02390449e-01  3.03208229e-01  3.04626824e-01  3.06340142e-01
  3.06485765e-01  3.06954855e-01  3.09363619e-01  3.11217360e-01
  3.14427800e-01  3.16014516e-01  3.16633932e-01  3.16737386e-01
  3.16821089e-01  3.18015630e-01  3.26022158e-01  3.26432391e-01
  3.36409388e-01  3.36511752e-01  3.47032284e-01  3.51440892e-01
  3.53955052e-01  3.54485686e-01  3.59569139e-01  3.60243116e-01
  3.60886971e-01  3.61263205e-01  3.62589748e-01  3.68116787e-01
  3.70165510e-01  3.71472207e-01  3.71724244e-01  3.73980817e-01
  3.80798737e-01  3.82362927e-01  3.85578787e-01  3.88213356e-01
  3.92885446e-01  3.96411439e-01  3.96539156e-01  3.99355323e-01
  4.01133604e-01  4.15001012e-01  4.16465530e-01  4.19325915e-01
  4.23159669e-01  4.23759027e-01  4.29102483e-01  4.30501930e-01
  4.33121775e-01  4.33544583e-01  4.50354314e-01  4.50904672e-01
  4.54500447e-01  4.55453216e-01  4.55894779e-01  4.58326891e-01
  4.60194744e-01  4.60348682e-01  4.66795664e-01  4.68873303e-01
  4.69628899e-01  4.70314790e-01  4.78550563e-01  4.80096586e-01
  4.81745823e-01  4.82222941e-01  4.82909581e-01  4.85653301e-01
  4.90439546e-01  4.93767012e-01  4.94080919e-01  4.98407549e-01
  5.00234755e-01  5.01102899e-01  5.07891185e-01  5.08033656e-01
  5.12042301e-01  5.12278154e-01  5.14103674e-01  5.15886565e-01
  5.16353141e-01  5.25791839e-01  5.27046660e-01  5.27223641e-01
  5.28264622e-01  5.29959519e-01  5.31434158e-01  5.33162350e-01
  5.37241920e-01  5.39721439e-01  5.40797341e-01  5.41454622e-01
  5.47146316e-01  5.57403596e-01  5.60338824e-01  5.70421273e-01
  5.71136922e-01  5.74209108e-01  5.78143072e-01  5.78725481e-01
  5.85004527e-01  5.85510994e-01  5.85900914e-01  5.93475798e-01
  5.96495666e-01  5.99489007e-01  6.00796635e-01  6.08514624e-01
  6.10658785e-01  6.11550640e-01  6.15772957e-01  6.18805876e-01
  6.21699544e-01  6.22851522e-01  6.24369093e-01  6.33099098e-01
  6.33342149e-01  6.35114593e-01  6.41235187e-01  6.44393199e-01
  6.48810482e-01  6.49552184e-01  6.64833434e-01  6.65052034e-01
  6.69545216e-01  6.74197400e-01  6.85757679e-01  6.95725604e-01
  7.03214441e-01  7.05176757e-01  7.09982719e-01  7.13395687e-01
  7.23221428e-01  7.28243807e-01  7.29993766e-01  7.33795401e-01
  7.34729623e-01  7.44212750e-01  7.45487267e-01  7.50920931e-01
  7.53565978e-01  7.55695141e-01  7.63838918e-01  7.75411168e-01
  7.78672866e-01  7.80797173e-01  7.84999349e-01  7.93184373e-01
  7.94508765e-01  7.96393360e-01  7.99685781e-01  8.03832453e-01
  8.09045433e-01  8.12847378e-01  8.14381090e-01  8.22356307e-01
  8.38314911e-01  8.43651020e-01  8.44384777e-01  9.10520759e-01
  9.21532914e-01  9.30089110e-01  9.51775105e-01  9.55505712e-01
  9.90155057e-01  9.96968672e-01  1.00195694e+00  1.00349030e+00
  1.01571852e+00  1.05616024e+00  1.05733284e+00  1.05784113e+00]

  warnings.warn(

2022-11-03 10:54:15,198:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-6.40090365e-02 -4.98985329e-02 -4.73172154e-02 -4.53116030e-02
 -4.44484462e-02 -4.20104887e-02 -4.15771019e-02 -3.96267906e-02
 -3.52227510e-02 -3.43542338e-02 -3.26493034e-02 -3.20317853e-02
 -2.72458256e-02 -2.48602644e-02 -2.46400522e-02 -2.32216575e-02
 -2.27100540e-02 -2.22612799e-02 -2.12695788e-02 -2.11292494e-02
 -2.01389164e-02 -1.93534115e-02 -1.91066956e-02 -1.83356948e-02
 -1.77296938e-02 -1.69091163e-02 -1.68300153e-02 -1.67772564e-02
 -1.65481649e-02 -1.43852208e-02 -1.40813152e-02 -1.28637102e-02
 -1.24657455e-02 -1.24248128e-02 -1.23967615e-02 -1.22537990e-02
 -1.10729946e-02 -1.08865900e-02 -1.08733121e-02 -1.08634088e-02
 -1.01772277e-02 -1.00237866e-02 -9.82405183e-03 -8.83060659e-03
 -8.78265936e-03 -8.59209628e-03 -7.95820930e-03 -7.89081282e-03
 -7.20213087e-03 -7.14061957e-03 -6.74110797e-03 -6.09283963e-03
 -5.74146853e-03 -5.34425869e-03 -5.30443916e-03 -4.72690184e-03
 -4.42731181e-03 -4.13730593e-03 -3.52208072e-03 -3.22157199e-03
 -3.17393736e-03 -2.68826131e-03 -2.63338130e-03 -2.60217356e-03
 -2.39370034e-03 -2.20391345e-03 -2.08565948e-03 -1.89284525e-03
 -1.77002346e-03 -1.76002638e-03 -1.18161485e-03 -1.05689667e-03
  3.78809697e-05  1.07911602e-04  3.44816624e-04  4.96365597e-04
  5.51867914e-04  6.28705146e-04  7.15434047e-04  9.76819673e-04
  1.02410148e-03  1.23534304e-03  1.75480029e-03  1.82982543e-03
  2.71399616e-03  3.37594582e-03  3.56875285e-03  4.04920486e-03
  4.09421175e-03  4.96252899e-03  5.17774692e-03  5.70365108e-03
  5.77452678e-03  6.28606267e-03  6.99976921e-03  7.15245745e-03
  8.32082630e-03  8.40013797e-03  9.64191883e-03  9.69871397e-03
  9.83304599e-03  1.05781563e-02  1.08552038e-02  1.13350702e-02
  1.18169298e-02  1.19564088e-02  1.24300713e-02  1.25658645e-02
  1.34401903e-02  1.34408114e-02  1.41610613e-02  1.45007697e-02
  1.54409729e-02  1.62506811e-02  1.63796979e-02  1.67184555e-02
  2.04710820e-02  2.22107693e-02  2.25769219e-02  2.26158377e-02
  2.30736139e-02  2.33074354e-02  2.35920374e-02  2.52384841e-02
  2.56201202e-02  2.61858048e-02  2.64360579e-02  2.81129410e-02
  3.03012231e-02  3.05650305e-02  3.10070195e-02  3.14490300e-02
  3.19703036e-02  3.20704134e-02  3.23129174e-02  3.26380477e-02
  3.37235642e-02  3.40568463e-02  3.53102274e-02  3.59003459e-02
  3.64329798e-02  3.70980142e-02  3.77498730e-02  3.89356531e-02
  3.95787565e-02  4.09170988e-02  4.24118809e-02  4.24257271e-02
  4.25755228e-02  4.29975876e-02  4.41886014e-02  4.75725181e-02
  4.85541393e-02  4.89693056e-02  4.96801543e-02  4.99273161e-02
  5.02381305e-02  5.07912657e-02  5.17838827e-02  5.21447128e-02
  5.28221401e-02  5.29013870e-02  5.37205225e-02  5.44316075e-02
  5.50006391e-02  5.79565977e-02  5.90986397e-02  5.92285108e-02
  5.99670081e-02  6.08586776e-02  6.16718114e-02  6.30352290e-02
  6.39540907e-02  6.40274257e-02  6.44593407e-02  6.54561156e-02
  6.57152378e-02  6.62228027e-02  6.77172531e-02  7.13233918e-02
  7.20859373e-02  7.26096785e-02  7.41700597e-02  7.53640166e-02
  7.63835341e-02  7.67157479e-02  7.71255219e-02  7.82320989e-02
  7.95377192e-02  8.06411841e-02  8.35583874e-02  8.38930782e-02
  8.72043527e-02  8.80664364e-02  8.83864693e-02  8.84150971e-02
  8.98670854e-02  9.22258055e-02  9.30299054e-02  9.39566774e-02
  9.62036777e-02  9.63258169e-02  9.64377308e-02  9.66947323e-02
  9.75999347e-02  9.92401667e-02  9.96707943e-02  9.98154971e-02
  1.00909531e-01  1.01505092e-01  1.02554626e-01  1.06015718e-01
  1.06341989e-01  1.06563845e-01  1.06695306e-01  1.07294002e-01
  1.07748220e-01  1.08761141e-01  1.09427977e-01  1.09854217e-01
  1.12023440e-01  1.13812571e-01  1.15519008e-01  1.18007728e-01
  1.18229733e-01  1.20593275e-01  1.23247797e-01  1.23835592e-01
  1.24042255e-01  1.26064673e-01  1.26631434e-01  1.29362833e-01
  1.30293244e-01  1.31184171e-01  1.33671595e-01  1.35204603e-01
  1.37157085e-01  1.37193363e-01  1.37742655e-01  1.40268647e-01
  1.42287403e-01  1.42410102e-01  1.44233200e-01  1.44593701e-01
  1.46116561e-01  1.49905178e-01  1.52220457e-01  1.52270151e-01
  1.53983105e-01  1.54596616e-01  1.55294736e-01  1.60227742e-01
  1.61002620e-01  1.61524305e-01  1.61651029e-01  1.61711426e-01
  1.63714092e-01  1.67193687e-01  1.67243572e-01  1.67495782e-01
  1.68145567e-01  1.71162332e-01  1.71678788e-01  1.73157932e-01
  1.73599437e-01  1.75312884e-01  1.77113365e-01  1.77325536e-01
  1.79997558e-01  1.80100770e-01  1.80193191e-01  1.83009924e-01
  1.84315818e-01  1.87612480e-01  1.88009712e-01  1.88637511e-01
  1.88779240e-01  1.89954066e-01  1.90009544e-01  1.94730310e-01
  1.95551793e-01  1.96027521e-01  1.98036376e-01  1.98891810e-01
  2.01127540e-01  2.01973160e-01  2.07942888e-01  2.07957979e-01
  2.08459732e-01  2.08775320e-01  2.09828127e-01  2.10726067e-01
  2.10938795e-01  2.13815868e-01  2.14768926e-01  2.18855718e-01
  2.21784487e-01  2.25227209e-01  2.26666719e-01  2.35656200e-01
  2.36757586e-01  2.38416545e-01  2.38833114e-01  2.42237164e-01
  2.49411045e-01  2.50361291e-01  2.50912084e-01  2.51598861e-01
  2.60289183e-01  2.61411216e-01  2.64426157e-01  2.65189612e-01
  2.67446902e-01  2.69765219e-01  2.70699064e-01  2.72440043e-01
  2.73033386e-01  2.78983833e-01  2.86178378e-01  2.86286662e-01
  2.87239692e-01  2.88824395e-01  2.92130152e-01  2.94258289e-01
  2.95273598e-01  2.95513859e-01  3.00763554e-01  3.00773018e-01
  3.01117269e-01  3.03373135e-01  3.04542775e-01  3.08769752e-01
  3.12512253e-01  3.14188365e-01  3.16884974e-01  3.17162745e-01
  3.24626842e-01  3.29573784e-01  3.30706740e-01  3.36502548e-01
  3.42771660e-01  3.46632812e-01  3.47528803e-01  3.49981163e-01
  3.53665564e-01  3.63641517e-01  3.65966288e-01  3.69496138e-01
  3.71640177e-01  3.74283176e-01  3.77561089e-01  3.77669474e-01
  3.79098788e-01  3.82058662e-01  3.84685203e-01  3.84738624e-01
  3.88351294e-01  3.93556155e-01  3.96488661e-01  3.99356327e-01
  4.00106923e-01  4.00448056e-01  4.04109104e-01  4.04660443e-01
  4.07478139e-01  4.07529584e-01  4.10812362e-01  4.12193659e-01
  4.16868692e-01  4.19608249e-01  4.21470584e-01  4.22230028e-01
  4.30211310e-01  4.35038118e-01  4.36293162e-01  4.37855664e-01
  4.38080600e-01  4.40469489e-01  4.42962238e-01  4.45078376e-01
  4.48955688e-01  4.49082344e-01  4.52238679e-01  4.52926422e-01
  4.57192683e-01  4.61140955e-01  4.61881690e-01  4.63763469e-01
  4.70317379e-01  4.86676582e-01  4.90654916e-01  4.91393695e-01
  4.91861700e-01  4.94827386e-01  4.98543233e-01  4.99084858e-01
  5.08433853e-01  5.17079236e-01  5.22573311e-01  5.23184403e-01
  5.23687415e-01  5.25506656e-01  5.28876434e-01  5.32208055e-01
  5.33946425e-01  5.37140825e-01  5.38051443e-01  5.54054989e-01
  5.55078312e-01  5.65144729e-01  5.74173198e-01  5.76245038e-01
  5.78166333e-01  5.80273452e-01  5.95715279e-01  5.97621383e-01
  5.99364873e-01  5.99371960e-01  6.00712496e-01  6.09268636e-01
  6.11412599e-01  6.14691025e-01  6.15999847e-01  6.19915302e-01
  6.26976437e-01  6.27586211e-01  6.28424942e-01  6.32804833e-01
  6.41856830e-01  6.41907539e-01  6.55538486e-01  6.57267373e-01
  6.67273851e-01  6.72383875e-01  6.75806366e-01  6.78716365e-01
  6.81576754e-01  6.83528633e-01  6.91594198e-01  6.92648847e-01
  6.96271736e-01  7.00431199e-01  7.05906166e-01  7.08985526e-01
  7.13270692e-01  7.14516066e-01  7.15700866e-01  7.17789293e-01
  7.21205708e-01  7.21838426e-01  7.34474233e-01  7.39367525e-01
  7.41033755e-01  7.43633138e-01  7.45950431e-01  7.59939582e-01
  7.63347036e-01  7.64207751e-01  7.65477258e-01  7.79170993e-01
  7.83550856e-01  7.87543315e-01  7.91009577e-01  7.93303420e-01
  7.94314272e-01  7.97311266e-01  7.97515598e-01  8.07842851e-01
  8.21555150e-01  8.23596820e-01  8.31274771e-01  8.33315674e-01
  8.35790085e-01  8.37705480e-01  8.47900308e-01  8.50377675e-01
  8.61313249e-01  8.63043859e-01  8.63980319e-01  8.82455802e-01
  8.82742360e-01  8.85305005e-01  8.86169566e-01  8.95960790e-01
  9.06169433e-01  9.12936031e-01  9.18577365e-01  9.25414697e-01
  9.37189904e-01  9.54922214e-01  9.64926171e-01  9.65232923e-01
  9.81147575e-01]

  warnings.warn(

2022-11-03 10:54:15,198:INFO:Calculating mean and std
2022-11-03 10:54:15,198:INFO:Creating metrics dataframe
2022-11-03 10:54:15,198:INFO:Uploading results into container
2022-11-03 10:54:15,214:INFO:Uploading model into container now
2022-11-03 10:54:15,214:INFO:master_model_container: 36
2022-11-03 10:54:15,214:INFO:display_container: 2
2022-11-03 10:54:15,214:INFO:LGBMRegressor(random_state=4411)
2022-11-03 10:54:15,214:INFO:create_model() successfully completed......................................
2022-11-03 10:54:15,471:ERROR:create_model() for LGBMRegressor(random_state=4411) raised an exception or returned all 0.0:
2022-11-03 10:54:15,479:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:54:15,479:INFO:Initializing Dummy Regressor
2022-11-03 10:54:15,479:INFO:Total runtime is 5.659499406814575 minutes
2022-11-03 10:54:15,479:INFO:SubProcess create_model() called ==================================
2022-11-03 10:54:15,479:INFO:Initializing create_model()
2022-11-03 10:54:15,479:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:54:15,479:INFO:Checking exceptions
2022-11-03 10:54:15,487:INFO:Importing libraries
2022-11-03 10:54:15,487:INFO:Copying training dataset
2022-11-03 10:54:15,503:INFO:Defining folds
2022-11-03 10:54:15,503:INFO:Declaring metric variables
2022-11-03 10:54:15,503:INFO:Importing untrained model
2022-11-03 10:54:15,503:INFO:Dummy Regressor Imported successfully
2022-11-03 10:54:15,503:INFO:Starting cross validation
2022-11-03 10:54:15,503:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:54:19,472:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26031102]

  warnings.warn(

2022-11-03 10:54:19,480:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26031102]

  warnings.warn(

2022-11-03 10:54:19,526:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25986027]

  warnings.warn(

2022-11-03 10:54:19,605:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26076178]

  warnings.warn(

2022-11-03 10:54:19,628:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2625648]

  warnings.warn(

2022-11-03 10:54:19,645:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26121253]

  warnings.warn(

2022-11-03 10:54:19,661:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26324093]

  warnings.warn(

2022-11-03 10:54:19,685:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25918413]

  warnings.warn(

2022-11-03 10:54:22,088:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25963489]

  warnings.warn(

2022-11-03 10:54:22,112:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26143791]

  warnings.warn(

2022-11-03 10:54:22,112:INFO:Calculating mean and std
2022-11-03 10:54:22,120:INFO:Creating metrics dataframe
2022-11-03 10:54:22,128:INFO:Uploading results into container
2022-11-03 10:54:22,128:INFO:Uploading model into container now
2022-11-03 10:54:22,128:INFO:master_model_container: 37
2022-11-03 10:54:22,128:INFO:display_container: 2
2022-11-03 10:54:22,128:INFO:DummyRegressor()
2022-11-03 10:54:22,128:INFO:create_model() successfully completed......................................
2022-11-03 10:54:22,389:WARNING:create_model() for DummyRegressor() raised an exception or returned all 0.0, trying without fit_kwargs:
2022-11-03 10:54:22,389:WARNING:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

2022-11-03 10:54:22,389:INFO:Initializing create_model()
2022-11-03 10:54:22,389:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000022E7CABEF50>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022E163C8D00>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:54:22,389:INFO:Checking exceptions
2022-11-03 10:54:22,405:INFO:Importing libraries
2022-11-03 10:54:22,405:INFO:Copying training dataset
2022-11-03 10:54:22,421:INFO:Defining folds
2022-11-03 10:54:22,421:INFO:Declaring metric variables
2022-11-03 10:54:22,421:INFO:Importing untrained model
2022-11-03 10:54:22,421:INFO:Dummy Regressor Imported successfully
2022-11-03 10:54:22,421:INFO:Starting cross validation
2022-11-03 10:54:22,436:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:54:26,403:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26031102]

  warnings.warn(

2022-11-03 10:54:26,403:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26031102]

  warnings.warn(

2022-11-03 10:54:26,481:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25986027]

  warnings.warn(

2022-11-03 10:54:26,505:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2625648]

  warnings.warn(

2022-11-03 10:54:26,518:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26076178]

  warnings.warn(

2022-11-03 10:54:26,622:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26324093]

  warnings.warn(

2022-11-03 10:54:26,622:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26121253]

  warnings.warn(

2022-11-03 10:54:26,622:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25918413]

  warnings.warn(

2022-11-03 10:54:28,954:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25963489]

  warnings.warn(

2022-11-03 10:54:28,986:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 74, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 261, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.26143791]

  warnings.warn(

2022-11-03 10:54:28,994:INFO:Calculating mean and std
2022-11-03 10:54:28,994:INFO:Creating metrics dataframe
2022-11-03 10:54:29,002:INFO:Uploading results into container
2022-11-03 10:54:29,002:INFO:Uploading model into container now
2022-11-03 10:54:29,002:INFO:master_model_container: 38
2022-11-03 10:54:29,002:INFO:display_container: 2
2022-11-03 10:54:29,002:INFO:DummyRegressor()
2022-11-03 10:54:29,002:INFO:create_model() successfully completed......................................
2022-11-03 10:54:29,257:ERROR:create_model() for DummyRegressor() raised an exception or returned all 0.0:
2022-11-03 10:54:29,257:ERROR:Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 795, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 811, in compare_models
    assert (
AssertionError

2022-11-03 10:54:29,257:INFO:master_model_container: 38
2022-11-03 10:54:29,257:INFO:display_container: 2
2022-11-03 10:54:29,257:INFO:[]
2022-11-03 10:54:29,257:INFO:compare_models() successfully completed......................................
2022-11-03 10:54:29,311:INFO:Initializing save_model()
2022-11-03 10:54:29,311:INFO:save_model(model=[], model_name=best_model, prep_pipe_=Pipeline(memory=Memory(location=C:\Users\HARISH~1\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(include=['Senior Citizen', 'tenure',
                                             'Monthly Charges'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Customer ID', 'Ge...
                                                                    'Payment '
                                                                    'Method'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['Customer ID', 'Total Charges'],
                                    transformer=LeaveOneOutEncoder(cols=['Customer '
                                                                         'ID',
                                                                         'Total '
                                                                         'Charges'],
                                                                   handle_missing='return_nan',
                                                                   random_state=4411))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-11-03 10:54:29,311:INFO:Adding model into prep_pipe
2022-11-03 10:54:29,389:INFO:best_model.pkl saved in current working directory
2022-11-03 10:54:29,465:INFO:Pipeline(memory=Memory(location=C:\Users\HARISH~1\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(include=['Senior Citizen', 'tenure',
                                             'Monthly Charges'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Customer ID', 'Ge...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['Customer ID', 'Total Charges'],
                                    transformer=LeaveOneOutEncoder(cols=['Customer '
                                                                         'ID',
                                                                         'Total '
                                                                         'Charges'],
                                                                   handle_missing='return_nan',
                                                                   random_state=4411))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', [])])
2022-11-03 10:54:29,465:INFO:save_model() successfully completed......................................
2022-11-03 10:56:16,965:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:56:16,965:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:56:16,965:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:56:16,965:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-11-03 10:56:17,733:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2022-11-03 10:58:35,682:INFO:PyCaret ClassificationExperiment
2022-11-03 10:58:35,682:INFO:Logging name: clf-default-name
2022-11-03 10:58:35,682:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2022-11-03 10:58:35,682:INFO:version 3.0.0.rc4
2022-11-03 10:58:35,683:INFO:Initializing setup()
2022-11-03 10:58:35,683:INFO:self.USI: 7870
2022-11-03 10:58:35,683:INFO:self.variable_keys: {'target_param', 'fold_shuffle_param', 'fix_imbalance', 'log_plots_param', 'variable_keys', 'n_jobs_param', '_available_plots', '_all_models', '_is_multiclass', 'master_model_container', 'memory', 'USI', 'y_test', 'X_train', 'exp_name_log', 'display_container', 'pipeline', 'y_train', '_all_models_internal', 'seed', 'logging_param', 'exp_id', 'html_param', 'fold_generator', '_gpu_n_jobs_param', 'X_test', 'y', 'gpu_param', '_ml_usecase', 'X', 'idx', 'fold_groups_param', 'data', '_all_metrics'}
2022-11-03 10:58:35,683:INFO:Checking environment
2022-11-03 10:58:35,683:INFO:python_version: 3.10.7
2022-11-03 10:58:35,683:INFO:python_build: ('tags/v3.10.7:6cc6b13', 'Sep  5 2022 14:08:36')
2022-11-03 10:58:35,683:INFO:machine: AMD64
2022-11-03 10:58:35,707:INFO:platform: Windows-10-10.0.22000-SP0
2022-11-03 10:58:35,707:INFO:Memory: svmem(total=16901767168, available=5846822912, percent=65.4, used=11054944256, free=5846822912)
2022-11-03 10:58:35,707:INFO:Physical Core: 4
2022-11-03 10:58:35,707:INFO:Logical Core: 8
2022-11-03 10:58:35,707:INFO:Checking libraries
2022-11-03 10:58:35,707:INFO:System:
2022-11-03 10:58:35,707:INFO:    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
2022-11-03 10:58:35,707:INFO:executable: C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\python.exe
2022-11-03 10:58:35,707:INFO:   machine: Windows-10-10.0.22000-SP0
2022-11-03 10:58:35,707:INFO:PyCaret required dependencies:
2022-11-03 10:58:35,707:INFO:                 pip: 22.3
2022-11-03 10:58:35,707:INFO:          setuptools: 63.2.0
2022-11-03 10:58:35,707:INFO:             pycaret: 3.0.0rc4
2022-11-03 10:58:35,707:INFO:             IPython: 8.5.0
2022-11-03 10:58:35,707:INFO:          ipywidgets: 8.0.2
2022-11-03 10:58:35,707:INFO:                tqdm: 4.64.1
2022-11-03 10:58:35,707:INFO:               numpy: 1.22.4
2022-11-03 10:58:35,707:INFO:              pandas: 1.4.4
2022-11-03 10:58:35,707:INFO:              jinja2: 3.1.2
2022-11-03 10:58:35,707:INFO:               scipy: 1.8.1
2022-11-03 10:58:35,707:INFO:              joblib: 1.2.0
2022-11-03 10:58:35,707:INFO:             sklearn: 1.1.2
2022-11-03 10:58:35,707:INFO:                pyod: 1.0.6
2022-11-03 10:58:35,715:INFO:            imblearn: 0.9.1
2022-11-03 10:58:35,715:INFO:   category_encoders: 2.5.1.post0
2022-11-03 10:58:35,715:INFO:            lightgbm: 3.3.3
2022-11-03 10:58:35,715:INFO:               numba: 0.55.2
2022-11-03 10:58:35,715:INFO:            requests: 2.28.1
2022-11-03 10:58:35,715:INFO:          matplotlib: 3.5.3
2022-11-03 10:58:35,715:INFO:          scikitplot: 0.3.7
2022-11-03 10:58:35,715:INFO:         yellowbrick: 1.5
2022-11-03 10:58:35,715:INFO:              plotly: 5.10.0
2022-11-03 10:58:35,715:INFO:             kaleido: 0.2.1
2022-11-03 10:58:35,715:INFO:         statsmodels: 0.13.4
2022-11-03 10:58:35,715:INFO:              sktime: 0.13.4
2022-11-03 10:58:35,715:INFO:               tbats: 1.1.1
2022-11-03 10:58:35,715:INFO:            pmdarima: 1.8.5
2022-11-03 10:58:35,715:INFO:              psutil: 5.9.2
2022-11-03 10:58:35,715:INFO:PyCaret optional dependencies:
2022-11-03 10:58:35,758:INFO:                shap: Not installed
2022-11-03 10:58:35,758:INFO:           interpret: Not installed
2022-11-03 10:58:35,758:INFO:                umap: Not installed
2022-11-03 10:58:35,758:INFO:    pandas_profiling: 3.4.0
2022-11-03 10:58:35,758:INFO:  explainerdashboard: Not installed
2022-11-03 10:58:35,758:INFO:             autoviz: Not installed
2022-11-03 10:58:35,758:INFO:           fairlearn: Not installed
2022-11-03 10:58:35,758:INFO:             xgboost: 1.6.2
2022-11-03 10:58:35,758:INFO:            catboost: Not installed
2022-11-03 10:58:35,758:INFO:              kmodes: Not installed
2022-11-03 10:58:35,758:INFO:             mlxtend: Not installed
2022-11-03 10:58:35,758:INFO:       statsforecast: Not installed
2022-11-03 10:58:35,758:INFO:        tune_sklearn: Not installed
2022-11-03 10:58:35,758:INFO:                 ray: Not installed
2022-11-03 10:58:35,758:INFO:            hyperopt: Not installed
2022-11-03 10:58:35,758:INFO:              optuna: Not installed
2022-11-03 10:58:35,758:INFO:               skopt: Not installed
2022-11-03 10:58:35,758:INFO:              mlflow: Not installed
2022-11-03 10:58:35,758:INFO:              gradio: Not installed
2022-11-03 10:58:35,758:INFO:             fastapi: Not installed
2022-11-03 10:58:35,758:INFO:             uvicorn: Not installed
2022-11-03 10:58:35,758:INFO:              m2cgen: Not installed
2022-11-03 10:58:35,758:INFO:           evidently: Not installed
2022-11-03 10:58:35,758:INFO:                nltk: Not installed
2022-11-03 10:58:35,758:INFO:            pyLDAvis: Not installed
2022-11-03 10:58:35,758:INFO:              gensim: Not installed
2022-11-03 10:58:35,766:INFO:               spacy: Not installed
2022-11-03 10:58:35,766:INFO:           wordcloud: Not installed
2022-11-03 10:58:35,766:INFO:            textblob: Not installed
2022-11-03 10:58:35,766:INFO:               fugue: Not installed
2022-11-03 10:58:35,766:INFO:           streamlit: 1.14.0
2022-11-03 10:58:35,766:INFO:             prophet: Not installed
2022-11-03 10:58:35,766:INFO:None
2022-11-03 10:58:35,766:INFO:Set up data.
2022-11-03 10:58:35,854:INFO:Set up train/test split.
2022-11-03 10:58:35,903:INFO:Set up index.
2022-11-03 10:58:35,903:INFO:Assigning column types.
2022-11-03 10:58:35,911:INFO:Set up folding strategy.
2022-11-03 10:58:35,911:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-11-03 10:58:36,044:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-03 10:58:36,075:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2022-11-03 10:58:36,180:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:58:36,467:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:58:36,592:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-03 10:58:36,592:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2022-11-03 10:58:36,686:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:58:36,704:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:58:36,704:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-11-03 10:58:36,862:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2022-11-03 10:58:36,950:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:58:36,966:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:58:37,101:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2022-11-03 10:58:37,198:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:58:37,198:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:58:37,198:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2022-11-03 10:58:37,428:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:58:37,444:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:58:37,673:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:58:37,691:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:58:37,691:INFO:Preparing preprocessing pipeline...
2022-11-03 10:58:37,699:INFO:Set up label encoding.
2022-11-03 10:58:37,699:INFO:Set up simple imputation.
2022-11-03 10:58:37,723:INFO:Set up encoding of ordinal features.
2022-11-03 10:58:37,732:INFO:Set up encoding of categorical features.
2022-11-03 10:58:37,732:INFO:Set up variance threshold.
2022-11-03 10:58:41,210:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:58:42,984:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:58:43,749:INFO:Finished creating preprocessing pipeline.
2022-11-03 10:58:43,796:INFO:Pipeline: Pipeline(memory=Memory(location=C:\Users\HARISH~1\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Senior Citizen', 'tenure',
                                             'Monthly Charges'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fi...
                                    include=['Customer ID', 'Total Charges'],
                                    transformer=LeaveOneOutEncoder(cols=['Customer '
                                                                         'ID',
                                                                         'Total '
                                                                         'Charges'],
                                                                   drop_invariant=False,
                                                                   handle_missing='return_nan',
                                                                   handle_unknown='value',
                                                                   random_state=3967,
                                                                   return_df=True,
                                                                   sigma=None,
                                                                   verbose=0))),
                ('low_variance',
                 TransformerWrapper(exclude=[], include=None,
                                    transformer=VarianceThreshold(threshold=0)))],
         verbose=False)
2022-11-03 10:58:43,796:INFO:Creating final display dataframe.
2022-11-03 10:58:47,202:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:225: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(transformer, X, y)

2022-11-03 10:58:48,788:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:225: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(transformer, X, y)

2022-11-03 10:58:52,249:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:225: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(transformer, X, y)

2022-11-03 10:59:02,735:INFO:Setup display_container:                     Description             Value
0                    Session id              3967
1                        Target             Churn
2                   Target type            Binary
3                Target mapping     No: 0, Yes: 1
4           Original data shape        (7044, 21)
5        Transformed data shape        (7044, 41)
6   Transformed train set shape        (4930, 41)
7    Transformed test set shape        (2114, 41)
8              Ordinal features                 5
9              Numeric features                 3
10         Categorical features                17
11                   Preprocess              True
12              Imputation type            simple
13           Numeric imputation              mean
14       Categorical imputation          constant
15     Maximum one-hot encoding                 5
16              Encoding method              None
17       Low variance threshold                 0
18               Fold Generator   StratifiedKFold
19                  Fold Number                10
20                     CPU Jobs                -1
21                      Use GPU             False
22               Log Experiment             False
23              Experiment Name  clf-default-name
24                          USI              7870
2022-11-03 10:59:03,032:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:59:03,035:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:59:03,278:INFO:Soft dependency imported: xgboost: 1.6.2
2022-11-03 10:59:03,278:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-03 10:59:03,313:INFO:setup() successfully completed in 27.64s...............
2022-11-03 10:59:03,329:INFO:Initializing compare_models()
2022-11-03 10:59:03,329:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2022-11-03 10:59:03,329:INFO:Checking exceptions
2022-11-03 10:59:03,346:INFO:Preparing display monitor
2022-11-03 10:59:03,359:INFO:Initializing Logistic Regression
2022-11-03 10:59:03,359:INFO:Total runtime is 0.0 minutes
2022-11-03 10:59:03,359:INFO:SubProcess create_model() called ==================================
2022-11-03 10:59:03,359:INFO:Initializing create_model()
2022-11-03 10:59:03,359:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:59:03,359:INFO:Checking exceptions
2022-11-03 10:59:03,373:INFO:Importing libraries
2022-11-03 10:59:03,373:INFO:Copying training dataset
2022-11-03 10:59:03,385:INFO:Defining folds
2022-11-03 10:59:03,385:INFO:Declaring metric variables
2022-11-03 10:59:03,385:INFO:Importing untrained model
2022-11-03 10:59:03,385:INFO:Logistic Regression Imported successfully
2022-11-03 10:59:03,385:INFO:Starting cross validation
2022-11-03 10:59:03,393:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:59:11,050:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:15,048:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:15,148:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:15,148:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.24s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:15,181:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:15,246:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.20s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:15,312:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:15,547:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:18,478:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:18,631:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:18,658:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:18,675:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:18,761:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:18,909:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:19,197:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:19,961:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:19,970:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:20,112:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:20,126:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:20,146:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:20,263:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:20,345:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:20,597:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:32,201:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:32,387:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:34,154:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:34,440:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:192: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y, fitted_transformer = self._memory_fit(

2022-11-03 10:59:40,665:INFO:Calculating mean and std
2022-11-03 10:59:40,665:INFO:Creating metrics dataframe
2022-11-03 10:59:40,681:INFO:Uploading results into container
2022-11-03 10:59:40,681:INFO:Uploading model into container now
2022-11-03 10:59:40,681:INFO:master_model_container: 1
2022-11-03 10:59:40,681:INFO:display_container: 2
2022-11-03 10:59:40,681:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=3967, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2022-11-03 10:59:40,681:INFO:create_model() successfully completed......................................
2022-11-03 10:59:40,956:INFO:SubProcess create_model() end ==================================
2022-11-03 10:59:40,956:INFO:Creating metrics dataframe
2022-11-03 10:59:40,972:INFO:Initializing K Neighbors Classifier
2022-11-03 10:59:40,972:INFO:Total runtime is 0.6268824100494385 minutes
2022-11-03 10:59:40,972:INFO:SubProcess create_model() called ==================================
2022-11-03 10:59:40,972:INFO:Initializing create_model()
2022-11-03 10:59:40,972:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:59:40,972:INFO:Checking exceptions
2022-11-03 10:59:40,972:INFO:Importing libraries
2022-11-03 10:59:40,972:INFO:Copying training dataset
2022-11-03 10:59:40,989:INFO:Defining folds
2022-11-03 10:59:40,989:INFO:Declaring metric variables
2022-11-03 10:59:40,989:INFO:Importing untrained model
2022-11-03 10:59:40,989:INFO:K Neighbors Classifier Imported successfully
2022-11-03 10:59:40,989:INFO:Starting cross validation
2022-11-03 10:59:41,005:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:59:51,147:INFO:Calculating mean and std
2022-11-03 10:59:51,155:INFO:Creating metrics dataframe
2022-11-03 10:59:51,164:INFO:Uploading results into container
2022-11-03 10:59:51,164:INFO:Uploading model into container now
2022-11-03 10:59:51,164:INFO:master_model_container: 2
2022-11-03 10:59:51,164:INFO:display_container: 2
2022-11-03 10:59:51,164:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2022-11-03 10:59:51,164:INFO:create_model() successfully completed......................................
2022-11-03 10:59:51,419:INFO:SubProcess create_model() end ==================================
2022-11-03 10:59:51,419:INFO:Creating metrics dataframe
2022-11-03 10:59:51,435:INFO:Initializing Naive Bayes
2022-11-03 10:59:51,435:INFO:Total runtime is 0.8012621363004049 minutes
2022-11-03 10:59:51,435:INFO:SubProcess create_model() called ==================================
2022-11-03 10:59:51,435:INFO:Initializing create_model()
2022-11-03 10:59:51,435:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 10:59:51,435:INFO:Checking exceptions
2022-11-03 10:59:51,450:INFO:Importing libraries
2022-11-03 10:59:51,450:INFO:Copying training dataset
2022-11-03 10:59:51,468:INFO:Defining folds
2022-11-03 10:59:51,468:INFO:Declaring metric variables
2022-11-03 10:59:51,468:INFO:Importing untrained model
2022-11-03 10:59:51,468:INFO:Naive Bayes Imported successfully
2022-11-03 10:59:51,468:INFO:Starting cross validation
2022-11-03 10:59:51,468:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 10:59:59,900:INFO:Calculating mean and std
2022-11-03 10:59:59,900:INFO:Creating metrics dataframe
2022-11-03 10:59:59,907:INFO:Uploading results into container
2022-11-03 10:59:59,916:INFO:Uploading model into container now
2022-11-03 10:59:59,916:INFO:master_model_container: 3
2022-11-03 10:59:59,916:INFO:display_container: 2
2022-11-03 10:59:59,916:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2022-11-03 10:59:59,916:INFO:create_model() successfully completed......................................
2022-11-03 11:00:00,178:INFO:SubProcess create_model() end ==================================
2022-11-03 11:00:00,178:INFO:Creating metrics dataframe
2022-11-03 11:00:00,178:INFO:Initializing Decision Tree Classifier
2022-11-03 11:00:00,194:INFO:Total runtime is 0.9469834049542745 minutes
2022-11-03 11:00:00,194:INFO:SubProcess create_model() called ==================================
2022-11-03 11:00:00,195:INFO:Initializing create_model()
2022-11-03 11:00:00,195:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:00:00,195:INFO:Checking exceptions
2022-11-03 11:00:00,195:INFO:Importing libraries
2022-11-03 11:00:00,195:INFO:Copying training dataset
2022-11-03 11:00:00,211:INFO:Defining folds
2022-11-03 11:00:00,211:INFO:Declaring metric variables
2022-11-03 11:00:00,211:INFO:Importing untrained model
2022-11-03 11:00:00,211:INFO:Decision Tree Classifier Imported successfully
2022-11-03 11:00:00,211:INFO:Starting cross validation
2022-11-03 11:00:00,211:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 11:00:08,589:INFO:Calculating mean and std
2022-11-03 11:00:08,589:INFO:Creating metrics dataframe
2022-11-03 11:00:08,605:INFO:Uploading results into container
2022-11-03 11:00:08,605:INFO:Uploading model into container now
2022-11-03 11:00:08,605:INFO:master_model_container: 4
2022-11-03 11:00:08,605:INFO:display_container: 2
2022-11-03 11:00:08,605:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=3967, splitter='best')
2022-11-03 11:00:08,605:INFO:create_model() successfully completed......................................
2022-11-03 11:00:08,872:INFO:SubProcess create_model() end ==================================
2022-11-03 11:00:08,872:INFO:Creating metrics dataframe
2022-11-03 11:00:08,888:INFO:Initializing SVM - Linear Kernel
2022-11-03 11:00:08,888:INFO:Total runtime is 1.0921430309613545 minutes
2022-11-03 11:00:08,888:INFO:SubProcess create_model() called ==================================
2022-11-03 11:00:08,888:INFO:Initializing create_model()
2022-11-03 11:00:08,888:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:00:08,888:INFO:Checking exceptions
2022-11-03 11:00:08,888:INFO:Importing libraries
2022-11-03 11:00:08,888:INFO:Copying training dataset
2022-11-03 11:00:08,904:INFO:Defining folds
2022-11-03 11:00:08,904:INFO:Declaring metric variables
2022-11-03 11:00:08,913:INFO:Importing untrained model
2022-11-03 11:00:08,913:INFO:SVM - Linear Kernel Imported successfully
2022-11-03 11:00:08,913:INFO:Starting cross validation
2022-11-03 11:00:08,913:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 11:00:13,390:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2022-11-03 11:00:16,235:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2022-11-03 11:00:16,251:INFO:Calculating mean and std
2022-11-03 11:00:16,267:INFO:Creating metrics dataframe
2022-11-03 11:00:16,267:INFO:Uploading results into container
2022-11-03 11:00:16,267:INFO:Uploading model into container now
2022-11-03 11:00:16,267:INFO:master_model_container: 5
2022-11-03 11:00:16,267:INFO:display_container: 2
2022-11-03 11:00:16,267:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=3967, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2022-11-03 11:00:16,267:INFO:create_model() successfully completed......................................
2022-11-03 11:00:16,542:INFO:SubProcess create_model() end ==================================
2022-11-03 11:00:16,542:INFO:Creating metrics dataframe
2022-11-03 11:00:16,558:INFO:Initializing Ridge Classifier
2022-11-03 11:00:16,558:INFO:Total runtime is 1.2199769814809163 minutes
2022-11-03 11:00:16,558:INFO:SubProcess create_model() called ==================================
2022-11-03 11:00:16,558:INFO:Initializing create_model()
2022-11-03 11:00:16,558:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:00:16,558:INFO:Checking exceptions
2022-11-03 11:00:16,573:INFO:Importing libraries
2022-11-03 11:00:16,573:INFO:Copying training dataset
2022-11-03 11:00:16,589:INFO:Defining folds
2022-11-03 11:00:16,589:INFO:Declaring metric variables
2022-11-03 11:00:16,589:INFO:Importing untrained model
2022-11-03 11:00:16,589:INFO:Ridge Classifier Imported successfully
2022-11-03 11:00:16,589:INFO:Starting cross validation
2022-11-03 11:00:16,589:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 11:00:23,582:INFO:Calculating mean and std
2022-11-03 11:00:23,598:INFO:Creating metrics dataframe
2022-11-03 11:00:23,598:INFO:Uploading results into container
2022-11-03 11:00:23,598:INFO:Uploading model into container now
2022-11-03 11:00:23,613:INFO:master_model_container: 6
2022-11-03 11:00:23,613:INFO:display_container: 2
2022-11-03 11:00:23,613:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=3967, solver='auto', tol=0.001)
2022-11-03 11:00:23,613:INFO:create_model() successfully completed......................................
2022-11-03 11:00:23,913:INFO:SubProcess create_model() end ==================================
2022-11-03 11:00:23,913:INFO:Creating metrics dataframe
2022-11-03 11:00:23,930:INFO:Initializing Random Forest Classifier
2022-11-03 11:00:23,930:INFO:Total runtime is 1.3428431113560995 minutes
2022-11-03 11:00:23,930:INFO:SubProcess create_model() called ==================================
2022-11-03 11:00:23,930:INFO:Initializing create_model()
2022-11-03 11:00:23,930:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:00:23,930:INFO:Checking exceptions
2022-11-03 11:00:23,938:INFO:Importing libraries
2022-11-03 11:00:23,938:INFO:Copying training dataset
2022-11-03 11:00:23,946:INFO:Defining folds
2022-11-03 11:00:23,946:INFO:Declaring metric variables
2022-11-03 11:00:23,946:INFO:Importing untrained model
2022-11-03 11:00:23,946:INFO:Random Forest Classifier Imported successfully
2022-11-03 11:00:23,946:INFO:Starting cross validation
2022-11-03 11:00:23,962:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 11:00:35,911:INFO:Calculating mean and std
2022-11-03 11:00:35,911:INFO:Creating metrics dataframe
2022-11-03 11:00:35,919:INFO:Uploading results into container
2022-11-03 11:00:35,927:INFO:Uploading model into container now
2022-11-03 11:00:35,927:INFO:master_model_container: 7
2022-11-03 11:00:35,927:INFO:display_container: 2
2022-11-03 11:00:35,927:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=3967, verbose=0, warm_start=False)
2022-11-03 11:00:35,927:INFO:create_model() successfully completed......................................
2022-11-03 11:00:36,183:INFO:SubProcess create_model() end ==================================
2022-11-03 11:00:36,183:INFO:Creating metrics dataframe
2022-11-03 11:00:36,183:INFO:Initializing Quadratic Discriminant Analysis
2022-11-03 11:00:36,183:INFO:Total runtime is 1.5470641096433004 minutes
2022-11-03 11:00:36,183:INFO:SubProcess create_model() called ==================================
2022-11-03 11:00:36,183:INFO:Initializing create_model()
2022-11-03 11:00:36,199:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:00:36,199:INFO:Checking exceptions
2022-11-03 11:00:36,199:INFO:Importing libraries
2022-11-03 11:00:36,199:INFO:Copying training dataset
2022-11-03 11:00:36,215:INFO:Defining folds
2022-11-03 11:00:36,215:INFO:Declaring metric variables
2022-11-03 11:00:36,215:INFO:Importing untrained model
2022-11-03 11:00:36,215:INFO:Quadratic Discriminant Analysis Imported successfully
2022-11-03 11:00:36,215:INFO:Starting cross validation
2022-11-03 11:00:36,215:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 11:00:39,136:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:887: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2022-11-03 11:00:39,136:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:887: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2022-11-03 11:00:39,145:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:887: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2022-11-03 11:00:39,162:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:887: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2022-11-03 11:00:39,226:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:887: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2022-11-03 11:00:39,242:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:887: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2022-11-03 11:00:39,258:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:887: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2022-11-03 11:00:39,328:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:887: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2022-11-03 11:00:43,344:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:887: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2022-11-03 11:00:43,399:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:887: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2022-11-03 11:00:45,044:INFO:Calculating mean and std
2022-11-03 11:00:45,044:INFO:Creating metrics dataframe
2022-11-03 11:00:45,044:INFO:Uploading results into container
2022-11-03 11:00:45,044:INFO:Uploading model into container now
2022-11-03 11:00:45,044:INFO:master_model_container: 8
2022-11-03 11:00:45,044:INFO:display_container: 2
2022-11-03 11:00:45,044:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2022-11-03 11:00:45,044:INFO:create_model() successfully completed......................................
2022-11-03 11:00:45,319:INFO:SubProcess create_model() end ==================================
2022-11-03 11:00:45,319:INFO:Creating metrics dataframe
2022-11-03 11:00:45,335:INFO:Initializing Ada Boost Classifier
2022-11-03 11:00:45,335:INFO:Total runtime is 1.699593722820282 minutes
2022-11-03 11:00:45,335:INFO:SubProcess create_model() called ==================================
2022-11-03 11:00:45,335:INFO:Initializing create_model()
2022-11-03 11:00:45,335:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:00:45,335:INFO:Checking exceptions
2022-11-03 11:00:45,343:INFO:Importing libraries
2022-11-03 11:00:45,343:INFO:Copying training dataset
2022-11-03 11:00:45,351:INFO:Defining folds
2022-11-03 11:00:45,351:INFO:Declaring metric variables
2022-11-03 11:00:45,351:INFO:Importing untrained model
2022-11-03 11:00:45,351:INFO:Ada Boost Classifier Imported successfully
2022-11-03 11:00:45,351:INFO:Starting cross validation
2022-11-03 11:00:45,351:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 11:00:55,881:INFO:Calculating mean and std
2022-11-03 11:00:55,881:INFO:Creating metrics dataframe
2022-11-03 11:00:55,903:INFO:Uploading results into container
2022-11-03 11:00:55,903:INFO:Uploading model into container now
2022-11-03 11:00:55,903:INFO:master_model_container: 9
2022-11-03 11:00:55,903:INFO:display_container: 2
2022-11-03 11:00:55,903:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=3967)
2022-11-03 11:00:55,903:INFO:create_model() successfully completed......................................
2022-11-03 11:00:56,153:INFO:SubProcess create_model() end ==================================
2022-11-03 11:00:56,153:INFO:Creating metrics dataframe
2022-11-03 11:00:56,169:INFO:Initializing Gradient Boosting Classifier
2022-11-03 11:00:56,169:INFO:Total runtime is 1.8801662961641947 minutes
2022-11-03 11:00:56,169:INFO:SubProcess create_model() called ==================================
2022-11-03 11:00:56,169:INFO:Initializing create_model()
2022-11-03 11:00:56,169:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:00:56,169:INFO:Checking exceptions
2022-11-03 11:00:56,185:INFO:Importing libraries
2022-11-03 11:00:56,185:INFO:Copying training dataset
2022-11-03 11:00:56,185:INFO:Defining folds
2022-11-03 11:00:56,185:INFO:Declaring metric variables
2022-11-03 11:00:56,185:INFO:Importing untrained model
2022-11-03 11:00:56,185:INFO:Gradient Boosting Classifier Imported successfully
2022-11-03 11:00:56,185:INFO:Starting cross validation
2022-11-03 11:00:56,201:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 11:01:09,474:INFO:Calculating mean and std
2022-11-03 11:01:09,474:INFO:Creating metrics dataframe
2022-11-03 11:01:09,474:INFO:Uploading results into container
2022-11-03 11:01:09,474:INFO:Uploading model into container now
2022-11-03 11:01:09,490:INFO:master_model_container: 10
2022-11-03 11:01:09,490:INFO:display_container: 2
2022-11-03 11:01:09,490:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=3967, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2022-11-03 11:01:09,490:INFO:create_model() successfully completed......................................
2022-11-03 11:01:09,747:INFO:SubProcess create_model() end ==================================
2022-11-03 11:01:09,755:INFO:Creating metrics dataframe
2022-11-03 11:01:09,763:INFO:Initializing Linear Discriminant Analysis
2022-11-03 11:01:09,763:INFO:Total runtime is 2.106737538178762 minutes
2022-11-03 11:01:09,763:INFO:SubProcess create_model() called ==================================
2022-11-03 11:01:09,763:INFO:Initializing create_model()
2022-11-03 11:01:09,763:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:01:09,771:INFO:Checking exceptions
2022-11-03 11:01:09,772:INFO:Importing libraries
2022-11-03 11:01:09,772:INFO:Copying training dataset
2022-11-03 11:01:09,788:INFO:Defining folds
2022-11-03 11:01:09,788:INFO:Declaring metric variables
2022-11-03 11:01:09,788:INFO:Importing untrained model
2022-11-03 11:01:09,788:INFO:Linear Discriminant Analysis Imported successfully
2022-11-03 11:01:09,788:INFO:Starting cross validation
2022-11-03 11:01:09,788:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 11:01:18,420:INFO:Calculating mean and std
2022-11-03 11:01:18,428:INFO:Creating metrics dataframe
2022-11-03 11:01:18,436:INFO:Uploading results into container
2022-11-03 11:01:18,436:INFO:Uploading model into container now
2022-11-03 11:01:18,436:INFO:master_model_container: 11
2022-11-03 11:01:18,436:INFO:display_container: 2
2022-11-03 11:01:18,436:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2022-11-03 11:01:18,436:INFO:create_model() successfully completed......................................
2022-11-03 11:01:18,700:INFO:SubProcess create_model() end ==================================
2022-11-03 11:01:18,700:INFO:Creating metrics dataframe
2022-11-03 11:01:18,716:INFO:Initializing Extra Trees Classifier
2022-11-03 11:01:18,716:INFO:Total runtime is 2.255943584442139 minutes
2022-11-03 11:01:18,716:INFO:SubProcess create_model() called ==================================
2022-11-03 11:01:18,716:INFO:Initializing create_model()
2022-11-03 11:01:18,716:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:01:18,716:INFO:Checking exceptions
2022-11-03 11:01:18,724:INFO:Importing libraries
2022-11-03 11:01:18,724:INFO:Copying training dataset
2022-11-03 11:01:18,732:INFO:Defining folds
2022-11-03 11:01:18,740:INFO:Declaring metric variables
2022-11-03 11:01:18,740:INFO:Importing untrained model
2022-11-03 11:01:18,740:INFO:Extra Trees Classifier Imported successfully
2022-11-03 11:01:18,740:INFO:Starting cross validation
2022-11-03 11:01:18,748:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 11:01:31,455:INFO:Calculating mean and std
2022-11-03 11:01:31,471:INFO:Creating metrics dataframe
2022-11-03 11:01:31,471:INFO:Uploading results into container
2022-11-03 11:01:31,471:INFO:Uploading model into container now
2022-11-03 11:01:31,471:INFO:master_model_container: 12
2022-11-03 11:01:31,486:INFO:display_container: 2
2022-11-03 11:01:31,488:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=3967, verbose=0, warm_start=False)
2022-11-03 11:01:31,488:INFO:create_model() successfully completed......................................
2022-11-03 11:01:31,750:INFO:SubProcess create_model() end ==================================
2022-11-03 11:01:31,750:INFO:Creating metrics dataframe
2022-11-03 11:01:31,767:INFO:Initializing Extreme Gradient Boosting
2022-11-03 11:01:31,767:INFO:Total runtime is 2.473457757631938 minutes
2022-11-03 11:01:31,782:INFO:SubProcess create_model() called ==================================
2022-11-03 11:01:31,782:INFO:Initializing create_model()
2022-11-03 11:01:31,782:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:01:31,782:INFO:Checking exceptions
2022-11-03 11:01:31,782:INFO:Importing libraries
2022-11-03 11:01:31,782:INFO:Copying training dataset
2022-11-03 11:01:31,798:INFO:Defining folds
2022-11-03 11:01:31,798:INFO:Declaring metric variables
2022-11-03 11:01:31,798:INFO:Importing untrained model
2022-11-03 11:01:31,798:INFO:Extreme Gradient Boosting Imported successfully
2022-11-03 11:01:31,798:INFO:Starting cross validation
2022-11-03 11:01:31,814:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 11:01:47,021:INFO:Calculating mean and std
2022-11-03 11:01:47,021:INFO:Creating metrics dataframe
2022-11-03 11:01:47,037:INFO:Uploading results into container
2022-11-03 11:01:47,037:INFO:Uploading model into container now
2022-11-03 11:01:47,037:INFO:master_model_container: 13
2022-11-03 11:01:47,037:INFO:display_container: 2
2022-11-03 11:01:47,037:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, gamma=None,
              gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic',
              predictor=None, random_state=3967, reg_alpha=None, ...)
2022-11-03 11:01:47,037:INFO:create_model() successfully completed......................................
2022-11-03 11:01:47,299:INFO:SubProcess create_model() end ==================================
2022-11-03 11:01:47,299:INFO:Creating metrics dataframe
2022-11-03 11:01:47,315:INFO:Initializing Light Gradient Boosting Machine
2022-11-03 11:01:47,315:INFO:Total runtime is 2.7326045751571657 minutes
2022-11-03 11:01:47,315:INFO:SubProcess create_model() called ==================================
2022-11-03 11:01:47,319:INFO:Initializing create_model()
2022-11-03 11:01:47,319:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:01:47,319:INFO:Checking exceptions
2022-11-03 11:01:47,319:INFO:Importing libraries
2022-11-03 11:01:47,319:INFO:Copying training dataset
2022-11-03 11:01:47,335:INFO:Defining folds
2022-11-03 11:01:47,335:INFO:Declaring metric variables
2022-11-03 11:01:47,335:INFO:Importing untrained model
2022-11-03 11:01:47,335:INFO:Light Gradient Boosting Machine Imported successfully
2022-11-03 11:01:47,335:INFO:Starting cross validation
2022-11-03 11:01:47,343:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 11:01:56,535:INFO:Calculating mean and std
2022-11-03 11:01:56,551:INFO:Creating metrics dataframe
2022-11-03 11:01:56,551:INFO:Uploading results into container
2022-11-03 11:01:56,551:INFO:Uploading model into container now
2022-11-03 11:01:56,551:INFO:master_model_container: 14
2022-11-03 11:01:56,551:INFO:display_container: 2
2022-11-03 11:01:56,551:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3967, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2022-11-03 11:01:56,551:INFO:create_model() successfully completed......................................
2022-11-03 11:01:56,817:INFO:SubProcess create_model() end ==================================
2022-11-03 11:01:56,817:INFO:Creating metrics dataframe
2022-11-03 11:01:56,832:INFO:Initializing Dummy Classifier
2022-11-03 11:01:56,832:INFO:Total runtime is 2.891221392154694 minutes
2022-11-03 11:01:56,832:INFO:SubProcess create_model() called ==================================
2022-11-03 11:01:56,832:INFO:Initializing create_model()
2022-11-03 11:01:56,832:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A837DD3C0>, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:01:56,832:INFO:Checking exceptions
2022-11-03 11:01:56,848:INFO:Importing libraries
2022-11-03 11:01:56,848:INFO:Copying training dataset
2022-11-03 11:01:56,848:INFO:Defining folds
2022-11-03 11:01:56,848:INFO:Declaring metric variables
2022-11-03 11:01:56,848:INFO:Importing untrained model
2022-11-03 11:01:56,848:INFO:Dummy Classifier Imported successfully
2022-11-03 11:01:56,848:INFO:Starting cross validation
2022-11-03 11:01:56,864:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-03 11:02:02,072:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2022-11-03 11:02:02,137:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2022-11-03 11:02:02,137:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2022-11-03 11:02:02,154:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2022-11-03 11:02:02,214:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2022-11-03 11:02:02,214:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2022-11-03 11:02:02,230:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2022-11-03 11:02:02,246:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2022-11-03 11:02:05,448:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2022-11-03 11:02:05,463:WARNING:C:\Users\HarishChandranManoha\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2022-11-03 11:02:05,479:INFO:Calculating mean and std
2022-11-03 11:02:05,479:INFO:Creating metrics dataframe
2022-11-03 11:02:05,496:INFO:Uploading results into container
2022-11-03 11:02:05,496:INFO:Uploading model into container now
2022-11-03 11:02:05,496:INFO:master_model_container: 15
2022-11-03 11:02:05,496:INFO:display_container: 2
2022-11-03 11:02:05,496:INFO:DummyClassifier(constant=None, random_state=3967, strategy='prior')
2022-11-03 11:02:05,496:INFO:create_model() successfully completed......................................
2022-11-03 11:02:05,752:INFO:SubProcess create_model() end ==================================
2022-11-03 11:02:05,752:INFO:Creating metrics dataframe
2022-11-03 11:02:05,777:INFO:Initializing create_model()
2022-11-03 11:02:05,777:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A8715C760>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=3967, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-11-03 11:02:05,777:INFO:Checking exceptions
2022-11-03 11:02:05,777:INFO:Importing libraries
2022-11-03 11:02:05,777:INFO:Copying training dataset
2022-11-03 11:02:05,793:INFO:Defining folds
2022-11-03 11:02:05,793:INFO:Declaring metric variables
2022-11-03 11:02:05,793:INFO:Importing untrained model
2022-11-03 11:02:05,793:INFO:Declaring custom model
2022-11-03 11:02:05,793:INFO:Gradient Boosting Classifier Imported successfully
2022-11-03 11:02:05,809:INFO:Cross validation set to False
2022-11-03 11:02:05,809:INFO:Fitting Model
2022-11-03 11:02:11,544:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=3967, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2022-11-03 11:02:11,544:INFO:create_model() successfully completed......................................
2022-11-03 11:02:11,878:INFO:master_model_container: 15
2022-11-03 11:02:11,878:INFO:display_container: 2
2022-11-03 11:02:11,878:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=3967, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2022-11-03 11:02:11,878:INFO:compare_models() successfully completed......................................
2022-11-03 11:02:11,946:INFO:Initializing save_model()
2022-11-03 11:02:11,962:INFO:save_model(model=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=3967, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), model_name=best_model, prep_pipe_=Pipeline(memory=Memory(location=C:\Users\HARISH~1\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Senior Citizen', 'tenure',
                                             'Monthly Charges'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fi...
                                    include=['Customer ID', 'Total Charges'],
                                    transformer=LeaveOneOutEncoder(cols=['Customer '
                                                                         'ID',
                                                                         'Total '
                                                                         'Charges'],
                                                                   drop_invariant=False,
                                                                   handle_missing='return_nan',
                                                                   handle_unknown='value',
                                                                   random_state=3967,
                                                                   return_df=True,
                                                                   sigma=None,
                                                                   verbose=0))),
                ('low_variance',
                 TransformerWrapper(exclude=[], include=None,
                                    transformer=VarianceThreshold(threshold=0)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2022-11-03 11:02:11,962:INFO:Adding model into prep_pipe
2022-11-03 11:02:12,027:INFO:best_model.pkl saved in current working directory
2022-11-03 11:02:12,070:INFO:Pipeline(memory=Memory(location=C:\Users\HARISH~1\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Senior Citizen', 'tenure',
                                             'Monthly Charges'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fi...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=3967, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2022-11-03 11:02:12,070:INFO:save_model() successfully completed......................................
